{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Xin Chen's implementation of the MBE-NN network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Author: Xin Chen\n",
    "* Email: Bismarrck@me.com\n",
    "\n",
    "This jupyter notebook is used to repeat the work of http://doi.org/10.1021/acs.jctc.6b00994. \n",
    "\n",
    "The test cluster is $\\textrm{C}_{9}\\textrm{H}_{7}\\textrm{N}$. The reference energies are calculated with DFTB.\n",
    "\n",
    "<img src=\"./C9H7N.png\" alt=\"network\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The structure of the deep convolutional neural network for the $Pt_{13}$ cluster is as follows:\n",
    "\n",
    "<img src=\"./convnet.jpg\" alt=\"network\" style=\"width: 800px;\"/>\n",
    "\n",
    "The input features are transformed interatomic distances. The output node represents the estimated DFT energies.\n",
    "The detailed explanantion of this convolutionary neural network will be given in the following section **Inference**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this section we shall import python modules and declare global constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import h5py\n",
    "import sys\n",
    "import time\n",
    "import hashlib\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from fnmatch import fnmatch\n",
    "from itertools import repeat\n",
    "from os.path import isfile, isdir, join, basename, splitext\n",
    "from os import makedirs, listdir\n",
    "from scipy.misc import comb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import pairwise_distances, r2_score\n",
    "from itertools import combinations\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import ops\n",
    "from tebismarrck@me.comnsorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.contrib.learn import ModeKeys\n",
    "sns.set(font=\"serif\")\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The patch size is always 1x1\n",
    "PATCH_SIZE = 1\n",
    "\n",
    "# The HDF5 database file.\n",
    "HDF5_DATABASE_FILE = \"c9h7n.hdf5\"\n",
    "\n",
    "# The dimension of the cluster.\n",
    "NUM_SITES = 17\n",
    "\n",
    "# The total number of structures in the XYZ file should be 209660.\n",
    "XYZ_FILE = \"../datasets/C9H7N.xyz\"\n",
    "TOTAL_SIZE = 5000\n",
    "\n",
    "# Setup the size.\n",
    "LOAD_SIZE = 5000\n",
    "\n",
    "# Setup the random seed.\n",
    "SEED = 235\n",
    "\n",
    "# Setup the precision of floats. \n",
    "TF_TYPE = tf.float32\n",
    "NP_TYPE = np.float32\n",
    "\n",
    "# Cuda\n",
    "CUDA_ON = True\n",
    "\n",
    "# The fixed constants: C(N,k) and C(k,2) where k is 4.\n",
    "CNK = comb(NUM_SITES, 4, exact=True)\n",
    "CK2 = comb(4, 2, exact=True)\n",
    "\n",
    "# The pyykko radius (+1) for each element.\n",
    "pyykko = {\n",
    "  'Ac': 1.86, 'Ag': 1.28, 'Al': 1.26, 'Am': 1.66, 'Ar': 0.96, 'As': 1.21,\n",
    "  'At': 1.47, 'Au': 1.24, 'B': 0.85, 'Ba': 1.96, 'Be': 1.02, 'Bh': 1.41,\n",
    "  'Bi': 1.51, 'Bk': 1.68, 'Br': 1.14, 'C': 0.75, 'Ca': 1.71, 'Cd': 1.36,\n",
    "  'Ce': 1.63, 'Cf': 1.68, 'Cl': 0.99, 'Cm': 1.66, 'Co': 1.11, 'Cr': 1.22,\n",
    "  'Cs': 2.32, 'Cu': 1.12, 'Db': 1.49, 'Ds': 1.28, 'Dy': 1.67, 'Er': 1.65,\n",
    "  'Es': 1.65, 'Eu': 1.68, 'F': 0.64, 'Fe': 1.16, 'Fm': 1.67, 'Fr': 2.23,\n",
    "  'Ga': 1.24, 'Gd': 1.69, 'Ge': 1.21, 'H': 0.32, 'He': 0.46, 'Hf': 1.52,\n",
    "  'Hg': 1.33, 'Ho': 1.66, 'Hs': 1.34, 'I': 1.33, 'In': 1.42, 'Ir': 1.22,\n",
    "  'K': 1.96, 'Kr': 1.17, 'La': 1.8, 'Li': 1.33, 'Lu': 1.62, 'Md': 1.73,\n",
    "  'Mg': 1.39, 'Mn': 1.19, 'Mo': 1.38, 'Mt': 1.29, 'N': 0.71, 'Na': 1.55,\n",
    "  'Nb': 1.47, 'Nd': 1.74, 'Ne': 0.67, 'Ni': 1.1, 'No': 1.76, 'Np': 1.71,\n",
    "  'O': 0.63, 'Os': 1.29, 'P': 1.11, 'Pa': 1.69, 'Pb': 1.44, 'Pd': 1.2,\n",
    "  'Pm': 1.73, 'Po': 1.45, 'Pr': 1.76, 'Pt': 1.23, 'Pu': 1.72, 'Ra': 2.01,\n",
    "  'Rb': 2.1, 'Re': 1.31, 'Rf': 1.57, 'Rh': 1.25, 'Rn': 1.42, 'Ru': 1.25,\n",
    "  'S': 1.03, 'Sb': 1.4, 'Sc': 1.48, 'Se': 1.16, 'Sg': 1.43, 'Si': 1.16,\n",
    "  'Sm': 1.72, 'Sn': 1.4, 'Sr': 1.85, 'Ta': 1.46, 'Tb': 1.68, 'Tc': 1.28,\n",
    "  'Te': 1.36, 'Th': 1.75, 'Ti': 1.36, 'Tl': 1.44, 'Tm': 1.64, 'U': 1.7,\n",
    "  'V': 1.34, 'W': 1.37, 'X': 0.32, 'Xe': 1.31, 'Y': 1.63, 'Yb': 1.7,\n",
    "  'Zn': 1.18, 'Zr': 1.54\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following cell defines the pyykko radii matrix for exponentially scaling interactomic distances. In this $\\textrm{C}_{9}\\textrm{H}_{7}\\textrm{N}$ case, the pyykko bond lengths of C-H, C-N and C-C varys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [12, 9]\n",
    "factor = 1.5\n",
    "pyykko_radii = {\"N\": 0.71, \"C\": 0.75, \"H\": 0.32}\n",
    "species = [\"N\"] + list(repeat(\"C\", 9)) + list(repeat(\"H\", 7))\n",
    "rr = np.asarray([pyykko_radii[specie] for specie in species])[:, np.newaxis]\n",
    "lmat = factor * (rr + rr.T).flatten()\n",
    "sns.heatmap(lmat.reshape((17, 17)), xticklabels=species, yticklabels=species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Invoke the interactive session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "sess = tf.InteractiveSession(graph=graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Some global helper functions are declared here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def brange(start, stop, batchsize):\n",
    "  \"\"\"\n",
    "  Range from `start` to `stop` given a batch size and return the start and stop of each batch.\n",
    "  \n",
    "  Args:\n",
    "    start: int, the start number of a sequence.\n",
    "    stop: int, the end number of a sequence.\n",
    "    batchsize: int, the size of each batch.\n",
    "  \n",
    "  \"\"\"\n",
    "  istart = start\n",
    "  while istart < stop:\n",
    "    istop = min(istart + batchsize, stop)\n",
    "    yield istart, istop\n",
    "    istart = istop\n",
    "\n",
    "\n",
    "def exponential(x, l=4.0):\n",
    "  \"\"\"\n",
    "  Exponentially scale the input value(s).  \n",
    "  \"\"\"\n",
    "  return np.exp(-x / l)\n",
    "\n",
    "\n",
    "def md5(filename):\n",
    "  \"\"\" \n",
    "  Return the md5 checksum of the given file.\n",
    "\n",
    "  Args:\n",
    "    filename: a file.\n",
    "\n",
    "  Returns:\n",
    "    checksum: the MD5 checksum of the file.\n",
    "\n",
    "  \"\"\"\n",
    "  hash_md5 = hashlib.md5()\n",
    "  with open(filename, \"rb\") as f:\n",
    "    for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "      hash_md5.update(chunk)\n",
    "  return hash_md5.hexdigest()\n",
    "\n",
    "\n",
    "def root_mean_squred(x):\n",
    "  \"\"\"\n",
    "  Return the root mean squared of the given array.\n",
    "  \"\"\"\n",
    "  return np.sqrt(np.mean(np.square(x)))\n",
    "\n",
    "\n",
    "def mean_abs_error(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  Return the mean absolute error.\n",
    "  \"\"\"\n",
    "  return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "\n",
    "def get_time_id():\n",
    "  \"\"\"\n",
    "  Return a string as the ID of this run.\n",
    "  \"\"\"\n",
    "  return datetime.strftime(datetime.fromtimestamp(time.time()), \"%m%d%H%M\")\n",
    "\n",
    "\n",
    "def get_pyykko_scaling_matrix(species, factor=1.5, flat=True):\n",
    "  \"\"\"\n",
    "  Return the pyykko-bond-length based scaling matrix.\n",
    "  \n",
    "  Args:\n",
    "    species: List[str], a list of species.\n",
    "  \n",
    "  \"\"\"\n",
    "  rr = np.asarray([pyykko_radii[specie] for specie in species])[:, np.newaxis]\n",
    "  lmat = factor * (rr + rr.T)\n",
    "  if flat:\n",
    "    return lmat.flatten()\n",
    "  else:\n",
    "    return lmat\n",
    "\n",
    "\n",
    "def smooth(x, window_len=11, window='hanning'):\n",
    "  \"\"\"\n",
    "  smooth the data using a window with requested size.\n",
    "\n",
    "  This method is based on the convolution of a scaled window with the signal.\n",
    "  The signal is prepared by introducing reflected copies of the signal\n",
    "  (with the window size) in both ends so that transient parts are minimized\n",
    "  in the begining and end part of the output signal.\n",
    "\n",
    "  Args:\n",
    "    x: the input signal\n",
    "    window_len: the dimension of the smoothing window; should be an odd integer\n",
    "    window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman' \n",
    "      flat window will produce a moving average smoothing.\n",
    "\n",
    "  output:\n",
    "    the smoothed signal\n",
    "  \"\"\"\n",
    "\n",
    "  if x.ndim != 1:\n",
    "    raise ValueError(\"smooth only accepts 1 dimension arrays.\")\n",
    "\n",
    "  if x.size < window_len:\n",
    "    raise ValueError(\"Input vector needs to be bigger than window size.\")\n",
    "\n",
    "  if window_len < 3:\n",
    "    return x\n",
    "\n",
    "  if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "    raise ValueError(\"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\")\n",
    "\n",
    "  s = np.r_[x[window_len-1:0:-1],x,x[-1:-window_len:-1]]\n",
    "  if window == 'flat':\n",
    "    w = np.ones(window_len,'d')\n",
    "  else:\n",
    "    w = eval('np.'+ window +'(window_len)')\n",
    "\n",
    "  return np.convolve(w / w.sum(), s, mode='valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This convnet is a slightly different from normal convolutionary neural networks. Suppose N is the batch size:\n",
    "\n",
    "* The shape of the input tensor is: $[N, 1, C_{N}^{k}, C_{k}^{2}]$\n",
    "* The **k** defines the many-body expansion. In this paper, k is selected to be 4.\n",
    "* The patch (kernel) is always 1x1.\n",
    "* The padding scheme is 'SAME' and the strides are 1 in both directions.\n",
    "* A shape transpose action should be taken between layer 4 and 5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this section we define some local helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(tensor):\n",
    "  \"\"\"\n",
    "  Attach a lot of summaries to a Tensor (for TensorBoard visualization).\n",
    "\n",
    "  Args:\n",
    "    tensor: a Tensor.\n",
    "\n",
    "  \"\"\"\n",
    "  with tf.name_scope('summaries'):\n",
    "    mean = tf.reduce_mean(tensor)\n",
    "    tf.summary.scalar('mean', mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "      stddev = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(tensor, mean))))\n",
    "      tf.summary.scalar('stddev', stddev)\n",
    "    tf.summary.scalar('max', tf.reduce_max(tensor))\n",
    "    tf.summary.scalar('min', tf.reduce_min(tensor))\n",
    "    tf.summary.histogram('histogram', tensor)\n",
    "\n",
    "    \n",
    "def print_activations(t):\n",
    "  \"\"\"\n",
    "  Print the name and shape of the input Tensor.\n",
    "\n",
    "  Args:\n",
    "    t: a Tensor.\n",
    "\n",
    "  \"\"\"\n",
    "  print(\"%-21s : %s\" % (t.op.name, t.get_shape().as_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "All layers except the last in this MBE-NN network are convolutionary layers. The activation functions are ``tanh`` and ``softplus`` and there is no pooling operation before the output layer. So this function is used to create the conv2d layers of this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def mbe_conv2d(tensor, n_in, n_out, name, activate=tf.tanh, verbose=True):\n",
    "  \"\"\" A lazy inner function to create a `tf.nn.conv2d` Tensor.\n",
    "\n",
    "  Args:\n",
    "    tensor: a Tensor, [index, 1, w, n_in]\n",
    "    n_in: the number of input channels.\n",
    "    n_out: the number of output channels.\n",
    "    name: the name of this layer.\n",
    "    activate: the activation function, defaults to `tf.tanh`.\n",
    "    verbose: print the layer if True\n",
    "\n",
    "  Returns:\n",
    "    activated: a Tensor of activated `tf.nn.conv2d`.\n",
    "\n",
    "  \"\"\"\n",
    "  with tf.name_scope(name):\n",
    "    with tf.name_scope(\"filter\"):陈\n",
    "      kernel = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "          [1, 1, n_in, n_out], stddev=0.1, seed=SEED, dtype=TF_TYPE), \n",
    "        name=\"kernel\")\n",
    "      variable_summaries(kernel)\n",
    "    conv = tf.nn.conv2d(\n",
    "      tensor, kernel, [1, 1, 1, 1], padding=\"SAME\", use_cudnn_on_gpu=CUDA_ON)\n",
    "    with tf.name_scope(\"biases\"):\n",
    "      biases = tf.Variable(\n",
    "        tf.zeros([n_out], dtype=TF_TYPE), name=\"biases\")\n",
    "      variable_summaries(biases)\n",
    "    bias = tf.nn.bias_add(conv, biases)\n",
    "    activated = activate(bias)\n",
    "    if verbose:\n",
    "      print_activations(activated)\n",
    "  return activated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we can infer the MBE-NN neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def inference(input_tensor, mode=ModeKeys.INFER, verbose=True):\n",
    "  \"\"\"\n",
    "  Return the infered MBE-NN-M deep neural network model.\n",
    "\n",
    "  Args:\n",
    "    input_tensor: a 4D dataset Tensor as the input layer, [batch, 1, C(N,k), C(k,2)]\n",
    "    mode: inference mode, 'INFER', 'EVAL' or 'TRAIN'.\n",
    "    verbose: print layer details if True.\n",
    "\n",
    "  Returns:\n",
    "    pool: the last Tensor of `tf.nn.avg_pool`, [batch, 1, 1, 1]\n",
    "\n",
    "  References:\n",
    "    Alexandrova, A. N. (2016). http://doi.org/10.1021/acs.jctc.6b00994\n",
    "\n",
    "  \"\"\"\n",
    "  if verbose:\n",
    "    print(\"-> Inference the MBE-NN-M model ...\")\n",
    "    print(\"\")\n",
    "\n",
    "  # Build the first three MBE layers.\n",
    "  # The shape of the input data tensor is [n, 1, C(N,k), C(k,2)].\n",
    "  # To fit Fk, the NN connection is localized in the second dimension, and the\n",
    "  # layer size of the first dimension is kept fixed. The weights and biases of\n",
    "  # NN connection are shared among different indices of the first dimension,\n",
    "  # so that the fitted function form of Fk is kept consistent among different\n",
    "  # k-body terms. The MBE part is composed of four layers with the following\n",
    "  # sizes:\n",
    "  # (C(N,k), C(k,2)) - (C(N,k), 40) - (C(N,k), 70) - (C(N,k), 60) - (C(N,k), 2).\n",
    "  conv1 = mbe_conv2d(input_tensor, 6,  16, \"Conv1\", activate=tf.nn.tanh)\n",
    "  conv2 = mbe_conv2d(conv1, 16, 32, \"Conv2\", activate=tf.nn.tanh)\n",
    "  conv3 = mbe_conv2d(conv2, 32, 48, \"Conv3\", activate=tf.nn.tanh)\n",
    "\n",
    "  # Then we build the three mixing layers.\n",
    "  # The mixing part is used to fit G. Within this part the NN connection is\n",
    "  # localized in the first dimension, and the size of the second dimension is\n",
    "  # kept fixed. The parameters of NN connection in this part are shared among\n",
    "  # different indices of the second dimension. In this work, the mixing part is\n",
    "  # composed of two layers with the following sizes:\n",
    "  # (C(N, k), 2) - (40, 2) - (10, 2).\n",
    "  conv4 = mbe_conv2d(conv3, 48, 2, \"Conv4\", activate=tf.nn.softplus)\n",
    "  with tf.name_scope(\"F2G\"):\n",
    "    tensor = tf.reshape(conv4, (-1, 1, 2, CNK))\n",
    "  conv5 = mbe_conv2d(tensor, CNK, 40, \"Conv5\", activate=tf.nn.softplus)\n",
    "  conv6 = mbe_conv2d(conv5, 40, 10, \"Conv6\", activate=tf.nn.softplus)\n",
    "\n",
    "  # The last part is used to transform the output of mixing part to a single\n",
    "  # value, representing the energy. The average-pooling is used, which means\n",
    "  # that we take the average value of all elements in the matrix of the previous\n",
    "  # layer as the final output. In this work, the pooling part is composed of one\n",
    "  # layer of the size:\n",
    "  # (10, 2) - (1).\n",
    "  with tf.name_scope(\"Pool7\"):\n",
    "    pool7 = tf.nn.avg_pool(tf.reshape(conv6, [-1, 10, 2, 1]),\n",
    "                           ksize=[1, 10, 2, 1],\n",
    "                           strides=[1, 1, 1, 1],\n",
    "                           padding=\"VALID\")\n",
    "    if verbose:\n",
    "      print_activations(pool7)\n",
    "      print(\"\")\n",
    "  \n",
    "  with tf.name_scope(\"Output\"):\n",
    "    return tf.reshape(pool7, (-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4. Preparing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.1 Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It is known that Gaussian coordinates are not suitable to direcly used as input of NN. Therefor, two transformations are adapted here on the input sample data in Cartesian coordiantes $\\{r_{j}\\}$:\n",
    "\n",
    "1. Transform each set of combination indices $\\{j\\}$ ($k=4$) to interatomic distances $\\{d_{i,j}\\}$ ($C_{k}^{2}=6$). \n",
    "2. Dump these interatomic distances with the exponential function: $d_{i,j}' = e^{-d_{i,j}/L}$. $L$ is a fixed parameter and 4.0 was used in this paper.\n",
    "\n",
    "The following figure demonstrates the workflow of these transformations:\n",
    "\n",
    "<img src=\"./input_transform.png\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
    "\n",
    "The shape of the final input matrix should be: [1, $C_{N}^{k}$, $C_{k}^{2}$]\n",
    "\n",
    "* The width is 1\n",
    "* The height is $C_{N}^{k}$\n",
    "* The depth is $C_{k}^{2}$\n",
    "\n",
    "So that we can use it in a convolutionary neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def transform_coords(coords, chunksize, mapping, l=4.0, verbose=True):\n",
    "  \"\"\"\n",
    "  Transform the cartesian coordinates to input features.\n",
    "\n",
    "  Args:\n",
    "    coords: a 3D array with shape [M,N,3] representing the cartesian coordinates.\n",
    "    chunksize: the transformed array is too large. So save it piece by piece.\n",
    "    mapping: a `h5py.Dataset`, which is a symbolic to the real data on disk.\n",
    "    l: the exponential parameter.\n",
    "    verbose: print the transformation progress if True.\n",
    "\n",
    "  \"\"\"\n",
    "  ntotal, n = coords.shape[:2]\n",
    "  cnkv = comb(n, 4, exact=True)\n",
    "  ck2v = comb(4, 2, exact=True)\n",
    "  cnkl = list(combinations(range(NUM_SITES), 4))\n",
    "  # Using mapping indices can increase the speed 30 times!\n",
    "  indices = np.zeros((ck2v, cnkv), dtype=int)\n",
    "  for i in range(cnkv):\n",
    "    for j, (vi, vj) in enumerate(combinations(cnkl[i], 2)):\n",
    "      indices[j, i] = vi * n + vj\n",
    "  dataset = np.zeros((chunksize, 1, cnkv, ck2v), dtype=NP_TYPE)\n",
    "  tic = time.time()\n",
    "  if verbose:\n",
    "    print(\"Transform the cartesian coordinates ...\\n\")\n",
    "  for i, inext in brange(0, ntotal, chunksize):\n",
    "    for j in range(i, inext):\n",
    "      dists = pairwise_distances(coords[j]).flatten()\n",
    "      dists = exponential(dists, l=l)\n",
    "      for k in range(ck2v):\n",
    "        dataset[j - i, 0, :, k] = dists[indices[k]]\n",
    "      del dists\n",
    "    batch_size = inext - i\n",
    "    mapping[i: inext, ...] = dataset[:batch_size, ...]\n",
    "    if verbose:\n",
    "      sys.stdout.write(\"\\rProgress: %7d  /  %7d\" % (inext, ntotal))\n",
    "    dataset.fill(0.0)\n",
    "  del indices\n",
    "  del dataset\n",
    "  if verbose:\n",
    "    print(\"\")\n",
    "    print(\"Total time: %.3f s\\n\" % (time.time() - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.2 Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Extract the XYZ coordinates and atomic symbols from the raw file. The raw file is not a standard XYZ file, so we need to write a helper function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_xyz(filename, verbose=True):\n",
    "  \"\"\"\n",
    "  Extract symbols, coordiantes and forces (for later usage) from the raw file.\n",
    "  \n",
    "  Args:\n",
    "    filename: a str.\n",
    "    verbose: a bool.\n",
    "\n",
    "  Returns\n",
    "    energies: Array[N,]\n",
    "    coordinates: Array[N, 17, 3], a 3D array containing the atomic coordinates.\n",
    "    forces: Array[N, 17, 3], a 3D array containing the atomic forces.\n",
    "  \n",
    "  \"\"\"\n",
    "  group = \"raw\"\n",
    "  hdb = h5py.File(HDF5_DATABASE_FILE)\n",
    "  if group not in hdb:\n",
    "    hdb.create_group(group)\n",
    "  \n",
    "  try:\n",
    "    energies = hdb[group][\"energies\"][:]\n",
    "    coordinates = hdb[group][\"coordinates\"][:]\n",
    "    forces = hdb[group][\"forces\"][:]\n",
    "\n",
    "  except Exception:\n",
    "    energies = np.zeros((TOTAL_SIZE,), dtype=NP_TYPE)\n",
    "    coordinates = np.zeros((TOTAL_SIZE, NUM_SITES, 3), dtype=NP_TYPE)\n",
    "    forces = np.zeros((TOTAL_SIZE, NUM_SITES, 3), dtype=NP_TYPE)\n",
    "    stage = 0\n",
    "    i = 0\n",
    "    j = 0\n",
    "    energy_patt = re.compile(r\".*energy=([\\d.-]+).*\")\n",
    "    string_patt = re.compile(r\"([A-Za-z]{1,2})\\s+([\\d.-]+)\\s+([\\d.-]+)\\s+([\\d.-]+)\\s+\"\n",
    "                              \"\\d+\\s+\\d.\\d+\\s+\\d+\\s+([\\d.-]+)\\s+([\\d.-]+)\\s+([\\d.-]+)\")\n",
    "    tic = time.time()\n",
    "    if verbose:\n",
    "      sys.stdout.write(\"Extract cartesian coordinates ...\\n\")\n",
    "    with open(filename) as f:\n",
    "      for line in f:\n",
    "        if i == TOTAL_SIZE:\n",
    "          break\n",
    "        l = line.strip()\n",
    "        if l == \"\":\n",
    "          continue\n",
    "        if stage == 0:\n",
    "          if l.isdigit():\n",
    "            n = int(l)\n",
    "            if n != NUM_SITES:\n",
    "              raise ValueError(\"The parsed size %d != NUM_SITES\" % n)\n",
    "            stage += 1\n",
    "        elif stage == 1:\n",
    "          m = energy_patt.search(l)\n",
    "          if m:\n",
    "            energies[i] = float(m.group(1))\n",
    "            stage += 1\n",
    "        elif stage == 2:\n",
    "          m = string_patt.search(l)\n",
    "          if m:\n",
    "            coordinates[i, j, :] = float(m.group(2)), float(m.group(3)), float(m.group(4))\n",
    "            forces[i, j, :] = float(m.group(5)), float(m.group(6)), float(m.group(7))\n",
    "            j += 1\n",
    "            if j == NUM_SITES:\n",
    "              j = 0\n",
    "              stage = 0\n",
    "              i += 1\n",
    "              if verbose and i % 1000 == 0:\n",
    "                sys.stdout.write(\"\\rProgress: %7d  /  %7d\" % (i, TOTAL_SIZE))\n",
    "      if verbose:\n",
    "        print(\"\")\n",
    "        print(\"Total time: %.3f s\\n\" % (time.time() - tic))\n",
    "      hdb[group].create_dataset(\"energies\", data=energies)\n",
    "      hdb[group].create_dataset(\"coordinates\", data=coordinates)\n",
    "      hdb[group].create_dataset(\"forces\", data=forces)\n",
    "  \n",
    "  finally:\n",
    "    hdb.close()\n",
    "  \n",
    "  return energies, coordinates, forces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "energies, coordinates, _ = extract_xyz(XYZ_FILE)\n",
    "ax = sns.distplot(energies)\n",
    "ax.set_xlabel(\"DFTB Energy (eV)\", fontsize=14)\n",
    "ax.set_ylabel(\"Probablity\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.3 Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Build the dataset and split it into training, validation and testing datasets:\n",
    "\n",
    "1. Initialize a new hdf5 file if it can not be accessed.\n",
    "1. Extract symbols, energies, atomic coordinates and forces from the file.\n",
    "2. Transform and scale the coordinates to build the 4D array $[N, 1, C_{N}^{k}, C_{k}^{2}]$.\n",
    "\n",
    "**Warning: in this case ($\\textrm{C}_{9}\\textrm{H}_{7}\\textrm{N}$), all samples are considered unique!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def may_build_dataset(filename, l=4.0, verbose=True):\n",
    "  \"\"\"\n",
    "  Build the training, validation and testing dataset and targets from a XYZ file.\n",
    "\n",
    "  Args:\n",
    "    filename: str, a file with CP2K/XYZ format.\n",
    "    l: float, the exponential parameter.\n",
    "    verbose: bool, print the building progress if True.\n",
    "\n",
    "  Returns:\n",
    "    features: a 4D array as the transformed input features.\n",
    "    targets: a 2D array as the scaled ([0, 1]) targets.\n",
    "    scaler: a ``sklearn.preprocessing.MinMaxScaler``.\n",
    "\n",
    "  \"\"\"\n",
    "  # Compute the MD5 checksum of the xyzfile\n",
    "  checksum = md5(filename)\n",
    "\n",
    "  if verbose:\n",
    "    print(\"-> Load the training, validation and testing datasets ...\\n\")\n",
    "\n",
    "  coords, energies = None, None\n",
    "  features, targets = None, None\n",
    "  backup_hdf5 = False\n",
    "  extract_coords = True\n",
    "  build_features = True\n",
    "\n",
    "  # If the HDF5 file is already existed, we try to load dataset and targets from\n",
    "  # the HDF5 file directly if the checksums are equal.\n",
    "  if isfile(HDF5_DATABASE_FILE):\n",
    "    with h5py.File(HDF5_DATABASE_FILE, \"r\") as hdb:\n",
    "      if hdb.attrs.get(\"checksum\", 0) == checksum:\n",
    "        # There are two main groups in this HDF5 file:\n",
    "        # 1. the first group is 'train' where training data and training targets\n",
    "        # are stored.\n",
    "        if \"train\" in hdb:\n",
    "          features = hdb[\"train\"][\"dataset\"][:LOAD_SIZE]\n",
    "          targets = hdb[\"train\"][\"targets\"][:LOAD_SIZE]\n",
    "          build_features = False\n",
    "          extract_coords = False\n",
    "        # 2. the second group is 'unique' where uniquified cartesian coordinates\n",
    "        # and their energies extracted from a CP2K/XYZ file are saved.\n",
    "        elif \"unique\" in hdb.keys():\n",
    "          coords = hdb[\"unique\"][\"coords\"][:]\n",
    "          energies = hdb[\"unique\"][\"energies\"][:]\n",
    "          extract_coords = False\n",
    "      # The checksum are not equal, so we backup the existed HDF5 databse by\n",
    "      # renaming it.\n",
    "      else:\n",
    "        backup_hdf5 = True\n",
    "    if backup_hdf5:\n",
    "      if verbose:\n",
    "        print(\"MD5 checksums mismatched. Build a new dataset.\\n\")\n",
    "      shutil.move(HDF5_DATABASE_FILE, HDF5_DATABASE_FILE + \".bak\")\n",
    "\n",
    "  # Extract the raw cartesian coordinates and energis (eV) from the CP2K/XYZ\n",
    "  # file and save these data into group 'raw'. All data are compressed with the\n",
    "  # lossless gzip filter.\n",
    "  if extract_coords:\n",
    "    energies, coords, _ = extract_xyz(filename, verbose=verbose)\n",
    "    # Remove the duplicates to reduce the dataset\n",
    "    # coords, energies = remove_duplicates(coords, energies, verbose=verbose)\n",
    "    with h5py.File(HDF5_DATABASE_FILE) as hdb:\n",
    "      # Delete the previous group `unique`. This should not happen, but it may\n",
    "      # be inserted manually for debugging.\n",
    "      group = \"unique\"\n",
    "      if group in hdb.keys():\n",
    "        del hdb[group]\n",
    "      hdb.attrs[\"checksum\"] = checksum\n",
    "      hdb.create_group(group)\n",
    "      hdb[group].create_dataset(\"coords\", data=coords, compression=\"gzip\")\n",
    "      hdb[group].create_dataset(\"energies\", data=energies, compression=\"gzip\")\n",
    "  elif verbose:\n",
    "    print(\"Use existed coordinates and energies.\\n\")\n",
    "\n",
    "  # Transform the cartesian coordinates to a 4D dataset. Permute this dataset\n",
    "  # several times and then we split it into three parts: training, validation\n",
    "  # and testing. Save these datasets and their targets into group 'cnn'.\n",
    "  if build_features:\n",
    "    # Allocate the disk space and then write transformed data piece by piece\n",
    "    # because my little computer only has 16GB memory.\n",
    "    shape = [len(energies), 1, comb(NUM_SITES, 4, True), comb(4, 2, True)]\n",
    "    hdb = h5py.File(HDF5_DATABASE_FILE)\n",
    "    try:\n",
    "      group = hdb.require_group(\"train\")\n",
    "      group.create_dataset(\"targets\", data=energies)\n",
    "      mapping = group.create_dataset(\n",
    "        \"dataset\", shape=shape, dtype=np.float32)\n",
    "      # Set the chunksize to 10000.\n",
    "      chunksize = 10000\n",
    "      transform_coords(coords, chunksize, mapping, l=l, verbose=verbose)\n",
    "    except Exception as excp:\n",
    "      del hdb[\"train\"]\n",
    "      raise excp\n",
    "    finally:\n",
    "      hdb.close()\n",
    "    # After the transformation we now load the whole dataset into memory.\n",
    "    with h5py.File(HDF5_DATABASE_FILE) as hdb:\n",
    "      features = hdb[\"train\"][\"dataset\"][:LOAD_SIZE]\n",
    "      targets = np.array(energies[:LOAD_SIZE], copy=False)\n",
    "    if verbose:\n",
    "      print(\"Dataset size (MB)     : \", features.nbytes / 1024 / 1024)\n",
    "      print(\"Targets size (MB)     : \", targets.nbytes / 1024 / 1024)\n",
    "      print(\"\")\n",
    "    del coords\n",
    "  elif verbose:\n",
    "    print(\"Use existed features and targets.\\n\")\n",
    "\n",
    "  # Determine the maximum and minimum energy. The energies should be scaled to\n",
    "  # [0, 1] during training.\n",
    "  scaler = MinMaxScaler()\n",
    "  targets = scaler.fit_transform(np.atleast_2d(targets).T)\n",
    "\n",
    "  if verbose:\n",
    "    print(\"-> Datasets and targets are loaded into memories.\")\n",
    "    print(\"\")\n",
    "  return features, targets, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "features, targets, scaler = may_build_dataset(XYZ_FILE, l=lmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.4 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this section further data preprocessing procedures will be taken. See http://cs231n.github.io/neural-networks-2/#batchnorm for explanantions.\n",
    "\n",
    "Before preprocessing, let's first demonstrate the value distributions of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.distplot欣(features.flatten()[::20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then, before we start training, the dataset should also be splited. ``sklearn.model_selection.train_test_split`` can be used to achieve this. Remember to set the random state!\n",
    "\n",
    "Some important notes:\n",
    "\n",
    "1. **In practice: ** it is very important to zero-center the data.\n",
    "2. **Common pitfall: ** An important point to make about the preprocessing is that any preprocessing statistics (e.g. the data mean) must only be computed on the training data, and then applied to the validation / test data. E.g. computing the mean and subtracting it from every image across the entire dataset and then splitting the data into train/val/test splits would be a mistake. Instead, the mean must be computed only over the training data and then subtracted equally from all splits (train/val/test).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Split the datatset into training and testing datasets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, random_state=SEED, test_size=0.1)\n",
    "\n",
    "# Further split the training dataset into training and validation datasets.\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, random_state=SEED, test_size=0.1)\n",
    "\n",
    "# Make the data zero-centered.\n",
    "mean = X_train.mean()\n",
    "X_train -= mean\n",
    "X_test -= mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Display the value distributions of ``X_train`` and ``X_test``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=[14, 7])\n",
    "\n",
    "ax = sns.distplot(X_train.flatten()[::20], ax=axes[0])\n",
    "ax.set_xlabel(\"Scaled Distance\", fontsize=12)\n",
    "ax.set_ylabel(\"Relative Numbers\", fontsize=12)\n",
    "ax.set_title(\"X_train\", fontsize=14)\n",
    "\n",
    "ax = sns.distplot(X_test.flatten()[::2], ax=axes[1])\n",
    "ax.set_xlabel(\"Scaled Distance\", fontsize=12)\n",
    "ax.set_ylabel(\"Relative Numbers\", fontsize=12)\n",
    "ax.set_title(\"X_test\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we can start training this network. The training settings defined in this paper are summerized here:\n",
    "\n",
    "* Root-mean-squared loss.\n",
    "* Mini-batch stochastic gradient descent with momentum\n",
    "     * Batch size: 50.\n",
    "     * Momentum factor: 0.7.\n",
    "* Step Decay Function: $s_{i} = s_{0}r/(r + i)$\n",
    "     * $s_0$ is the initial step length, which is 0.1 in this paper.\n",
    "     * r is a predefined factor and r = 60 in this paper.\n",
    "* Regularization factor is not mentioned in this paper.\n",
    "     * $\\lambda$ is set to None.\n",
    "* The exponentia ldecay rate is 0.9 in this implementaion.\n",
    "* The total number of epochs for the reference dataset is 1400.\n",
    "\n",
    "These parameters are declared in the following section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "momentum_factor = 0.7\n",
    "start_learning_rate = 0.1\n",
    "decay_rate = 0.9\n",
    "decay_factor = 60\n",
    "rlambda = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we can begin the training. Initialize a new ``graph`` and use it as the default graph. Then we get the infered network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_batch = tf.placeholder(TF_TYPE, [batch_size, 1, CNK, CK2])\n",
    "y_batch = tf.placeholder(TF_TYPE, [batch_size, 1])\n",
    "\n",
    "estimates = inference(X_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Add helper functions to visualize the graph. These codes are copied from the official example **deepdream**. Uncomment the last line can display the network graph inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script src=\"//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js\"></script>\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "\n",
    "# Uncomment the following line to visualize the graph in this notebook. \n",
    "# show_graph(graph.as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Setup the total loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "  rms = tf.sqrt(tf.losses.mean_squared_error(estimates, y_batch))\n",
    "  tf.summary.scalar(\"rms\", rms)\n",
    "  if rlambda is not None:\n",
    "    with tf.name_scope(\"regularizer\"):\n",
    "      r = tf.multiply(\n",
    "        tf.add_n(\n",
    "          [tf.nn.l2_loss(v) for v in tf.trainable_variables() \n",
    "           if \"bias\" not in v.name]), rlambda)\n",
    "      tf.summary.scalar(\"l2\", r)\n",
    "    loss = rms + r\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "  else:\n",
    "    loss = rms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Setup the learning rate decay and the momentum optimizer. The learning rate $r_i$ computed by the inverse decay function in this paper is implemented here:\n",
    "$$r_{i} = \\frac{rs_{0}}{r + i}$$\n",
    "where $i$ is the epoch, $r$ is a constant decay factor and $s_0$ is the initial learning rate. $r$ is set to 60 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def inverse_decay(init_learning_rate, epoch, decay_factor, name=None):\n",
    "  \"\"\"\n",
    "  The inverse decay function.\n",
    "  \n",
    "  Args:\n",
    "    init_learning_rate: A scalar `float32` or `float64` `Tensor` or a\n",
    "      Python number.  The initial learning rate.\n",
    "    global_epoch: A scalar `int32` or `int64` `Tensor` or a Python number.\n",
    "      Global epoch to use for the decay computation.  Must not be negative.\n",
    "    decay_factor: A scalar `int32` or `int64` `Tensor` or a Python number.\n",
    "      Must be positive.  See the decay equation above.\n",
    "    name: String.  Optional name of the operation.  Defaults to\n",
    "      'ExponentialDecay'.\n",
    "\n",
    "  Returns:\n",
    "    A scalar `Tensor` of the same type as `learning_rate`.  The decayed\n",
    "    learning rate.\n",
    "  \n",
    "  \"\"\"\n",
    "  if epoch is None:\n",
    "    raise ValueError(\"global_step is required for inv_decay.\")\n",
    "  with ops.name_scope(name, \"InvDecay\", \n",
    "                      [init_learning_rate, epoch, decay_factor]) as name:\n",
    "    init_learning_rate = ops.convert_to_tensor(\n",
    "      init_learning_rate, name=\"init_learning_rate\")\n",
    "    dtype = init_learning_rate.dtype\n",
    "    epoch = math_ops.cast(epoch, dtype)\n",
    "    decay_factor = math_ops.cast(decay_factor, dtype)\n",
    "    top = math_ops.multiply(init_learning_rate陈欣, decay_factor)\n",
    "    return math_ops.div(top, math_ops.add(epoch, decay_factor), name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "global_epoch = tf.Variable(0, dtype=tf.int64, trainable=False, name=\"global_epoch\")\n",
    "global_epoch_op = tf.assign(global_epoch, global_epoch + 1)\n",
    "learning_rate = inverse_decay(start_learning_rate, global_epoch, decay_factor)\n",
    "tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "\n",
    "batch = tf.Variable(0, dtype=tf.int64, trainable=False)\n",
    "\n",
    "# learning_rate = tf.train.exponential_decay(\n",
    "#   start_learning_rate, \n",
    "#   tf.multiply(batch, batch_size), \n",
    "#   len(X_train), \n",
    "#   decay_rate,\n",
    "# )\n",
    "# Test the Adagrad optimzier\n",
    "# trainer = tf.train.AdagradDAOptimizer(\n",
    "#   learning_rate=learning_rate, \n",
    "#   initial_gradient_squared_accumulator_value=0.1,\n",
    "#   global_step=batch\n",
    "# )\n",
    "\n",
    "# Use the Momentum optimizer陈欣\n",
    "trainer = tf.train.MomentumOptimizer(learning_rate, momentum_factor)\n",
    "optimizer = trainer.minimize(loss, global_step=batch)\n",
    "\n",
    "# Merge all the summaries.\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(logdir=\"./events/%s\" % get_time_id(), graph=graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.2 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "These variabls control the key settings of a training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 200陈欣\n",
    "log_frequency = 50\n",
    "eval_frequency = 100\n",
    "save_frequency = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Setup the saving path and the checkpoint file so that we can reuse this model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "save_path = \"./saves\"\n",
    "chk_file = join(save_path, \"c9h7n.ckpt\")\n",
    "if not isdir(save_path):\n",
    "  makedirs(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Small utility function to evaluate a dataset by feeding batches of data to ``{eval_dataset}`` and pulling the results \n",
    "from ``{eval_values}``. Saves memory and enables this to run on smaller GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def eval_in_batches(data):\n",
    "  \"\"\" Get all predictions for a dataset by running it in small batches. \"\"\"\n",
    "  size = data.shape[0]\n",
    "  if size < batch_size:\n",
    "    raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\n",
    "  eval_values = np.ndarray(shape=(size, 1), dtype=NP_TYPE)\n",
    "  for i, inext in brange(0, size, batch_size):\n",
    "    eval_values[i: inext] = sess.run(\n",
    "      estimates,\n",
    "      feed_dict={X_batch: data[i: inext, ...]})\n",
    "  return eval_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This helper function is used to restore the latest checkpoint if existed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def restore_latest_from_ckpt(save_dir):\n",
    "  \"\"\"\n",
    "  Restore the latest checkpoint from the 'scratch' if possible.\n",
    "  \"\"\"\n",
    "  if not isdir(save_dir):\n",
    "    return None\n",
    "  ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "  if ckpt and ckpt.model_checkpoint_path:\n",
    "    return ckpt.model_checkpoint_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally we invoke a new session and start this training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Build an initialization operation.\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Register a model saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Load the previous checkpoint if existed.\n",
    "checkpoint_path = restore_latest_from_ckpt(save_path)\n",
    "load = False\n",
    "if load and checkpoint_path:\n",
    "  saver.restore(sess, checkpoint_path)\n",
    "  print(\"Restore from the lastest checkpoint file %s ...\" % checkpoint_path)\n",
    "  print(\"\")\n",
    "\n",
    "print(\"Initialized!\")\n",
    "print(\"\")\n",
    "print(\"Training Samples      :\", len(X_train))\n",
    "print(\"Batch Size            :\", batch_size)\n",
    "print(\"Number of Epochs      :\", num_epochs)\n",
    "print(\"Log Frequency         :\", log_frequency)\n",
    "print(\"Eval Frequency        :\", eval_frequency)\n",
    "print(\"\")\n",
    "\n",
    "tic = time.time()\n",
    "tstart = time.time()\n",
    "\n",
    "# Keep the validation errors locally\n",
    "valid_hists = []\n",
    "\n",
    "# Loop through training steps.\n",
    "for step in range(int(num_epochs * len(X_train)) // batch_size):\n",
    "  # Compute the offset of the current minibatch in the data.\n",
    "  # The dataset was already shuffled in assignment 1 so we do not need to\n",
    "  # randomize it.\n",
    "  offset = (step * batch_size) % (len(X_train) - batch_size)\n",
    "  batch_dataset = X_train[offset: (offset + batch_size), ...]\n",
    "  batch_targets = y_train[offset: (offset + batch_size), ...]\n",
    "  # Build the feed dict to feed previous defined placeholders.\n",
    "  feed_dict = {X_batch: batch_dataset, y_batch: batch_targets}\n",
    "  # Run the optimization session.\n",
    "  sess.run([optimizer], feed_dict=feed_dict)\n",
    "  # Run the step decay function\n",
    "  if step > 0 and step % (len(X_train) // batch_size) == 0:\n",
    "    sess.run([global_epoch_op])\n",
    "  # Save the training accuracy every 100 steps.\n",
    "  if step % log_frequency == 0:\n",
    "    summary, error = sess.run([merged, loss], feed_dict=feed_dict)\n",
    "    elapsed_time = time.time() - tic\n",
    "    tic = time.time()\n",
    "    eval_run = (step % eval_frequency == 0) and (step > 0)\n",
    "    if not eval_run:\n",
    "      print(\"Step %6d (epoch %5.2f), loss: %.6f, time: %.3f s\" % (\n",
    "        step, float(step) * batch_size / len(X_train), error, elapsed_time))\n",
    "    writer.add_summary(summary, step)\n",
    "    # Every `eval_frequency` steps we shall take several extra operations,\n",
    "    # including printing the validation accuracy and updating the learning\n",
    "    # rate.\n",
    "    if eval_run:\n",
    "      lr = sess.run(learning_rate)\n",
    "      valid_error = root_mean_squred(y_test - eval_in_batches(X_test))\n",
    "      valid_hists.append([step, valid_error])\n",
    "      print(\"Step %6d (epoch %5.2f), loss: %.6f, time: %.3f s, error: %.6f\" % (\n",
    "        step, float(step) * batch_size / len(X_train), error, elapsed_time, valid_error))\n",
    "    sys.stdout.flush()\n",
    "  # Save the trained model every 1000 steps.\n",
    "  if step % save_frequency == 0:\n",
    "    print(\"\")\n",
    "    print(\"Time since beginning  : %.3f s\" % (time.time() - tstart))\n",
    "    print(\"\")\n",
    "    saver.save(sess, save_path=chk_file, global_step=batch)\n",
    "# Close the writer\n",
    "writer.close()\n",
    "# Finally the training is completed. Now let me see if this MBE model can\n",
    "# really estimate DFT energies.\n",
    "print(\"\")\n",
    "print(\"-> Test error         : %.6f\" % root_mean_squred(y_test - eval_in_batches(X_test)))\n",
    "print(\"\")\n",
    "# Do not forget to save the model one last time!\n",
    "saver.save(sess, save_path=chk_file, global_step=batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.3 Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this section we will display some figures to analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Plot the smoothed validation errors\n",
    "xy = np.asarray(valid_hists)\n",
    "size = 20\n",
    "x, y = xy[:, 0], smooth(xy[:, 1], size)[:len(xy)]\n",
    "fig, ax = plt.subplots(1, 1, figsize=[12, 7])\n",
    "ax.plot(x, y, \"r-\")\n",
    "ax.set_ylabel(\"Validation RMSE\", fontsize=12)\n",
    "ax.set_xlabel(\"Steps\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Transform the estimates and targets back to energies\n",
    "y_train_est = eval_in_batches(X_train)\n",
    "y_test_est = eval_in_batches(X_test)\n",
    "energies_train = scaler.inverse_transform(y_train)\n",
    "energies_train_pred = scaler.inverse_transform(y_train_est)\n",
    "energies_test = scaler.inverse_transform(y_test)\n",
    "energies_test_pred = scaler.inverse_transform(y_test_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Plot the regression figure using ``seaborn.regplot``.\n",
    "fig, axes = plt.subplots(1, 2, figsize=[16, 8], sharey=False)\n",
    "\n",
    "def plot_regression(y_true, y_pred, iax, title):\n",
    "  r2 = r2_score(y_true, y_pred)\n",
    "  mae = mean_abs_error(y_true, y_pred)\n",
    "  diff = y_true - y_pred\n",
    "  stddev = np.var(diff)\n",
    "  ax = sns.regplot(x=y_true.flatten(), y=y_pred.flatten(), ax=axes[iax])\n",
    "  ax.set_ylabel(\"DFTB Energy (eV)\", fontsize=12)\n",
    "  ax.set_xlabel(\"DNN Energy (eV)\", fontsize=12)\n",
    "  ax.set_title(\"%s, $R^2$=%.3f, MAE=%.3f, stddev=%.3f\" % (title, r2, mae, stddev), fontsize=12)\n",
    "\n",
    "# Plot the training result\n",
    "plot_regression(energies_train, energies_train_pred, 0, \"Training Data\")\n",
    "\n",
    "# Plot the testing result\n",
    "plot_regression(energies_test, energies_test_pred, 1, \"Testing Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
