{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Xin Chen's implementation of the MBE-NN network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Author: Xin Chen\n",
    "* Email: Bismarrck@me.com\n",
    "\n",
    "This jupyter notebook is used to repeat the work of http://doi.org/10.1021/acs.jctc.6b00994. \n",
    "\n",
    "The test cluster is $\\textrm{C}_{9}\\textrm{H}_{7}\\textrm{N}$. The reference energies are calculated with DFTB.\n",
    "\n",
    "<img src=\"./images/C9H7N.png\" alt=\"network\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The structure of the deep convolutional neural network for the $Pt_{13}$ cluster is as follows:\n",
    "\n",
    "<img src=\"./images/convnet.jpg\" alt=\"network\" style=\"width: 800px;\"/>\n",
    "\n",
    "The input features are transformed interatomic distances. The output node represents the estimated DFT energies.\n",
    "The detailed explanantion of this convolutionary neural network will be given in the following section **Inference**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this section we shall import python modules and declare global constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport models\n",
    "%aimport similarity\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import h5py\n",
    "import sys\n",
    "import time\n",
    "import hashlib\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from fnmatch import fnmatch\n",
    "from itertools import repeat\n",
    "from os.path import isfile, isdir, join, basename, splitext\n",
    "from os import makedirs, listdir\n",
    "from scipy.misc import comb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import pairwise_distances, r2_score\n",
    "from itertools import combinations\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework.ops import GraphKeys\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.contrib.learn import ModeKeys\n",
    "sns.set(font=\"serif\")\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The patch size is always 1x1\n",
    "PATCH_SIZE = 1\n",
    "\n",
    "# The HDF5 database file.\n",
    "HDF5_DATABASE_FILE = \"B20.hdf5\"\n",
    "\n",
    "# The dimension of the cluster.\n",
    "NUM_SITES = 20\n",
    "\n",
    "# The total number of structures in the XYZ file should be 209660.\n",
    "XYZ_FILE = \"../datasets/B20pbe.xyz\"\n",
    "XYZ_FORMAT = 'cp2k'\n",
    "TOTAL_SIZE = 209660\n",
    "# TOTAL_SIZE = 2410\n",
    "\n",
    "# Setup the size.\n",
    "LOAD_SIZE = 209660\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# Setup the random seed.\n",
    "SEED = 235\n",
    "\n",
    "# Setup the precision of floats. \n",
    "TF_TYPE = tf.float32\n",
    "NP_TYPE = np.float32\n",
    "\n",
    "# Cuda\n",
    "CUDA_ON = True\n",
    "\n",
    "# Set this to True to make input features zero-centered.\n",
    "ZERO_CENTER = False\n",
    "\n",
    "# Sort the features on the last axis.\n",
    "SORTED = True\n",
    "\n",
    "# The fixed constants: C(N,k) and C(k,2) where k is 4.\n",
    "CNK = comb(NUM_SITES, 4, exact=True)\n",
    "CK2 = comb(4, 2, exact=True)\n",
    "\n",
    "# The pyykko radius (+1) for each element.\n",
    "pyykko = {\n",
    "  'Ac': 1.86, 'Ag': 1.28, 'Al': 1.26, 'Am': 1.66, 'Ar': 0.96, 'As': 1.21,\n",
    "  'At': 1.47, 'Au': 1.24, 'B': 0.85, 'Ba': 1.96, 'Be': 1.02, 'Bh': 1.41,\n",
    "  'Bi': 1.51, 'Bk': 1.68, 'Br': 1.14, 'C': 0.75, 'Ca': 1.71, 'Cd': 1.36,\n",
    "  'Ce': 1.63, 'Cf': 1.68, 'Cl': 0.99, 'Cm': 1.66, 'Co': 1.11, 'Cr': 1.22,\n",
    "  'Cs': 2.32, 'Cu': 1.12, 'Db': 1.49, 'Ds': 1.28, 'Dy': 1.67, 'Er': 1.65,\n",
    "  'Es': 1.65, 'Eu': 1.68, 'F': 0.64, 'Fe': 1.16, 'Fm': 1.67, 'Fr': 2.23,\n",
    "  'Ga': 1.24, 'Gd': 1.69, 'Ge': 1.21, 'H': 0.32, 'He': 0.46, 'Hf': 1.52,\n",
    "  'Hg': 1.33, 'Ho': 1.66, 'Hs': 1.34, 'I': 1.33, 'In': 1.42, 'Ir': 1.22,\n",
    "  'K': 1.96, 'Kr': 1.17, 'La': 1.8, 'Li': 1.33, 'Lu': 1.62, 'Md': 1.73,\n",
    "  'Mg': 1.39, 'Mn': 1.19, 'Mo': 1.38, 'Mt': 1.29, 'N': 0.71, 'Na': 1.55,\n",
    "  'Nb': 1.47, 'Nd': 1.74, 'Ne': 0.67, 'Ni': 1.1, 'No': 1.76, 'Np': 1.71,\n",
    "  'O': 0.63, 'Os': 1.29, 'P': 1.11, 'Pa': 1.69, 'Pb': 1.44, 'Pd': 1.2,\n",
    "  'Pm': 1.73, 'Po': 1.45, 'Pr': 1.76, 'Pt': 1.23, 'Pu': 1.72, 'Ra': 2.01,\n",
    "  'Rb': 2.1, 'Re': 1.31, 'Rf': 1.57, 'Rh': 1.25, 'Rn': 1.42, 'Ru': 1.25,\n",
    "  'S': 1.03, 'Sb': 1.4, 'Sc': 1.48, 'Se': 1.16, 'Sg': 1.43, 'Si': 1.16,\n",
    "  'Sm': 1.72, 'Sn': 1.4, 'Sr': 1.85, 'Ta': 1.46, 'Tb': 1.68, 'Tc': 1.28,\n",
    "  'Te': 1.36, 'Th': 1.75, 'Ti': 1.36, 'Tl': 1.44, 'Tm': 1.64, 'U': 1.7,\n",
    "  'V': 1.34, 'W': 1.37, 'X': 0.32, 'Xe': 1.31, 'Y': 1.63, 'Yb': 1.7,\n",
    "  'Zn': 1.18, 'Zr': 1.54\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Some global helper functions are declared here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def brange(start, stop, batchsize):\n",
    "  \"\"\"\n",
    "  Range from `start` to `stop` given a batch size and return the start and stop of each batch.\n",
    "  \n",
    "  Args:\n",
    "    start: int, the start number of a sequence.\n",
    "    stop: int, the end number of a sequence.\n",
    "    batchsize: int, the size of each batch.\n",
    "  \n",
    "  \"\"\"\n",
    "  istart = start\n",
    "  while istart < stop:\n",
    "    istop = min(istart + batchsize, stop)\n",
    "    yield istart, istop\n",
    "    istart = istop\n",
    "\n",
    "\n",
    "def exponential(x, l=4.0):\n",
    "  \"\"\"\n",
    "  Exponentially scale the input value(s).  \n",
    "  \"\"\"\n",
    "  return np.exp(-x / l)\n",
    "\n",
    "\n",
    "def md5(filename):\n",
    "  \"\"\" \n",
    "  Return the md5 checksum of the given file.\n",
    "\n",
    "  Args:\n",
    "    filename: a file.\n",
    "\n",
    "  Returns:\n",
    "    checksum: the MD5 checksum of the file.\n",
    "\n",
    "  \"\"\"\n",
    "  hash_md5 = hashlib.md5()\n",
    "  with open(filename, \"rb\") as f:\n",
    "    for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "      hash_md5.update(chunk)\n",
    "  return hash_md5.hexdigest()\n",
    "\n",
    "\n",
    "def root_mean_squred(x):\n",
    "  \"\"\"\n",
    "  Return the root mean squared of the given array.\n",
    "  \"\"\"\n",
    "  return np.sqrt(np.mean(np.square(x)))\n",
    "\n",
    "\n",
    "def mean_abs_error(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  Return the mean absolute error.\n",
    "  \"\"\"\n",
    "  return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "\n",
    "def get_time_id():\n",
    "  \"\"\"\n",
    "  Return a string as the ID of this run.\n",
    "  \"\"\"\n",
    "  return datetime.strftime(datetime.fromtimestamp(time.time()), \"%m%d%H%M\")\n",
    "\n",
    "\n",
    "def smooth(x, window_len=11, window='hanning'):\n",
    "  \"\"\"\n",
    "  smooth the data using a window with requested size.\n",
    "\n",
    "  This method is based on the convolution of a scaled window with the signal.\n",
    "  The signal is prepared by introducing reflected copies of the signal\n",
    "  (with the window size) in both ends so that transient parts are minimized\n",
    "  in the begining and end part of the output signal.\n",
    "\n",
    "  Args:\n",
    "    x: the input signal\n",
    "    window_len: the dimension of the smoothing window; should be an odd integer\n",
    "    window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman' \n",
    "      flat window will produce a moving average smoothing.\n",
    "\n",
    "  output:\n",
    "    the smoothed signal\n",
    "  \"\"\"\n",
    "\n",
    "  if x.ndim != 1:\n",
    "    raise ValueError(\"smooth only accepts 1 dimension arrays.\")\n",
    "\n",
    "  if x.size < window_len:\n",
    "    raise ValueError(\"Input vector needs to be bigger than window size.\")\n",
    "\n",
    "  if window_len < 3:\n",
    "    return x\n",
    "\n",
    "  if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "    raise ValueError(\"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\")\n",
    "\n",
    "  s = np.r_[x[window_len-1:0:-1],x,x[-1:-window_len:-1]]\n",
    "  if window == 'flat':\n",
    "    w = np.ones(window_len,'d')\n",
    "  else:\n",
    "    w = eval('np.'+ window +'(window_len)')\n",
    "\n",
    "  return np.convolve(w / w.sum(), s, mode='valid')\n",
    "\n",
    "\n",
    "def get_pyykko_bonds_matrix(species, factor=1.5, flatten=True):\n",
    "  \"\"\"\n",
    "  Return the pyykko-bonds matrix given a list of atomic symbols.\n",
    "  \n",
    "  Args:\n",
    "    species: List[str], a list of atomic symbols.\n",
    "    factor: a float, the scaling factor.\n",
    "    flatten: a bool. The bonds matrix will be flatten to a 1D array if True.\n",
    "\n",
    "  Returns:\n",
    "    bonds: the bonds matrix (or vector if `flatten` is True).\n",
    "  \n",
    "  \"\"\"\n",
    "  rr = np.asarray([pyykko[specie] for specie in species])[:, np.newaxis]\n",
    "  lmat = factor * (rr + rr.T)\n",
    "  if flatten:\n",
    "    return lmat.flatten()\n",
    "  else:\n",
    "    return lmat\n",
    "\n",
    "\n",
    "def plot_pyykko_bonds_matrix(bonds, species):\n",
    "  \"\"\"\n",
    "  Plot the heatmap of the given pyykko bonds matrix.\n",
    "  \"\"\"\n",
    "  if len(bonds.shape) == 1:\n",
    "    bonds = bonds.reshape((len(species), len(species)))\n",
    "  sns.heatmap(bonds, xticklabels=species, yticklabels=species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This convnet is a slightly different from normal convolutionary neural networks. Suppose N is the batch size:\n",
    "\n",
    "* The shape of the input tensor is: $[N, 1, C_{N}^{k}, C_{k}^{2}]$\n",
    "* The **k** defines the many-body expansion. In this paper, k is selected to be 4.\n",
    "* The patch (kernel) is always 1x1.\n",
    "* The padding scheme is 'SAME' and the strides are 1 in both directions.\n",
    "\n",
    "The ``MBE-NN-M`` model and its variants are implemented in **``model.py``**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4. Preparing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.1 Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It is known that Gaussian coordinates are not suitable to direcly used as input of NN. Therefor, two transformations are adapted here on the input sample data in Cartesian coordiantes $\\{r_{j}\\}$:\n",
    "\n",
    "1. Transform each set of combination indices $\\{j\\}$ ($k=4$) to interatomic distances $\\{d_{i,j}\\}$ ($C_{k}^{2}=6$). \n",
    "2. Dump these interatomic distances with the exponential function: $d_{i,j}' = e^{-d_{i,j}/L}$. $L$ is a fixed parameter and 4.0 was used in this paper.\n",
    "\n",
    "The following figure demonstrates the workflow of these transformations:\n",
    "\n",
    "<img src=\"./images/input_transform.png\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
    "\n",
    "The shape of the final input matrix should be: [1, $C_{N}^{k}$, $C_{k}^{2}$]\n",
    "\n",
    "* The width is 1\n",
    "* The height is $C_{N}^{k}$\n",
    "* The depth is $C_{k}^{2}$\n",
    "\n",
    "So that we can use it in a convolutionary neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def transform_coords(coords, chunksize, mapping, l=4.0, verbose=True, sort=False):\n",
    "  \"\"\"\n",
    "  Transform the cartesian coordinates to input features.\n",
    "\n",
    "  Args:\n",
    "    coords: a 3D array with shape [M,N,3] representing the cartesian coordinates.\n",
    "    chunksize: the transformed array is too large. So save it piece by piece.\n",
    "    mapping: a `h5py.Dataset`, which is a symbolic to the real data on disk.\n",
    "    l: the exponential parameter.\n",
    "    verbose: print the transformation progress if True.\n",
    "\n",
    "  \"\"\"\n",
    "  ntotal, n = coords.shape[:2]\n",
    "  cnkv = comb(n, 4, exact=True)\n",
    "  ck2v = comb(4, 2, exact=True)\n",
    "  cnkl = list(combinations(range(NUM_SITES), 4))\n",
    "  # Using mapping indices can increase the speed 30 times!\n",
    "  indices = np.zeros((ck2v, cnkv), dtype=int)\n",
    "  for i in range(cnkv):\n",
    "    for j, (vi, vj) in enumerate(combinations(cnkl[i], 2)):\n",
    "      indices[j, i] = vi * n + vj\n",
    "  dataset = np.zeros((chunksize, 1, cnkv, ck2v), dtype=NP_TYPE)\n",
    "  tic = time.time()\n",
    "  if verbose:\n",
    "    print(\"Transform the cartesian coordinates ...\\n\")\n",
    "  for i, inext in brange(0, ntotal, chunksize):\n",
    "    for j in range(i, inext):\n",
    "      dists = pairwise_distances(coords[j]).flatten()\n",
    "      dists = exponential(dists, l=l)\n",
    "      for k in range(ck2v):\n",
    "        dataset[j - i, 0, :, k] = dists[indices[k]]\n",
    "      del dists\n",
    "    batch_size = inext - i\n",
    "    if sort:\n",
    "      dataset.sort(axis=-1)\n",
    "    mapping[i: inext, ...] = dataset[:batch_size, ...]\n",
    "    if verbose:\n",
    "      sys.stdout.write(\"\\rProgress: %7d  /  %7d\" % (inext, ntotal))\n",
    "    dataset.fill(0.0)\n",
    "  del indices\n",
    "  del dataset\n",
    "  if verbose:\n",
    "    print(\"\")\n",
    "    print(\"Total time: %.3f s\\n\" % (time.time() - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.2 Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Extract the XYZ coordinates and atomic symbols from the raw file. The raw file is not a standard XYZ file, so we need to write a helper function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_xyz(filename, verbose=True, xyz_format='grendel'):\n",
    "  \"\"\"\n",
    "  Extract symbols, coordiantes and forces (for later usage) from the file.\n",
    "  \n",
    "  Args:\n",
    "    filename: a str, the file to parse.\n",
    "    verbose: a bool.\n",
    "    xyz_format: a string representing the format of the given xyz file.\n",
    "\n",
    "  Returns\n",
    "    species: Array[NUM_SITES], an array of the atomic symbols.\n",
    "    energies: Array[N,], a 1D array of the atomic energies.\n",
    "    coordinates: Array[N, 17, 3], a 3D array of the atomic coordinates.\n",
    "    forces: Array[N, 17, 3], a 3D array of the atomic forces.\n",
    "  \n",
    "  \"\"\"\n",
    "  group = \"raw\"\n",
    "  hdb = h5py.File(HDF5_DATABASE_FILE)\n",
    "  if group not in hdb:\n",
    "    hdb.create_group(group)\n",
    "  \n",
    "  try:\n",
    "    energies = hdb[group][\"energies\"][:]\n",
    "    coordinates = hdb[group][\"coordinates\"][:]\n",
    "    forces = hdb[group][\"forces\"][:]\n",
    "    species = hdb[group][\"species\"][:]\n",
    "\n",
    "  except Exception:\n",
    "    energies = np.zeros((TOTAL_SIZE,), dtype=NP_TYPE)\n",
    "    coordinates = np.zeros((TOTAL_SIZE, NUM_SITES, 3), dtype=NP_TYPE)\n",
    "    forces = np.zeros((TOTAL_SIZE, NUM_SITES, 3), dtype=NP_TYPE)\n",
    "    species = []\n",
    "    parse_species = True\n",
    "    parse_forces = False\n",
    "    stage = 0\n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    if xyz_format.lower() == 'grendel':\n",
    "      energy_patt = re.compile(r\".*energy=([\\d.-]+).*\")\n",
    "      string_patt = re.compile(r\"([A-Za-z]{1,2})\\s+([\\d.-]+)\\s+([\\d.-]+)\\s+([\\d.-]+)\\s+\"\n",
    "                              \"\\d+\\s+\\d.\\d+\\s+\\d+\\s+([\\d.-]+)\\s+([\\d.-]+)\\s+([\\d.-]+)\")\n",
    "      parse_forces = True\n",
    "    elif xyz_format.lower() == 'cp2k':\n",
    "      energy_patt = re.compile(r\"i\\s=\\s+\\d+,\\sE\\s=\\s+([\\w.-]+)\")\n",
    "      string_patt = re.compile(r\"([A-Za-z]+)\\s+([\\w.-]+)\\s+([\\w.-]+)\\s+([\\w.-]+)\")\n",
    "    elif xyz_format.lower() == 'xyz':\n",
    "      energy_patt = re.compile(r\"([\\w.-]+)\")\n",
    "      string_patt = re.compile(r\"([A-Za-z]+)\\s+([\\w.-]+)\\s+([\\w.-]+)\\s+([\\w.-]+)\")\n",
    "    else:\n",
    "      raise ValueError(\"The file format of %s is not supported!\" % xyz_format)\n",
    "    \n",
    "    tic = time.time()\n",
    "    if verbose:\n",
    "      sys.stdout.write(\"Extract cartesian coordinates ...\\n\")\n",
    "    with open(filename) as f:\n",
    "      for line in f:\n",
    "        if i == TOTAL_SIZE:\n",
    "          break\n",
    "        l = line.strip()\n",
    "        if l == \"\":\n",
    "          continue\n",
    "        if stage == 0:\n",
    "          if l.isdigit():\n",
    "            n = int(l)\n",
    "            if n != NUM_SITES:\n",
    "              raise ValueError(\"The parsed size %d != NUM_SITES\" % n)\n",
    "            stage += 1\n",
    "        elif stage == 1:\n",
    "          m = energy_patt.search(l)\n",
    "          if m:\n",
    "            energies[i] = float(m.group(1))\n",
    "            stage += 1\n",
    "        elif stage == 2:\n",
    "          m = string_patt.search(l)\n",
    "          if m:\n",
    "            coordinates[i, j, :] = float(m.group(2)), float(m.group(3)), float(m.group(4))\n",
    "            if parse_forces:\n",
    "              forces[i, j, :] = float(m.group(5)), float(m.group(6)), float(m.group(7))\n",
    "            if parse_species:\n",
    "              species.append(m.group(1))\n",
    "              if len(species) == NUM_SITES:\n",
    "                species = np.asarray(species, dtype=object)\n",
    "                parse_species = False\n",
    "            j += 1\n",
    "            if j == NUM_SITES:\n",
    "              j = 0\n",
    "              stage = 0\n",
    "              i += 1\n",
    "              if verbose and i % 1000 == 0:\n",
    "                sys.stdout.write(\"\\rProgress: %7d  /  %7d\" % (i, TOTAL_SIZE))\n",
    "      if verbose:\n",
    "        print(\"\")\n",
    "        print(\"Total time: %.3f s\\n\" % (time.time() - tic))\n",
    "      hdb[group].create_dataset(\"energies\", data=energies)\n",
    "      hdb[group].create_dataset(\"coordinates\", data=coordinates)\n",
    "      hdb[group].create_dataset(\"forces\", data=forces)\n",
    "      hdb[group].create_dataset(\"species\", data=species, dtype=h5py.special_dtype(vlen=str))\n",
    "  \n",
    "  finally:\n",
    "    hdb.close()\n",
    "  \n",
    "  return species, energies, coordinates, forces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fb27b404860>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3IAAAK9CAYAAABhHl40AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb27b41d550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "species, energies, coordinates, _ = extract_xyz(XYZ_FILE, xyz_format=XYZ_FORMAT)\n",
    "ax = sns.distplot(energies)\n",
    "ax.set_xlabel(\"Energy (eV)\", fontsize=14)\n",
    "ax.set_ylabel(\"Probablity\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.3 Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Build the dataset and split it into training, validation and testing datasets:\n",
    "\n",
    "1. Initialize a new hdf5 file if it can not be accessed.\n",
    "1. Extract symbols, energies, atomic coordinates and forces from the file.\n",
    "2. Transform and scale the coordinates to build the 4D array $[N, 1, C_{N}^{k}, C_{k}^{2}]$.\n",
    "\n",
    "**Warning: in this case ($\\textrm{C}_{9}\\textrm{H}_{7}\\textrm{N}$), all samples are considered unique!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def may_build_dataset(filename, l=None, verbose=True, sort=False, **kwargs):\n",
    "  \"\"\"\n",
    "  Build the training, validation and testing dataset and targets from a XYZ file.\n",
    "\n",
    "  Args:\n",
    "    filename: str, a file with CP2K/XYZ format.\n",
    "    l: a float, a flatten vector or a 2D matrix, the exponential parameter(s). \n",
    "      If None, the pyykko bonds matrix will be used.\n",
    "    verbose: bool, print the building progress if True.\n",
    "    kwargs: additional arguments for extracting xyz files.\n",
    "\n",
    "  Returns:\n",
    "    features: a 4D array as the transformed input features.\n",
    "    targets: a 2D array as the scaled ([0, 1]) targets.\n",
    "    scaler: a ``sklearn.preprocessing.MinMaxScaler``.\n",
    "\n",
    "  \"\"\"\n",
    "  # Compute the MD5 checksum of the xyzfile\n",
    "  checksum = md5(filename)\n",
    "\n",
    "  if verbose:\n",
    "    print(\"-> Load the training, validation and testing datasets ...\\n\")\n",
    "\n",
    "  coords, energies, species = None, None, None\n",
    "  features, targets = None, None\n",
    "  backup_hdf5 = False\n",
    "  extract_coords = True\n",
    "  build_features = True\n",
    "\n",
    "  # If the HDF5 file is already existed, we try to load dataset and targets from\n",
    "  # the HDF5 file directly if the checksums are equal.\n",
    "  if isfile(HDF5_DATABASE_FILE):\n",
    "    with h5py.File(HDF5_DATABASE_FILE, \"r\") as hdb:\n",
    "      if hdb.attrs.get(\"checksum\", 0) == checksum:\n",
    "        # There are two main groups in this HDF5 file:\n",
    "        # 1. the first group is 'train' where training data and training targets\n",
    "        # are stored.\n",
    "        if \"train\" in hdb:\n",
    "          features = hdb[\"train\"][\"dataset\"][:LOAD_SIZE]\n",
    "          targets = hdb[\"train\"][\"targets\"][:LOAD_SIZE]\n",
    "          build_features = False\n",
    "          extract_coords = False\n",
    "        # 2. the second group is 'unique' where uniquified cartesian coordinates\n",
    "        # and their energies extracted from a CP2K/XYZ file are saved.\n",
    "        elif \"unique\" in hdb.keys():\n",
    "          coords = hdb[\"unique\"][\"coords\"][:]\n",
    "          energies = hdb[\"unique\"][\"energies\"][:]\n",
    "          species = hdb[\"unique\"][\"species\"][:]\n",
    "          extract_coords = False\n",
    "      # The checksum are not equal, so we backup the existed HDF5 databse by\n",
    "      # renaming it.\n",
    "      else:\n",
    "        backup_hdf5 = True\n",
    "    if backup_hdf5:\n",
    "      if verbose:\n",
    "        print(\"MD5 checksums mismatched. Build a new dataset.\\n\")\n",
    "      shutil.move(HDF5_DATABASE_FILE, HDF5_DATABASE_FILE + \".bak\")\n",
    "\n",
    "  # Extract the raw cartesian coordinates and energis (eV) from the CP2K/XYZ\n",
    "  # file and save these data into group 'raw'. All data are compressed with the\n",
    "  # lossless gzip filter.\n",
    "  if extract_coords:\n",
    "    species, energies, coords, _ = extract_xyz(\n",
    "      filename, \n",
    "      verbose=verbose, \n",
    "      **kwargs\n",
    "    )\n",
    "    # Remove the duplicates to reduce the dataset\n",
    "    energies, coords = similarity.remove_duplicates(\n",
    "      coords, \n",
    "      energies, \n",
    "      HDF5_DATABASE_FILE,\n",
    "      threshold=0.985,\n",
    "      verbose=verbose,\n",
    "    )\n",
    "    with h5py.File(HDF5_DATABASE_FILE) as hdb:\n",
    "      # Delete the previous group `unique`. This should not happen, but it may\n",
    "      # be inserted manually for debugging.\n",
    "      group = \"unique\"\n",
    "      if group in hdb.keys():\n",
    "        del hdb[group]\n",
    "      hdb.attrs[\"checksum\"] = checksum\n",
    "      hdb.create_group(group)\n",
    "      hdb[group].create_dataset(\"coords\", data=coords, compression=\"gzip\")\n",
    "      hdb[group].create_dataset(\"energies\", data=energies, compression=\"gzip\")\n",
    "      hdb[group].create_dataset(\"species\", data=species, \n",
    "                                dtype=h5py.special_dtype(vlen=str))\n",
    "  elif verbose:\n",
    "    print(\"Use existed coordinates and energies.\\n\")\n",
    "\n",
    "  # Transform the cartesian coordinates to a 4D dataset. Permute this dataset\n",
    "  # several times and then we split it into three parts: training, validation\n",
    "  # and testing. Save these datasets and their targets into group 'cnn'.\n",
    "  if build_features:\n",
    "    # Allocate the disk space and then write transformed data piece by piece\n",
    "    # because my little computer only has 16GB memory.\n",
    "    if l is None:\n",
    "      l = get_pyykko_bonds_matrix(species, flatten=True)\n",
    "    shape = [len(energies), 1, comb(NUM_SITES, 4, True), comb(4, 2, True)]\n",
    "    hdb = h5py.File(HDF5_DATABASE_FILE)\n",
    "    try:\n",
    "      group = hdb.require_group(\"train\")\n",
    "      group.create_dataset(\"targets\", data=energies)\n",
    "      mapping = group.create_dataset(\n",
    "        \"dataset\", shape=shape, dtype=np.float32)\n",
    "      # Set the chunksize to 10000.\n",
    "      chunksize = 10000\n",
    "      transform_coords(\n",
    "        coords, \n",
    "        chunksize, \n",
    "        mapping, \n",
    "        l=l, \n",
    "        verbose=verbose,\n",
    "        sort=sort,\n",
    "      )\n",
    "    except Exception as excp:\n",
    "      del hdb[\"train\"]\n",
    "      raise excp\n",
    "    finally:\n",
    "      hdb.close()\n",
    "    # After the transformation we now load the whole dataset into memory.\n",
    "    with h5py.File(HDF5_DATABASE_FILE) as hdb:\n",
    "      features = hdb[\"train\"][\"dataset\"][:LOAD_SIZE]\n",
    "      targets = np.array(energies[:LOAD_SIZE], copy=False)\n",
    "    if verbose:\n",
    "      print(\"Dataset size (MB)     : %8.2f\", features.nbytes / 1024 / 1024)\n",
    "      print(\"Targets size (MB)     : %8.2f\", targets.nbytes / 1024 / 1024)\n",
    "      print(\"\")\n",
    "    del coords\n",
    "  elif verbose:\n",
    "    print(\"Use existed features and targets.\\n\")\n",
    "\n",
    "  # Determine the maximum and minimum energy. The energies should be scaled to\n",
    "  # [0, 1] during training.\n",
    "  scaler = MinMaxScaler()\n",
    "  targets = scaler.fit_transform(np.atleast_2d(targets).T)\n",
    "\n",
    "  if verbose:\n",
    "    print(\"-> Datasets and targets are loaded into memories.\")\n",
    "    print(\"\")\n",
    "  return features, targets, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Load the training, validation and testing datasets ...\n",
      "\n",
      "Use existed coordinates and energies.\n",
      "\n",
      "Use existed features and targets.\n",
      "\n",
      "-> Datasets and targets are loaded into memories.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features, targets, scaler = may_build_dataset(XYZ_FILE, xyz_format=XYZ_FORMAT, sort=SORTED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.4 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this section further data preprocessing procedures will be taken. See http://cs231n.github.io/neural-networks-2/#batchnorm for explanantions.\n",
    "\n",
    "Before preprocessing, let's first demonstrate the value distributions of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb273328160>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2AAAAKqCAYAAABclj3CAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0CT/P/Ap5j5Q6sAIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb273389eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(features.flatten()[::1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then, before we start training, the dataset should also be splited. ``sklearn.model_selection.train_test_split`` can be used to achieve this. Remember to set the random state!\n",
    "\n",
    "Some important notes:\n",
    "\n",
    "1. **In practice: ** it is very important to zero-center the data.\n",
    "2. **Common pitfall: ** An important point to make about the preprocessing is that any preprocessing statistics (e.g. the data mean) must only be computed on the training data, and then applied to the validation / test data. E.g. computing the mean and subtracting it from every image across the entire dataset and then splitting the data into train/val/test splits would be a mistake. Instead, the mean must be computed only over the training data and then subtracted equally from all splits (train/val/test).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Split the datatset into training and testing datasets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  features, \n",
    "  targets, \n",
    "  random_state=SEED, \n",
    "  test_size=TEST_SIZE\n",
    ")\n",
    "\n",
    "# Further split the training dataset into training and validation datasets.\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, random_state=SEED, test_size=0.1)\n",
    "\n",
    "# Make the data zero-centered.\n",
    "if ZERO_CENTER:\n",
    "  mean = X_train.mean()\n",
    "  X_train -= mean\n",
    "  X_test -= mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Display the value distributions of ``X_train`` and ``X_test``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Only display these figures if TOTAL_SIZE is not too large!\n",
    "if TOTAL_SIZE < 10000:\n",
    "  fig, axes = plt.subplots(1, 2, figsize=[14, 7])\n",
    "\n",
    "  ax = sns.distplot(X_train.flatten()[::20], ax=axes[0])\n",
    "  ax.set_xlabel(\"Scaled Distance\", fontsize=12)\n",
    "  ax.set_ylabel(\"Relative Numbers\", fontsize=12)\n",
    "  ax.set_title(\"X_train\", fontsize=14)\n",
    "\n",
    "  ax = sns.distplot(X_test.flatten()[::2], ax=axes[1])\n",
    "  ax.set_xlabel(\"Scaled Distance\", fontsize=12)\n",
    "  ax.set_ylabel(\"Relative Numbers\", fontsize=12)\n",
    "  ax.set_title(\"X_test\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fb2782541d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzoAAAG8CAYAAAAWx9BYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDgsdIiIiIiLyOyx0iIiIiIjI7/x/oSncBFI7//cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb2782eebe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=[14, 7])\n",
    "\n",
    "ax = sns.distplot(y_train.flatten(), ax=axes[0])\n",
    "ax.set_xlabel(\"Scaled Energies\", fontsize=12)\n",
    "ax.set_ylabel(\"Relative Numbers\", fontsize=12)\n",
    "ax.set_title(\"y_train\", fontsize=14)\n",
    "\n",
    "ax = sns.distplot(y_test.flatten(), ax=axes[1])\n",
    "ax.set_xlabel(\"Scaled Energies\", fontsize=12)\n",
    "ax.set_ylabel(\"Relative Numbers\", fontsize=12)\n",
    "ax.set_title(\"y_test\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we can start training this network. The training settings defined in this paper are summerized here:\n",
    "\n",
    "* Root-mean-squared loss.\n",
    "* Mini-batch stochastic gradient descent with momentum\n",
    "     * Batch size: 50.\n",
    "     * Momentum factor: 0.7.\n",
    "* Step Decay Function: $s_{i} = s_{0}r/(r + i)$\n",
    "     * $s_0$ is the initial step length, which is 0.1 in this paper.\n",
    "     * r is a predefined factor and r = 60 in this paper.\n",
    "* Regularization factor is not mentioned in this paper.\n",
    "     * $\\lambda$ is set to None.\n",
    "* The exponentia ldecay rate is 0.9 in this implementaion.\n",
    "* The total number of epochs for the reference dataset is 1400.\n",
    "\n",
    "These parameters are declared in the following section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "momentum_factor = 0.7\n",
    "start_learning_rate = 0.1\n",
    "decay_rate = 0.90\n",
    "decay_factor = 60\n",
    "rlambda = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Invoke the interactive session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "sess = tf.InteractiveSession(graph=graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we can begin the training. Initialize a new ``graph`` and use it as the default graph. Then we get the infered network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1/Tanh            : [     -1,      1,   4845,     40]\n",
      "Conv2/Tanh            : [     -1,      1,   4845,     70]\n",
      "Conv3/Tanh            : [     -1,      1,   4845,     60]\n",
      "Conv4/Softplus        : [     -1,      1,   4845,      2]\n",
      "Switch                : [     -1,      1,      2,   4845]\n",
      "Conv5/Softplus        : [     -1,      1,      2,    200]\n",
      "Conv6/Softplus        : [     -1,      1,      2,     20]\n",
      "Flatten/Reshape       : [     -1,     40]\n",
      "Output                : [     -1,      1]\n"
     ]
    }
   ],
   "source": [
    "X_batch = tf.placeholder(TF_TYPE, [None, 1, CNK, CK2], name=\"X_batch\")\n",
    "y_batch = tf.placeholder(TF_TYPE, [None, 1], name=\"y_batch\")\n",
    "keep_prob = tf.placeholder(TF_TYPE, name=\"conv_keep\")\n",
    "dense_keep_prob = tf.placeholder(TF_TYPE, name=\"fc_keep\")\n",
    "\n",
    "# y_pred = models.inference(\n",
    "#   X_batch, \n",
    "#   \"mbe-nn-m-fc\", \n",
    "#   conv_keep_prob=keep_prob, \n",
    "#   dense_keep_prob=dense_keep_prob, \n",
    "#   dropouts=(6,), \n",
    "#   dense_dims=[200, 20], \n",
    "#   dense_funcs=[tf.nn.relu, tf.nn.relu, tf.nn.tanh],\n",
    "#   conv_dims=[40, 70, 60, 2, 400, 200],\n",
    "#   verbose=True\n",
    "# )\n",
    "\n",
    "y_pred = models.inference(\n",
    "  X_batch,\n",
    "  \"mbe-nn-m\",\n",
    "  dims=[40, 70, 60, 2, 200, 20],\n",
    "  verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Output the total number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1/filter/kernel:0      240\n",
      "Conv1/biases/biases:0      40\n",
      "Conv2/filter/kernel:0      2800\n",
      "Conv2/biases/biases:0      70\n",
      "Conv3/filter/kernel:0      4200\n",
      "Conv3/biases/biases:0      60\n",
      "Conv4/filter/kernel:0      120\n",
      "Conv4/biases/biases:0      2\n",
      "Conv5/filter/kernel:0      969000\n",
      "Conv5/biases/biases:0      200\n",
      "Conv6/filter/kernel:0      4000\n",
      "Conv6/biases/biases:0      20\n",
      "\n",
      "Total number of parameters: 980752\n"
     ]
    }
   ],
   "source": [
    "models.get_number_of_trainable_parameters(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Add helper functions to visualize the graph. These codes are copied from the official example **deepdream**. Uncomment the last line can display the network graph inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def strip_consts(graph_def, max_const_size=32):\n",
    "  \"\"\"\n",
    "  Strip large constant values from graph_def.\n",
    "  \"\"\"\n",
    "  strip_def = tf.GraphDef()\n",
    "  for n0 in graph_def.node:\n",
    "    n = strip_def.node.add() \n",
    "    n.MergeFrom(n0)\n",
    "    if n.op == 'Const':\n",
    "      tensor = n.attr['value'].tensor\n",
    "      size = len(tensor.tensor_content)\n",
    "      if size > max_const_size:\n",
    "        tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "  return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "  \"\"\"\n",
    "  Visualize TensorFlow graph.\n",
    "  \"\"\"\n",
    "  if hasattr(graph_def, 'as_graph_def'):\n",
    "    graph_def = graph_def.as_graph_def()\n",
    "  strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "  code = \"\"\"\n",
    "    <script src=\"//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js\"></script>\n",
    "    <script>\n",
    "      function load() {{\n",
    "        document.getElementById(\"{id}\").pbtxt = {data};\n",
    "      }}\n",
    "    </script>\n",
    "    <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "    <div style=\"height:600px\">\n",
    "    <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "    </div>\n",
    "  \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "  iframe = \"\"\"\n",
    "    <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "  \"\"\".format(code.replace('\"', '&quot;'))\n",
    "  display(HTML(iframe))\n",
    "\n",
    "# Uncomment the following line to visualize the graph in this notebook. \n",
    "# show_graph(graph.as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Setup the total loss. Add L2 loss for the weights of dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "  \n",
    "  regvars = tf.get_collection(GraphKeys.REGULARIZATION_LOSSES)\n",
    "  if len(regvars) > 0 and rlambda is not None:\n",
    "    def _nameit(_w):\n",
    "      return \"%sL2\" % _w.name.split(\"/\")[0]\n",
    "\n",
    "    with tf.name_scope(\"regularizer\"):\n",
    "      l2 = tf.multiply(\n",
    "        tf.add_n([tf.nn.l2_loss(w, name=_nameit(w)) for w in regvars]), rlambda)\n",
    "      tf.summary.scalar(\"l2\", l2)\n",
    "  else:\n",
    "    l2 = tf.constant(0.0)\n",
    "\n",
    "  rms = tf.sqrt(tf.losses.mean_squared_error(y_pred, y_batch, scope=\"RMSE\"))\n",
    "  loss = tf.add(rms, l2)\n",
    "  tf.summary.scalar(\"rms\", rms)\n",
    "  tf.summary.scalar(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Setup the learning rate decay and the momentum optimizer. The learning rate $r_i$ computed by the inverse decay function in this paper is implemented here:\n",
    "$$r_{i} = \\frac{rs_{0}}{r + i}$$\n",
    "where $i$ is the epoch, $r$ is a constant decay factor and $s_0$ is the initial learning rate. $r$ is set to 60 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def inverse_decay(init_learning_rate, epoch, decay_factor, name=None):\n",
    "  \"\"\"\n",
    "  The inverse decay function.\n",
    "  \n",
    "  Args:\n",
    "    init_learning_rate: A scalar `float32` or `float64` `Tensor` or a\n",
    "      Python number.  The initial learning rate.\n",
    "    global_epoch: A scalar `int32` or `int64` `Tensor` or a Python number.\n",
    "      Global epoch to use for the decay computation.  Must not be negative.\n",
    "    decay_factor: A scalar `int32` or `int64` `Tensor` or a Python number.\n",
    "      Must be positive.  See the decay equation above.\n",
    "    name: String.  Optional name of the operation.  Defaults to\n",
    "      'ExponentialDecay'.\n",
    "\n",
    "  Returns:\n",
    "    A scalar `Tensor` of the same type as `learning_rate`.  The decayed\n",
    "    learning rate.\n",
    "  \n",
    "  \"\"\"\n",
    "  if epoch is None:\n",
    "    raise ValueError(\"global_step is required for inv_decay.\")\n",
    "  with ops.name_scope(name, \"InvDecay\", \n",
    "                      [init_learning_rate, epoch, decay_factor]) as name:\n",
    "    init_learning_rate = ops.convert_to_tensor(\n",
    "      init_learning_rate, name=\"init_learning_rate\")\n",
    "    dtype = init_learning_rate.dtype\n",
    "    epoch = math_ops.cast(epoch, dtype)\n",
    "    decay_factor = math_ops.cast(decay_factor, dtype)\n",
    "    top = math_ops.multiply(init_learning_rate, decay_factor)\n",
    "    return math_ops.div(top, math_ops.add(epoch, decay_factor), name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "global_epoch = tf.Variable(0, dtype=tf.int64, trainable=False, name=\"global_epoch\")\n",
    "global_epoch_op = tf.assign(global_epoch, global_epoch + 1)\n",
    "global_step = tf.Variable(0, dtype=tf.int64, trainable=False, name=\"global_step\")\n",
    "learning_rate = inverse_decay(start_learning_rate, global_epoch, decay_factor)\n",
    "\n",
    "# learning_rate = tf.train.exponential_decay(\n",
    "#   start_learning_rate, \n",
    "#   tf.multiply(global_step, batch_size), \n",
    "#   len(X_train), \n",
    "#   decay_rate,\n",
    "#   staircase=True,\n",
    "# )\n",
    "\n",
    "# Test the Adagrad optimzier\n",
    "# trainer = tf.train.AdagradDAOptimizer(\n",
    "#   learning_rate=learning_rate, \n",
    "#   initial_gradient_squared_accumulator_value=0.1,\n",
    "#   global_step=batch\n",
    "# )\n",
    "\n",
    "tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "\n",
    "# Use the Momentum optimizer\n",
    "trainer = tf.train.MomentumOptimizer(learning_rate, momentum_factor)\n",
    "optimizer = trainer.minimize(loss, global_step=global_step)\n",
    "\n",
    "# Merge all the summaries.\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(logdir=\"./events/%s\" % get_time_id(), graph=graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.2 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "These variabls control the key settings of a training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "log_frequency = 50\n",
    "eval_frequency = 400\n",
    "save_frequency = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Setup the saving path and the checkpoint file so that we can reuse this model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "save_path = \"./saves\"\n",
    "chk_file = join(save_path, \"c9h7n.ckpt\")\n",
    "if not isdir(save_path):\n",
    "  makedirs(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Small utility function to evaluate a dataset by feeding batches of data to ``{eval_dataset}`` and pulling the results \n",
    "from ``{eval_values}``. Saves memory and enables this to run on smaller GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def eval_in_batches(data, eval_size=50):\n",
    "  \"\"\" Get all predictions for a dataset by running it in small batches. \"\"\"\n",
    "  size = data.shape[0]\n",
    "  if size < eval_size:\n",
    "    raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\n",
    "  eval_values = np.ndarray(shape=(size, 1), dtype=NP_TYPE)\n",
    "  for i, inext in brange(0, size, eval_size):\n",
    "    eval_values[i: inext] = sess.run(\n",
    "      y_pred,\n",
    "      feed_dict={X_batch: data[i: inext, ...], keep_prob: 1.0, dense_keep_prob: 1.0})\n",
    "  return eval_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This helper function is used to restore the latest checkpoint if existed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def restore_latest_from_ckpt(save_dir):\n",
    "  \"\"\"\n",
    "  Restore the latest checkpoint from the 'scratch' if possible.\n",
    "  \"\"\"\n",
    "  if not isdir(save_dir):\n",
    "    return None\n",
    "  ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "  if ckpt and ckpt.model_checkpoint_path:\n",
    "    return ckpt.model_checkpoint_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally we invoke a new session and start this training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\n",
      "\n",
      "Training Samples      : 43034\n",
      "Batch Size            : 50\n",
      "Number of Epochs      : 500\n",
      "Log Frequency         : 50\n",
      "Eval Frequency        : 400\n",
      "\n",
      "Step      0 (epoch    0.00), loss: 0.171963, time:  0.393 s\n",
      "\n",
      "Time since beginning  : 0.394 s\n",
      "\n",
      "Step     50 (epoch    0.06), loss: 0.117599, time:  5.567 s\n",
      "Step    100 (epoch    0.12), loss: 0.102275, time:  5.405 s\n",
      "Step    150 (epoch    0.17), loss: 0.104771, time:  5.420 s\n",
      "Step    200 (epoch    0.23), loss: 0.096757, time:  5.450 s\n",
      "Step    250 (epoch    0.29), loss: 0.098901, time:  5.424 s\n",
      "Step    300 (epoch    0.35), loss: 0.096377, time:  5.435 s\n",
      "Step    350 (epoch    0.41), loss: 0.103996, time:  5.408 s\n",
      "Step    400 (epoch    0.46), loss: 0.084656, time:  5.424 s, error: 0.102301\n",
      "Step    450 (epoch    0.52), loss: 0.110176, time:  8.634 s\n",
      "Step    500 (epoch    0.58), loss: 0.098678, time:  5.414 s\n",
      "Step    550 (epoch    0.64), loss: 0.106657, time:  5.443 s\n",
      "Step    600 (epoch    0.70), loss: 0.096576, time:  5.420 s\n",
      "Step    650 (epoch    0.76), loss: 0.100514, time:  5.435 s\n",
      "Step    700 (epoch    0.81), loss: 0.113191, time:  5.420 s\n",
      "Step    750 (epoch    0.87), loss: 0.105866, time:  5.412 s\n",
      "Step    800 (epoch    0.93), loss: 0.096624, time:  5.423 s, error: 0.101126\n",
      "Step    850 (epoch    0.99), loss: 0.097856, time:  8.605 s\n",
      "Step    900 (epoch    1.05), loss: 0.091933, time:  5.456 s\n",
      "Step    950 (epoch    1.10), loss: 0.095042, time:  5.436 s\n",
      "Step   1000 (epoch    1.16), loss: 0.109236, time:  5.445 s\n",
      "Step   1050 (epoch    1.22), loss: 0.105485, time:  5.437 s\n",
      "Step   1100 (epoch    1.28), loss: 0.091145, time:  5.441 s\n",
      "Step   1150 (epoch    1.34), loss: 0.081864, time:  5.414 s\n",
      "Step   1200 (epoch    1.39), loss: 0.095701, time:  5.430 s, error: 0.095377\n",
      "Step   1250 (epoch    1.45), loss: 0.091425, time:  8.628 s\n",
      "Step   1300 (epoch    1.51), loss: 0.090179, time:  5.434 s\n",
      "Step   1350 (epoch    1.57), loss: 0.088985, time:  5.436 s\n",
      "Step   1400 (epoch    1.63), loss: 0.085296, time:  5.431 s\n",
      "Step   1450 (epoch    1.68), loss: 0.107697, time:  5.427 s\n",
      "Step   1500 (epoch    1.74), loss: 0.082532, time:  5.428 s\n",
      "Step   1550 (epoch    1.80), loss: 0.088872, time:  5.434 s\n",
      "Step   1600 (epoch    1.86), loss: 0.090230, time:  5.423 s, error: 0.084589\n",
      "Step   1650 (epoch    1.92), loss: 0.084257, time:  8.692 s\n",
      "Step   1700 (epoch    1.98), loss: 0.083543, time:  5.428 s\n",
      "Step   1750 (epoch    2.03), loss: 0.082191, time:  5.432 s\n",
      "Step   1800 (epoch    2.09), loss: 0.100821, time:  5.432 s\n",
      "Step   1850 (epoch    2.15), loss: 0.120395, time:  5.422 s\n",
      "Step   1900 (epoch    2.21), loss: 0.094851, time:  5.432 s\n",
      "Step   1950 (epoch    2.27), loss: 0.065885, time:  5.427 s\n",
      "Step   2000 (epoch    2.32), loss: 0.107752, time:  5.456 s, error: 0.104290\n",
      "\n",
      "Time since beginning  : 233.760 s\n",
      "\n",
      "Step   2050 (epoch    2.38), loss: 0.101407, time:  8.739 s\n",
      "Step   2100 (epoch    2.44), loss: 0.102748, time:  5.429 s\n",
      "Step   2150 (epoch    2.50), loss: 0.120052, time:  5.452 s\n",
      "Step   2200 (epoch    2.56), loss: 0.106022, time:  5.443 s\n",
      "Step   2250 (epoch    2.61), loss: 0.083711, time:  5.454 s\n",
      "Step   2300 (epoch    2.67), loss: 0.085881, time:  5.438 s\n",
      "Step   2350 (epoch    2.73), loss: 0.088961, time:  5.456 s\n",
      "Step   2400 (epoch    2.79), loss: 0.081953, time:  5.440 s, error: 0.101017\n",
      "Step   2450 (epoch    2.85), loss: 0.066669, time:  8.638 s\n",
      "Step   2500 (epoch    2.90), loss: 0.094270, time:  5.429 s\n",
      "Step   2550 (epoch    2.96), loss: 0.095160, time:  5.451 s\n",
      "Step   2600 (epoch    3.02), loss: 0.078925, time:  5.428 s\n",
      "Step   2650 (epoch    3.08), loss: 0.062827, time:  5.436 s\n",
      "Step   2700 (epoch    3.14), loss: 0.093074, time:  5.442 s\n",
      "Step   2750 (epoch    3.20), loss: 0.088627, time:  5.441 s\n",
      "Step   2800 (epoch    3.25), loss: 0.099633, time:  5.441 s, error: 0.096796\n",
      "Step   2850 (epoch    3.31), loss: 0.064643, time:  8.665 s\n",
      "Step   2900 (epoch    3.37), loss: 0.082913, time:  5.414 s\n",
      "Step   2950 (epoch    3.43), loss: 0.078033, time:  5.429 s\n",
      "Step   3000 (epoch    3.49), loss: 0.079085, time:  5.440 s\n",
      "Step   3050 (epoch    3.54), loss: 0.089716, time:  5.456 s\n",
      "Step   3100 (epoch    3.60), loss: 0.078218, time:  5.449 s\n",
      "Step   3150 (epoch    3.66), loss: 0.062736, time:  5.440 s\n",
      "Step   3200 (epoch    3.72), loss: 0.075625, time:  5.448 s, error: 0.081832\n",
      "Step   3250 (epoch    3.78), loss: 0.083767, time:  8.667 s\n",
      "Step   3300 (epoch    3.83), loss: 0.125726, time:  5.451 s\n",
      "Step   3350 (epoch    3.89), loss: 0.071843, time:  5.436 s\n",
      "Step   3400 (epoch    3.95), loss: 0.056610, time:  5.444 s\n",
      "Step   3450 (epoch    4.01), loss: 0.062450, time:  5.445 s\n",
      "Step   3500 (epoch    4.07), loss: 0.067455, time:  5.415 s\n",
      "Step   3550 (epoch    4.12), loss: 0.088374, time:  5.426 s\n",
      "Step   3600 (epoch    4.18), loss: 0.063022, time:  5.420 s, error: 0.070360\n",
      "Step   3650 (epoch    4.24), loss: 0.054387, time:  8.629 s\n",
      "Step   3700 (epoch    4.30), loss: 0.056157, time:  5.442 s\n",
      "Step   3750 (epoch    4.36), loss: 0.062051, time:  5.426 s\n",
      "Step   3800 (epoch    4.42), loss: 0.054487, time:  5.458 s\n",
      "Step   3850 (epoch    4.47), loss: 0.056727, time:  5.442 s\n",
      "Step   3900 (epoch    4.53), loss: 0.053789, time:  5.456 s\n",
      "Step   3950 (epoch    4.59), loss: 0.049348, time:  5.454 s\n",
      "Step   4000 (epoch    4.65), loss: 0.070395, time:  5.447 s, error: 0.084440\n",
      "\n",
      "Time since beginning  : 467.505 s\n",
      "\n",
      "Step   4050 (epoch    4.71), loss: 0.054095, time:  8.707 s\n",
      "Step   4100 (epoch    4.76), loss: 0.066206, time:  5.431 s\n",
      "Step   4150 (epoch    4.82), loss: 0.062873, time:  5.443 s\n",
      "Step   4200 (epoch    4.88), loss: 0.052144, time:  5.431 s\n",
      "Step   4250 (epoch    4.94), loss: 0.044224, time:  5.437 s\n",
      "Step   4300 (epoch    5.00), loss: 0.078372, time:  5.457 s\n",
      "Step   4350 (epoch    5.05), loss: 0.080033, time:  5.450 s\n",
      "Step   4400 (epoch    5.11), loss: 0.062670, time:  5.430 s, error: 0.058654\n",
      "Step   4450 (epoch    5.17), loss: 0.071988, time:  8.649 s\n",
      "Step   4500 (epoch    5.23), loss: 0.057350, time:  5.461 s\n",
      "Step   4550 (epoch    5.29), loss: 0.052536, time:  5.426 s\n",
      "Step   4600 (epoch    5.34), loss: 0.044691, time:  5.440 s\n",
      "Step   4650 (epoch    5.40), loss: 0.042177, time:  5.443 s\n",
      "Step   4700 (epoch    5.46), loss: 0.048379, time:  5.436 s\n",
      "Step   4750 (epoch    5.52), loss: 0.057738, time:  5.449 s\n",
      "Step   4800 (epoch    5.58), loss: 0.045078, time:  5.453 s, error: 0.052368\n",
      "Step   4850 (epoch    5.64), loss: 0.059159, time:  8.684 s\n",
      "Step   4900 (epoch    5.69), loss: 0.061134, time:  5.437 s\n",
      "Step   4950 (epoch    5.75), loss: 0.064240, time:  5.450 s\n",
      "Step   5000 (epoch    5.81), loss: 0.067759, time:  5.437 s\n",
      "Step   5050 (epoch    5.87), loss: 0.064139, time:  5.436 s\n",
      "Step   5100 (epoch    5.93), loss: 0.072265, time:  5.436 s\n",
      "Step   5150 (epoch    5.98), loss: 0.060695, time:  5.458 s\n",
      "Step   5200 (epoch    6.04), loss: 0.059700, time:  5.457 s, error: 0.070547\n",
      "Step   5250 (epoch    6.10), loss: 0.046455, time:  8.727 s\n",
      "Step   5300 (epoch    6.16), loss: 0.045776, time:  5.449 s\n",
      "Step   5350 (epoch    6.22), loss: 0.045096, time:  5.452 s\n",
      "Step   5400 (epoch    6.27), loss: 0.044010, time:  5.433 s\n",
      "Step   5450 (epoch    6.33), loss: 0.060425, time:  5.441 s\n",
      "Step   5500 (epoch    6.39), loss: 0.048643, time:  5.451 s\n",
      "Step   5550 (epoch    6.45), loss: 0.052802, time:  5.448 s\n",
      "Step   5600 (epoch    6.51), loss: 0.041591, time:  5.444 s, error: 0.043992\n",
      "Step   5650 (epoch    6.56), loss: 0.030909, time:  8.642 s\n",
      "Step   5700 (epoch    6.62), loss: 0.044615, time:  5.439 s\n",
      "Step   5750 (epoch    6.68), loss: 0.055491, time:  5.446 s\n",
      "Step   5800 (epoch    6.74), loss: 0.048147, time:  5.427 s\n",
      "Step   5850 (epoch    6.80), loss: 0.041569, time:  5.449 s\n",
      "Step   5900 (epoch    6.86), loss: 0.044368, time:  5.435 s\n",
      "Step   5950 (epoch    6.91), loss: 0.030029, time:  5.455 s\n",
      "Step   6000 (epoch    6.97), loss: 0.026996, time:  5.447 s, error: 0.042688\n",
      "\n",
      "Time since beginning  : 701.445 s\n",
      "\n",
      "Step   6050 (epoch    7.03), loss: 0.061883, time:  8.726 s\n",
      "Step   6100 (epoch    7.09), loss: 0.053657, time:  5.434 s\n",
      "Step   6150 (epoch    7.15), loss: 0.032157, time:  5.432 s\n",
      "Step   6200 (epoch    7.20), loss: 0.038507, time:  5.439 s\n",
      "Step   6250 (epoch    7.26), loss: 0.032822, time:  5.443 s\n",
      "Step   6300 (epoch    7.32), loss: 0.026115, time:  5.450 s\n",
      "Step   6350 (epoch    7.38), loss: 0.029019, time:  5.442 s\n",
      "Step   6400 (epoch    7.44), loss: 0.027774, time:  5.430 s, error: 0.038323\n",
      "Step   6450 (epoch    7.49), loss: 0.039587, time:  8.643 s\n",
      "Step   6500 (epoch    7.55), loss: 0.027911, time:  5.431 s\n",
      "Step   6550 (epoch    7.61), loss: 0.036332, time:  5.443 s\n",
      "Step   6600 (epoch    7.67), loss: 0.032431, time:  5.430 s\n",
      "Step   6650 (epoch    7.73), loss: 0.043981, time:  5.430 s\n",
      "Step   6700 (epoch    7.78), loss: 0.029944, time:  5.449 s\n",
      "Step   6750 (epoch    7.84), loss: 0.041357, time:  5.445 s\n",
      "Step   6800 (epoch    7.90), loss: 0.036655, time:  5.435 s, error: 0.036278\n",
      "Step   6850 (epoch    7.96), loss: 0.036688, time:  8.667 s\n",
      "Step   6900 (epoch    8.02), loss: 0.031721, time:  5.447 s\n",
      "Step   6950 (epoch    8.08), loss: 0.035800, time:  5.436 s\n",
      "Step   7000 (epoch    8.13), loss: 0.027809, time:  5.440 s\n",
      "Step   7050 (epoch    8.19), loss: 0.037022, time:  5.469 s\n",
      "Step   7100 (epoch    8.25), loss: 0.035354, time:  5.449 s\n",
      "Step   7150 (epoch    8.31), loss: 0.030215, time:  5.434 s\n",
      "Step   7200 (epoch    8.37), loss: 0.038445, time:  5.442 s, error: 0.037634\n",
      "Step   7250 (epoch    8.42), loss: 0.027112, time:  8.632 s\n",
      "Step   7300 (epoch    8.48), loss: 0.028476, time:  5.434 s\n",
      "Step   7350 (epoch    8.54), loss: 0.035470, time:  5.437 s\n",
      "Step   7400 (epoch    8.60), loss: 0.047350, time:  5.461 s\n",
      "Step   7450 (epoch    8.66), loss: 0.025615, time:  5.442 s\n",
      "Step   7500 (epoch    8.71), loss: 0.033224, time:  5.430 s\n",
      "Step   7550 (epoch    8.77), loss: 0.035622, time:  5.436 s\n",
      "Step   7600 (epoch    8.83), loss: 0.026959, time:  5.425 s, error: 0.033769\n",
      "Step   7650 (epoch    8.89), loss: 0.026834, time:  8.636 s\n",
      "Step   7700 (epoch    8.95), loss: 0.028399, time:  5.436 s\n",
      "Step   7750 (epoch    9.00), loss: 0.026785, time:  5.449 s\n",
      "Step   7800 (epoch    9.06), loss: 0.026173, time:  5.439 s\n",
      "Step   7850 (epoch    9.12), loss: 0.021673, time:  5.444 s\n",
      "Step   7900 (epoch    9.18), loss: 0.032880, time:  5.431 s\n",
      "Step   7950 (epoch    9.24), loss: 0.025096, time:  5.429 s\n",
      "Step   8000 (epoch    9.29), loss: 0.030819, time:  5.427 s, error: 0.032579\n",
      "\n",
      "Time since beginning  : 935.101 s\n",
      "\n",
      "Step   8050 (epoch    9.35), loss: 0.031995, time:  8.720 s\n",
      "Step   8100 (epoch    9.41), loss: 0.020851, time:  5.456 s\n",
      "Step   8150 (epoch    9.47), loss: 0.020178, time:  5.434 s\n",
      "Step   8200 (epoch    9.53), loss: 0.034116, time:  5.433 s\n",
      "Step   8250 (epoch    9.59), loss: 0.025774, time:  5.440 s\n",
      "Step   8300 (epoch    9.64), loss: 0.023882, time:  5.432 s\n",
      "Step   8350 (epoch    9.70), loss: 0.023892, time:  5.433 s\n",
      "Step   8400 (epoch    9.76), loss: 0.029950, time:  5.445 s, error: 0.032767\n",
      "Step   8450 (epoch    9.82), loss: 0.029384, time:  8.653 s\n",
      "Step   8500 (epoch    9.88), loss: 0.027573, time:  5.455 s\n",
      "Step   8550 (epoch    9.93), loss: 0.027118, time:  5.432 s\n",
      "Step   8600 (epoch    9.99), loss: 0.026098, time:  5.439 s\n",
      "Step   8650 (epoch   10.05), loss: 0.022438, time:  5.448 s\n",
      "Step   8700 (epoch   10.11), loss: 0.024401, time:  5.441 s\n",
      "Step   8750 (epoch   10.17), loss: 0.019782, time:  5.429 s\n",
      "Step   8800 (epoch   10.22), loss: 0.024721, time:  5.448 s, error: 0.036961\n",
      "Step   8850 (epoch   10.28), loss: 0.031168, time:  8.718 s\n",
      "Step   8900 (epoch   10.34), loss: 0.018148, time:  5.438 s\n",
      "Step   8950 (epoch   10.40), loss: 0.026581, time:  5.445 s\n",
      "Step   9000 (epoch   10.46), loss: 0.019469, time:  5.444 s\n",
      "Step   9050 (epoch   10.51), loss: 0.018875, time:  5.443 s\n",
      "Step   9100 (epoch   10.57), loss: 0.020145, time:  5.431 s\n",
      "Step   9150 (epoch   10.63), loss: 0.016490, time:  5.444 s\n",
      "Step   9200 (epoch   10.69), loss: 0.022515, time:  5.441 s, error: 0.029298\n",
      "Step   9250 (epoch   10.75), loss: 0.018238, time:  8.629 s\n",
      "Step   9300 (epoch   10.81), loss: 0.023846, time:  5.438 s\n",
      "Step   9350 (epoch   10.86), loss: 0.023047, time:  5.451 s\n",
      "Step   9400 (epoch   10.92), loss: 0.024991, time:  5.440 s\n",
      "Step   9450 (epoch   10.98), loss: 0.015902, time:  5.441 s\n",
      "Step   9500 (epoch   11.04), loss: 0.030264, time:  5.436 s\n",
      "Step   9550 (epoch   11.10), loss: 0.021444, time:  5.457 s\n",
      "Step   9600 (epoch   11.15), loss: 0.023247, time:  5.434 s, error: 0.032292\n",
      "Step   9650 (epoch   11.21), loss: 0.018802, time:  8.634 s\n",
      "Step   9700 (epoch   11.27), loss: 0.019136, time:  5.435 s\n",
      "Step   9750 (epoch   11.33), loss: 0.017789, time:  5.448 s\n",
      "Step   9800 (epoch   11.39), loss: 0.023418, time:  5.441 s\n",
      "Step   9850 (epoch   11.44), loss: 0.018750, time:  5.444 s\n",
      "Step   9900 (epoch   11.50), loss: 0.018659, time:  5.443 s\n",
      "Step   9950 (epoch   11.56), loss: 0.021596, time:  5.447 s\n",
      "Step  10000 (epoch   11.62), loss: 0.021458, time:  5.439 s, error: 0.036532\n",
      "\n",
      "Time since beginning  : 1168.904 s\n",
      "\n",
      "Step  10050 (epoch   11.68), loss: 0.019762, time:  8.717 s\n",
      "Step  10100 (epoch   11.73), loss: 0.020505, time:  5.436 s\n",
      "Step  10150 (epoch   11.79), loss: 0.018366, time:  5.430 s\n",
      "Step  10200 (epoch   11.85), loss: 0.021890, time:  5.447 s\n",
      "Step  10250 (epoch   11.91), loss: 0.018019, time:  5.450 s\n",
      "Step  10300 (epoch   11.97), loss: 0.020406, time:  5.448 s\n",
      "Step  10350 (epoch   12.03), loss: 0.015109, time:  5.444 s\n",
      "Step  10400 (epoch   12.08), loss: 0.017490, time:  5.434 s, error: 0.027029\n",
      "Step  10450 (epoch   12.14), loss: 0.017785, time:  8.651 s\n",
      "Step  10500 (epoch   12.20), loss: 0.016831, time:  5.440 s\n",
      "Step  10550 (epoch   12.26), loss: 0.017247, time:  5.439 s\n",
      "Step  10600 (epoch   12.32), loss: 0.017882, time:  5.440 s\n",
      "Step  10650 (epoch   12.37), loss: 0.019297, time:  5.457 s\n",
      "Step  10700 (epoch   12.43), loss: 0.017950, time:  5.446 s\n",
      "Step  10750 (epoch   12.49), loss: 0.019694, time:  5.435 s\n",
      "Step  10800 (epoch   12.55), loss: 0.017981, time:  5.442 s, error: 0.027276\n",
      "Step  10850 (epoch   12.61), loss: 0.018223, time:  8.627 s\n",
      "Step  10900 (epoch   12.66), loss: 0.023829, time:  5.440 s\n",
      "Step  10950 (epoch   12.72), loss: 0.016307, time:  5.430 s\n",
      "Step  11000 (epoch   12.78), loss: 0.034823, time:  5.454 s\n",
      "Step  11050 (epoch   12.84), loss: 0.021821, time:  5.437 s\n",
      "Step  11100 (epoch   12.90), loss: 0.015633, time:  5.444 s\n",
      "Step  11150 (epoch   12.95), loss: 0.012271, time:  5.434 s\n",
      "Step  11200 (epoch   13.01), loss: 0.017552, time:  5.439 s, error: 0.027253\n",
      "Step  11250 (epoch   13.07), loss: 0.016113, time:  8.626 s\n",
      "Step  11300 (epoch   13.13), loss: 0.024576, time:  5.430 s\n",
      "Step  11350 (epoch   13.19), loss: 0.022751, time:  5.454 s\n",
      "Step  11400 (epoch   13.25), loss: 0.018601, time:  5.438 s\n",
      "Step  11450 (epoch   13.30), loss: 0.013909, time:  5.436 s\n",
      "Step  11500 (epoch   13.36), loss: 0.017288, time:  5.424 s\n",
      "Step  11550 (epoch   13.42), loss: 0.015080, time:  5.445 s\n",
      "Step  11600 (epoch   13.48), loss: 0.020100, time:  5.436 s, error: 0.025285\n",
      "Step  11650 (epoch   13.54), loss: 0.018561, time:  8.633 s\n",
      "Step  11700 (epoch   13.59), loss: 0.015131, time:  5.457 s\n",
      "Step  11750 (epoch   13.65), loss: 0.015470, time:  5.436 s\n",
      "Step  11800 (epoch   13.71), loss: 0.019650, time:  5.430 s\n",
      "Step  11850 (epoch   13.77), loss: 0.019155, time:  5.428 s\n",
      "Step  11900 (epoch   13.83), loss: 0.014224, time:  5.450 s\n",
      "Step  11950 (epoch   13.88), loss: 0.025253, time:  5.443 s\n",
      "Step  12000 (epoch   13.94), loss: 0.019928, time:  5.450 s, error: 0.027619\n",
      "\n",
      "Time since beginning  : 1402.584 s\n",
      "\n",
      "Step  12050 (epoch   14.00), loss: 0.016917, time:  8.739 s\n",
      "Step  12100 (epoch   14.06), loss: 0.014792, time:  5.437 s\n",
      "Step  12150 (epoch   14.12), loss: 0.015744, time:  5.424 s\n",
      "Step  12200 (epoch   14.17), loss: 0.012546, time:  5.439 s\n",
      "Step  12250 (epoch   14.23), loss: 0.014835, time:  5.436 s\n",
      "Step  12300 (epoch   14.29), loss: 0.018929, time:  5.436 s\n",
      "Step  12350 (epoch   14.35), loss: 0.017743, time:  5.442 s\n",
      "Step  12400 (epoch   14.41), loss: 0.011092, time:  5.430 s, error: 0.025077\n",
      "Step  12450 (epoch   14.47), loss: 0.026266, time:  8.705 s\n",
      "Step  12500 (epoch   14.52), loss: 0.014335, time:  5.439 s\n",
      "Step  12550 (epoch   14.58), loss: 0.013251, time:  5.448 s\n",
      "Step  12600 (epoch   14.64), loss: 0.019937, time:  5.451 s\n",
      "Step  12650 (epoch   14.70), loss: 0.017842, time:  5.470 s\n",
      "Step  12700 (epoch   14.76), loss: 0.014675, time:  5.471 s\n",
      "Step  12750 (epoch   14.81), loss: 0.017076, time:  5.467 s\n",
      "Step  12800 (epoch   14.87), loss: 0.017609, time:  5.470 s, error: 0.028517\n",
      "Step  12850 (epoch   14.93), loss: 0.015777, time:  8.686 s\n",
      "Step  12900 (epoch   14.99), loss: 0.018389, time:  5.460 s\n",
      "Step  12950 (epoch   15.05), loss: 0.010149, time:  5.434 s\n",
      "Step  13000 (epoch   15.10), loss: 0.015672, time:  5.438 s\n",
      "Step  13050 (epoch   15.16), loss: 0.034997, time:  5.441 s\n",
      "Step  13100 (epoch   15.22), loss: 0.015674, time:  5.438 s\n",
      "Step  13150 (epoch   15.28), loss: 0.020400, time:  5.448 s\n",
      "Step  13200 (epoch   15.34), loss: 0.012443, time:  5.447 s, error: 0.024667\n",
      "Step  13250 (epoch   15.39), loss: 0.017792, time:  8.682 s\n",
      "Step  13300 (epoch   15.45), loss: 0.017153, time:  5.470 s\n",
      "Step  13350 (epoch   15.51), loss: 0.014581, time:  5.451 s\n",
      "Step  13400 (epoch   15.57), loss: 0.012713, time:  5.432 s\n",
      "Step  13450 (epoch   15.63), loss: 0.013102, time:  5.433 s\n",
      "Step  13500 (epoch   15.69), loss: 0.013344, time:  5.427 s\n",
      "Step  13550 (epoch   15.74), loss: 0.012865, time:  5.441 s\n",
      "Step  13600 (epoch   15.80), loss: 0.012739, time:  5.440 s, error: 0.025042\n",
      "Step  13650 (epoch   15.86), loss: 0.017064, time:  8.647 s\n",
      "Step  13700 (epoch   15.92), loss: 0.022599, time:  5.445 s\n",
      "Step  13750 (epoch   15.98), loss: 0.020453, time:  5.449 s\n",
      "Step  13800 (epoch   16.03), loss: 0.014828, time:  5.450 s\n",
      "Step  13850 (epoch   16.09), loss: 0.015378, time:  5.448 s\n",
      "Step  13900 (epoch   16.15), loss: 0.011381, time:  5.445 s\n",
      "Step  13950 (epoch   16.21), loss: 0.017514, time:  5.427 s\n",
      "Step  14000 (epoch   16.27), loss: 0.017559, time:  5.426 s, error: 0.024880\n",
      "\n",
      "Time since beginning  : 1636.578 s\n",
      "\n",
      "Step  14050 (epoch   16.32), loss: 0.015691, time:  8.687 s\n",
      "Step  14100 (epoch   16.38), loss: 0.011435, time:  5.443 s\n",
      "Step  14150 (epoch   16.44), loss: 0.015703, time:  5.453 s\n",
      "Step  14200 (epoch   16.50), loss: 0.012246, time:  5.425 s\n",
      "Step  14250 (epoch   16.56), loss: 0.012968, time:  5.435 s\n",
      "Step  14300 (epoch   16.61), loss: 0.012841, time:  5.446 s\n",
      "Step  14350 (epoch   16.67), loss: 0.015408, time:  5.447 s\n",
      "Step  14400 (epoch   16.73), loss: 0.014911, time:  5.433 s, error: 0.026278\n",
      "Step  14450 (epoch   16.79), loss: 0.012090, time:  8.649 s\n",
      "Step  14500 (epoch   16.85), loss: 0.011205, time:  5.432 s\n",
      "Step  14550 (epoch   16.91), loss: 0.016712, time:  5.440 s\n",
      "Step  14600 (epoch   16.96), loss: 0.011174, time:  5.434 s\n",
      "Step  14650 (epoch   17.02), loss: 0.015081, time:  5.445 s\n",
      "Step  14700 (epoch   17.08), loss: 0.016612, time:  5.431 s\n",
      "Step  14750 (epoch   17.14), loss: 0.014131, time:  5.428 s\n",
      "Step  14800 (epoch   17.20), loss: 0.011105, time:  5.428 s, error: 0.023210\n",
      "Step  14850 (epoch   17.25), loss: 0.013582, time:  8.626 s\n",
      "Step  14900 (epoch   17.31), loss: 0.017442, time:  5.437 s\n",
      "Step  14950 (epoch   17.37), loss: 0.034798, time:  5.444 s\n",
      "Step  15000 (epoch   17.43), loss: 0.016883, time:  5.442 s\n",
      "Step  15050 (epoch   17.49), loss: 0.014646, time:  5.440 s\n",
      "Step  15100 (epoch   17.54), loss: 0.028010, time:  5.440 s\n",
      "Step  15150 (epoch   17.60), loss: 0.014246, time:  5.440 s\n",
      "Step  15200 (epoch   17.66), loss: 0.015300, time:  5.445 s, error: 0.022951\n",
      "Step  15250 (epoch   17.72), loss: 0.018340, time:  8.611 s\n",
      "Step  15300 (epoch   17.78), loss: 0.012966, time:  5.430 s\n",
      "Step  15350 (epoch   17.83), loss: 0.011932, time:  5.449 s\n",
      "Step  15400 (epoch   17.89), loss: 0.010511, time:  5.423 s\n",
      "Step  15450 (epoch   17.95), loss: 0.011918, time:  5.425 s\n",
      "Step  15500 (epoch   18.01), loss: 0.012669, time:  5.442 s\n",
      "Step  15550 (epoch   18.07), loss: 0.010003, time:  5.431 s\n",
      "Step  15600 (epoch   18.13), loss: 0.020020, time:  5.443 s, error: 0.022995\n",
      "Step  15650 (epoch   18.18), loss: 0.010469, time:  8.671 s\n",
      "Step  15700 (epoch   18.24), loss: 0.012365, time:  5.435 s\n",
      "Step  15750 (epoch   18.30), loss: 0.009418, time:  5.460 s\n",
      "Step  15800 (epoch   18.36), loss: 0.014165, time:  5.439 s\n",
      "Step  15850 (epoch   18.42), loss: 0.012999, time:  5.429 s\n",
      "Step  15900 (epoch   18.47), loss: 0.011653, time:  5.434 s\n",
      "Step  15950 (epoch   18.53), loss: 0.017553, time:  5.438 s\n",
      "Step  16000 (epoch   18.59), loss: 0.011653, time:  5.437 s, error: 0.022715\n",
      "\n",
      "Time since beginning  : 1870.142 s\n",
      "\n",
      "Step  16050 (epoch   18.65), loss: 0.013417, time:  8.688 s\n",
      "Step  16100 (epoch   18.71), loss: 0.010412, time:  5.452 s\n",
      "Step  16150 (epoch   18.76), loss: 0.016662, time:  5.436 s\n",
      "Step  16200 (epoch   18.82), loss: 0.013022, time:  5.436 s\n",
      "Step  16250 (epoch   18.88), loss: 0.013317, time:  5.437 s\n",
      "Step  16300 (epoch   18.94), loss: 0.016003, time:  5.430 s\n",
      "Step  16350 (epoch   19.00), loss: 0.012453, time:  5.444 s\n",
      "Step  16400 (epoch   19.05), loss: 0.012635, time:  5.424 s, error: 0.022228\n",
      "Step  16450 (epoch   19.11), loss: 0.012293, time:  8.642 s\n",
      "Step  16500 (epoch   19.17), loss: 0.020461, time:  5.431 s\n",
      "Step  16550 (epoch   19.23), loss: 0.012721, time:  5.427 s\n",
      "Step  16600 (epoch   19.29), loss: 0.011312, time:  5.435 s\n",
      "Step  16650 (epoch   19.35), loss: 0.013211, time:  5.437 s\n",
      "Step  16700 (epoch   19.40), loss: 0.014519, time:  5.428 s\n",
      "Step  16750 (epoch   19.46), loss: 0.008623, time:  5.445 s\n",
      "Step  16800 (epoch   19.52), loss: 0.016587, time:  5.443 s, error: 0.026163\n",
      "Step  16850 (epoch   19.58), loss: 0.009631, time:  8.772 s\n",
      "Step  16900 (epoch   19.64), loss: 0.009200, time:  5.443 s\n",
      "Step  16950 (epoch   19.69), loss: 0.011114, time:  5.436 s\n",
      "Step  17000 (epoch   19.75), loss: 0.010993, time:  5.434 s\n",
      "Step  17050 (epoch   19.81), loss: 0.010527, time:  5.425 s\n",
      "Step  17100 (epoch   19.87), loss: 0.014924, time:  5.423 s\n",
      "Step  17150 (epoch   19.93), loss: 0.013990, time:  5.437 s\n",
      "Step  17200 (epoch   19.98), loss: 0.009937, time:  5.459 s, error: 0.022471\n",
      "Step  17250 (epoch   20.04), loss: 0.015239, time:  8.632 s\n",
      "Step  17300 (epoch   20.10), loss: 0.010419, time:  5.449 s\n",
      "Step  17350 (epoch   20.16), loss: 0.011588, time:  5.434 s\n",
      "Step  17400 (epoch   20.22), loss: 0.012695, time:  5.430 s\n",
      "Step  17450 (epoch   20.27), loss: 0.009747, time:  5.429 s\n",
      "Step  17500 (epoch   20.33), loss: 0.015032, time:  5.431 s\n",
      "Step  17550 (epoch   20.39), loss: 0.009871, time:  5.440 s\n",
      "Step  17600 (epoch   20.45), loss: 0.009161, time:  5.437 s, error: 0.022032\n",
      "Step  17650 (epoch   20.51), loss: 0.015640, time:  8.617 s\n",
      "Step  17700 (epoch   20.57), loss: 0.008253, time:  5.422 s\n",
      "Step  17750 (epoch   20.62), loss: 0.011896, time:  5.445 s\n",
      "Step  17800 (epoch   20.68), loss: 0.019245, time:  5.455 s\n",
      "Step  17850 (epoch   20.74), loss: 0.011952, time:  5.442 s\n",
      "Step  17900 (epoch   20.80), loss: 0.009753, time:  5.435 s\n",
      "Step  17950 (epoch   20.86), loss: 0.013023, time:  5.452 s\n",
      "Step  18000 (epoch   20.91), loss: 0.020799, time:  5.426 s, error: 0.023007\n",
      "\n",
      "Time since beginning  : 2103.793 s\n",
      "\n",
      "Step  18050 (epoch   20.97), loss: 0.009047, time:  8.696 s\n",
      "Step  18100 (epoch   21.03), loss: 0.011538, time:  5.424 s\n",
      "Step  18150 (epoch   21.09), loss: 0.011608, time:  5.431 s\n",
      "Step  18200 (epoch   21.15), loss: 0.012169, time:  5.422 s\n",
      "Step  18250 (epoch   21.20), loss: 0.010319, time:  5.443 s\n",
      "Step  18300 (epoch   21.26), loss: 0.010775, time:  5.445 s\n",
      "Step  18350 (epoch   21.32), loss: 0.012235, time:  5.435 s\n",
      "Step  18400 (epoch   21.38), loss: 0.014371, time:  5.433 s, error: 0.022865\n",
      "Step  18450 (epoch   21.44), loss: 0.010622, time:  8.687 s\n",
      "Step  18500 (epoch   21.49), loss: 0.016001, time:  5.443 s\n",
      "Step  18550 (epoch   21.55), loss: 0.008437, time:  5.445 s\n",
      "Step  18600 (epoch   21.61), loss: 0.010548, time:  5.422 s\n",
      "Step  18650 (epoch   21.67), loss: 0.015089, time:  5.451 s\n",
      "Step  18700 (epoch   21.73), loss: 0.019376, time:  5.440 s\n",
      "Step  18750 (epoch   21.79), loss: 0.011616, time:  5.436 s\n",
      "Step  18800 (epoch   21.84), loss: 0.010651, time:  5.436 s, error: 0.021929\n",
      "Step  18850 (epoch   21.90), loss: 0.008942, time:  8.605 s\n",
      "Step  18900 (epoch   21.96), loss: 0.011967, time:  5.430 s\n",
      "Step  18950 (epoch   22.02), loss: 0.013286, time:  5.437 s\n",
      "Step  19000 (epoch   22.08), loss: 0.010396, time:  5.439 s\n",
      "Step  19050 (epoch   22.13), loss: 0.010997, time:  5.441 s\n",
      "Step  19100 (epoch   22.19), loss: 0.010555, time:  5.435 s\n",
      "Step  19150 (epoch   22.25), loss: 0.016159, time:  5.424 s\n",
      "Step  19200 (epoch   22.31), loss: 0.008217, time:  5.432 s, error: 0.021739\n",
      "Step  19250 (epoch   22.37), loss: 0.011248, time:  8.610 s\n",
      "Step  19300 (epoch   22.42), loss: 0.012045, time:  5.417 s\n",
      "Step  19350 (epoch   22.48), loss: 0.009283, time:  5.427 s\n",
      "Step  19400 (epoch   22.54), loss: 0.008596, time:  5.455 s\n",
      "Step  19450 (epoch   22.60), loss: 0.014662, time:  5.444 s\n",
      "Step  19500 (epoch   22.66), loss: 0.013294, time:  5.444 s\n",
      "Step  19550 (epoch   22.71), loss: 0.011278, time:  5.445 s\n",
      "Step  19600 (epoch   22.77), loss: 0.010132, time:  5.438 s, error: 0.021272\n",
      "Step  19650 (epoch   22.83), loss: 0.012927, time:  8.671 s\n",
      "Step  19700 (epoch   22.89), loss: 0.015329, time:  5.431 s\n",
      "Step  19750 (epoch   22.95), loss: 0.008870, time:  5.454 s\n",
      "Step  19800 (epoch   23.01), loss: 0.010916, time:  5.418 s\n",
      "Step  19850 (epoch   23.06), loss: 0.012773, time:  5.419 s\n",
      "Step  19900 (epoch   23.12), loss: 0.009740, time:  5.428 s\n",
      "Step  19950 (epoch   23.18), loss: 0.008515, time:  5.421 s\n",
      "Step  20000 (epoch   23.24), loss: 0.009462, time:  5.446 s, error: 0.021109\n",
      "\n",
      "Time since beginning  : 2337.275 s\n",
      "\n",
      "Step  20050 (epoch   23.30), loss: 0.008241, time:  8.681 s\n",
      "Step  20100 (epoch   23.35), loss: 0.021710, time:  5.453 s\n",
      "Step  20150 (epoch   23.41), loss: 0.015563, time:  5.442 s\n",
      "Step  20200 (epoch   23.47), loss: 0.011669, time:  5.448 s\n",
      "Step  20250 (epoch   23.53), loss: 0.011633, time:  5.446 s\n",
      "Step  20300 (epoch   23.59), loss: 0.005972, time:  5.452 s\n",
      "Step  20350 (epoch   23.64), loss: 0.011300, time:  5.444 s\n",
      "Step  20400 (epoch   23.70), loss: 0.011119, time:  5.432 s, error: 0.020979\n",
      "Step  20450 (epoch   23.76), loss: 0.018390, time:  8.616 s\n",
      "Step  20500 (epoch   23.82), loss: 0.011516, time:  5.448 s\n",
      "Step  20550 (epoch   23.88), loss: 0.012618, time:  5.442 s\n",
      "Step  20600 (epoch   23.93), loss: 0.008146, time:  5.422 s\n",
      "Step  20650 (epoch   23.99), loss: 0.007432, time:  5.439 s\n",
      "Step  20700 (epoch   24.05), loss: 0.008310, time:  5.441 s\n",
      "Step  20750 (epoch   24.11), loss: 0.013144, time:  5.436 s\n",
      "Step  20800 (epoch   24.17), loss: 0.010567, time:  5.446 s, error: 0.021987\n",
      "Step  20850 (epoch   24.23), loss: 0.009542, time:  8.631 s\n",
      "Step  20900 (epoch   24.28), loss: 0.010846, time:  5.439 s\n",
      "Step  20950 (epoch   24.34), loss: 0.012916, time:  5.446 s\n",
      "Step  21000 (epoch   24.40), loss: 0.009317, time:  5.424 s\n",
      "Step  21050 (epoch   24.46), loss: 0.007284, time:  5.438 s\n",
      "Step  21100 (epoch   24.52), loss: 0.010609, time:  5.431 s\n",
      "Step  21150 (epoch   24.57), loss: 0.007312, time:  5.418 s\n",
      "Step  21200 (epoch   24.63), loss: 0.009450, time:  5.432 s, error: 0.020881\n",
      "Step  21250 (epoch   24.69), loss: 0.010605, time:  8.762 s\n",
      "Step  21300 (epoch   24.75), loss: 0.013668, time:  5.428 s\n",
      "Step  21350 (epoch   24.81), loss: 0.009932, time:  5.445 s\n",
      "Step  21400 (epoch   24.86), loss: 0.014365, time:  5.440 s\n",
      "Step  21450 (epoch   24.92), loss: 0.009162, time:  5.437 s\n",
      "Step  21500 (epoch   24.98), loss: 0.008571, time:  5.430 s\n",
      "Step  21550 (epoch   25.04), loss: 0.008683, time:  5.442 s\n",
      "Step  21600 (epoch   25.10), loss: 0.009973, time:  5.448 s, error: 0.020621\n",
      "Step  21650 (epoch   25.15), loss: 0.015688, time:  8.619 s\n",
      "Step  21700 (epoch   25.21), loss: 0.010284, time:  5.448 s\n",
      "Step  21750 (epoch   25.27), loss: 0.011041, time:  5.435 s\n",
      "Step  21800 (epoch   25.33), loss: 0.009111, time:  5.439 s\n",
      "Step  21850 (epoch   25.39), loss: 0.013174, time:  5.434 s\n",
      "Step  21900 (epoch   25.44), loss: 0.010405, time:  5.452 s\n",
      "Step  21950 (epoch   25.50), loss: 0.011669, time:  5.453 s\n",
      "Step  22000 (epoch   25.56), loss: 0.010054, time:  5.423 s, error: 0.020439\n",
      "\n",
      "Time since beginning  : 2570.972 s\n",
      "\n",
      "Step  22050 (epoch   25.62), loss: 0.009536, time:  8.704 s\n",
      "Step  22100 (epoch   25.68), loss: 0.010676, time:  5.425 s\n",
      "Step  22150 (epoch   25.74), loss: 0.013656, time:  5.419 s\n",
      "Step  22200 (epoch   25.79), loss: 0.010604, time:  5.423 s\n",
      "Step  22250 (epoch   25.85), loss: 0.008478, time:  5.428 s\n",
      "Step  22300 (epoch   25.91), loss: 0.010029, time:  5.444 s\n",
      "Step  22350 (epoch   25.97), loss: 0.010537, time:  5.436 s\n",
      "Step  22400 (epoch   26.03), loss: 0.010293, time:  5.423 s, error: 0.020470\n",
      "Step  22450 (epoch   26.08), loss: 0.009828, time:  8.614 s\n",
      "Step  22500 (epoch   26.14), loss: 0.010217, time:  5.427 s\n",
      "Step  22550 (epoch   26.20), loss: 0.010243, time:  5.415 s\n",
      "Step  22600 (epoch   26.26), loss: 0.009611, time:  5.425 s\n",
      "Step  22650 (epoch   26.32), loss: 0.010797, time:  5.432 s\n",
      "Step  22700 (epoch   26.37), loss: 0.009489, time:  5.433 s\n",
      "Step  22750 (epoch   26.43), loss: 0.014133, time:  5.423 s\n",
      "Step  22800 (epoch   26.49), loss: 0.010994, time:  5.424 s, error: 0.021807\n",
      "Step  22850 (epoch   26.55), loss: 0.006848, time:  8.625 s\n",
      "Step  22900 (epoch   26.61), loss: 0.010027, time:  5.431 s\n",
      "Step  22950 (epoch   26.66), loss: 0.008481, time:  5.427 s\n",
      "Step  23000 (epoch   26.72), loss: 0.008254, time:  5.436 s\n",
      "Step  23050 (epoch   26.78), loss: 0.011781, time:  5.444 s\n",
      "Step  23100 (epoch   26.84), loss: 0.009095, time:  5.435 s\n",
      "Step  23150 (epoch   26.90), loss: 0.013447, time:  5.428 s\n",
      "Step  23200 (epoch   26.96), loss: 0.010158, time:  5.426 s, error: 0.021725\n",
      "Step  23250 (epoch   27.01), loss: 0.007129, time:  8.630 s\n",
      "Step  23300 (epoch   27.07), loss: 0.009698, time:  5.442 s\n",
      "Step  23350 (epoch   27.13), loss: 0.010546, time:  5.434 s\n",
      "Step  23400 (epoch   27.19), loss: 0.012805, time:  5.448 s\n",
      "Step  23450 (epoch   27.25), loss: 0.012818, time:  5.426 s\n",
      "Step  23500 (epoch   27.30), loss: 0.011778, time:  5.426 s\n",
      "Step  23550 (epoch   27.36), loss: 0.011162, time:  5.429 s\n",
      "Step  23600 (epoch   27.42), loss: 0.009424, time:  5.435 s, error: 0.022208\n",
      "Step  23650 (epoch   27.48), loss: 0.012597, time:  8.606 s\n",
      "Step  23700 (epoch   27.54), loss: 0.009965, time:  5.440 s\n",
      "Step  23750 (epoch   27.59), loss: 0.009696, time:  5.451 s\n",
      "Step  23800 (epoch   27.65), loss: 0.020284, time:  5.454 s\n",
      "Step  23850 (epoch   27.71), loss: 0.010051, time:  5.451 s\n",
      "Step  23900 (epoch   27.77), loss: 0.009688, time:  5.445 s\n",
      "Step  23950 (epoch   27.83), loss: 0.009863, time:  5.442 s\n",
      "Step  24000 (epoch   27.88), loss: 0.009839, time:  5.431 s, error: 0.021216\n",
      "\n",
      "Time since beginning  : 2804.300 s\n",
      "\n",
      "Step  24050 (epoch   27.94), loss: 0.008367, time:  8.703 s\n",
      "Step  24100 (epoch   28.00), loss: 0.010480, time:  5.437 s\n",
      "Step  24150 (epoch   28.06), loss: 0.007582, time:  5.462 s\n",
      "Step  24200 (epoch   28.12), loss: 0.011434, time:  5.453 s\n",
      "Step  24250 (epoch   28.18), loss: 0.007058, time:  5.446 s\n",
      "Step  24300 (epoch   28.23), loss: 0.010735, time:  5.446 s\n",
      "Step  24350 (epoch   28.29), loss: 0.013924, time:  5.444 s\n",
      "Step  24400 (epoch   28.35), loss: 0.009325, time:  5.437 s, error: 0.020577\n",
      "Step  24450 (epoch   28.41), loss: 0.014422, time:  8.673 s\n",
      "Step  24500 (epoch   28.47), loss: 0.009793, time:  5.452 s\n",
      "Step  24550 (epoch   28.52), loss: 0.009881, time:  5.447 s\n",
      "Step  24600 (epoch   28.58), loss: 0.008190, time:  5.436 s\n",
      "Step  24650 (epoch   28.64), loss: 0.013013, time:  5.431 s\n",
      "Step  24700 (epoch   28.70), loss: 0.009264, time:  5.425 s\n",
      "Step  24750 (epoch   28.76), loss: 0.014103, time:  5.422 s\n",
      "Step  24800 (epoch   28.81), loss: 0.012840, time:  5.411 s, error: 0.020766\n",
      "Step  24850 (epoch   28.87), loss: 0.006349, time:  8.637 s\n",
      "Step  24900 (epoch   28.93), loss: 0.008627, time:  5.451 s\n",
      "Step  24950 (epoch   28.99), loss: 0.009091, time:  5.431 s\n",
      "Step  25000 (epoch   29.05), loss: 0.006496, time:  5.442 s\n",
      "Step  25050 (epoch   29.10), loss: 0.013639, time:  5.422 s\n",
      "Step  25100 (epoch   29.16), loss: 0.010036, time:  5.421 s\n",
      "Step  25150 (epoch   29.22), loss: 0.010883, time:  5.431 s\n",
      "Step  25200 (epoch   29.28), loss: 0.008154, time:  5.426 s, error: 0.020722\n",
      "Step  25250 (epoch   29.34), loss: 0.010535, time:  8.697 s\n",
      "Step  25300 (epoch   29.40), loss: 0.011087, time:  5.428 s\n",
      "Step  25350 (epoch   29.45), loss: 0.008990, time:  5.428 s\n",
      "Step  25400 (epoch   29.51), loss: 0.008212, time:  5.457 s\n",
      "Step  25450 (epoch   29.57), loss: 0.008169, time:  5.448 s\n",
      "Step  25500 (epoch   29.63), loss: 0.009099, time:  5.444 s\n",
      "Step  25550 (epoch   29.69), loss: 0.008956, time:  5.411 s\n",
      "Step  25600 (epoch   29.74), loss: 0.011830, time:  5.446 s, error: 0.023508\n",
      "Step  25650 (epoch   29.80), loss: 0.007648, time:  8.688 s\n",
      "Step  25700 (epoch   29.86), loss: 0.013576, time:  5.428 s\n",
      "Step  25750 (epoch   29.92), loss: 0.011251, time:  5.431 s\n",
      "Step  25800 (epoch   29.98), loss: 0.006270, time:  5.417 s\n",
      "Step  25850 (epoch   30.03), loss: 0.009110, time:  5.429 s\n",
      "Step  25900 (epoch   30.09), loss: 0.009490, time:  5.439 s\n",
      "Step  25950 (epoch   30.15), loss: 0.010749, time:  5.434 s\n",
      "Step  26000 (epoch   30.21), loss: 0.006755, time:  5.452 s, error: 0.020763\n",
      "\n",
      "Time since beginning  : 3037.964 s\n",
      "\n",
      "Step  26050 (epoch   30.27), loss: 0.010716, time:  8.697 s\n",
      "Step  26100 (epoch   30.32), loss: 0.009334, time:  5.444 s\n",
      "Step  26150 (epoch   30.38), loss: 0.008853, time:  5.442 s\n",
      "Step  26200 (epoch   30.44), loss: 0.011201, time:  5.421 s\n",
      "Step  26250 (epoch   30.50), loss: 0.010874, time:  5.420 s\n",
      "Step  26300 (epoch   30.56), loss: 0.010724, time:  5.434 s\n",
      "Step  26350 (epoch   30.62), loss: 0.009028, time:  5.450 s\n",
      "Step  26400 (epoch   30.67), loss: 0.012375, time:  5.434 s, error: 0.021792\n",
      "Step  26450 (epoch   30.73), loss: 0.008058, time:  8.653 s\n",
      "Step  26500 (epoch   30.79), loss: 0.006067, time:  5.426 s\n",
      "Step  26550 (epoch   30.85), loss: 0.007820, time:  5.428 s\n",
      "Step  26600 (epoch   30.91), loss: 0.012426, time:  5.424 s\n",
      "Step  26650 (epoch   30.96), loss: 0.011724, time:  5.431 s\n",
      "Step  26700 (epoch   31.02), loss: 0.009804, time:  5.453 s\n",
      "Step  26750 (epoch   31.08), loss: 0.012363, time:  5.435 s\n",
      "Step  26800 (epoch   31.14), loss: 0.009098, time:  5.438 s, error: 0.020001\n",
      "Step  26850 (epoch   31.20), loss: 0.010898, time:  8.669 s\n",
      "Step  26900 (epoch   31.25), loss: 0.008427, time:  5.451 s\n",
      "Step  26950 (epoch   31.31), loss: 0.009710, time:  5.443 s\n",
      "Step  27000 (epoch   31.37), loss: 0.007365, time:  5.430 s\n",
      "Step  27050 (epoch   31.43), loss: 0.007347, time:  5.419 s\n",
      "Step  27100 (epoch   31.49), loss: 0.013385, time:  5.428 s\n",
      "Step  27150 (epoch   31.54), loss: 0.011474, time:  5.433 s\n",
      "Step  27200 (epoch   31.60), loss: 0.009453, time:  5.424 s, error: 0.020223\n",
      "Step  27250 (epoch   31.66), loss: 0.006017, time:  8.642 s\n",
      "Step  27300 (epoch   31.72), loss: 0.010432, time:  5.427 s\n",
      "Step  27350 (epoch   31.78), loss: 0.008207, time:  5.445 s\n",
      "Step  27400 (epoch   31.84), loss: 0.008259, time:  5.424 s\n",
      "Step  27450 (epoch   31.89), loss: 0.011214, time:  5.450 s\n",
      "Step  27500 (epoch   31.95), loss: 0.006672, time:  5.425 s\n",
      "Step  27550 (epoch   32.01), loss: 0.009230, time:  5.431 s\n",
      "Step  27600 (epoch   32.07), loss: 0.010287, time:  5.426 s, error: 0.020483\n",
      "Step  27650 (epoch   32.13), loss: 0.008590, time:  8.623 s\n",
      "Step  27700 (epoch   32.18), loss: 0.008281, time:  5.440 s\n",
      "Step  27750 (epoch   32.24), loss: 0.007416, time:  5.431 s\n",
      "Step  27800 (epoch   32.30), loss: 0.008473, time:  5.444 s\n",
      "Step  27850 (epoch   32.36), loss: 0.009850, time:  5.431 s\n",
      "Step  27900 (epoch   32.42), loss: 0.012281, time:  5.419 s\n",
      "Step  27950 (epoch   32.47), loss: 0.010425, time:  5.426 s\n",
      "Step  28000 (epoch   32.53), loss: 0.007671, time:  5.432 s, error: 0.020415\n",
      "\n",
      "Time since beginning  : 3271.414 s\n",
      "\n",
      "Step  28050 (epoch   32.59), loss: 0.011981, time:  8.687 s\n",
      "Step  28100 (epoch   32.65), loss: 0.012735, time:  5.429 s\n",
      "Step  28150 (epoch   32.71), loss: 0.006763, time:  5.441 s\n",
      "Step  28200 (epoch   32.76), loss: 0.007567, time:  5.424 s\n",
      "Step  28250 (epoch   32.82), loss: 0.006950, time:  5.422 s\n",
      "Step  28300 (epoch   32.88), loss: 0.011762, time:  5.433 s\n",
      "Step  28350 (epoch   32.94), loss: 0.006640, time:  5.434 s\n",
      "Step  28400 (epoch   33.00), loss: 0.008832, time:  5.438 s, error: 0.019593\n",
      "Step  28450 (epoch   33.06), loss: 0.009055, time:  8.617 s\n",
      "Step  28500 (epoch   33.11), loss: 0.011520, time:  5.450 s\n",
      "Step  28550 (epoch   33.17), loss: 0.007506, time:  5.439 s\n",
      "Step  28600 (epoch   33.23), loss: 0.010141, time:  5.418 s\n",
      "Step  28650 (epoch   33.29), loss: 0.012513, time:  5.420 s\n",
      "Step  28700 (epoch   33.35), loss: 0.007067, time:  5.419 s\n",
      "Step  28750 (epoch   33.40), loss: 0.009190, time:  5.419 s\n",
      "Step  28800 (epoch   33.46), loss: 0.009504, time:  5.415 s, error: 0.022576\n",
      "Step  28850 (epoch   33.52), loss: 0.010180, time:  8.650 s\n",
      "Step  28900 (epoch   33.58), loss: 0.008850, time:  5.447 s\n",
      "Step  28950 (epoch   33.64), loss: 0.009514, time:  5.425 s\n",
      "Step  29000 (epoch   33.69), loss: 0.010077, time:  5.441 s\n",
      "Step  29050 (epoch   33.75), loss: 0.011238, time:  5.449 s\n",
      "Step  29100 (epoch   33.81), loss: 0.006640, time:  5.429 s\n",
      "Step  29150 (epoch   33.87), loss: 0.010898, time:  5.442 s\n",
      "Step  29200 (epoch   33.93), loss: 0.007416, time:  5.430 s, error: 0.021132\n",
      "Step  29250 (epoch   33.98), loss: 0.008912, time:  8.635 s\n",
      "Step  29300 (epoch   34.04), loss: 0.007807, time:  5.429 s\n",
      "Step  29350 (epoch   34.10), loss: 0.009674, time:  5.418 s\n",
      "Step  29400 (epoch   34.16), loss: 0.008254, time:  5.426 s\n",
      "Step  29450 (epoch   34.22), loss: 0.007535, time:  5.410 s\n",
      "Step  29500 (epoch   34.28), loss: 0.009033, time:  5.435 s\n",
      "Step  29550 (epoch   34.33), loss: 0.008420, time:  5.442 s\n",
      "Step  29600 (epoch   34.39), loss: 0.009933, time:  5.435 s, error: 0.019938\n",
      "Step  29650 (epoch   34.45), loss: 0.008284, time:  8.698 s\n",
      "Step  29700 (epoch   34.51), loss: 0.008864, time:  5.437 s\n",
      "Step  29750 (epoch   34.57), loss: 0.007926, time:  5.431 s\n",
      "Step  29800 (epoch   34.62), loss: 0.006960, time:  5.440 s\n",
      "Step  29850 (epoch   34.68), loss: 0.007849, time:  5.424 s\n",
      "Step  29900 (epoch   34.74), loss: 0.007824, time:  5.424 s\n",
      "Step  29950 (epoch   34.80), loss: 0.014770, time:  5.437 s\n",
      "Step  30000 (epoch   34.86), loss: 0.008554, time:  5.449 s, error: 0.020509\n",
      "\n",
      "Time since beginning  : 3504.897 s\n",
      "\n",
      "Step  30050 (epoch   34.91), loss: 0.006580, time:  8.791 s\n",
      "Step  30100 (epoch   34.97), loss: 0.006136, time:  5.421 s\n",
      "Step  30150 (epoch   35.03), loss: 0.006900, time:  5.428 s\n",
      "Step  30200 (epoch   35.09), loss: 0.009829, time:  5.434 s\n",
      "Step  30250 (epoch   35.15), loss: 0.013156, time:  5.422 s\n",
      "Step  30300 (epoch   35.20), loss: 0.009359, time:  5.425 s\n",
      "Step  30350 (epoch   35.26), loss: 0.008910, time:  5.428 s\n",
      "Step  30400 (epoch   35.32), loss: 0.008608, time:  5.442 s, error: 0.019889\n",
      "Step  30450 (epoch   35.38), loss: 0.009486, time:  8.634 s\n",
      "Step  30500 (epoch   35.44), loss: 0.007429, time:  5.444 s\n",
      "Step  30550 (epoch   35.50), loss: 0.006693, time:  5.445 s\n",
      "Step  30600 (epoch   35.55), loss: 0.007384, time:  5.438 s\n",
      "Step  30650 (epoch   35.61), loss: 0.008270, time:  5.417 s\n",
      "Step  30700 (epoch   35.67), loss: 0.006068, time:  5.438 s\n",
      "Step  30750 (epoch   35.73), loss: 0.007184, time:  5.437 s\n",
      "Step  30800 (epoch   35.79), loss: 0.010128, time:  5.424 s, error: 0.019853\n",
      "Step  30850 (epoch   35.84), loss: 0.007869, time:  8.645 s\n",
      "Step  30900 (epoch   35.90), loss: 0.006762, time:  5.414 s\n",
      "Step  30950 (epoch   35.96), loss: 0.008423, time:  5.427 s\n",
      "Step  31000 (epoch   36.02), loss: 0.010403, time:  5.420 s\n",
      "Step  31050 (epoch   36.08), loss: 0.014226, time:  5.441 s\n",
      "Step  31100 (epoch   36.13), loss: 0.006458, time:  5.447 s\n",
      "Step  31150 (epoch   36.19), loss: 0.006976, time:  5.438 s\n",
      "Step  31200 (epoch   36.25), loss: 0.011743, time:  5.421 s, error: 0.021140\n",
      "Step  31250 (epoch   36.31), loss: 0.006532, time:  8.654 s\n",
      "Step  31300 (epoch   36.37), loss: 0.008004, time:  5.444 s\n",
      "Step  31350 (epoch   36.42), loss: 0.008476, time:  5.434 s\n",
      "Step  31400 (epoch   36.48), loss: 0.012485, time:  5.434 s\n",
      "Step  31450 (epoch   36.54), loss: 0.006720, time:  5.446 s\n",
      "Step  31500 (epoch   36.60), loss: 0.004475, time:  5.434 s\n",
      "Step  31550 (epoch   36.66), loss: 0.008906, time:  5.425 s\n",
      "Step  31600 (epoch   36.72), loss: 0.008481, time:  5.430 s, error: 0.019359\n",
      "Step  31650 (epoch   36.77), loss: 0.006724, time:  8.618 s\n",
      "Step  31700 (epoch   36.83), loss: 0.016949, time:  5.449 s\n",
      "Step  31750 (epoch   36.89), loss: 0.006984, time:  5.427 s\n",
      "Step  31800 (epoch   36.95), loss: 0.009492, time:  5.440 s\n",
      "Step  31850 (epoch   37.01), loss: 0.010125, time:  5.462 s\n",
      "Step  31900 (epoch   37.06), loss: 0.008719, time:  5.433 s\n",
      "Step  31950 (epoch   37.12), loss: 0.008429, time:  5.437 s\n",
      "Step  32000 (epoch   37.18), loss: 0.007424, time:  5.435 s, error: 0.019160\n",
      "\n",
      "Time since beginning  : 3738.321 s\n",
      "\n",
      "Step  32050 (epoch   37.24), loss: 0.005526, time:  8.687 s\n",
      "Step  32100 (epoch   37.30), loss: 0.010140, time:  5.436 s\n",
      "Step  32150 (epoch   37.35), loss: 0.008784, time:  5.426 s\n",
      "Step  32200 (epoch   37.41), loss: 0.010172, time:  5.433 s\n",
      "Step  32250 (epoch   37.47), loss: 0.009189, time:  5.437 s\n",
      "Step  32300 (epoch   37.53), loss: 0.007607, time:  5.416 s\n",
      "Step  32350 (epoch   37.59), loss: 0.014294, time:  5.424 s\n",
      "Step  32400 (epoch   37.64), loss: 0.009252, time:  5.436 s, error: 0.019445\n",
      "Step  32450 (epoch   37.70), loss: 0.014501, time:  8.620 s\n",
      "Step  32500 (epoch   37.76), loss: 0.007050, time:  5.437 s\n",
      "Step  32550 (epoch   37.82), loss: 0.010796, time:  5.449 s\n",
      "Step  32600 (epoch   37.88), loss: 0.007373, time:  5.448 s\n",
      "Step  32650 (epoch   37.94), loss: 0.007219, time:  5.440 s\n",
      "Step  32700 (epoch   37.99), loss: 0.004779, time:  5.442 s\n",
      "Step  32750 (epoch   38.05), loss: 0.010054, time:  5.411 s\n",
      "Step  32800 (epoch   38.11), loss: 0.006969, time:  5.441 s, error: 0.019081\n",
      "Step  32850 (epoch   38.17), loss: 0.007969, time:  8.606 s\n",
      "Step  32900 (epoch   38.23), loss: 0.011842, time:  5.432 s\n",
      "Step  32950 (epoch   38.28), loss: 0.010538, time:  5.440 s\n",
      "Step  33000 (epoch   38.34), loss: 0.010737, time:  5.427 s\n",
      "Step  33050 (epoch   38.40), loss: 0.008179, time:  5.443 s\n",
      "Step  33100 (epoch   38.46), loss: 0.009202, time:  5.441 s\n",
      "Step  33150 (epoch   38.52), loss: 0.005836, time:  5.434 s\n",
      "Step  33200 (epoch   38.57), loss: 0.007903, time:  5.423 s, error: 0.019074\n",
      "Step  33250 (epoch   38.63), loss: 0.005857, time:  8.613 s\n",
      "Step  33300 (epoch   38.69), loss: 0.006408, time:  5.452 s\n",
      "Step  33350 (epoch   38.75), loss: 0.009379, time:  5.433 s\n",
      "Step  33400 (epoch   38.81), loss: 0.009664, time:  5.428 s\n",
      "Step  33450 (epoch   38.86), loss: 0.015977, time:  5.423 s\n",
      "Step  33500 (epoch   38.92), loss: 0.008379, time:  5.433 s\n",
      "Step  33550 (epoch   38.98), loss: 0.007790, time:  5.432 s\n",
      "Step  33600 (epoch   39.04), loss: 0.005943, time:  5.440 s, error: 0.019546\n",
      "Step  33650 (epoch   39.10), loss: 0.006088, time:  8.679 s\n",
      "Step  33700 (epoch   39.16), loss: 0.008846, time:  5.439 s\n",
      "Step  33750 (epoch   39.21), loss: 0.010201, time:  5.417 s\n",
      "Step  33800 (epoch   39.27), loss: 0.008661, time:  5.430 s\n",
      "Step  33850 (epoch   39.33), loss: 0.009959, time:  5.441 s\n",
      "Step  33900 (epoch   39.39), loss: 0.009128, time:  5.438 s\n",
      "Step  33950 (epoch   39.45), loss: 0.009005, time:  5.437 s\n",
      "Step  34000 (epoch   39.50), loss: 0.007174, time:  5.438 s, error: 0.019354\n",
      "\n",
      "Time since beginning  : 3971.776 s\n",
      "\n",
      "Step  34050 (epoch   39.56), loss: 0.015611, time:  8.750 s\n",
      "Step  34100 (epoch   39.62), loss: 0.008380, time:  5.435 s\n",
      "Step  34150 (epoch   39.68), loss: 0.008825, time:  5.433 s\n",
      "Step  34200 (epoch   39.74), loss: 0.008160, time:  5.427 s\n",
      "Step  34250 (epoch   39.79), loss: 0.006146, time:  5.422 s\n",
      "Step  34300 (epoch   39.85), loss: 0.007648, time:  5.425 s\n",
      "Step  34350 (epoch   39.91), loss: 0.010168, time:  5.444 s\n",
      "Step  34400 (epoch   39.97), loss: 0.006613, time:  5.448 s, error: 0.019203\n",
      "Step  34450 (epoch   40.03), loss: 0.008626, time:  8.697 s\n",
      "Step  34500 (epoch   40.08), loss: 0.010107, time:  5.429 s\n",
      "Step  34550 (epoch   40.14), loss: 0.008521, time:  5.429 s\n",
      "Step  34600 (epoch   40.20), loss: 0.010438, time:  5.434 s\n",
      "Step  34650 (epoch   40.26), loss: 0.010805, time:  5.443 s\n",
      "Step  34700 (epoch   40.32), loss: 0.007809, time:  5.437 s\n",
      "Step  34750 (epoch   40.38), loss: 0.008870, time:  5.432 s\n",
      "Step  34800 (epoch   40.43), loss: 0.009196, time:  5.457 s, error: 0.019853\n",
      "Step  34850 (epoch   40.49), loss: 0.012160, time:  8.650 s\n",
      "Step  34900 (epoch   40.55), loss: 0.006425, time:  5.427 s\n",
      "Step  34950 (epoch   40.61), loss: 0.008476, time:  5.437 s\n",
      "Step  35000 (epoch   40.67), loss: 0.007222, time:  5.435 s\n",
      "Step  35050 (epoch   40.72), loss: 0.007797, time:  5.429 s\n",
      "Step  35100 (epoch   40.78), loss: 0.010382, time:  5.426 s\n",
      "Step  35150 (epoch   40.84), loss: 0.010905, time:  5.462 s\n",
      "Step  35200 (epoch   40.90), loss: 0.009030, time:  5.447 s, error: 0.019224\n",
      "Step  35250 (epoch   40.96), loss: 0.006074, time:  8.635 s\n",
      "Step  35300 (epoch   41.01), loss: 0.008498, time:  5.459 s\n",
      "Step  35350 (epoch   41.07), loss: 0.008528, time:  5.441 s\n",
      "Step  35400 (epoch   41.13), loss: 0.010469, time:  5.438 s\n",
      "Step  35450 (epoch   41.19), loss: 0.006349, time:  5.438 s\n",
      "Step  35500 (epoch   41.25), loss: 0.014803, time:  5.458 s\n",
      "Step  35550 (epoch   41.30), loss: 0.008315, time:  5.420 s\n",
      "Step  35600 (epoch   41.36), loss: 0.008380, time:  5.424 s, error: 0.019609\n",
      "Step  35650 (epoch   41.42), loss: 0.011162, time:  8.658 s\n",
      "Step  35700 (epoch   41.48), loss: 0.008992, time:  5.450 s\n",
      "Step  35750 (epoch   41.54), loss: 0.009216, time:  5.425 s\n",
      "Step  35800 (epoch   41.60), loss: 0.005448, time:  5.428 s\n",
      "Step  35850 (epoch   41.65), loss: 0.011954, time:  5.450 s\n",
      "Step  35900 (epoch   41.71), loss: 0.008198, time:  5.433 s\n",
      "Step  35950 (epoch   41.77), loss: 0.006223, time:  5.418 s\n",
      "Step  36000 (epoch   41.83), loss: 0.009970, time:  5.445 s, error: 0.019368\n",
      "\n",
      "Time since beginning  : 4205.408 s\n",
      "\n",
      "Step  36050 (epoch   41.89), loss: 0.006823, time:  8.716 s\n",
      "Step  36100 (epoch   41.94), loss: 0.006851, time:  5.432 s\n",
      "Step  36150 (epoch   42.00), loss: 0.010482, time:  5.440 s\n",
      "Step  36200 (epoch   42.06), loss: 0.006850, time:  5.443 s\n",
      "Step  36250 (epoch   42.12), loss: 0.005806, time:  5.450 s\n",
      "Step  36300 (epoch   42.18), loss: 0.007076, time:  5.440 s\n",
      "Step  36350 (epoch   42.23), loss: 0.007483, time:  5.437 s\n",
      "Step  36400 (epoch   42.29), loss: 0.006580, time:  5.448 s, error: 0.018998\n",
      "Step  36450 (epoch   42.35), loss: 0.005919, time:  8.649 s\n",
      "Step  36500 (epoch   42.41), loss: 0.008360, time:  5.439 s\n",
      "Step  36550 (epoch   42.47), loss: 0.006765, time:  5.442 s\n",
      "Step  36600 (epoch   42.52), loss: 0.006038, time:  5.456 s\n",
      "Step  36650 (epoch   42.58), loss: 0.009227, time:  5.441 s\n",
      "Step  36700 (epoch   42.64), loss: 0.008522, time:  5.443 s\n",
      "Step  36750 (epoch   42.70), loss: 0.007390, time:  5.438 s\n",
      "Step  36800 (epoch   42.76), loss: 0.009900, time:  5.449 s, error: 0.020871\n",
      "Step  36850 (epoch   42.81), loss: 0.015562, time:  8.653 s\n",
      "Step  36900 (epoch   42.87), loss: 0.006179, time:  5.435 s\n",
      "Step  36950 (epoch   42.93), loss: 0.006136, time:  5.455 s\n",
      "Step  37000 (epoch   42.99), loss: 0.005979, time:  5.438 s\n",
      "Step  37050 (epoch   43.05), loss: 0.011344, time:  5.452 s\n",
      "Step  37100 (epoch   43.11), loss: 0.008141, time:  5.435 s\n",
      "Step  37150 (epoch   43.16), loss: 0.007072, time:  5.430 s\n",
      "Step  37200 (epoch   43.22), loss: 0.012451, time:  5.430 s, error: 0.023486\n",
      "Step  37250 (epoch   43.28), loss: 0.005936, time:  8.643 s\n",
      "Step  37300 (epoch   43.34), loss: 0.007026, time:  5.450 s\n",
      "Step  37350 (epoch   43.40), loss: 0.006523, time:  5.444 s\n",
      "Step  37400 (epoch   43.45), loss: 0.011729, time:  5.441 s\n",
      "Step  37450 (epoch   43.51), loss: 0.007826, time:  5.436 s\n",
      "Step  37500 (epoch   43.57), loss: 0.006883, time:  5.435 s\n",
      "Step  37550 (epoch   43.63), loss: 0.006595, time:  5.442 s\n",
      "Step  37600 (epoch   43.69), loss: 0.008965, time:  5.439 s, error: 0.019489\n",
      "Step  37650 (epoch   43.74), loss: 0.011330, time:  8.652 s\n",
      "Step  37700 (epoch   43.80), loss: 0.007288, time:  5.462 s\n",
      "Step  37750 (epoch   43.86), loss: 0.010865, time:  5.431 s\n",
      "Step  37800 (epoch   43.92), loss: 0.004159, time:  5.437 s\n",
      "Step  37850 (epoch   43.98), loss: 0.006300, time:  5.441 s\n",
      "Step  37900 (epoch   44.03), loss: 0.006033, time:  5.427 s\n",
      "Step  37950 (epoch   44.09), loss: 0.009445, time:  5.442 s\n",
      "Step  38000 (epoch   44.15), loss: 0.008543, time:  5.441 s, error: 0.019025\n",
      "\n",
      "Time since beginning  : 4439.146 s\n",
      "\n",
      "Step  38050 (epoch   44.21), loss: 0.011228, time:  8.709 s\n",
      "Step  38100 (epoch   44.27), loss: 0.004270, time:  5.446 s\n",
      "Step  38150 (epoch   44.33), loss: 0.005659, time:  5.433 s\n",
      "Step  38200 (epoch   44.38), loss: 0.007131, time:  5.420 s\n",
      "Step  38250 (epoch   44.44), loss: 0.007990, time:  5.445 s\n",
      "Step  38300 (epoch   44.50), loss: 0.006160, time:  5.440 s\n",
      "Step  38350 (epoch   44.56), loss: 0.006043, time:  5.423 s\n",
      "Step  38400 (epoch   44.62), loss: 0.009117, time:  5.424 s, error: 0.020464\n",
      "Step  38450 (epoch   44.67), loss: 0.012713, time:  8.707 s\n",
      "Step  38500 (epoch   44.73), loss: 0.008311, time:  5.421 s\n",
      "Step  38550 (epoch   44.79), loss: 0.006155, time:  5.423 s\n",
      "Step  38600 (epoch   44.85), loss: 0.008415, time:  5.424 s\n",
      "Step  38650 (epoch   44.91), loss: 0.007032, time:  5.441 s\n",
      "Step  38700 (epoch   44.96), loss: 0.008263, time:  5.432 s\n",
      "Step  38750 (epoch   45.02), loss: 0.008473, time:  5.425 s\n",
      "Step  38800 (epoch   45.08), loss: 0.008046, time:  5.451 s, error: 0.019058\n",
      "Step  38850 (epoch   45.14), loss: 0.008895, time:  8.623 s\n",
      "Step  38900 (epoch   45.20), loss: 0.006535, time:  5.435 s\n",
      "Step  38950 (epoch   45.25), loss: 0.005391, time:  5.442 s\n",
      "Step  39000 (epoch   45.31), loss: 0.006625, time:  5.435 s\n",
      "Step  39050 (epoch   45.37), loss: 0.007020, time:  5.436 s\n",
      "Step  39100 (epoch   45.43), loss: 0.014328, time:  5.444 s\n",
      "Step  39150 (epoch   45.49), loss: 0.007984, time:  5.450 s\n",
      "Step  39200 (epoch   45.55), loss: 0.007418, time:  5.439 s, error: 0.018574\n",
      "Step  39250 (epoch   45.60), loss: 0.005505, time:  8.628 s\n",
      "Step  39300 (epoch   45.66), loss: 0.007358, time:  5.435 s\n",
      "Step  39350 (epoch   45.72), loss: 0.008221, time:  5.439 s\n",
      "Step  39400 (epoch   45.78), loss: 0.007348, time:  5.441 s\n",
      "Step  39450 (epoch   45.84), loss: 0.008125, time:  5.443 s\n",
      "Step  39500 (epoch   45.89), loss: 0.009267, time:  5.447 s\n",
      "Step  39550 (epoch   45.95), loss: 0.005014, time:  5.447 s\n",
      "Step  39600 (epoch   46.01), loss: 0.007568, time:  5.429 s, error: 0.018787\n",
      "Step  39650 (epoch   46.07), loss: 0.009586, time:  8.631 s\n",
      "Step  39700 (epoch   46.13), loss: 0.014918, time:  5.426 s\n",
      "Step  39750 (epoch   46.18), loss: 0.006957, time:  5.435 s\n",
      "Step  39800 (epoch   46.24), loss: 0.007360, time:  5.428 s\n",
      "Step  39850 (epoch   46.30), loss: 0.004841, time:  5.450 s\n",
      "Step  39900 (epoch   46.36), loss: 0.006499, time:  5.463 s\n",
      "Step  39950 (epoch   46.42), loss: 0.008405, time:  5.449 s\n",
      "Step  40000 (epoch   46.47), loss: 0.008230, time:  5.434 s, error: 0.018794\n",
      "\n",
      "Time since beginning  : 4672.740 s\n",
      "\n",
      "Step  40050 (epoch   46.53), loss: 0.005837, time:  8.676 s\n",
      "Step  40100 (epoch   46.59), loss: 0.007561, time:  5.429 s\n",
      "Step  40150 (epoch   46.65), loss: 0.009292, time:  5.426 s\n",
      "Step  40200 (epoch   46.71), loss: 0.005272, time:  5.446 s\n",
      "Step  40250 (epoch   46.77), loss: 0.006992, time:  5.433 s\n",
      "Step  62650 (epoch   72.79), loss: 0.006681, time:  5.431 s\n",
      "Step  62700 (epoch   72.85), loss: 0.005599, time:  5.426 s\n",
      "Step  62750 (epoch   72.91), loss: 0.008428, time:  5.444 s\n",
      "Step  62800 (epoch   72.97), loss: 0.006403, time:  5.443 s, error: 0.017782\n",
      "Step  62850 (epoch   73.02), loss: 0.004448, time:  8.651 s\n",
      "Step  62900 (epoch   73.08), loss: 0.004673, time:  5.436 s\n",
      "Step  62950 (epoch   73.14), loss: 0.005599, time:  5.453 s\n",
      "Step  63000 (epoch   73.20), loss: 0.006153, time:  5.427 s\n",
      "Step  63050 (epoch   73.26), loss: 0.005047, time:  5.427 s\n",
      "Step  63100 (epoch   73.31), loss: 0.006859, time:  5.443 s\n",
      "Step  63150 (epoch   73.37), loss: 0.004884, time:  5.430 s\n",
      "Step  63200 (epoch   73.43), loss: 0.006748, time:  5.434 s, error: 0.018319\n",
      "Step  63250 (epoch   73.49), loss: 0.006007, time:  8.660 s\n",
      "Step  63300 (epoch   73.55), loss: 0.008011, time:  5.450 s\n",
      "Step  63350 (epoch   73.60), loss: 0.005775, time:  5.441 s\n",
      "Step  63400 (epoch   73.66), loss: 0.006705, time:  5.438 s\n",
      "Step  63450 (epoch   73.72), loss: 0.006845, time:  5.433 s\n",
      "Step  63500 (epoch   73.78), loss: 0.005954, time:  5.442 s\n",
      "Step  63550 (epoch   73.84), loss: 0.006689, time:  5.430 s\n",
      "Step  63600 (epoch   73.90), loss: 0.006031, time:  5.415 s, error: 0.017789\n",
      "Step  63650 (epoch   73.95), loss: 0.006175, time:  8.639 s\n",
      "Step  63700 (epoch   74.01), loss: 0.006368, time:  5.457 s\n",
      "Step  63750 (epoch   74.07), loss: 0.007900, time:  5.440 s\n",
      "Step  63800 (epoch   74.13), loss: 0.004961, time:  5.422 s\n",
      "Step  63850 (epoch   74.19), loss: 0.004819, time:  5.441 s\n",
      "Step  63900 (epoch   74.24), loss: 0.005414, time:  5.451 s\n",
      "Step  63950 (epoch   74.30), loss: 0.005606, time:  5.435 s\n",
      "Step  64000 (epoch   74.36), loss: 0.005698, time:  5.455 s, error: 0.019168\n",
      "\n",
      "Time since beginning  : 7475.598 s\n",
      "\n",
      "Step  64050 (epoch   74.42), loss: 0.006697, time:  8.736 s\n",
      "Step  64100 (epoch   74.48), loss: 0.005959, time:  5.447 s\n",
      "Step  64150 (epoch   74.53), loss: 0.004696, time:  5.448 s\n",
      "Step  64200 (epoch   74.59), loss: 0.005547, time:  5.440 s\n",
      "Step  64250 (epoch   74.65), loss: 0.006076, time:  5.430 s\n",
      "Step  64300 (epoch   74.71), loss: 0.009063, time:  5.424 s\n",
      "Step  64350 (epoch   74.77), loss: 0.007334, time:  5.436 s\n",
      "Step  64400 (epoch   74.82), loss: 0.007663, time:  5.437 s, error: 0.018678\n",
      "Step  64450 (epoch   74.88), loss: 0.004952, time:  8.769 s\n",
      "Step  64500 (epoch   74.94), loss: 0.004799, time:  5.439 s\n",
      "Step  64550 (epoch   75.00), loss: 0.006103, time:  5.447 s\n",
      "Step  64600 (epoch   75.06), loss: 0.007935, time:  5.443 s\n",
      "Step  64650 (epoch   75.12), loss: 0.003931, time:  5.438 s\n",
      "Step  64700 (epoch   75.17), loss: 0.005638, time:  5.436 s\n",
      "Step  64750 (epoch   75.23), loss: 0.003115, time:  5.443 s\n",
      "Step  64800 (epoch   75.29), loss: 0.005041, time:  5.449 s, error: 0.019213\n",
      "Step  64850 (epoch   75.35), loss: 0.006479, time:  8.683 s\n",
      "Step  64900 (epoch   75.41), loss: 0.008551, time:  5.446 s\n",
      "Step  64950 (epoch   75.46), loss: 0.005005, time:  5.438 s\n",
      "Step  65000 (epoch   75.52), loss: 0.003932, time:  5.438 s\n",
      "Step  65050 (epoch   75.58), loss: 0.005315, time:  5.439 s\n",
      "Step  65100 (epoch   75.64), loss: 0.007679, time:  5.455 s\n",
      "Step  65150 (epoch   75.70), loss: 0.005194, time:  5.458 s\n",
      "Step  65200 (epoch   75.75), loss: 0.004778, time:  5.436 s, error: 0.018036\n",
      "Step  65250 (epoch   75.81), loss: 0.004257, time:  8.651 s\n",
      "Step  65300 (epoch   75.87), loss: 0.005798, time:  5.421 s\n",
      "Step  65350 (epoch   75.93), loss: 0.005597, time:  5.431 s\n",
      "Step  65400 (epoch   75.99), loss: 0.005550, time:  5.437 s\n",
      "Step  65450 (epoch   76.04), loss: 0.005546, time:  5.456 s\n",
      "Step  65500 (epoch   76.10), loss: 0.005751, time:  5.449 s\n",
      "Step  65550 (epoch   76.16), loss: 0.004299, time:  5.447 s\n",
      "Step  65600 (epoch   76.22), loss: 0.005732, time:  5.446 s, error: 0.017944\n",
      "Step  65650 (epoch   76.28), loss: 0.005414, time:  8.628 s\n",
      "Step  65700 (epoch   76.33), loss: 0.005196, time:  5.431 s\n",
      "Step  65750 (epoch   76.39), loss: 0.005459, time:  5.442 s\n",
      "Step  65800 (epoch   76.45), loss: 0.006426, time:  5.420 s\n",
      "Step  65850 (epoch   76.51), loss: 0.005213, time:  5.447 s\n",
      "Step  65900 (epoch   76.57), loss: 0.005811, time:  5.459 s\n",
      "Step  65950 (epoch   76.63), loss: 0.007657, time:  5.440 s\n",
      "Step  66000 (epoch   76.68), loss: 0.004228, time:  5.442 s, error: 0.017591\n",
      "\n",
      "Time since beginning  : 7709.529 s\n",
      "\n",
      "Step  66050 (epoch   76.74), loss: 0.007260, time:  8.746 s\n",
      "Step  66100 (epoch   76.80), loss: 0.005084, time:  5.441 s\n",
      "Step  66150 (epoch   76.86), loss: 0.006962, time:  5.443 s\n",
      "Step  66200 (epoch   76.92), loss: 0.004063, time:  5.441 s\n",
      "Step  66250 (epoch   76.97), loss: 0.006368, time:  5.461 s\n",
      "Step  66300 (epoch   77.03), loss: 0.007448, time:  5.447 s\n",
      "Step  66350 (epoch   77.09), loss: 0.009618, time:  5.446 s\n",
      "Step  66400 (epoch   77.15), loss: 0.005908, time:  5.430 s, error: 0.017630\n",
      "Step  66450 (epoch   77.21), loss: 0.006837, time:  8.616 s\n",
      "Step  66500 (epoch   77.26), loss: 0.003761, time:  5.445 s\n",
      "Step  66550 (epoch   77.32), loss: 0.005203, time:  5.433 s\n",
      "Step  66600 (epoch   77.38), loss: 0.005113, time:  5.445 s\n",
      "Step  66650 (epoch   77.44), loss: 0.005651, time:  5.443 s\n",
      "Step  66700 (epoch   77.50), loss: 0.004904, time:  5.417 s\n",
      "Step  66750 (epoch   77.55), loss: 0.004253, time:  5.432 s\n",
      "Step  66800 (epoch   77.61), loss: 0.005787, time:  5.414 s, error: 0.019182\n",
      "Step  66850 (epoch   77.67), loss: 0.003701, time:  8.628 s\n",
      "Step  66900 (epoch   77.73), loss: 0.006213, time:  5.433 s\n",
      "Step  66950 (epoch   77.79), loss: 0.005422, time:  5.443 s\n",
      "Step  67000 (epoch   77.85), loss: 0.005772, time:  5.444 s\n",
      "Step  67050 (epoch   77.90), loss: 0.007255, time:  5.443 s\n",
      "Step  67100 (epoch   77.96), loss: 0.006635, time:  5.430 s\n",
      "Step  67150 (epoch   78.02), loss: 0.005826, time:  5.430 s\n",
      "Step  67200 (epoch   78.08), loss: 0.007806, time:  5.436 s, error: 0.018638\n",
      "Step  67250 (epoch   78.14), loss: 0.005464, time:  8.623 s\n",
      "Step  67300 (epoch   78.19), loss: 0.003444, time:  5.417 s\n",
      "Step  67350 (epoch   78.25), loss: 0.005792, time:  5.458 s\n",
      "Step  67400 (epoch   78.31), loss: 0.004573, time:  5.420 s\n",
      "Step  67450 (epoch   78.37), loss: 0.004362, time:  5.439 s\n",
      "Step  67500 (epoch   78.43), loss: 0.005686, time:  5.429 s\n",
      "Step  67550 (epoch   78.48), loss: 0.004381, time:  5.437 s\n",
      "Step  67600 (epoch   78.54), loss: 0.005660, time:  5.432 s, error: 0.017812\n",
      "Step  67650 (epoch   78.60), loss: 0.005947, time:  8.612 s\n",
      "Step  67700 (epoch   78.66), loss: 0.003442, time:  5.451 s\n",
      "Step  67750 (epoch   78.72), loss: 0.005571, time:  5.428 s\n",
      "Step  67800 (epoch   78.77), loss: 0.006697, time:  5.422 s\n",
      "Step  67850 (epoch   78.83), loss: 0.005856, time:  5.432 s\n",
      "Step  67900 (epoch   78.89), loss: 0.006053, time:  5.455 s\n",
      "Step  67950 (epoch   78.95), loss: 0.006235, time:  5.457 s\n",
      "Step  68000 (epoch   79.01), loss: 0.004619, time:  5.442 s, error: 0.017670\n",
      "\n",
      "Time since beginning  : 7943.057 s\n",
      "\n",
      "Step  68050 (epoch   79.07), loss: 0.008047, time:  8.734 s\n",
      "Step  68100 (epoch   79.12), loss: 0.005471, time:  5.441 s\n",
      "Step  68150 (epoch   79.18), loss: 0.005445, time:  5.426 s\n",
      "Step  68200 (epoch   79.24), loss: 0.006098, time:  5.444 s\n",
      "Step  68250 (epoch   79.30), loss: 0.006399, time:  5.437 s\n",
      "Step  68300 (epoch   79.36), loss: 0.007304, time:  5.421 s\n",
      "Step  68350 (epoch   79.41), loss: 0.005330, time:  5.428 s\n",
      "Step  68400 (epoch   79.47), loss: 0.008306, time:  5.434 s, error: 0.018352\n",
      "Step  68450 (epoch   79.53), loss: 0.006393, time:  8.678 s\n",
      "Step  68500 (epoch   79.59), loss: 0.004458, time:  5.442 s\n",
      "Step  68550 (epoch   79.65), loss: 0.006318, time:  5.436 s\n",
      "Step  68600 (epoch   79.70), loss: 0.006258, time:  5.450 s\n",
      "Step  68650 (epoch   79.76), loss: 0.004661, time:  5.446 s\n",
      "Step  68700 (epoch   79.82), loss: 0.004724, time:  5.431 s\n",
      "Step  68750 (epoch   79.88), loss: 0.006039, time:  5.416 s\n",
      "Step  68800 (epoch   79.94), loss: 0.004382, time:  5.454 s, error: 0.018038\n",
      "Step  68850 (epoch   79.99), loss: 0.005114, time:  8.703 s\n",
      "Step  68900 (epoch   80.05), loss: 0.008997, time:  5.432 s\n",
      "Step  68950 (epoch   80.11), loss: 0.005381, time:  5.444 s\n",
      "Step  69000 (epoch   80.17), loss: 0.005466, time:  5.449 s\n",
      "Step  69050 (epoch   80.23), loss: 0.004691, time:  5.434 s\n",
      "Step  69100 (epoch   80.29), loss: 0.004966, time:  5.434 s\n",
      "Step  69150 (epoch   80.34), loss: 0.006313, time:  5.428 s\n",
      "Step  69200 (epoch   80.40), loss: 0.004906, time:  5.447 s, error: 0.019158\n",
      "Step  69250 (epoch   80.46), loss: 0.009101, time:  8.611 s\n",
      "Step  69300 (epoch   80.52), loss: 0.002819, time:  5.442 s\n",
      "Step  69350 (epoch   80.58), loss: 0.004409, time:  5.434 s\n",
      "Step  69400 (epoch   80.63), loss: 0.005407, time:  5.433 s\n",
      "Step  69450 (epoch   80.69), loss: 0.004884, time:  5.426 s\n",
      "Step  69500 (epoch   80.75), loss: 0.004591, time:  5.437 s\n",
      "Step  69550 (epoch   80.81), loss: 0.004273, time:  5.441 s\n",
      "Step  69600 (epoch   80.87), loss: 0.007531, time:  5.437 s, error: 0.018546\n",
      "Step  69650 (epoch   80.92), loss: 0.006762, time:  8.626 s\n",
      "Step  69700 (epoch   80.98), loss: 0.020377, time:  5.437 s\n",
      "Step  69750 (epoch   81.04), loss: 0.004952, time:  5.442 s\n",
      "Step  69800 (epoch   81.10), loss: 0.009434, time:  5.436 s\n",
      "Step  69850 (epoch   81.16), loss: 0.003634, time:  5.449 s\n",
      "Step  69900 (epoch   81.21), loss: 0.006234, time:  5.450 s\n",
      "Step  69950 (epoch   81.27), loss: 0.004552, time:  5.444 s\n",
      "Step  70000 (epoch   81.33), loss: 0.005574, time:  5.440 s, error: 0.017970\n",
      "\n",
      "Time since beginning  : 8176.710 s\n",
      "\n",
      "Step  70050 (epoch   81.39), loss: 0.004680, time:  8.719 s\n",
      "Step  70100 (epoch   81.45), loss: 0.003933, time:  5.429 s\n",
      "Step  70150 (epoch   81.51), loss: 0.006842, time:  5.435 s\n",
      "Step  70200 (epoch   81.56), loss: 0.005494, time:  5.431 s\n",
      "Step  70250 (epoch   81.62), loss: 0.006448, time:  5.441 s\n",
      "Step  70300 (epoch   81.68), loss: 0.005252, time:  5.455 s\n",
      "Step  70350 (epoch   81.74), loss: 0.006665, time:  5.452 s\n",
      "Step  70400 (epoch   81.80), loss: 0.004128, time:  5.442 s, error: 0.018418\n",
      "Step  70450 (epoch   81.85), loss: 0.007469, time:  8.625 s\n",
      "Step  70500 (epoch   81.91), loss: 0.004223, time:  5.454 s\n",
      "Step  70550 (epoch   81.97), loss: 0.009281, time:  5.423 s\n",
      "Step  70600 (epoch   82.03), loss: 0.005025, time:  5.427 s\n",
      "Step  70650 (epoch   82.09), loss: 0.005811, time:  5.451 s\n",
      "Step  70700 (epoch   82.14), loss: 0.005131, time:  5.440 s\n",
      "Step  70750 (epoch   82.20), loss: 0.005405, time:  5.439 s\n",
      "Step  70800 (epoch   82.26), loss: 0.006386, time:  5.451 s, error: 0.017608\n",
      "Step  70850 (epoch   82.32), loss: 0.005372, time:  8.653 s\n",
      "Step  70900 (epoch   82.38), loss: 0.002903, time:  5.430 s\n",
      "Step  70950 (epoch   82.43), loss: 0.006591, time:  5.447 s\n",
      "Step  71000 (epoch   82.49), loss: 0.004606, time:  5.459 s\n",
      "Step  71050 (epoch   82.55), loss: 0.004958, time:  5.454 s\n",
      "Step  71100 (epoch   82.61), loss: 0.007323, time:  5.452 s\n",
      "Step  71150 (epoch   82.67), loss: 0.003927, time:  5.442 s\n",
      "Step  71200 (epoch   82.73), loss: 0.007262, time:  5.445 s, error: 0.018532\n",
      "Step  71250 (epoch   82.78), loss: 0.005297, time:  8.614 s\n",
      "Step  71300 (epoch   82.84), loss: 0.007431, time:  5.436 s\n",
      "Step  71350 (epoch   82.90), loss: 0.005963, time:  5.453 s\n",
      "Step  71400 (epoch   82.96), loss: 0.005380, time:  5.437 s\n",
      "Step  71450 (epoch   83.02), loss: 0.004799, time:  5.438 s\n",
      "Step  71500 (epoch   83.07), loss: 0.008899, time:  5.428 s\n",
      "Step  71550 (epoch   83.13), loss: 0.003746, time:  5.435 s\n",
      "Step  71600 (epoch   83.19), loss: 0.003977, time:  5.437 s, error: 0.018082\n",
      "Step  71650 (epoch   83.25), loss: 0.004984, time:  8.668 s\n",
      "Step  71700 (epoch   83.31), loss: 0.004593, time:  5.457 s\n",
      "Step  71750 (epoch   83.36), loss: 0.005079, time:  5.447 s\n",
      "Step  71800 (epoch   83.42), loss: 0.007560, time:  5.432 s\n",
      "Step  71850 (epoch   83.48), loss: 0.003760, time:  5.447 s\n",
      "Step  71900 (epoch   83.54), loss: 0.006035, time:  5.448 s\n",
      "Step  71950 (epoch   83.60), loss: 0.004642, time:  5.450 s\n",
      "Step  72000 (epoch   83.65), loss: 0.005979, time:  5.438 s, error: 0.017786\n",
      "\n",
      "Time since beginning  : 8410.460 s\n",
      "\n",
      "Step  72050 (epoch   83.71), loss: 0.003677, time:  8.686 s\n",
      "Step  72100 (epoch   83.77), loss: 0.004830, time:  5.460 s\n",
      "Step  72150 (epoch   83.83), loss: 0.005438, time:  5.423 s\n",
      "Step  72200 (epoch   83.89), loss: 0.005068, time:  5.437 s\n",
      "Step  72250 (epoch   83.95), loss: 0.005457, time:  5.437 s\n",
      "Step  72300 (epoch   84.00), loss: 0.004609, time:  5.450 s\n",
      "Step  72350 (epoch   84.06), loss: 0.005609, time:  5.432 s\n",
      "Step  72400 (epoch   84.12), loss: 0.004809, time:  5.445 s, error: 0.017582\n",
      "Step  72450 (epoch   84.18), loss: 0.006668, time:  8.642 s\n",
      "Step  72500 (epoch   84.24), loss: 0.005253, time:  5.443 s\n",
      "Step  72550 (epoch   84.29), loss: 0.005731, time:  5.442 s\n",
      "Step  72600 (epoch   84.35), loss: 0.006885, time:  5.451 s\n",
      "Step  72650 (epoch   84.41), loss: 0.005036, time:  5.435 s\n",
      "Step  72700 (epoch   84.47), loss: 0.004057, time:  5.427 s\n",
      "Step  72750 (epoch   84.53), loss: 0.006425, time:  5.434 s\n",
      "Step  72800 (epoch   84.58), loss: 0.005462, time:  5.440 s, error: 0.017662\n",
      "Step  72850 (epoch   84.64), loss: 0.007444, time:  8.742 s\n",
      "Step  72900 (epoch   84.70), loss: 0.003822, time:  5.430 s\n",
      "Step  72950 (epoch   84.76), loss: 0.007849, time:  5.440 s\n",
      "Step  73000 (epoch   84.82), loss: 0.006447, time:  5.452 s\n",
      "Step  73050 (epoch   84.87), loss: 0.004758, time:  5.448 s\n",
      "Step  73100 (epoch   84.93), loss: 0.006056, time:  5.455 s\n",
      "Step  73150 (epoch   84.99), loss: 0.013640, time:  5.433 s\n",
      "Step  73200 (epoch   85.05), loss: 0.007284, time:  5.458 s, error: 0.017912\n",
      "Step  73250 (epoch   85.11), loss: 0.004897, time:  8.657 s\n",
      "Step  73300 (epoch   85.17), loss: 0.003404, time:  5.445 s\n",
      "Step  73350 (epoch   85.22), loss: 0.003601, time:  5.451 s\n",
      "Step  73400 (epoch   85.28), loss: 0.005210, time:  5.433 s\n",
      "Step  73450 (epoch   85.34), loss: 0.007091, time:  5.440 s\n",
      "Step  73500 (epoch   85.40), loss: 0.006159, time:  5.424 s\n",
      "Step  73550 (epoch   85.46), loss: 0.005282, time:  5.444 s\n",
      "Step  73600 (epoch   85.51), loss: 0.003414, time:  5.440 s, error: 0.018176\n",
      "Step  73650 (epoch   85.57), loss: 0.002877, time:  8.633 s\n",
      "Step  73700 (epoch   85.63), loss: 0.006368, time:  5.435 s\n",
      "Step  73750 (epoch   85.69), loss: 0.005718, time:  5.456 s\n",
      "Step  73800 (epoch   85.75), loss: 0.005447, time:  5.444 s\n",
      "Step  73850 (epoch   85.80), loss: 0.004112, time:  5.441 s\n",
      "Step  73900 (epoch   85.86), loss: 0.004862, time:  5.450 s\n",
      "Step  73950 (epoch   85.92), loss: 0.003941, time:  5.454 s\n",
      "Step  74000 (epoch   85.98), loss: 0.006500, time:  5.424 s, error: 0.018903\n",
      "\n",
      "Time since beginning  : 8644.273 s\n",
      "\n",
      "Step  74050 (epoch   86.04), loss: 0.006205, time:  8.703 s\n",
      "Step  74100 (epoch   86.09), loss: 0.005178, time:  5.419 s\n",
      "Step  74150 (epoch   86.15), loss: 0.005178, time:  5.437 s\n",
      "Step  74200 (epoch   86.21), loss: 0.006136, time:  5.439 s\n",
      "Step  74250 (epoch   86.27), loss: 0.005689, time:  5.423 s\n",
      "Step  74300 (epoch   86.33), loss: 0.005302, time:  5.439 s\n",
      "Step  74350 (epoch   86.39), loss: 0.005743, time:  5.433 s\n",
      "Step  74400 (epoch   86.44), loss: 0.005779, time:  5.417 s, error: 0.017381\n",
      "Step  74450 (epoch   86.50), loss: 0.006874, time:  8.633 s\n",
      "Step  74500 (epoch   86.56), loss: 0.006885, time:  5.447 s\n",
      "Step  74550 (epoch   86.62), loss: 0.005768, time:  5.448 s\n",
      "Step  74600 (epoch   86.68), loss: 0.003984, time:  5.431 s\n",
      "Step  74650 (epoch   86.73), loss: 0.005497, time:  5.442 s\n",
      "Step  74700 (epoch   86.79), loss: 0.005529, time:  5.422 s\n",
      "Step  74750 (epoch   86.85), loss: 0.006294, time:  5.429 s\n",
      "Step  74800 (epoch   86.91), loss: 0.006191, time:  5.424 s, error: 0.018347\n",
      "Step  74850 (epoch   86.97), loss: 0.005209, time:  8.667 s\n",
      "Step  74900 (epoch   87.02), loss: 0.004887, time:  5.439 s\n",
      "Step  74950 (epoch   87.08), loss: 0.007403, time:  5.428 s\n",
      "Step  75000 (epoch   87.14), loss: 0.005084, time:  5.440 s\n",
      "Step  75050 (epoch   87.20), loss: 0.006568, time:  5.433 s\n",
      "Step  75100 (epoch   87.26), loss: 0.003895, time:  5.452 s\n",
      "Step  75150 (epoch   87.31), loss: 0.005387, time:  5.457 s\n",
      "Step  75200 (epoch   87.37), loss: 0.005543, time:  5.426 s, error: 0.017404\n",
      "Step  75250 (epoch   87.43), loss: 0.007203, time:  8.628 s\n",
      "Step  75300 (epoch   87.49), loss: 0.003626, time:  5.434 s\n",
      "Step  75350 (epoch   87.55), loss: 0.004705, time:  5.440 s\n",
      "Step  75400 (epoch   87.61), loss: 0.007313, time:  5.458 s\n",
      "Step  75450 (epoch   87.66), loss: 0.005652, time:  5.430 s\n",
      "Step  75500 (epoch   87.72), loss: 0.006308, time:  5.427 s\n",
      "Step  75550 (epoch   87.78), loss: 0.004201, time:  5.440 s\n",
      "Step  75600 (epoch   87.84), loss: 0.005377, time:  5.419 s, error: 0.018082\n",
      "Step  75650 (epoch   87.90), loss: 0.004400, time:  8.651 s\n",
      "Step  75700 (epoch   87.95), loss: 0.006771, time:  5.440 s\n",
      "Step  75750 (epoch   88.01), loss: 0.005021, time:  5.454 s\n",
      "Step  75800 (epoch   88.07), loss: 0.005289, time:  5.436 s\n",
      "Step  75850 (epoch   88.13), loss: 0.003589, time:  5.440 s\n",
      "Step  75900 (epoch   88.19), loss: 0.004658, time:  5.433 s\n",
      "Step  75950 (epoch   88.24), loss: 0.005102, time:  5.424 s\n",
      "Step  76000 (epoch   88.30), loss: 0.005839, time:  5.445 s, error: 0.017801\n",
      "\n",
      "Time since beginning  : 8877.801 s\n",
      "\n",
      "Step  76050 (epoch   88.36), loss: 0.004739, time:  8.697 s\n",
      "Step  76100 (epoch   88.42), loss: 0.004738, time:  5.448 s\n",
      "Step  76150 (epoch   88.48), loss: 0.002979, time:  5.426 s\n",
      "Step  76200 (epoch   88.53), loss: 0.008893, time:  5.417 s\n",
      "Step  76250 (epoch   88.59), loss: 0.005370, time:  5.443 s\n",
      "Step  76300 (epoch   88.65), loss: 0.007020, time:  5.445 s\n",
      "Step  76350 (epoch   88.71), loss: 0.003957, time:  5.428 s\n",
      "Step  76400 (epoch   88.77), loss: 0.004982, time:  5.426 s, error: 0.017929\n",
      "Step  76450 (epoch   88.83), loss: 0.004890, time:  8.635 s\n",
      "Step  76500 (epoch   88.88), loss: 0.006695, time:  5.457 s\n",
      "Step  76550 (epoch   88.94), loss: 0.005655, time:  5.429 s\n",
      "Step  76600 (epoch   89.00), loss: 0.004087, time:  5.438 s\n",
      "Step  76650 (epoch   89.06), loss: 0.007121, time:  5.439 s\n",
      "Step  76700 (epoch   89.12), loss: 0.003941, time:  5.434 s\n",
      "Step  76750 (epoch   89.17), loss: 0.005467, time:  5.450 s\n",
      "Step  76800 (epoch   89.23), loss: 0.008170, time:  5.446 s, error: 0.019982\n",
      "Step  76850 (epoch   89.29), loss: 0.008445, time:  8.640 s\n",
      "Step  76900 (epoch   89.35), loss: 0.008719, time:  5.429 s\n",
      "Step  76950 (epoch   89.41), loss: 0.005790, time:  5.416 s\n",
      "Step  77000 (epoch   89.46), loss: 0.005110, time:  5.431 s\n",
      "Step  77050 (epoch   89.52), loss: 0.005784, time:  5.427 s\n",
      "Step  77100 (epoch   89.58), loss: 0.004267, time:  5.425 s\n",
      "Step  77150 (epoch   89.64), loss: 0.003645, time:  5.416 s\n",
      "Step  77200 (epoch   89.70), loss: 0.003949, time:  5.430 s, error: 0.017422\n",
      "Step  77250 (epoch   89.75), loss: 0.004548, time:  8.750 s\n",
      "Step  77300 (epoch   89.81), loss: 0.007340, time:  5.431 s\n",
      "Step  77350 (epoch   89.87), loss: 0.004273, time:  5.431 s\n",
      "Step  77400 (epoch   89.93), loss: 0.005090, time:  5.426 s\n",
      "Step  77450 (epoch   89.99), loss: 0.005843, time:  5.426 s\n",
      "Step  77500 (epoch   90.05), loss: 0.006660, time:  5.430 s\n",
      "Step  77550 (epoch   90.10), loss: 0.004703, time:  5.435 s\n",
      "Step  77600 (epoch   90.16), loss: 0.007815, time:  5.444 s, error: 0.017737\n",
      "Step  77650 (epoch   90.22), loss: 0.006785, time:  8.661 s\n",
      "Step  77700 (epoch   90.28), loss: 0.004153, time:  5.449 s\n",
      "Step  77750 (epoch   90.34), loss: 0.007198, time:  5.438 s\n",
      "Step  77800 (epoch   90.39), loss: 0.004547, time:  5.430 s\n",
      "Step  77850 (epoch   90.45), loss: 0.005205, time:  5.430 s\n",
      "Step  77900 (epoch   90.51), loss: 0.005753, time:  5.417 s\n",
      "Step  77950 (epoch   90.57), loss: 0.003422, time:  5.435 s\n",
      "Step  78000 (epoch   90.63), loss: 0.004260, time:  5.430 s, error: 0.017354\n",
      "\n",
      "Time since beginning  : 9111.340 s\n",
      "\n",
      "Step  78050 (epoch   90.68), loss: 0.005782, time:  8.701 s\n",
      "Step  78100 (epoch   90.74), loss: 0.007670, time:  5.440 s\n",
      "Step  78150 (epoch   90.80), loss: 0.004313, time:  5.435 s\n",
      "Step  78200 (epoch   90.86), loss: 0.005753, time:  5.426 s\n",
      "Step  78250 (epoch   90.92), loss: 0.004522, time:  5.424 s\n",
      "Step  78300 (epoch   90.97), loss: 0.005376, time:  5.442 s\n",
      "Step  78350 (epoch   91.03), loss: 0.005396, time:  5.441 s\n",
      "Step  78400 (epoch   91.09), loss: 0.004916, time:  5.442 s, error: 0.017501\n",
      "Step  78450 (epoch   91.15), loss: 0.003879, time:  8.637 s\n",
      "Step  78500 (epoch   91.21), loss: 0.003691, time:  5.410 s\n",
      "Step  78550 (epoch   91.27), loss: 0.003497, time:  5.430 s\n",
      "Step  78600 (epoch   91.32), loss: 0.006519, time:  5.442 s\n",
      "Step  78650 (epoch   91.38), loss: 0.005492, time:  5.428 s\n",
      "Step  78700 (epoch   91.44), loss: 0.006316, time:  5.438 s\n",
      "Step  78750 (epoch   91.50), loss: 0.004823, time:  5.422 s\n",
      "Step  78800 (epoch   91.56), loss: 0.004182, time:  5.433 s, error: 0.017752\n",
      "Step  78850 (epoch   91.61), loss: 0.004783, time:  8.678 s\n",
      "Step  78900 (epoch   91.67), loss: 0.006412, time:  5.437 s\n",
      "Step  78950 (epoch   91.73), loss: 0.004640, time:  5.437 s\n",
      "Step  79000 (epoch   91.79), loss: 0.006358, time:  5.426 s\n",
      "Step  79050 (epoch   91.85), loss: 0.005306, time:  5.436 s\n",
      "Step  79100 (epoch   91.90), loss: 0.005037, time:  5.440 s\n",
      "Step  79150 (epoch   91.96), loss: 0.005254, time:  5.450 s\n",
      "Step  79200 (epoch   92.02), loss: 0.004764, time:  5.440 s, error: 0.017309\n",
      "Step  79250 (epoch   92.08), loss: 0.005036, time:  8.658 s\n",
      "Step  79300 (epoch   92.14), loss: 0.004186, time:  5.435 s\n",
      "Step  79350 (epoch   92.19), loss: 0.006607, time:  5.438 s\n",
      "Step  79400 (epoch   92.25), loss: 0.004113, time:  5.446 s\n",
      "Step  79450 (epoch   92.31), loss: 0.003489, time:  5.442 s\n",
      "Step  79500 (epoch   92.37), loss: 0.004498, time:  5.436 s\n",
      "Step  79550 (epoch   92.43), loss: 0.009523, time:  5.447 s\n",
      "Step  79600 (epoch   92.49), loss: 0.006250, time:  5.446 s, error: 0.017644\n",
      "Step  79650 (epoch   92.54), loss: 0.004622, time:  8.624 s\n",
      "Step  79700 (epoch   92.60), loss: 0.005970, time:  5.425 s\n",
      "Step  79750 (epoch   92.66), loss: 0.005060, time:  5.437 s\n",
      "Step  79800 (epoch   92.72), loss: 0.006414, time:  5.447 s\n",
      "Step  79850 (epoch   92.78), loss: 0.004593, time:  5.435 s\n",
      "Step  79900 (epoch   92.83), loss: 0.008130, time:  5.435 s\n",
      "Step  79950 (epoch   92.89), loss: 0.007721, time:  5.433 s\n",
      "Step  80000 (epoch   92.95), loss: 0.005465, time:  5.420 s, error: 0.018234\n",
      "\n",
      "Time since beginning  : 9344.881 s\n",
      "\n",
      "Step  80050 (epoch   93.01), loss: 0.004575, time:  8.695 s\n",
      "Step  80100 (epoch   93.07), loss: 0.006697, time:  5.435 s\n",
      "Step  80150 (epoch   93.12), loss: 0.004704, time:  5.452 s\n",
      "Step  80200 (epoch   93.18), loss: 0.004499, time:  5.442 s\n",
      "Step  80250 (epoch   93.24), loss: 0.005563, time:  5.444 s\n",
      "Step  80300 (epoch   93.30), loss: 0.002930, time:  5.444 s\n",
      "Step  80350 (epoch   93.36), loss: 0.004412, time:  5.443 s\n",
      "Step  80400 (epoch   93.41), loss: 0.007664, time:  5.446 s, error: 0.018212\n",
      "Step  80450 (epoch   93.47), loss: 0.004944, time:  8.633 s\n",
      "Step  80500 (epoch   93.53), loss: 0.005128, time:  5.450 s\n",
      "Step  80550 (epoch   93.59), loss: 0.003788, time:  5.429 s\n",
      "Step  80600 (epoch   93.65), loss: 0.006605, time:  5.438 s\n",
      "Step  80650 (epoch   93.70), loss: 0.004879, time:  5.440 s\n",
      "Step  80700 (epoch   93.76), loss: 0.005076, time:  5.445 s\n",
      "Step  80750 (epoch   93.82), loss: 0.004319, time:  5.424 s\n",
      "Step  80800 (epoch   93.88), loss: 0.004335, time:  5.429 s, error: 0.017646\n",
      "Step  80850 (epoch   93.94), loss: 0.005087, time:  8.681 s\n",
      "Step  80900 (epoch   94.00), loss: 0.003116, time:  5.428 s\n",
      "Step  80950 (epoch   94.05), loss: 0.005250, time:  5.432 s\n",
      "Step  81000 (epoch   94.11), loss: 0.004879, time:  5.427 s\n",
      "Step  81050 (epoch   94.17), loss: 0.004290, time:  5.431 s\n",
      "Step  81100 (epoch   94.23), loss: 0.003178, time:  5.447 s\n",
      "Step  81150 (epoch   94.29), loss: 0.002679, time:  5.433 s\n",
      "Step  81200 (epoch   94.34), loss: 0.004741, time:  5.420 s, error: 0.017638\n",
      "Step  81250 (epoch   94.40), loss: 0.005661, time:  8.659 s\n",
      "Step  81300 (epoch   94.46), loss: 0.005323, time:  5.441 s\n",
      "Step  81350 (epoch   94.52), loss: 0.006135, time:  5.431 s\n",
      "Step  81400 (epoch   94.58), loss: 0.004880, time:  5.440 s\n",
      "Step  81450 (epoch   94.63), loss: 0.007062, time:  5.443 s\n",
      "Step  81500 (epoch   94.69), loss: 0.004672, time:  5.443 s\n",
      "Step  81550 (epoch   94.75), loss: 0.003635, time:  5.429 s\n",
      "Step  81600 (epoch   94.81), loss: 0.007076, time:  5.439 s, error: 0.017534\n",
      "Step  81650 (epoch   94.87), loss: 0.004523, time:  8.677 s\n",
      "Step  81700 (epoch   94.92), loss: 0.004823, time:  5.444 s\n",
      "Step  81750 (epoch   94.98), loss: 0.004960, time:  5.437 s\n",
      "Step  81800 (epoch   95.04), loss: 0.006568, time:  5.437 s\n",
      "Step  81850 (epoch   95.10), loss: 0.004915, time:  5.446 s\n",
      "Step  81900 (epoch   95.16), loss: 0.004371, time:  5.438 s\n",
      "Step  81950 (epoch   95.22), loss: 0.005907, time:  5.433 s\n",
      "Step  82000 (epoch   95.27), loss: 0.005302, time:  5.452 s, error: 0.018369\n",
      "\n",
      "Time since beginning  : 9578.546 s\n",
      "\n",
      "Step  82050 (epoch   95.33), loss: 0.006816, time:  8.688 s\n",
      "Step  82100 (epoch   95.39), loss: 0.005727, time:  5.427 s\n",
      "Step  82150 (epoch   95.45), loss: 0.006060, time:  5.427 s\n",
      "Step  82200 (epoch   95.51), loss: 0.006720, time:  5.437 s\n",
      "Step  82250 (epoch   95.56), loss: 0.003418, time:  5.439 s\n",
      "Step  82300 (epoch   95.62), loss: 0.004689, time:  5.416 s\n",
      "Step  82350 (epoch   95.68), loss: 0.003567, time:  5.447 s\n",
      "Step  82400 (epoch   95.74), loss: 0.005812, time:  5.438 s, error: 0.017536\n",
      "Step  82450 (epoch   95.80), loss: 0.006453, time:  8.651 s\n",
      "Step  82500 (epoch   95.85), loss: 0.004567, time:  5.432 s\n",
      "Step  82550 (epoch   95.91), loss: 0.005418, time:  5.442 s\n",
      "Step  82600 (epoch   95.97), loss: 0.005485, time:  5.443 s\n",
      "Step  82650 (epoch   96.03), loss: 0.005032, time:  5.435 s\n",
      "Step  82700 (epoch   96.09), loss: 0.003769, time:  5.463 s\n",
      "Step  82750 (epoch   96.14), loss: 0.003630, time:  5.436 s\n",
      "Step  82800 (epoch   96.20), loss: 0.004384, time:  5.434 s, error: 0.017450\n",
      "Step  82850 (epoch   96.26), loss: 0.005596, time:  8.635 s\n",
      "Step  82900 (epoch   96.32), loss: 0.006450, time:  5.435 s\n",
      "Step  82950 (epoch   96.38), loss: 0.004405, time:  5.437 s\n",
      "Step  83000 (epoch   96.44), loss: 0.005312, time:  5.435 s\n",
      "Step  83050 (epoch   96.49), loss: 0.005841, time:  5.422 s\n",
      "Step  83100 (epoch   96.55), loss: 0.004087, time:  5.433 s\n",
      "Step  83150 (epoch   96.61), loss: 0.004573, time:  5.426 s\n",
      "Step  83200 (epoch   96.67), loss: 0.005008, time:  5.428 s, error: 0.017205\n",
      "Step  83250 (epoch   96.73), loss: 0.007180, time:  8.640 s\n",
      "Step  83300 (epoch   96.78), loss: 0.005463, time:  5.428 s\n",
      "Step  83350 (epoch   96.84), loss: 0.003649, time:  5.434 s\n",
      "Step  83400 (epoch   96.90), loss: 0.006537, time:  5.438 s\n",
      "Step  83450 (epoch   96.96), loss: 0.004564, time:  5.449 s\n",
      "Step  83500 (epoch   97.02), loss: 0.006995, time:  5.437 s\n",
      "Step  83550 (epoch   97.07), loss: 0.007693, time:  5.433 s\n",
      "Step  83600 (epoch   97.13), loss: 0.003384, time:  5.422 s, error: 0.017430\n",
      "Step  83650 (epoch   97.19), loss: 0.004982, time:  8.639 s\n",
      "Step  83700 (epoch   97.25), loss: 0.005146, time:  5.431 s\n",
      "Step  83750 (epoch   97.31), loss: 0.004886, time:  5.443 s\n",
      "Step  83800 (epoch   97.36), loss: 0.004126, time:  5.447 s\n",
      "Step  83850 (epoch   97.42), loss: 0.005618, time:  5.427 s\n",
      "Step  83900 (epoch   97.48), loss: 0.006150, time:  5.426 s\n",
      "Step  83950 (epoch   97.54), loss: 0.005136, time:  5.443 s\n",
      "Step  84000 (epoch   97.60), loss: 0.005015, time:  5.436 s, error: 0.017774\n",
      "\n",
      "Time since beginning  : 9812.043 s\n",
      "\n",
      "Step  84050 (epoch   97.66), loss: 0.004082, time:  8.722 s\n",
      "Step  84100 (epoch   97.71), loss: 0.006584, time:  5.436 s\n",
      "Step  84150 (epoch   97.77), loss: 0.005796, time:  5.449 s\n",
      "Step  84200 (epoch   97.83), loss: 0.005047, time:  5.433 s\n",
      "Step  84250 (epoch   97.89), loss: 0.006126, time:  5.444 s\n",
      "Step  84300 (epoch   97.95), loss: 0.006079, time:  5.422 s\n",
      "Step  84350 (epoch   98.00), loss: 0.004585, time:  5.429 s\n",
      "Step  84400 (epoch   98.06), loss: 0.005248, time:  5.424 s, error: 0.017637\n",
      "Step  84450 (epoch   98.12), loss: 0.004230, time:  8.665 s\n",
      "Step  84500 (epoch   98.18), loss: 0.006156, time:  5.450 s\n",
      "Step  84550 (epoch   98.24), loss: 0.005153, time:  5.439 s\n",
      "Step  84600 (epoch   98.29), loss: 0.006284, time:  5.424 s\n",
      "Step  84650 (epoch   98.35), loss: 0.004766, time:  5.434 s\n",
      "Step  84700 (epoch   98.41), loss: 0.007887, time:  5.429 s\n",
      "Step  84750 (epoch   98.47), loss: 0.007871, time:  5.441 s\n",
      "Step  84800 (epoch   98.53), loss: 0.005067, time:  5.421 s, error: 0.018329\n",
      "Step  84850 (epoch   98.58), loss: 0.004897, time:  8.617 s\n",
      "Step  84900 (epoch   98.64), loss: 0.005923, time:  5.450 s\n",
      "Step  84950 (epoch   98.70), loss: 0.005563, time:  5.420 s\n",
      "Step  85000 (epoch   98.76), loss: 0.006386, time:  5.439 s\n",
      "Step  85050 (epoch   98.82), loss: 0.004409, time:  5.428 s\n",
      "Step  85100 (epoch   98.88), loss: 0.003934, time:  5.433 s\n",
      "Step  85150 (epoch   98.93), loss: 0.004737, time:  5.436 s\n",
      "Step  85200 (epoch   98.99), loss: 0.005455, time:  5.451 s, error: 0.017331\n",
      "Step  85250 (epoch   99.05), loss: 0.005030, time:  8.651 s\n",
      "Step  85300 (epoch   99.11), loss: 0.003608, time:  5.427 s\n",
      "Step  85350 (epoch   99.17), loss: 0.004213, time:  5.439 s\n",
      "Step  85400 (epoch   99.22), loss: 0.006018, time:  5.424 s\n",
      "Step  85450 (epoch   99.28), loss: 0.003482, time:  5.436 s\n",
      "Step  85500 (epoch   99.34), loss: 0.006400, time:  5.443 s\n",
      "Step  85550 (epoch   99.40), loss: 0.005361, time:  5.432 s\n",
      "Step  85600 (epoch   99.46), loss: 0.006650, time:  5.426 s, error: 0.019561\n",
      "Step  85650 (epoch   99.51), loss: 0.006960, time:  8.751 s\n",
      "Step  85700 (epoch   99.57), loss: 0.006040, time:  5.432 s\n",
      "Step  85750 (epoch   99.63), loss: 0.007477, time:  5.430 s\n",
      "Step  85800 (epoch   99.69), loss: 0.002926, time:  5.433 s\n",
      "Step  85850 (epoch   99.75), loss: 0.004999, time:  5.436 s\n",
      "Step  85900 (epoch   99.80), loss: 0.006557, time:  5.420 s\n",
      "Step  85950 (epoch   99.86), loss: 0.006104, time:  5.417 s\n",
      "Step  86000 (epoch   99.92), loss: 0.004147, time:  5.445 s, error: 0.017323\n",
      "\n",
      "Time since beginning  : 10045.606 s\n",
      "\n",
      "Step  86050 (epoch   99.98), loss: 0.004854, time:  8.692 s\n",
      "Step  86100 (epoch  100.04), loss: 0.004246, time:  5.425 s\n",
      "Step  86150 (epoch  100.10), loss: 0.003307, time:  5.424 s\n",
      "Step  86200 (epoch  100.15), loss: 0.004210, time:  5.426 s\n",
      "Step  86250 (epoch  100.21), loss: 0.004610, time:  5.435 s\n",
      "Step  86300 (epoch  100.27), loss: 0.004501, time:  5.435 s\n",
      "Step  86350 (epoch  100.33), loss: 0.005326, time:  5.434 s\n",
      "Step  86400 (epoch  100.39), loss: 0.003772, time:  5.437 s, error: 0.017416\n",
      "Step  86450 (epoch  100.44), loss: 0.003292, time:  8.631 s\n",
      "Step  86500 (epoch  100.50), loss: 0.004793, time:  5.424 s\n",
      "Step  86550 (epoch  100.56), loss: 0.004783, time:  5.433 s\n",
      "Step  86600 (epoch  100.62), loss: 0.004482, time:  5.433 s\n",
      "Step  86650 (epoch  100.68), loss: 0.005350, time:  5.414 s\n",
      "Step  86700 (epoch  100.73), loss: 0.004951, time:  5.431 s\n",
      "Step  86750 (epoch  100.79), loss: 0.005257, time:  5.451 s\n",
      "Step  86800 (epoch  100.85), loss: 0.004267, time:  5.450 s, error: 0.017710\n",
      "Step  86850 (epoch  100.91), loss: 0.003605, time:  8.635 s\n",
      "Step  86900 (epoch  100.97), loss: 0.004624, time:  5.436 s\n",
      "Step  86950 (epoch  101.02), loss: 0.004170, time:  5.421 s\n",
      "Step  87000 (epoch  101.08), loss: 0.003747, time:  5.430 s\n",
      "Step  87050 (epoch  101.14), loss: 0.005051, time:  5.438 s\n",
      "Step  87100 (epoch  101.20), loss: 0.004164, time:  5.459 s\n",
      "Step  87150 (epoch  101.26), loss: 0.004115, time:  5.436 s\n",
      "Step  87200 (epoch  101.32), loss: 0.005747, time:  5.451 s, error: 0.018862\n",
      "Step  87250 (epoch  101.37), loss: 0.005554, time:  8.666 s\n",
      "Step  87300 (epoch  101.43), loss: 0.003242, time:  5.445 s\n",
      "Step  87350 (epoch  101.49), loss: 0.007044, time:  5.440 s\n",
      "Step  87400 (epoch  101.55), loss: 0.004231, time:  5.432 s\n",
      "Step  87450 (epoch  101.61), loss: 0.005189, time:  5.451 s\n",
      "Step  87500 (epoch  101.66), loss: 0.003755, time:  5.441 s\n",
      "Step  87550 (epoch  101.72), loss: 0.004175, time:  5.442 s\n",
      "Step  87600 (epoch  101.78), loss: 0.005125, time:  5.440 s, error: 0.017204\n",
      "Step  87650 (epoch  101.84), loss: 0.005757, time:  8.640 s\n",
      "Step  87700 (epoch  101.90), loss: 0.005253, time:  5.436 s\n",
      "Step  87750 (epoch  101.95), loss: 0.004585, time:  5.445 s\n",
      "Step  87800 (epoch  102.01), loss: 0.005283, time:  5.452 s\n",
      "Step  87850 (epoch  102.07), loss: 0.005076, time:  5.440 s\n",
      "Step  87900 (epoch  102.13), loss: 0.006574, time:  5.440 s\n",
      "Step  87950 (epoch  102.19), loss: 0.006753, time:  5.441 s\n",
      "Step  88000 (epoch  102.24), loss: 0.005492, time:  5.431 s, error: 0.017131\n",
      "\n",
      "Time since beginning  : 10279.184 s\n",
      "\n",
      "Step  88050 (epoch  102.30), loss: 0.004040, time:  8.701 s\n",
      "Step  88100 (epoch  102.36), loss: 0.004416, time:  5.447 s\n",
      "Step  88150 (epoch  102.42), loss: 0.009450, time:  5.433 s\n",
      "Step  88200 (epoch  102.48), loss: 0.007732, time:  5.456 s\n",
      "Step  88250 (epoch  102.54), loss: 0.007042, time:  5.437 s\n",
      "Step  88300 (epoch  102.59), loss: 0.004796, time:  5.440 s\n",
      "Step  88350 (epoch  102.65), loss: 0.004868, time:  5.432 s\n",
      "Step  88400 (epoch  102.71), loss: 0.004184, time:  5.436 s, error: 0.017497\n",
      "Step  88450 (epoch  102.77), loss: 0.005170, time:  8.681 s\n",
      "Step  88500 (epoch  102.83), loss: 0.005947, time:  5.439 s\n",
      "Step  88550 (epoch  102.88), loss: 0.003810, time:  5.459 s\n",
      "Step  88600 (epoch  102.94), loss: 0.005757, time:  5.454 s\n",
      "Step  88650 (epoch  103.00), loss: 0.004826, time:  5.439 s\n",
      "Step  88700 (epoch  103.06), loss: 0.005893, time:  5.451 s\n",
      "Step  88750 (epoch  103.12), loss: 0.003189, time:  5.438 s\n",
      "Step  88800 (epoch  103.17), loss: 0.006187, time:  5.447 s, error: 0.017474\n",
      "Step  88850 (epoch  103.23), loss: 0.003974, time:  8.635 s\n",
      "Step  88900 (epoch  103.29), loss: 0.004714, time:  5.463 s\n",
      "Step  88950 (epoch  103.35), loss: 0.005604, time:  5.436 s\n",
      "Step  89000 (epoch  103.41), loss: 0.005140, time:  5.431 s\n",
      "Step  89050 (epoch  103.46), loss: 0.007291, time:  5.439 s\n",
      "Step  89100 (epoch  103.52), loss: 0.003676, time:  5.436 s\n",
      "Step  89150 (epoch  103.58), loss: 0.004489, time:  5.423 s\n",
      "Step  89200 (epoch  103.64), loss: 0.007011, time:  5.428 s, error: 0.019010\n",
      "Step  89250 (epoch  103.70), loss: 0.005268, time:  8.621 s\n",
      "Step  89300 (epoch  103.76), loss: 0.005118, time:  5.450 s\n",
      "Step  89350 (epoch  103.81), loss: 0.005118, time:  5.424 s\n",
      "Step  89400 (epoch  103.87), loss: 0.004646, time:  5.431 s\n",
      "Step  89450 (epoch  103.93), loss: 0.004496, time:  5.450 s\n",
      "Step  89500 (epoch  103.99), loss: 0.003733, time:  5.435 s\n",
      "Step  89550 (epoch  104.05), loss: 0.005599, time:  5.430 s\n",
      "Step  89600 (epoch  104.10), loss: 0.004211, time:  5.435 s, error: 0.017080\n",
      "Step  89650 (epoch  104.16), loss: 0.003390, time:  8.644 s\n",
      "Step  89700 (epoch  104.22), loss: 0.005743, time:  5.436 s\n",
      "Step  89750 (epoch  104.28), loss: 0.004098, time:  5.422 s\n",
      "Step  89800 (epoch  104.34), loss: 0.006206, time:  5.423 s\n",
      "Step  89850 (epoch  104.39), loss: 0.004956, time:  5.419 s\n",
      "Step  89900 (epoch  104.45), loss: 0.003865, time:  5.435 s\n",
      "Step  89950 (epoch  104.51), loss: 0.004581, time:  5.440 s\n",
      "Step  90000 (epoch  104.57), loss: 0.003763, time:  5.424 s, error: 0.017655\n",
      "\n",
      "Time since beginning  : 10512.918 s\n",
      "\n",
      "Step  90050 (epoch  104.63), loss: 0.006294, time:  8.832 s\n",
      "Step  90100 (epoch  104.68), loss: 0.003276, time:  5.426 s\n",
      "Step  90150 (epoch  104.74), loss: 0.004859, time:  5.442 s\n",
      "Step  90200 (epoch  104.80), loss: 0.003802, time:  5.444 s\n",
      "Step  90250 (epoch  104.86), loss: 0.003737, time:  5.445 s\n",
      "Step  90300 (epoch  104.92), loss: 0.003973, time:  5.434 s\n",
      "Step  90350 (epoch  104.98), loss: 0.006147, time:  5.436 s\n",
      "Step  90400 (epoch  105.03), loss: 0.006750, time:  5.458 s, error: 0.017220\n",
      "Step  90450 (epoch  105.09), loss: 0.004771, time:  8.652 s\n",
      "Step  90500 (epoch  105.15), loss: 0.005783, time:  5.432 s\n",
      "Step  90550 (epoch  105.21), loss: 0.006091, time:  5.429 s\n",
      "Step  90600 (epoch  105.27), loss: 0.005543, time:  5.448 s\n",
      "Step  90650 (epoch  105.32), loss: 0.003974, time:  5.435 s\n",
      "Step  90700 (epoch  105.38), loss: 0.003949, time:  5.444 s\n",
      "Step  90750 (epoch  105.44), loss: 0.004146, time:  5.442 s\n",
      "Step  90800 (epoch  105.50), loss: 0.003973, time:  5.438 s, error: 0.017018\n",
      "Step  90850 (epoch  105.56), loss: 0.004480, time:  8.614 s\n",
      "Step  90900 (epoch  105.61), loss: 0.005605, time:  5.424 s\n",
      "Step  90950 (epoch  105.67), loss: 0.007033, time:  5.418 s\n",
      "Step  91000 (epoch  105.73), loss: 0.005587, time:  5.429 s\n",
      "Step  91050 (epoch  105.79), loss: 0.005242, time:  5.441 s\n",
      "Step  91100 (epoch  105.85), loss: 0.004612, time:  5.455 s\n",
      "Step  91150 (epoch  105.90), loss: 0.004525, time:  5.452 s\n",
      "Step  91200 (epoch  105.96), loss: 0.004998, time:  5.447 s, error: 0.016971\n",
      "Step  91250 (epoch  106.02), loss: 0.004756, time:  8.670 s\n",
      "Step  91300 (epoch  106.08), loss: 0.003358, time:  5.451 s\n",
      "Step  91350 (epoch  106.14), loss: 0.005032, time:  5.442 s\n",
      "Step  91400 (epoch  106.20), loss: 0.004096, time:  5.421 s\n",
      "Step  91450 (epoch  106.25), loss: 0.004007, time:  5.420 s\n",
      "Step  91500 (epoch  106.31), loss: 0.004990, time:  5.452 s\n",
      "Step  91550 (epoch  106.37), loss: 0.005966, time:  5.430 s\n",
      "Step  91600 (epoch  106.43), loss: 0.004715, time:  5.427 s, error: 0.016887\n",
      "Step  91650 (epoch  106.49), loss: 0.003797, time:  8.639 s\n",
      "Step  91700 (epoch  106.54), loss: 0.005246, time:  5.437 s\n",
      "Step  91750 (epoch  106.60), loss: 0.005975, time:  5.428 s\n",
      "Step  91800 (epoch  106.66), loss: 0.003546, time:  5.436 s\n",
      "Step  91850 (epoch  106.72), loss: 0.004408, time:  5.454 s\n",
      "Step  91900 (epoch  106.78), loss: 0.003734, time:  5.426 s\n",
      "Step  91950 (epoch  106.83), loss: 0.004239, time:  5.432 s\n",
      "Step  92000 (epoch  106.89), loss: 0.003865, time:  5.430 s, error: 0.017462\n",
      "\n",
      "Time since beginning  : 10746.491 s\n",
      "\n",
      "Step  92050 (epoch  106.95), loss: 0.004779, time:  8.692 s\n",
      "Step  92100 (epoch  107.01), loss: 0.005192, time:  5.435 s\n",
      "Step  92150 (epoch  107.07), loss: 0.004460, time:  5.443 s\n",
      "Step  92200 (epoch  107.12), loss: 0.003164, time:  5.451 s\n",
      "Step  92250 (epoch  107.18), loss: 0.004831, time:  5.434 s\n",
      "Step  92300 (epoch  107.24), loss: 0.004907, time:  5.429 s\n",
      "Step  92350 (epoch  107.30), loss: 0.003654, time:  5.418 s\n",
      "Step  92400 (epoch  107.36), loss: 0.005077, time:  5.420 s, error: 0.017392\n",
      "Step  92450 (epoch  107.42), loss: 0.006282, time:  8.630 s\n",
      "Step  92500 (epoch  107.47), loss: 0.004781, time:  5.442 s\n",
      "Step  92550 (epoch  107.53), loss: 0.004424, time:  5.430 s\n",
      "Step  92600 (epoch  107.59), loss: 0.005864, time:  5.450 s\n",
      "Step  92650 (epoch  107.65), loss: 0.004894, time:  5.446 s\n",
      "Step  92700 (epoch  107.71), loss: 0.007398, time:  5.424 s\n",
      "Step  92750 (epoch  107.76), loss: 0.005069, time:  5.422 s\n",
      "Step  92800 (epoch  107.82), loss: 0.005838, time:  5.437 s, error: 0.018195\n",
      "Step  92850 (epoch  107.88), loss: 0.004823, time:  8.625 s\n",
      "Step  92900 (epoch  107.94), loss: 0.004314, time:  5.433 s\n",
      "Step  92950 (epoch  108.00), loss: 0.005523, time:  5.446 s\n",
      "Step  93000 (epoch  108.05), loss: 0.005871, time:  5.439 s\n",
      "Step  93050 (epoch  108.11), loss: 0.004732, time:  5.422 s\n",
      "Step  93100 (epoch  108.17), loss: 0.005755, time:  5.428 s\n",
      "Step  93150 (epoch  108.23), loss: 0.003441, time:  5.432 s\n",
      "Step  93200 (epoch  108.29), loss: 0.004835, time:  5.439 s, error: 0.017432\n",
      "Step  93250 (epoch  108.34), loss: 0.004669, time:  8.610 s\n",
      "Step  93300 (epoch  108.40), loss: 0.004757, time:  5.456 s\n",
      "Step  93350 (epoch  108.46), loss: 0.004338, time:  5.426 s\n",
      "Step  93400 (epoch  108.52), loss: 0.004464, time:  5.428 s\n",
      "Step  93450 (epoch  108.58), loss: 0.004546, time:  5.423 s\n",
      "Step  93500 (epoch  108.64), loss: 0.004638, time:  5.442 s\n",
      "Step  93550 (epoch  108.69), loss: 0.004503, time:  5.446 s\n",
      "Step  93600 (epoch  108.75), loss: 0.003742, time:  5.442 s, error: 0.017105\n",
      "Step  93650 (epoch  108.81), loss: 0.004157, time:  8.661 s\n",
      "Step  93700 (epoch  108.87), loss: 0.004591, time:  5.430 s\n",
      "Step  93750 (epoch  108.93), loss: 0.003893, time:  5.438 s\n",
      "Step  93800 (epoch  108.98), loss: 0.003557, time:  5.438 s\n",
      "Step  93850 (epoch  109.04), loss: 0.004343, time:  5.435 s\n",
      "Step  93900 (epoch  109.10), loss: 0.004529, time:  5.417 s\n",
      "Step  93950 (epoch  109.16), loss: 0.005156, time:  5.427 s\n",
      "Step  94000 (epoch  109.22), loss: 0.004500, time:  5.430 s, error: 0.017334\n",
      "\n",
      "Time since beginning  : 10979.933 s\n",
      "\n",
      "Step  94050 (epoch  109.27), loss: 0.004234, time:  8.741 s\n",
      "Step  94100 (epoch  109.33), loss: 0.004979, time:  5.439 s\n",
      "Step  94150 (epoch  109.39), loss: 0.005090, time:  5.430 s\n",
      "Step  94200 (epoch  109.45), loss: 0.005517, time:  5.444 s\n",
      "Step  94250 (epoch  109.51), loss: 0.005028, time:  5.423 s\n",
      "Step  94300 (epoch  109.56), loss: 0.004454, time:  5.434 s\n",
      "Step  94350 (epoch  109.62), loss: 0.005638, time:  5.442 s\n",
      "Step  94400 (epoch  109.68), loss: 0.003692, time:  5.439 s, error: 0.017027\n",
      "Step  94450 (epoch  109.74), loss: 0.005743, time:  8.689 s\n",
      "Step  94500 (epoch  109.80), loss: 0.003959, time:  5.418 s\n",
      "Step  94550 (epoch  109.85), loss: 0.004527, time:  5.430 s\n",
      "Step  94600 (epoch  109.91), loss: 0.004748, time:  5.434 s\n",
      "Step  94650 (epoch  109.97), loss: 0.004034, time:  5.436 s\n",
      "Step  94700 (epoch  110.03), loss: 0.006002, time:  5.427 s\n",
      "Step  94750 (epoch  110.09), loss: 0.005336, time:  5.426 s\n",
      "Step  94800 (epoch  110.15), loss: 0.004901, time:  5.449 s, error: 0.017404\n",
      "Step  94850 (epoch  110.20), loss: 0.004508, time:  8.611 s\n",
      "Step  94900 (epoch  110.26), loss: 0.004547, time:  5.418 s\n",
      "Step  94950 (epoch  110.32), loss: 0.005742, time:  5.433 s\n",
      "Step  95000 (epoch  110.38), loss: 0.004978, time:  5.447 s\n",
      "Step  95050 (epoch  110.44), loss: 0.006525, time:  5.432 s\n",
      "Step  95100 (epoch  110.49), loss: 0.003688, time:  5.424 s\n",
      "Step  95150 (epoch  110.55), loss: 0.004016, time:  5.439 s\n",
      "Step  95200 (epoch  110.61), loss: 0.006623, time:  5.427 s, error: 0.017638\n",
      "Step  95250 (epoch  110.67), loss: 0.006113, time:  8.627 s\n",
      "Step  95300 (epoch  110.73), loss: 0.006242, time:  5.443 s\n",
      "Step  95350 (epoch  110.78), loss: 0.003375, time:  5.423 s\n",
      "Step  95400 (epoch  110.84), loss: 0.004570, time:  5.431 s\n",
      "Step  95450 (epoch  110.90), loss: 0.004367, time:  5.436 s\n",
      "Step  95500 (epoch  110.96), loss: 0.003808, time:  5.438 s\n",
      "Step  95550 (epoch  111.02), loss: 0.005554, time:  5.430 s\n",
      "Step  95600 (epoch  111.07), loss: 0.004195, time:  5.434 s, error: 0.017048\n",
      "Step  95650 (epoch  111.13), loss: 0.003718, time:  8.641 s\n",
      "Step  95700 (epoch  111.19), loss: 0.004516, time:  5.438 s\n",
      "Step  95750 (epoch  111.25), loss: 0.003451, time:  5.431 s\n",
      "Step  95800 (epoch  111.31), loss: 0.004930, time:  5.423 s\n",
      "Step  95850 (epoch  111.37), loss: 0.004214, time:  5.433 s\n",
      "Step  95900 (epoch  111.42), loss: 0.006222, time:  5.432 s\n",
      "Step  95950 (epoch  111.48), loss: 0.003706, time:  5.442 s\n",
      "Step  96000 (epoch  111.54), loss: 0.004413, time:  5.433 s, error: 0.016926\n",
      "\n",
      "Time since beginning  : 11213.374 s\n",
      "\n",
      "Step  96050 (epoch  111.60), loss: 0.004944, time:  8.689 s\n",
      "Step  96100 (epoch  111.66), loss: 0.003111, time:  5.438 s\n",
      "Step  96150 (epoch  111.71), loss: 0.004459, time:  5.436 s\n",
      "Step  96200 (epoch  111.77), loss: 0.004616, time:  5.419 s\n",
      "Step  96250 (epoch  111.83), loss: 0.006950, time:  5.448 s\n",
      "Step  96300 (epoch  111.89), loss: 0.005161, time:  5.426 s\n",
      "Step  96350 (epoch  111.95), loss: 0.017056, time:  5.435 s\n",
      "Step  96400 (epoch  112.00), loss: 0.005562, time:  5.449 s, error: 0.018053\n",
      "Step  96450 (epoch  112.06), loss: 0.007254, time:  8.667 s\n",
      "Step  96500 (epoch  112.12), loss: 0.004008, time:  5.436 s\n",
      "Step  96550 (epoch  112.18), loss: 0.004583, time:  5.422 s\n",
      "Step  96600 (epoch  112.24), loss: 0.005519, time:  5.450 s\n",
      "Step  96650 (epoch  112.29), loss: 0.005301, time:  5.426 s\n",
      "Step  96700 (epoch  112.35), loss: 0.004575, time:  5.412 s\n",
      "Step  96750 (epoch  112.41), loss: 0.003887, time:  5.426 s\n",
      "Step  96800 (epoch  112.47), loss: 0.005708, time:  5.425 s, error: 0.017362\n",
      "Step  96850 (epoch  112.53), loss: 0.005003, time:  8.622 s\n",
      "Step  96900 (epoch  112.59), loss: 0.004491, time:  5.427 s\n",
      "Step  96950 (epoch  112.64), loss: 0.005330, time:  5.431 s\n",
      "Step  97000 (epoch  112.70), loss: 0.005995, time:  5.428 s\n",
      "Step  97050 (epoch  112.76), loss: 0.003607, time:  5.443 s\n",
      "Step  97100 (epoch  112.82), loss: 0.005452, time:  5.443 s\n",
      "Step  97150 (epoch  112.88), loss: 0.004030, time:  5.434 s\n",
      "Step  97200 (epoch  112.93), loss: 0.006677, time:  5.430 s, error: 0.017175\n",
      "Step  97250 (epoch  112.99), loss: 0.003925, time:  8.637 s\n",
      "Step  97300 (epoch  113.05), loss: 0.005287, time:  5.440 s\n",
      "Step  97350 (epoch  113.11), loss: 0.003456, time:  5.455 s\n",
      "Step  97400 (epoch  113.17), loss: 0.004439, time:  5.433 s\n",
      "Step  97450 (epoch  113.22), loss: 0.005058, time:  5.425 s\n",
      "Step  97500 (epoch  113.28), loss: 0.003859, time:  5.435 s\n",
      "Step  97550 (epoch  113.34), loss: 0.002700, time:  5.442 s\n",
      "Step  97600 (epoch  113.40), loss: 0.005738, time:  5.445 s, error: 0.017879\n",
      "Step  97650 (epoch  113.46), loss: 0.004544, time:  8.627 s\n",
      "Step  97700 (epoch  113.51), loss: 0.005961, time:  5.436 s\n",
      "Step  97750 (epoch  113.57), loss: 0.003510, time:  5.436 s\n",
      "Step  97800 (epoch  113.63), loss: 0.005259, time:  5.438 s\n",
      "Step  97850 (epoch  113.69), loss: 0.005246, time:  5.431 s\n",
      "Step  97900 (epoch  113.75), loss: 0.004190, time:  5.421 s\n",
      "Step  97950 (epoch  113.81), loss: 0.007380, time:  5.437 s\n",
      "Step  98000 (epoch  113.86), loss: 0.004360, time:  5.427 s, error: 0.017042\n",
      "\n",
      "Time since beginning  : 11446.782 s\n",
      "\n",
      "Step  98050 (epoch  113.92), loss: 0.004825, time:  8.705 s\n",
      "Step  98100 (epoch  113.98), loss: 0.005130, time:  5.433 s\n",
      "Step  98150 (epoch  114.04), loss: 0.004014, time:  5.429 s\n",
      "Step  98200 (epoch  114.10), loss: 0.003480, time:  5.430 s\n",
      "Step  98250 (epoch  114.15), loss: 0.003160, time:  5.429 s\n",
      "Step  98300 (epoch  114.21), loss: 0.004988, time:  5.424 s\n",
      "Step  98350 (epoch  114.27), loss: 0.003872, time:  5.431 s\n",
      "Step  98400 (epoch  114.33), loss: 0.004794, time:  5.435 s, error: 0.016913\n",
      "Step  98450 (epoch  114.39), loss: 0.003936, time:  8.723 s\n",
      "Step  98500 (epoch  114.44), loss: 0.004167, time:  5.436 s\n",
      "Step  98550 (epoch  114.50), loss: 0.005245, time:  5.433 s\n",
      "Step  98600 (epoch  114.56), loss: 0.004809, time:  5.428 s\n",
      "Step  98650 (epoch  114.62), loss: 0.003677, time:  5.433 s\n",
      "Step  98700 (epoch  114.68), loss: 0.003189, time:  5.429 s\n",
      "Step  98750 (epoch  114.73), loss: 0.004272, time:  5.439 s\n",
      "Step  98800 (epoch  114.79), loss: 0.004064, time:  5.449 s, error: 0.017681\n",
      "Step  98850 (epoch  114.85), loss: 0.004941, time:  8.654 s\n",
      "Step  98900 (epoch  114.91), loss: 0.003674, time:  5.442 s\n",
      "Step  98950 (epoch  114.97), loss: 0.004355, time:  5.451 s\n",
      "Step  99000 (epoch  115.03), loss: 0.006027, time:  5.433 s\n",
      "Step  99050 (epoch  115.08), loss: 0.003785, time:  5.429 s\n",
      "Step  99100 (epoch  115.14), loss: 0.008983, time:  5.418 s\n",
      "Step  99150 (epoch  115.20), loss: 0.004521, time:  5.434 s\n",
      "Step  99200 (epoch  115.26), loss: 0.005334, time:  5.448 s, error: 0.018708\n",
      "Step  99250 (epoch  115.32), loss: 0.005117, time:  8.636 s\n",
      "Step  99300 (epoch  115.37), loss: 0.004572, time:  5.439 s\n",
      "Step  99350 (epoch  115.43), loss: 0.003605, time:  5.431 s\n",
      "Step  99400 (epoch  115.49), loss: 0.004653, time:  5.434 s\n",
      "Step  99450 (epoch  115.55), loss: 0.004807, time:  5.432 s\n",
      "Step  99500 (epoch  115.61), loss: 0.003555, time:  5.422 s\n",
      "Step  99550 (epoch  115.66), loss: 0.003763, time:  5.436 s\n",
      "Step  99600 (epoch  115.72), loss: 0.008670, time:  5.427 s, error: 0.019847\n",
      "Step  99650 (epoch  115.78), loss: 0.004346, time:  8.673 s\n",
      "Step  99700 (epoch  115.84), loss: 0.003857, time:  5.429 s\n",
      "Step  99750 (epoch  115.90), loss: 0.004554, time:  5.429 s\n",
      "Step  99800 (epoch  115.95), loss: 0.013028, time:  5.424 s\n",
      "Step  99850 (epoch  116.01), loss: 0.005296, time:  5.433 s\n",
      "Step  99900 (epoch  116.07), loss: 0.004681, time:  5.442 s\n",
      "Step  99950 (epoch  116.13), loss: 0.003159, time:  5.423 s\n",
      "Step 100000 (epoch  116.19), loss: 0.004380, time:  5.426 s, error: 0.017972\n",
      "\n",
      "Time since beginning  : 11680.346 s\n",
      "\n",
      "Step 100050 (epoch  116.25), loss: 0.004300, time:  8.725 s\n",
      "Step 100100 (epoch  116.30), loss: 0.004759, time:  5.457 s\n",
      "Step 100150 (epoch  116.36), loss: 0.004266, time:  5.434 s\n",
      "Step 100200 (epoch  116.42), loss: 0.004393, time:  5.422 s\n",
      "Step 100250 (epoch  116.48), loss: 0.003207, time:  5.441 s\n",
      "Step 100300 (epoch  116.54), loss: 0.003933, time:  5.423 s\n",
      "Step 100350 (epoch  116.59), loss: 0.004231, time:  5.419 s\n",
      "Step 100400 (epoch  116.65), loss: 0.004146, time:  5.447 s, error: 0.017675\n",
      "Step 100450 (epoch  116.71), loss: 0.006558, time:  8.654 s\n",
      "Step 100500 (epoch  116.77), loss: 0.003715, time:  5.435 s\n",
      "Step 100550 (epoch  116.83), loss: 0.004285, time:  5.429 s\n",
      "Step 100600 (epoch  116.88), loss: 0.003856, time:  5.434 s\n",
      "Step 100650 (epoch  116.94), loss: 0.004150, time:  5.434 s\n",
      "Step 100700 (epoch  117.00), loss: 0.004600, time:  5.431 s\n",
      "Step 100750 (epoch  117.06), loss: 0.003692, time:  5.443 s\n",
      "Step 100800 (epoch  117.12), loss: 0.003373, time:  5.427 s, error: 0.017176\n",
      "Step 100850 (epoch  117.17), loss: 0.005475, time:  8.625 s\n",
      "Step 100900 (epoch  117.23), loss: 0.005204, time:  5.439 s\n",
      "Step 100950 (epoch  117.29), loss: 0.005792, time:  5.445 s\n",
      "Step 101000 (epoch  117.35), loss: 0.004619, time:  5.440 s\n",
      "Step 101050 (epoch  117.41), loss: 0.006048, time:  5.425 s\n",
      "Step 101100 (epoch  117.47), loss: 0.004291, time:  5.425 s\n",
      "Step 101150 (epoch  117.52), loss: 0.005958, time:  5.425 s\n",
      "Step 101200 (epoch  117.58), loss: 0.004631, time:  5.435 s, error: 0.017063\n",
      "Step 101250 (epoch  117.64), loss: 0.006461, time:  8.655 s\n",
      "Step 101300 (epoch  117.70), loss: 0.004347, time:  5.423 s\n",
      "Step 101350 (epoch  117.76), loss: 0.005027, time:  5.438 s\n",
      "Step 101400 (epoch  117.81), loss: 0.005153, time:  5.430 s\n",
      "Step 101450 (epoch  117.87), loss: 0.005012, time:  5.421 s\n",
      "Step 101500 (epoch  117.93), loss: 0.003839, time:  5.429 s\n",
      "Step 101550 (epoch  117.99), loss: 0.006198, time:  5.429 s\n",
      "Step 101600 (epoch  118.05), loss: 0.006362, time:  5.426 s, error: 0.018307\n",
      "Step 101650 (epoch  118.10), loss: 0.003559, time:  8.615 s\n",
      "Step 101700 (epoch  118.16), loss: 0.005594, time:  5.432 s\n",
      "Step 101750 (epoch  118.22), loss: 0.002972, time:  5.428 s\n",
      "Step 101800 (epoch  118.28), loss: 0.003887, time:  5.442 s\n",
      "Step 101850 (epoch  118.34), loss: 0.005074, time:  5.430 s\n",
      "Step 101900 (epoch  118.39), loss: 0.004384, time:  5.426 s\n",
      "Step 101950 (epoch  118.45), loss: 0.003890, time:  5.426 s\n",
      "Step 102000 (epoch  118.51), loss: 0.004607, time:  5.442 s, error: 0.017067\n",
      "\n",
      "Time since beginning  : 11913.735 s\n",
      "\n",
      "Step 102050 (epoch  118.57), loss: 0.004274, time:  8.689 s\n",
      "Step 102100 (epoch  118.63), loss: 0.005187, time:  5.443 s\n",
      "Step 102150 (epoch  118.69), loss: 0.006087, time:  5.433 s\n",
      "Step 102200 (epoch  118.74), loss: 0.004704, time:  5.427 s\n",
      "Step 102250 (epoch  118.80), loss: 0.003995, time:  5.442 s\n",
      "Step 102300 (epoch  118.86), loss: 0.004147, time:  5.441 s\n",
      "Step 102350 (epoch  118.92), loss: 0.004929, time:  5.442 s\n",
      "Step 102400 (epoch  118.98), loss: 0.004342, time:  5.439 s, error: 0.017792\n",
      "Step 102450 (epoch  119.03), loss: 0.006710, time:  8.654 s\n",
      "Step 102500 (epoch  119.09), loss: 0.004125, time:  5.422 s\n",
      "Step 102550 (epoch  119.15), loss: 0.004119, time:  5.417 s\n",
      "Step 102600 (epoch  119.21), loss: 0.003384, time:  5.438 s\n",
      "Step 102650 (epoch  119.27), loss: 0.003732, time:  5.439 s\n",
      "Step 102700 (epoch  119.32), loss: 0.002813, time:  5.438 s\n",
      "Step 102750 (epoch  119.38), loss: 0.004185, time:  5.419 s\n",
      "Step 102800 (epoch  119.44), loss: 0.002876, time:  5.437 s, error: 0.016779\n",
      "Step 102850 (epoch  119.50), loss: 0.006304, time:  8.737 s\n",
      "Step 102900 (epoch  119.56), loss: 0.005397, time:  5.421 s\n",
      "Step 102950 (epoch  119.61), loss: 0.004829, time:  5.423 s\n",
      "Step 103000 (epoch  119.67), loss: 0.002871, time:  5.434 s\n",
      "Step 103050 (epoch  119.73), loss: 0.007033, time:  5.418 s\n",
      "Step 103100 (epoch  119.79), loss: 0.003975, time:  5.444 s\n",
      "Step 103150 (epoch  119.85), loss: 0.004270, time:  5.434 s\n",
      "Step 103200 (epoch  119.91), loss: 0.004679, time:  5.451 s, error: 0.017349\n",
      "Step 103250 (epoch  119.96), loss: 0.003430, time:  8.619 s\n",
      "Step 103300 (epoch  120.02), loss: 0.004850, time:  5.443 s\n",
      "Step 103350 (epoch  120.08), loss: 0.003057, time:  5.425 s\n",
      "Step 103400 (epoch  120.14), loss: 0.004817, time:  5.432 s\n",
      "Step 103450 (epoch  120.20), loss: 0.006211, time:  5.436 s\n",
      "Step 103500 (epoch  120.25), loss: 0.007408, time:  5.431 s\n",
      "Step 103550 (epoch  120.31), loss: 0.004447, time:  5.436 s\n",
      "Step 103600 (epoch  120.37), loss: 0.003598, time:  5.427 s, error: 0.016742\n",
      "Step 103650 (epoch  120.43), loss: 0.003531, time:  8.629 s\n",
      "Step 103700 (epoch  120.49), loss: 0.003853, time:  5.430 s\n",
      "Step 103750 (epoch  120.54), loss: 0.003425, time:  5.426 s\n",
      "Step 103800 (epoch  120.60), loss: 0.003249, time:  5.437 s\n",
      "Step 103850 (epoch  120.66), loss: 0.002926, time:  5.432 s\n",
      "Step 103900 (epoch  120.72), loss: 0.004710, time:  5.436 s\n",
      "Step 103950 (epoch  120.78), loss: 0.005253, time:  5.423 s\n",
      "Step 104000 (epoch  120.83), loss: 0.003244, time:  5.419 s, error: 0.016974\n",
      "\n",
      "Time since beginning  : 12147.197 s\n",
      "\n",
      "Step 104050 (epoch  120.89), loss: 0.004443, time:  8.700 s\n",
      "Step 104100 (epoch  120.95), loss: 0.004058, time:  5.430 s\n",
      "Step 104150 (epoch  121.01), loss: 0.004592, time:  5.438 s\n",
      "Step 104200 (epoch  121.07), loss: 0.005340, time:  5.449 s\n",
      "Step 104250 (epoch  121.13), loss: 0.005333, time:  5.426 s\n",
      "Step 104300 (epoch  121.18), loss: 0.005448, time:  5.433 s\n",
      "Step 104350 (epoch  121.24), loss: 0.003607, time:  5.417 s\n",
      "Step 104400 (epoch  121.30), loss: 0.003166, time:  5.423 s, error: 0.017132\n",
      "Step 104450 (epoch  121.36), loss: 0.003811, time:  8.606 s\n",
      "Step 104500 (epoch  121.42), loss: 0.004071, time:  5.440 s\n",
      "Step 104550 (epoch  121.47), loss: 0.004013, time:  5.424 s\n",
      "Step 104600 (epoch  121.53), loss: 0.004442, time:  5.442 s\n",
      "Step 104650 (epoch  121.59), loss: 0.003638, time:  5.448 s\n",
      "Step 104700 (epoch  121.65), loss: 0.004820, time:  5.431 s\n",
      "Step 104750 (epoch  121.71), loss: 0.004790, time:  5.449 s\n",
      "Step 104800 (epoch  121.76), loss: 0.003768, time:  5.441 s, error: 0.016830\n",
      "Step 104850 (epoch  121.82), loss: 0.005797, time:  8.658 s\n",
      "Step 104900 (epoch  121.88), loss: 0.003329, time:  5.453 s\n",
      "Step 104950 (epoch  121.94), loss: 0.004685, time:  5.447 s\n",
      "Step 105000 (epoch  122.00), loss: 0.004593, time:  5.448 s\n",
      "Step 105050 (epoch  122.05), loss: 0.006492, time:  5.437 s\n",
      "Step 105100 (epoch  122.11), loss: 0.003653, time:  5.427 s\n",
      "Step 105150 (epoch  122.17), loss: 0.004811, time:  5.427 s\n",
      "Step 105200 (epoch  122.23), loss: 0.003174, time:  5.434 s, error: 0.017241\n",
      "Step 105250 (epoch  122.29), loss: 0.005175, time:  8.620 s\n",
      "Step 105300 (epoch  122.35), loss: 0.004050, time:  5.443 s\n",
      "Step 105350 (epoch  122.40), loss: 0.005305, time:  5.442 s\n",
      "Step 105400 (epoch  122.46), loss: 0.004168, time:  5.439 s\n",
      "Step 105450 (epoch  122.52), loss: 0.004038, time:  5.423 s\n",
      "Step 105500 (epoch  122.58), loss: 0.003465, time:  5.427 s\n",
      "Step 105550 (epoch  122.64), loss: 0.005908, time:  5.428 s\n",
      "Step 105600 (epoch  122.69), loss: 0.004147, time:  5.450 s, error: 0.017303\n",
      "Step 105650 (epoch  122.75), loss: 0.005092, time:  8.665 s\n",
      "Step 105700 (epoch  122.81), loss: 0.003603, time:  5.428 s\n",
      "Step 105750 (epoch  122.87), loss: 0.004681, time:  5.446 s\n",
      "Step 105800 (epoch  122.93), loss: 0.004603, time:  5.431 s\n",
      "Step 105850 (epoch  122.98), loss: 0.003808, time:  5.430 s\n",
      "Step 105900 (epoch  123.04), loss: 0.004520, time:  5.421 s\n",
      "Step 105950 (epoch  123.10), loss: 0.003620, time:  5.434 s\n",
      "Step 106000 (epoch  123.16), loss: 0.005211, time:  5.441 s, error: 0.017237\n",
      "\n",
      "Time since beginning  : 12380.728 s\n",
      "\n",
      "Step 106050 (epoch  123.22), loss: 0.005237, time:  8.747 s\n",
      "Step 106100 (epoch  123.27), loss: 0.004488, time:  5.456 s\n",
      "Step 106150 (epoch  123.33), loss: 0.003682, time:  5.434 s\n",
      "Step 106200 (epoch  123.39), loss: 0.005280, time:  5.435 s\n",
      "Step 106250 (epoch  123.45), loss: 0.003232, time:  5.438 s\n",
      "Step 106300 (epoch  123.51), loss: 0.005040, time:  5.438 s\n",
      "Step 106350 (epoch  123.57), loss: 0.004595, time:  5.452 s\n",
      "Step 106400 (epoch  123.62), loss: 0.004487, time:  5.440 s, error: 0.016947\n",
      "Step 106450 (epoch  123.68), loss: 0.006602, time:  8.632 s\n",
      "Step 106500 (epoch  123.74), loss: 0.003441, time:  5.429 s\n",
      "Step 106550 (epoch  123.80), loss: 0.005508, time:  5.432 s\n",
      "Step 106600 (epoch  123.86), loss: 0.007518, time:  5.429 s\n",
      "Step 106650 (epoch  123.91), loss: 0.004464, time:  5.453 s\n",
      "Step 106700 (epoch  123.97), loss: 0.004770, time:  5.435 s\n",
      "Step 106750 (epoch  124.03), loss: 0.005590, time:  5.439 s\n",
      "Step 106800 (epoch  124.09), loss: 0.003294, time:  5.433 s, error: 0.017165\n",
      "Step 106850 (epoch  124.15), loss: 0.005234, time:  8.692 s\n",
      "Step 106900 (epoch  124.20), loss: 0.005146, time:  5.437 s\n",
      "Step 106950 (epoch  124.26), loss: 0.002794, time:  5.458 s\n",
      "Step 107000 (epoch  124.32), loss: 0.003360, time:  5.442 s\n",
      "Step 107050 (epoch  124.38), loss: 0.006390, time:  5.435 s\n",
      "Step 107100 (epoch  124.44), loss: 0.005379, time:  5.446 s\n",
      "Step 107150 (epoch  124.49), loss: 0.003811, time:  5.424 s\n",
      "Step 107200 (epoch  124.55), loss: 0.004195, time:  5.442 s, error: 0.016803\n",
      "Step 107250 (epoch  124.61), loss: 0.005404, time:  8.672 s\n",
      "Step 107300 (epoch  124.67), loss: 0.004413, time:  5.436 s\n",
      "Step 107350 (epoch  124.73), loss: 0.006342, time:  5.424 s\n",
      "Step 107400 (epoch  124.79), loss: 0.004374, time:  5.453 s\n",
      "Step 107450 (epoch  124.84), loss: 0.003770, time:  5.442 s\n",
      "Step 107500 (epoch  124.90), loss: 0.004514, time:  5.447 s\n",
      "Step 107550 (epoch  124.96), loss: 0.004327, time:  5.429 s\n",
      "Step 107600 (epoch  125.02), loss: 0.004588, time:  5.453 s, error: 0.016692\n",
      "Step 107650 (epoch  125.08), loss: 0.005148, time:  8.680 s\n",
      "Step 107700 (epoch  125.13), loss: 0.004230, time:  5.436 s\n",
      "Step 107750 (epoch  125.19), loss: 0.003282, time:  5.425 s\n",
      "Step 107800 (epoch  125.25), loss: 0.004072, time:  5.452 s\n",
      "Step 107850 (epoch  125.31), loss: 0.004439, time:  5.445 s\n",
      "Step 107900 (epoch  125.37), loss: 0.005662, time:  5.432 s\n",
      "Step 107950 (epoch  125.42), loss: 0.002586, time:  5.454 s\n",
      "Step 108000 (epoch  125.48), loss: 0.003736, time:  5.427 s, error: 0.016766\n",
      "\n",
      "Time since beginning  : 12614.502 s\n",
      "\n",
      "Step 108050 (epoch  125.54), loss: 0.004401, time:  8.715 s\n",
      "Step 108100 (epoch  125.60), loss: 0.005102, time:  5.432 s\n",
      "Step 108150 (epoch  125.66), loss: 0.003308, time:  5.445 s\n",
      "Step 108200 (epoch  125.71), loss: 0.004415, time:  5.438 s\n",
      "Step 108250 (epoch  125.77), loss: 0.005661, time:  5.427 s\n",
      "Step 108300 (epoch  125.83), loss: 0.003909, time:  5.431 s\n",
      "Step 108350 (epoch  125.89), loss: 0.004769, time:  5.425 s\n",
      "Step 108400 (epoch  125.95), loss: 0.004827, time:  5.428 s, error: 0.017113\n",
      "Step 108450 (epoch  126.01), loss: 0.005346, time:  8.627 s\n",
      "Step 108500 (epoch  126.06), loss: 0.003504, time:  5.420 s\n",
      "Step 108550 (epoch  126.12), loss: 0.003003, time:  5.426 s\n",
      "Step 108600 (epoch  126.18), loss: 0.005212, time:  5.437 s\n",
      "Step 108650 (epoch  126.24), loss: 0.002447, time:  5.452 s\n",
      "Step 108700 (epoch  126.30), loss: 0.005148, time:  5.442 s\n",
      "Step 108750 (epoch  126.35), loss: 0.005347, time:  5.436 s\n",
      "Step 108800 (epoch  126.41), loss: 0.005892, time:  5.425 s, error: 0.016737\n",
      "Step 108850 (epoch  126.47), loss: 0.003456, time:  8.633 s\n",
      "Step 108900 (epoch  126.53), loss: 0.003552, time:  5.425 s\n",
      "Step 108950 (epoch  126.59), loss: 0.004074, time:  5.429 s\n",
      "Step 109000 (epoch  126.64), loss: 0.003214, time:  5.435 s\n",
      "Step 109050 (epoch  126.70), loss: 0.004931, time:  5.445 s\n",
      "Step 109100 (epoch  126.76), loss: 0.005337, time:  5.419 s\n",
      "Step 109150 (epoch  126.82), loss: 0.003222, time:  5.432 s\n",
      "Step 109200 (epoch  126.88), loss: 0.004819, time:  5.421 s, error: 0.016900\n",
      "Step 109250 (epoch  126.93), loss: 0.005754, time:  8.612 s\n",
      "Step 109300 (epoch  126.99), loss: 0.003764, time:  5.432 s\n",
      "Step 109350 (epoch  127.05), loss: 0.003459, time:  5.446 s\n",
      "Step 109400 (epoch  127.11), loss: 0.004700, time:  5.446 s\n",
      "Step 109450 (epoch  127.17), loss: 0.004491, time:  5.446 s\n",
      "Step 109500 (epoch  127.22), loss: 0.004217, time:  5.432 s\n",
      "Step 109550 (epoch  127.28), loss: 0.004920, time:  5.440 s\n",
      "Step 109600 (epoch  127.34), loss: 0.002938, time:  5.430 s, error: 0.016762\n",
      "Step 109650 (epoch  127.40), loss: 0.006089, time:  8.634 s\n",
      "Step 109700 (epoch  127.46), loss: 0.004335, time:  5.415 s\n",
      "Step 109750 (epoch  127.52), loss: 0.003746, time:  5.447 s\n",
      "Step 109800 (epoch  127.57), loss: 0.003183, time:  5.437 s\n",
      "Step 109850 (epoch  127.63), loss: 0.004048, time:  5.435 s\n",
      "Step 109900 (epoch  127.69), loss: 0.005947, time:  5.440 s\n",
      "Step 109950 (epoch  127.75), loss: 0.004621, time:  5.433 s\n",
      "Step 110000 (epoch  127.81), loss: 0.004234, time:  5.434 s, error: 0.016874\n",
      "\n",
      "Time since beginning  : 12847.899 s\n",
      "\n",
      "Step 110050 (epoch  127.86), loss: 0.003424, time:  8.694 s\n",
      "Step 110100 (epoch  127.92), loss: 0.003409, time:  5.440 s\n",
      "Step 110150 (epoch  127.98), loss: 0.004209, time:  5.442 s\n",
      "Step 110200 (epoch  128.04), loss: 0.007306, time:  5.431 s\n",
      "Step 110250 (epoch  128.10), loss: 0.002824, time:  5.424 s\n",
      "Step 110300 (epoch  128.15), loss: 0.004754, time:  5.430 s\n",
      "Step 110350 (epoch  128.21), loss: 0.005384, time:  5.432 s\n",
      "Step 110400 (epoch  128.27), loss: 0.004389, time:  5.440 s, error: 0.018105\n",
      "Step 110450 (epoch  128.33), loss: 0.003002, time:  8.622 s\n",
      "Step 110500 (epoch  128.39), loss: 0.004009, time:  5.455 s\n",
      "Step 110550 (epoch  128.44), loss: 0.004603, time:  5.418 s\n",
      "Step 110600 (epoch  128.50), loss: 0.003826, time:  5.429 s\n",
      "Step 110650 (epoch  128.56), loss: 0.004531, time:  5.436 s\n",
      "Step 110700 (epoch  128.62), loss: 0.004790, time:  5.435 s\n",
      "Step 110750 (epoch  128.68), loss: 0.005516, time:  5.425 s\n",
      "Step 110800 (epoch  128.74), loss: 0.004897, time:  5.432 s, error: 0.016747\n",
      "Step 110850 (epoch  128.79), loss: 0.004696, time:  8.655 s\n",
      "Step 110900 (epoch  128.85), loss: 0.005523, time:  5.431 s\n",
      "Step 110950 (epoch  128.91), loss: 0.005363, time:  5.432 s\n",
      "Step 111000 (epoch  128.97), loss: 0.005942, time:  5.435 s\n",
      "Step 111050 (epoch  129.03), loss: 0.004517, time:  5.433 s\n",
      "Step 111100 (epoch  129.08), loss: 0.004551, time:  5.429 s\n",
      "Step 111150 (epoch  129.14), loss: 0.005380, time:  5.441 s\n",
      "Step 111200 (epoch  129.20), loss: 0.005713, time:  5.427 s, error: 0.018039\n",
      "Step 111250 (epoch  129.26), loss: 0.002199, time:  8.736 s\n",
      "Step 111300 (epoch  129.32), loss: 0.003589, time:  5.427 s\n",
      "Step 111350 (epoch  129.37), loss: 0.004461, time:  5.430 s\n",
      "Step 111400 (epoch  129.43), loss: 0.004043, time:  5.435 s\n",
      "Step 111450 (epoch  129.49), loss: 0.002980, time:  5.442 s\n",
      "Step 111500 (epoch  129.55), loss: 0.004819, time:  5.441 s\n",
      "Step 111550 (epoch  129.61), loss: 0.003414, time:  5.431 s\n",
      "Step 111600 (epoch  129.66), loss: 0.005338, time:  5.459 s, error: 0.017274\n",
      "Step 111650 (epoch  129.72), loss: 0.006897, time:  8.647 s\n",
      "Step 111700 (epoch  129.78), loss: 0.003746, time:  5.440 s\n",
      "Step 111750 (epoch  129.84), loss: 0.003628, time:  5.435 s\n",
      "Step 111800 (epoch  129.90), loss: 0.004599, time:  5.429 s\n",
      "Step 111850 (epoch  129.96), loss: 0.004701, time:  5.424 s\n",
      "Step 111900 (epoch  130.01), loss: 0.004106, time:  5.440 s\n",
      "Step 111950 (epoch  130.07), loss: 0.004098, time:  5.448 s\n",
      "Step 112000 (epoch  130.13), loss: 0.003586, time:  5.451 s, error: 0.017012\n",
      "\n",
      "Time since beginning  : 13081.486 s\n",
      "\n",
      "Step 112050 (epoch  130.19), loss: 0.005009, time:  8.700 s\n",
      "Step 112100 (epoch  130.25), loss: 0.003840, time:  5.427 s\n",
      "Step 112150 (epoch  130.30), loss: 0.004195, time:  5.416 s\n",
      "Step 112200 (epoch  130.36), loss: 0.003568, time:  5.433 s\n",
      "Step 112250 (epoch  130.42), loss: 0.004167, time:  5.432 s\n",
      "Step 112300 (epoch  130.48), loss: 0.005707, time:  5.448 s\n",
      "Step 112350 (epoch  130.54), loss: 0.003874, time:  5.437 s\n",
      "Step 112400 (epoch  130.59), loss: 0.004653, time:  5.431 s, error: 0.017727\n",
      "Step 112450 (epoch  130.65), loss: 0.002406, time:  8.644 s\n",
      "Step 112500 (epoch  130.71), loss: 0.004211, time:  5.431 s\n",
      "Step 112550 (epoch  130.77), loss: 0.005280, time:  5.431 s\n",
      "Step 112600 (epoch  130.83), loss: 0.004966, time:  5.440 s\n",
      "Step 112650 (epoch  130.88), loss: 0.003528, time:  5.434 s\n",
      "Step 112700 (epoch  130.94), loss: 0.004940, time:  5.454 s\n",
      "Step 112750 (epoch  131.00), loss: 0.003260, time:  5.440 s\n",
      "Step 112800 (epoch  131.06), loss: 0.004830, time:  5.440 s, error: 0.017302\n",
      "Step 112850 (epoch  131.12), loss: 0.003574, time:  8.646 s\n",
      "Step 112900 (epoch  131.18), loss: 0.003368, time:  5.433 s\n",
      "Step 112950 (epoch  131.23), loss: 0.004330, time:  5.445 s\n",
      "Step 113000 (epoch  131.29), loss: 0.003324, time:  5.434 s\n",
      "Step 113050 (epoch  131.35), loss: 0.003460, time:  5.457 s\n",
      "Step 113100 (epoch  131.41), loss: 0.002064, time:  5.450 s\n",
      "Step 113150 (epoch  131.47), loss: 0.004137, time:  5.432 s\n",
      "Step 113200 (epoch  131.52), loss: 0.002563, time:  5.437 s, error: 0.016564\n",
      "Step 113250 (epoch  131.58), loss: 0.003330, time:  8.644 s\n",
      "Step 113300 (epoch  131.64), loss: 0.003580, time:  5.432 s\n",
      "Step 113350 (epoch  131.70), loss: 0.005006, time:  5.434 s\n",
      "Step 113400 (epoch  131.76), loss: 0.003856, time:  5.438 s\n",
      "Step 113450 (epoch  131.81), loss: 0.003742, time:  5.436 s\n",
      "Step 113500 (epoch  131.87), loss: 0.004802, time:  5.430 s\n",
      "Step 113550 (epoch  131.93), loss: 0.004709, time:  5.437 s\n",
      "Step 113600 (epoch  131.99), loss: 0.003888, time:  5.444 s, error: 0.016728\n",
      "Step 113650 (epoch  132.05), loss: 0.003392, time:  8.666 s\n",
      "Step 113700 (epoch  132.10), loss: 0.003664, time:  5.432 s\n",
      "Step 113750 (epoch  132.16), loss: 0.003773, time:  5.439 s\n",
      "Step 113800 (epoch  132.22), loss: 0.003354, time:  5.459 s\n",
      "Step 113850 (epoch  132.28), loss: 0.004199, time:  5.430 s\n",
      "Step 113900 (epoch  132.34), loss: 0.003180, time:  5.433 s\n",
      "Step 113950 (epoch  132.40), loss: 0.004681, time:  5.446 s\n",
      "Step 114000 (epoch  132.45), loss: 0.004118, time:  5.438 s, error: 0.016599\n",
      "\n",
      "Time since beginning  : 13315.118 s\n",
      "\n",
      "Step 114050 (epoch  132.51), loss: 0.004240, time:  8.721 s\n",
      "Step 114100 (epoch  132.57), loss: 0.006601, time:  5.417 s\n",
      "Step 114150 (epoch  132.63), loss: 0.004401, time:  5.437 s\n",
      "Step 114200 (epoch  132.69), loss: 0.003297, time:  5.432 s\n",
      "Step 114250 (epoch  132.74), loss: 0.004244, time:  5.431 s\n",
      "Step 114300 (epoch  132.80), loss: 0.005629, time:  5.434 s\n",
      "Step 114350 (epoch  132.86), loss: 0.004564, time:  5.437 s\n",
      "Step 114400 (epoch  132.92), loss: 0.004315, time:  5.430 s, error: 0.016882\n",
      "Step 114450 (epoch  132.98), loss: 0.005558, time:  8.634 s\n",
      "Step 114500 (epoch  133.03), loss: 0.002885, time:  5.455 s\n",
      "Step 114550 (epoch  133.09), loss: 0.005116, time:  5.425 s\n",
      "Step 114600 (epoch  133.15), loss: 0.005054, time:  5.436 s\n",
      "Step 114650 (epoch  133.21), loss: 0.004925, time:  5.430 s\n",
      "Step 114700 (epoch  133.27), loss: 0.003318, time:  5.431 s\n",
      "Step 114750 (epoch  133.32), loss: 0.003933, time:  5.418 s\n",
      "Step 114800 (epoch  133.38), loss: 0.006658, time:  5.445 s, error: 0.016741\n",
      "Step 114850 (epoch  133.44), loss: 0.004055, time:  8.636 s\n",
      "Step 114900 (epoch  133.50), loss: 0.005767, time:  5.443 s\n",
      "Step 114950 (epoch  133.56), loss: 0.004471, time:  5.416 s\n",
      "Step 115000 (epoch  133.62), loss: 0.003978, time:  5.441 s\n",
      "Step 115050 (epoch  133.67), loss: 0.004263, time:  5.423 s\n",
      "Step 115100 (epoch  133.73), loss: 0.003983, time:  5.438 s\n",
      "Step 115150 (epoch  133.79), loss: 0.005440, time:  5.442 s\n",
      "Step 115200 (epoch  133.85), loss: 0.004380, time:  5.430 s, error: 0.017144\n",
      "Step 115250 (epoch  133.91), loss: 0.004526, time:  8.642 s\n",
      "Step 115300 (epoch  133.96), loss: 0.004922, time:  5.457 s\n",
      "Step 115350 (epoch  134.02), loss: 0.003644, time:  5.434 s\n",
      "Step 115400 (epoch  134.08), loss: 0.004690, time:  5.419 s\n",
      "Step 115450 (epoch  134.14), loss: 0.005015, time:  5.449 s\n",
      "Step 115500 (epoch  134.20), loss: 0.003683, time:  5.448 s\n",
      "Step 115550 (epoch  134.25), loss: 0.004594, time:  5.427 s\n",
      "Step 115600 (epoch  134.31), loss: 0.004348, time:  5.439 s, error: 0.017000\n",
      "Step 115650 (epoch  134.37), loss: 0.005756, time:  8.736 s\n",
      "Step 115700 (epoch  134.43), loss: 0.004560, time:  5.427 s\n",
      "Step 115750 (epoch  134.49), loss: 0.002885, time:  5.423 s\n",
      "Step 115800 (epoch  134.54), loss: 0.003979, time:  5.436 s\n",
      "Step 115850 (epoch  134.60), loss: 0.004439, time:  5.419 s\n",
      "Step 115900 (epoch  134.66), loss: 0.002855, time:  5.430 s\n",
      "Step 115950 (epoch  134.72), loss: 0.003661, time:  5.426 s\n",
      "Step 116000 (epoch  134.78), loss: 0.004545, time:  5.434 s, error: 0.016872\n",
      "\n",
      "Time since beginning  : 13548.612 s\n",
      "\n",
      "Step 116050 (epoch  134.84), loss: 0.004043, time:  8.672 s\n",
      "Step 116100 (epoch  134.89), loss: 0.004199, time:  5.432 s\n",
      "Step 116150 (epoch  134.95), loss: 0.003146, time:  5.421 s\n",
      "Step 116200 (epoch  135.01), loss: 0.004014, time:  5.419 s\n",
      "Step 116250 (epoch  135.07), loss: 0.004366, time:  5.415 s\n",
      "Step 116300 (epoch  135.13), loss: 0.003462, time:  5.439 s\n",
      "Step 116350 (epoch  135.18), loss: 0.006447, time:  5.458 s\n",
      "Step 116400 (epoch  135.24), loss: 0.003855, time:  5.422 s, error: 0.016932\n",
      "Step 116450 (epoch  135.30), loss: 0.005739, time:  8.614 s\n",
      "Step 116500 (epoch  135.36), loss: 0.004749, time:  5.423 s\n",
      "Step 116550 (epoch  135.42), loss: 0.003897, time:  5.418 s\n",
      "Step 116600 (epoch  135.47), loss: 0.003907, time:  5.432 s\n",
      "Step 116650 (epoch  135.53), loss: 0.003526, time:  5.422 s\n",
      "Step 116700 (epoch  135.59), loss: 0.005126, time:  5.435 s\n",
      "Step 116750 (epoch  135.65), loss: 0.003206, time:  5.437 s\n",
      "Step 116800 (epoch  135.71), loss: 0.003961, time:  5.440 s, error: 0.016955\n",
      "Step 116850 (epoch  135.76), loss: 0.003319, time:  8.667 s\n",
      "Step 116900 (epoch  135.82), loss: 0.003651, time:  5.435 s\n",
      "Step 116950 (epoch  135.88), loss: 0.003945, time:  5.424 s\n",
      "Step 117000 (epoch  135.94), loss: 0.004670, time:  5.416 s\n",
      "Step 117050 (epoch  136.00), loss: 0.005829, time:  5.440 s\n",
      "Step 117100 (epoch  136.06), loss: 0.004002, time:  5.446 s\n",
      "Step 117150 (epoch  136.11), loss: 0.004947, time:  5.425 s\n",
      "Step 117200 (epoch  136.17), loss: 0.003267, time:  5.423 s, error: 0.016944\n",
      "Step 117250 (epoch  136.23), loss: 0.005217, time:  8.620 s\n",
      "Step 117300 (epoch  136.29), loss: 0.004046, time:  5.433 s\n",
      "Step 117350 (epoch  136.35), loss: 0.003158, time:  5.433 s\n",
      "Step 117400 (epoch  136.40), loss: 0.004165, time:  5.436 s\n",
      "Step 117450 (epoch  136.46), loss: 0.003777, time:  5.450 s\n",
      "Step 117500 (epoch  136.52), loss: 0.003495, time:  5.420 s\n",
      "Step 117550 (epoch  136.58), loss: 0.005306, time:  5.429 s\n",
      "Step 117600 (epoch  136.64), loss: 0.004953, time:  5.424 s, error: 0.016796\n",
      "Step 117650 (epoch  136.69), loss: 0.004342, time:  8.636 s\n",
      "Step 117700 (epoch  136.75), loss: 0.004221, time:  5.441 s\n",
      "Step 117750 (epoch  136.81), loss: 0.003925, time:  5.436 s\n",
      "Step 117800 (epoch  136.87), loss: 0.003095, time:  5.454 s\n",
      "Step 117850 (epoch  136.93), loss: 0.003925, time:  5.426 s\n",
      "Step 117900 (epoch  136.98), loss: 0.004411, time:  5.437 s\n",
      "Step 117950 (epoch  137.04), loss: 0.003022, time:  5.425 s\n",
      "Step 118000 (epoch  137.10), loss: 0.004382, time:  5.441 s, error: 0.016744\n",
      "\n",
      "Time since beginning  : 13781.952 s\n",
      "\n",
      "Step 118050 (epoch  137.16), loss: 0.003030, time:  8.707 s\n",
      "Step 118100 (epoch  137.22), loss: 0.002873, time:  5.430 s\n",
      "Step 118150 (epoch  137.28), loss: 0.003493, time:  5.439 s\n",
      "Step 118200 (epoch  137.33), loss: 0.004238, time:  5.441 s\n",
      "Step 118250 (epoch  137.39), loss: 0.003822, time:  5.420 s\n",
      "Step 118300 (epoch  137.45), loss: 0.003610, time:  5.448 s\n",
      "Step 118350 (epoch  137.51), loss: 0.004029, time:  5.415 s\n",
      "Step 118400 (epoch  137.57), loss: 0.005695, time:  5.432 s, error: 0.016812\n",
      "Step 118450 (epoch  137.62), loss: 0.003980, time:  8.618 s\n",
      "Step 118500 (epoch  137.68), loss: 0.003964, time:  5.426 s\n",
      "Step 118550 (epoch  137.74), loss: 0.002471, time:  5.446 s\n",
      "Step 118600 (epoch  137.80), loss: 0.002943, time:  5.429 s\n",
      "Step 118650 (epoch  137.86), loss: 0.003413, time:  5.433 s\n",
      "Step 118700 (epoch  137.91), loss: 0.004881, time:  5.449 s\n",
      "Step 118750 (epoch  137.97), loss: 0.003725, time:  5.448 s\n",
      "Step 118800 (epoch  138.03), loss: 0.004293, time:  5.444 s, error: 0.016769\n",
      "Step 118850 (epoch  138.09), loss: 0.002755, time:  8.654 s\n",
      "Step 118900 (epoch  138.15), loss: 0.004646, time:  5.455 s\n",
      "Step 118950 (epoch  138.20), loss: 0.004811, time:  5.447 s\n",
      "Step 119000 (epoch  138.26), loss: 0.003430, time:  5.439 s\n",
      "Step 119050 (epoch  138.32), loss: 0.004161, time:  5.437 s\n",
      "Step 119100 (epoch  138.38), loss: 0.005322, time:  5.428 s\n",
      "Step 119150 (epoch  138.44), loss: 0.003562, time:  5.428 s\n",
      "Step 119200 (epoch  138.50), loss: 0.004582, time:  5.423 s, error: 0.017293\n",
      "Step 119250 (epoch  138.55), loss: 0.005318, time:  8.616 s\n",
      "Step 119300 (epoch  138.61), loss: 0.004046, time:  5.435 s\n",
      "Step 119350 (epoch  138.67), loss: 0.004891, time:  5.484 s\n",
      "Step 119400 (epoch  138.73), loss: 0.004392, time:  5.427 s\n",
      "Step 119450 (epoch  138.79), loss: 0.004525, time:  5.442 s\n",
      "Step 119500 (epoch  138.84), loss: 0.003530, time:  5.446 s\n",
      "Step 119550 (epoch  138.90), loss: 0.002982, time:  5.428 s\n",
      "Step 119600 (epoch  138.96), loss: 0.005125, time:  5.431 s, error: 0.016969\n",
      "Step 119650 (epoch  139.02), loss: 0.004605, time:  8.674 s\n",
      "Step 119700 (epoch  139.08), loss: 0.004718, time:  5.429 s\n",
      "Step 119750 (epoch  139.13), loss: 0.005709, time:  5.431 s\n",
      "Step 119800 (epoch  139.19), loss: 0.004358, time:  5.439 s\n",
      "Step 119850 (epoch  139.25), loss: 0.004245, time:  5.425 s\n",
      "Step 119900 (epoch  139.31), loss: 0.002959, time:  5.428 s\n",
      "Step 119950 (epoch  139.37), loss: 0.002927, time:  5.439 s\n",
      "Step 120000 (epoch  139.42), loss: 0.003794, time:  5.430 s, error: 0.017031\n",
      "\n",
      "Time since beginning  : 14015.593 s\n",
      "\n",
      "Step 120050 (epoch  139.48), loss: 0.003039, time:  8.824 s\n",
      "Step 120100 (epoch  139.54), loss: 0.003443, time:  5.443 s\n",
      "Step 120150 (epoch  139.60), loss: 0.003719, time:  5.421 s\n",
      "Step 120200 (epoch  139.66), loss: 0.003867, time:  5.426 s\n",
      "Step 120250 (epoch  139.72), loss: 0.004176, time:  5.420 s\n",
      "Step 120300 (epoch  139.77), loss: 0.004464, time:  5.417 s\n",
      "Step 120350 (epoch  139.83), loss: 0.004174, time:  5.428 s\n",
      "Step 120400 (epoch  139.89), loss: 0.003437, time:  5.453 s, error: 0.016768\n",
      "Step 120450 (epoch  139.95), loss: 0.005084, time:  8.633 s\n",
      "Step 120500 (epoch  140.01), loss: 0.003468, time:  5.433 s\n",
      "Step 120550 (epoch  140.06), loss: 0.004919, time:  5.442 s\n",
      "Step 120600 (epoch  140.12), loss: 0.002467, time:  5.449 s\n",
      "Step 120650 (epoch  140.18), loss: 0.005461, time:  5.441 s\n",
      "Step 120700 (epoch  140.24), loss: 0.003925, time:  5.439 s\n",
      "Step 120750 (epoch  140.30), loss: 0.003945, time:  5.456 s\n",
      "Step 120800 (epoch  140.35), loss: 0.002802, time:  5.457 s, error: 0.016684\n",
      "Step 120850 (epoch  140.41), loss: 0.002846, time:  8.663 s\n",
      "Step 120900 (epoch  140.47), loss: 0.004067, time:  5.438 s\n",
      "Step 120950 (epoch  140.53), loss: 0.003561, time:  5.444 s\n",
      "Step 121000 (epoch  140.59), loss: 0.003749, time:  5.436 s\n",
      "Step 121050 (epoch  140.64), loss: 0.003103, time:  5.433 s\n",
      "Step 121100 (epoch  140.70), loss: 0.004901, time:  5.457 s\n",
      "Step 121150 (epoch  140.76), loss: 0.003720, time:  5.425 s\n",
      "Step 121200 (epoch  140.82), loss: 0.004810, time:  5.424 s, error: 0.016760\n",
      "Step 121250 (epoch  140.88), loss: 0.005463, time:  8.633 s\n",
      "Step 121300 (epoch  140.94), loss: 0.004922, time:  5.430 s\n",
      "Step 121350 (epoch  140.99), loss: 0.004798, time:  5.430 s\n",
      "Step 121400 (epoch  141.05), loss: 0.005105, time:  5.442 s\n",
      "Step 121450 (epoch  141.11), loss: 0.004158, time:  5.452 s\n",
      "Step 121500 (epoch  141.17), loss: 0.004200, time:  5.444 s\n",
      "Step 121550 (epoch  141.23), loss: 0.003249, time:  5.433 s\n",
      "Step 121600 (epoch  141.28), loss: 0.003995, time:  5.425 s, error: 0.017095\n",
      "Step 121650 (epoch  141.34), loss: 0.003847, time:  8.617 s\n",
      "Step 121700 (epoch  141.40), loss: 0.004768, time:  5.433 s\n",
      "Step 121750 (epoch  141.46), loss: 0.003242, time:  5.417 s\n",
      "Step 121800 (epoch  141.52), loss: 0.002675, time:  5.436 s\n",
      "Step 121850 (epoch  141.57), loss: 0.004644, time:  5.442 s\n",
      "Step 121900 (epoch  141.63), loss: 0.004870, time:  5.430 s\n",
      "Step 121950 (epoch  141.69), loss: 0.003661, time:  5.438 s\n",
      "Step 122000 (epoch  141.75), loss: 0.004263, time:  5.448 s, error: 0.017188\n",
      "\n",
      "Time since beginning  : 14249.153 s\n",
      "\n",
      "Step 122050 (epoch  141.81), loss: 0.003902, time:  8.708 s\n",
      "Step 122100 (epoch  141.86), loss: 0.004013, time:  5.414 s\n",
      "Step 122150 (epoch  141.92), loss: 0.003428, time:  5.435 s\n",
      "Step 122200 (epoch  141.98), loss: 0.004899, time:  5.446 s\n",
      "Step 122250 (epoch  142.04), loss: 0.004007, time:  5.433 s\n",
      "Step 122300 (epoch  142.10), loss: 0.004158, time:  5.431 s\n",
      "Step 122350 (epoch  142.16), loss: 0.003104, time:  5.422 s\n",
      "Step 122400 (epoch  142.21), loss: 0.002051, time:  5.436 s, error: 0.016817\n",
      "Step 122450 (epoch  142.27), loss: 0.004977, time:  8.603 s\n",
      "Step 122500 (epoch  142.33), loss: 0.003846, time:  5.432 s\n",
      "Step 122550 (epoch  142.39), loss: 0.005483, time:  5.455 s\n",
      "Step 122600 (epoch  142.45), loss: 0.002239, time:  5.430 s\n",
      "Step 122650 (epoch  142.50), loss: 0.004378, time:  5.422 s\n",
      "Step 122700 (epoch  142.56), loss: 0.003956, time:  5.434 s\n",
      "Step 122750 (epoch  142.62), loss: 0.003429, time:  5.425 s\n",
      "Step 122800 (epoch  142.68), loss: 0.003014, time:  5.428 s, error: 0.016510\n",
      "Step 122850 (epoch  142.74), loss: 0.004639, time:  8.643 s\n",
      "Step 122900 (epoch  142.79), loss: 0.005186, time:  5.433 s\n",
      "Step 122950 (epoch  142.85), loss: 0.007583, time:  5.437 s\n",
      "Step 123000 (epoch  142.91), loss: 0.013493, time:  5.431 s\n",
      "Step 123050 (epoch  142.97), loss: 0.003834, time:  5.421 s\n",
      "Step 123100 (epoch  143.03), loss: 0.004946, time:  5.430 s\n",
      "Step 123150 (epoch  143.08), loss: 0.003679, time:  5.438 s\n",
      "Step 123200 (epoch  143.14), loss: 0.003705, time:  5.446 s, error: 0.017291\n",
      "Step 123250 (epoch  143.20), loss: 0.004390, time:  8.622 s\n",
      "Step 123300 (epoch  143.26), loss: 0.005209, time:  5.450 s\n",
      "Step 123350 (epoch  143.32), loss: 0.003033, time:  5.421 s\n",
      "Step 123400 (epoch  143.38), loss: 0.004724, time:  5.435 s\n",
      "Step 123450 (epoch  143.43), loss: 0.004645, time:  5.435 s\n",
      "Step 123500 (epoch  143.49), loss: 0.003963, time:  5.432 s\n",
      "Step 123550 (epoch  143.55), loss: 0.003413, time:  5.436 s\n",
      "Step 123600 (epoch  143.61), loss: 0.004928, time:  5.438 s, error: 0.018176\n",
      "Step 123650 (epoch  143.67), loss: 0.005325, time:  8.631 s\n",
      "Step 123700 (epoch  143.72), loss: 0.002928, time:  5.416 s\n",
      "Step 123750 (epoch  143.78), loss: 0.004174, time:  5.432 s\n",
      "Step 123800 (epoch  143.84), loss: 0.004149, time:  5.429 s\n",
      "Step 123850 (epoch  143.90), loss: 0.004779, time:  5.438 s\n",
      "Step 123900 (epoch  143.96), loss: 0.003936, time:  5.434 s\n",
      "Step 123950 (epoch  144.01), loss: 0.003267, time:  5.440 s\n",
      "Step 124000 (epoch  144.07), loss: 0.003501, time:  5.447 s, error: 0.016619\n",
      "\n",
      "Time since beginning  : 14482.634 s\n",
      "\n",
      "Step 124050 (epoch  144.13), loss: 0.003734, time:  8.841 s\n",
      "Step 124100 (epoch  144.19), loss: 0.006107, time:  5.438 s\n",
      "Step 124150 (epoch  144.25), loss: 0.004941, time:  5.428 s\n",
      "Step 124200 (epoch  144.30), loss: 0.002578, time:  5.429 s\n",
      "Step 124250 (epoch  144.36), loss: 0.003411, time:  5.438 s\n",
      "Step 124300 (epoch  144.42), loss: 0.003224, time:  5.448 s\n",
      "Step 124350 (epoch  144.48), loss: 0.004298, time:  5.439 s\n",
      "Step 124400 (epoch  144.54), loss: 0.003086, time:  5.452 s, error: 0.016790\n",
      "Step 124450 (epoch  144.59), loss: 0.003656, time:  8.632 s\n",
      "Step 124500 (epoch  144.65), loss: 0.005179, time:  5.438 s\n",
      "Step 124550 (epoch  144.71), loss: 0.003893, time:  5.438 s\n",
      "Step 124600 (epoch  144.77), loss: 0.005897, time:  5.441 s\n",
      "Step 124650 (epoch  144.83), loss: 0.004028, time:  5.450 s\n",
      "Step 124700 (epoch  144.89), loss: 0.004390, time:  5.431 s\n",
      "Step 124750 (epoch  144.94), loss: 0.004671, time:  5.447 s\n",
      "Step 124800 (epoch  145.00), loss: 0.003743, time:  5.434 s, error: 0.017004\n",
      "Step 124850 (epoch  145.06), loss: 0.003669, time:  8.660 s\n",
      "Step 124900 (epoch  145.12), loss: 0.003243, time:  5.445 s\n",
      "Step 124950 (epoch  145.18), loss: 0.004393, time:  5.435 s\n",
      "Step 125000 (epoch  145.23), loss: 0.003851, time:  5.446 s\n",
      "Step 125050 (epoch  145.29), loss: 0.004636, time:  5.445 s\n",
      "Step 125100 (epoch  145.35), loss: 0.002543, time:  5.443 s\n",
      "Step 125150 (epoch  145.41), loss: 0.002939, time:  5.462 s\n",
      "Step 125200 (epoch  145.47), loss: 0.004097, time:  5.429 s, error: 0.016643\n",
      "Step 125250 (epoch  145.52), loss: 0.003510, time:  8.629 s\n",
      "Step 125300 (epoch  145.58), loss: 0.003888, time:  5.430 s\n",
      "Step 125350 (epoch  145.64), loss: 0.002815, time:  5.443 s\n",
      "Step 125400 (epoch  145.70), loss: 0.002904, time:  5.431 s\n",
      "Step 125450 (epoch  145.76), loss: 0.004960, time:  5.432 s\n",
      "Step 125500 (epoch  145.81), loss: 0.004061, time:  5.456 s\n",
      "Step 125550 (epoch  145.87), loss: 0.003469, time:  5.431 s\n",
      "Step 125600 (epoch  145.93), loss: 0.004474, time:  5.435 s, error: 0.017830\n",
      "Step 125650 (epoch  145.99), loss: 0.003149, time:  8.623 s\n",
      "Step 125700 (epoch  146.05), loss: 0.003695, time:  5.439 s\n",
      "Step 125750 (epoch  146.11), loss: 0.008142, time:  5.431 s\n",
      "Step 125800 (epoch  146.16), loss: 0.003144, time:  5.428 s\n",
      "Step 125850 (epoch  146.22), loss: 0.003797, time:  5.453 s\n",
      "Step 125900 (epoch  146.28), loss: 0.004488, time:  5.448 s\n",
      "Step 125950 (epoch  146.34), loss: 0.003043, time:  5.424 s\n",
      "Step 126000 (epoch  146.40), loss: 0.003199, time:  5.406 s, error: 0.016419\n",
      "\n",
      "Time since beginning  : 14716.246 s\n",
      "\n",
      "Step 126050 (epoch  146.45), loss: 0.004032, time:  8.723 s\n",
      "Step 126100 (epoch  146.51), loss: 0.004120, time:  5.445 s\n",
      "Step 126150 (epoch  146.57), loss: 0.003578, time:  5.443 s\n",
      "Step 126200 (epoch  146.63), loss: 0.004384, time:  5.448 s\n",
      "Step 126250 (epoch  146.69), loss: 0.005628, time:  5.445 s\n",
      "Step 126300 (epoch  146.74), loss: 0.004811, time:  5.442 s\n",
      "Step 126350 (epoch  146.80), loss: 0.004781, time:  5.433 s\n",
      "Step 126400 (epoch  146.86), loss: 0.003446, time:  5.435 s, error: 0.016706\n",
      "Step 126450 (epoch  146.92), loss: 0.012353, time:  8.624 s\n",
      "Step 126500 (epoch  146.98), loss: 0.003948, time:  5.418 s\n",
      "Step 126550 (epoch  147.03), loss: 0.002207, time:  5.430 s\n",
      "Step 126600 (epoch  147.09), loss: 0.004209, time:  5.447 s\n",
      "Step 126650 (epoch  147.15), loss: 0.003366, time:  5.428 s\n",
      "Step 126700 (epoch  147.21), loss: 0.002911, time:  5.433 s\n",
      "Step 126750 (epoch  147.27), loss: 0.005484, time:  5.415 s\n",
      "Step 126800 (epoch  147.33), loss: 0.002844, time:  5.428 s, error: 0.017133\n",
      "Step 126850 (epoch  147.38), loss: 0.004881, time:  8.623 s\n",
      "Step 126900 (epoch  147.44), loss: 0.003111, time:  5.422 s\n",
      "Step 126950 (epoch  147.50), loss: 0.004608, time:  5.452 s\n",
      "Step 127000 (epoch  147.56), loss: 0.003681, time:  5.420 s\n",
      "Step 127050 (epoch  147.62), loss: 0.003600, time:  5.433 s\n",
      "Step 127100 (epoch  147.67), loss: 0.003788, time:  5.425 s\n",
      "Step 127150 (epoch  147.73), loss: 0.003466, time:  5.446 s\n",
      "Step 127200 (epoch  147.79), loss: 0.003595, time:  5.430 s, error: 0.017265\n",
      "Step 127250 (epoch  147.85), loss: 0.005430, time:  8.630 s\n",
      "Step 127300 (epoch  147.91), loss: 0.003535, time:  5.443 s\n",
      "Step 127350 (epoch  147.96), loss: 0.003801, time:  5.433 s\n",
      "Step 127400 (epoch  148.02), loss: 0.003343, time:  5.435 s\n",
      "Step 127450 (epoch  148.08), loss: 0.003369, time:  5.433 s\n",
      "Step 127500 (epoch  148.14), loss: 0.004281, time:  5.438 s\n",
      "Step 127550 (epoch  148.20), loss: 0.004498, time:  5.448 s\n",
      "Step 127600 (epoch  148.25), loss: 0.004223, time:  5.433 s, error: 0.016976\n",
      "Step 127650 (epoch  148.31), loss: 0.003419, time:  8.651 s\n",
      "Step 127700 (epoch  148.37), loss: 0.005180, time:  5.461 s\n",
      "Step 127750 (epoch  148.43), loss: 0.003275, time:  5.450 s\n",
      "Step 127800 (epoch  148.49), loss: 0.004388, time:  5.423 s\n",
      "Step 127850 (epoch  148.55), loss: 0.004226, time:  5.428 s\n",
      "Step 127900 (epoch  148.60), loss: 0.004729, time:  5.420 s\n",
      "Step 127950 (epoch  148.66), loss: 0.003527, time:  5.441 s\n",
      "Step 128000 (epoch  148.72), loss: 0.004232, time:  5.436 s, error: 0.017016\n",
      "\n",
      "Time since beginning  : 14949.724 s\n",
      "\n",
      "Step 128050 (epoch  148.78), loss: 0.003250, time:  8.711 s\n",
      "Step 128100 (epoch  148.84), loss: 0.005494, time:  5.433 s\n",
      "Step 128150 (epoch  148.89), loss: 0.003056, time:  5.441 s\n",
      "Step 128200 (epoch  148.95), loss: 0.004901, time:  5.428 s\n",
      "Step 128250 (epoch  149.01), loss: 0.005772, time:  5.422 s\n",
      "Step 128300 (epoch  149.07), loss: 0.003869, time:  5.426 s\n",
      "Step 128350 (epoch  149.13), loss: 0.004489, time:  5.440 s\n",
      "Step 128400 (epoch  149.18), loss: 0.003542, time:  5.439 s, error: 0.016831\n",
      "Step 128450 (epoch  149.24), loss: 0.003785, time:  8.742 s\n",
      "Step 128500 (epoch  149.30), loss: 0.003681, time:  5.425 s\n",
      "Step 128550 (epoch  149.36), loss: 0.003682, time:  5.424 s\n",
      "Step 128600 (epoch  149.42), loss: 0.003222, time:  5.427 s\n",
      "Step 128650 (epoch  149.47), loss: 0.003512, time:  5.421 s\n",
      "Step 128700 (epoch  149.53), loss: 0.005154, time:  5.426 s\n",
      "Step 128750 (epoch  149.59), loss: 0.004263, time:  5.428 s\n",
      "Step 128800 (epoch  149.65), loss: 0.004318, time:  5.454 s, error: 0.016638\n",
      "Step 128850 (epoch  149.71), loss: 0.003297, time:  8.642 s\n",
      "Step 128900 (epoch  149.77), loss: 0.004217, time:  5.437 s\n",
      "Step 128950 (epoch  149.82), loss: 0.002838, time:  5.443 s\n",
      "Step 129000 (epoch  149.88), loss: 0.003730, time:  5.429 s\n",
      "Step 129050 (epoch  149.94), loss: 0.004887, time:  5.433 s\n",
      "Step 129100 (epoch  150.00), loss: 0.003659, time:  5.424 s\n",
      "Step 129150 (epoch  150.06), loss: 0.004561, time:  5.452 s\n",
      "Step 129200 (epoch  150.11), loss: 0.003934, time:  5.431 s, error: 0.017637\n",
      "Step 129250 (epoch  150.17), loss: 0.003434, time:  8.635 s\n",
      "Step 129300 (epoch  150.23), loss: 0.003293, time:  5.437 s\n",
      "Step 129350 (epoch  150.29), loss: 0.003998, time:  5.433 s\n",
      "Step 129400 (epoch  150.35), loss: 0.003214, time:  5.424 s\n",
      "Step 129450 (epoch  150.40), loss: 0.002533, time:  5.435 s\n",
      "Step 129500 (epoch  150.46), loss: 0.004912, time:  5.432 s\n",
      "Step 129550 (epoch  150.52), loss: 0.004670, time:  5.440 s\n",
      "Step 129600 (epoch  150.58), loss: 0.003805, time:  5.433 s, error: 0.016792\n",
      "Step 129650 (epoch  150.64), loss: 0.002780, time:  8.646 s\n",
      "Step 129700 (epoch  150.69), loss: 0.004098, time:  5.427 s\n",
      "Step 129750 (epoch  150.75), loss: 0.004292, time:  5.439 s\n",
      "Step 129800 (epoch  150.81), loss: 0.003392, time:  5.435 s\n",
      "Step 129850 (epoch  150.87), loss: 0.004412, time:  5.433 s\n",
      "Step 129900 (epoch  150.93), loss: 0.003207, time:  5.446 s\n",
      "Step 129950 (epoch  150.99), loss: 0.003734, time:  5.427 s\n",
      "Step 130000 (epoch  151.04), loss: 0.002953, time:  5.424 s, error: 0.017113\n",
      "\n",
      "Time since beginning  : 15183.235 s\n",
      "\n",
      "Step 130050 (epoch  151.10), loss: 0.003481, time:  8.685 s\n",
      "Step 130100 (epoch  151.16), loss: 0.004249, time:  5.445 s\n",
      "Step 130150 (epoch  151.22), loss: 0.006499, time:  5.431 s\n",
      "Step 130200 (epoch  151.28), loss: 0.004606, time:  5.427 s\n",
      "Step 130250 (epoch  151.33), loss: 0.003284, time:  5.451 s\n",
      "Step 130300 (epoch  151.39), loss: 0.004215, time:  5.429 s\n",
      "Step 130350 (epoch  151.45), loss: 0.003649, time:  5.425 s\n",
      "Step 130400 (epoch  151.51), loss: 0.004163, time:  5.428 s, error: 0.017511\n",
      "Step 130450 (epoch  151.57), loss: 0.004010, time:  8.667 s\n",
      "Step 130500 (epoch  151.62), loss: 0.002811, time:  5.429 s\n",
      "Step 130550 (epoch  151.68), loss: 0.003759, time:  5.438 s\n",
      "Step 130600 (epoch  151.74), loss: 0.004262, time:  5.446 s\n",
      "Step 130650 (epoch  151.80), loss: 0.003491, time:  5.424 s\n",
      "Step 130700 (epoch  151.86), loss: 0.003513, time:  5.412 s\n",
      "Step 130750 (epoch  151.91), loss: 0.003027, time:  5.431 s\n",
      "Step 130800 (epoch  151.97), loss: 0.004202, time:  5.427 s, error: 0.016854\n",
      "Step 130850 (epoch  152.03), loss: 0.004336, time:  8.629 s\n",
      "Step 130900 (epoch  152.09), loss: 0.004843, time:  5.429 s\n",
      "Step 130950 (epoch  152.15), loss: 0.004743, time:  5.452 s\n",
      "Step 131000 (epoch  152.21), loss: 0.003203, time:  5.447 s\n",
      "Step 131050 (epoch  152.26), loss: 0.003049, time:  5.430 s\n",
      "Step 131100 (epoch  152.32), loss: 0.003869, time:  5.429 s\n",
      "Step 131150 (epoch  152.38), loss: 0.004062, time:  5.435 s\n",
      "Step 131200 (epoch  152.44), loss: 0.003636, time:  5.444 s, error: 0.016538\n",
      "Step 131250 (epoch  152.50), loss: 0.004333, time:  8.641 s\n",
      "Step 131300 (epoch  152.55), loss: 0.002543, time:  5.439 s\n",
      "Step 131350 (epoch  152.61), loss: 0.003908, time:  5.449 s\n",
      "Step 131400 (epoch  152.67), loss: 0.004929, time:  5.425 s\n",
      "Step 131450 (epoch  152.73), loss: 0.002817, time:  5.432 s\n",
      "Step 131500 (epoch  152.79), loss: 0.005226, time:  5.450 s\n",
      "Step 131550 (epoch  152.84), loss: 0.005184, time:  5.423 s\n",
      "Step 131600 (epoch  152.90), loss: 0.004032, time:  5.424 s, error: 0.016664\n",
      "Step 131650 (epoch  152.96), loss: 0.002946, time:  8.629 s\n",
      "Step 131700 (epoch  153.02), loss: 0.005158, time:  5.431 s\n",
      "Step 131750 (epoch  153.08), loss: 0.002603, time:  5.424 s\n",
      "Step 131800 (epoch  153.13), loss: 0.004048, time:  5.428 s\n",
      "Step 131850 (epoch  153.19), loss: 0.002972, time:  5.430 s\n",
      "Step 131900 (epoch  153.25), loss: 0.004442, time:  5.424 s\n",
      "Step 131950 (epoch  153.31), loss: 0.003237, time:  5.421 s\n",
      "Step 132000 (epoch  153.37), loss: 0.005379, time:  5.452 s, error: 0.017289\n",
      "\n",
      "Time since beginning  : 15416.681 s\n",
      "\n",
      "Step 132050 (epoch  153.43), loss: 0.002676, time:  8.736 s\n",
      "Step 132100 (epoch  153.48), loss: 0.003996, time:  5.432 s\n",
      "Step 132150 (epoch  153.54), loss: 0.004510, time:  5.427 s\n",
      "Step 132200 (epoch  153.60), loss: 0.004690, time:  5.452 s\n",
      "Step 132250 (epoch  153.66), loss: 0.003330, time:  5.441 s\n",
      "Step 132300 (epoch  153.72), loss: 0.004418, time:  5.439 s\n",
      "Step 132350 (epoch  153.77), loss: 0.003074, time:  5.448 s\n",
      "Step 132400 (epoch  153.83), loss: 0.003654, time:  5.437 s, error: 0.016595\n",
      "Step 132450 (epoch  153.89), loss: 0.004433, time:  8.699 s\n",
      "Step 132500 (epoch  153.95), loss: 0.003054, time:  5.425 s\n",
      "Step 132550 (epoch  154.01), loss: 0.003333, time:  5.444 s\n",
      "Step 132600 (epoch  154.06), loss: 0.003442, time:  5.448 s\n",
      "Step 132650 (epoch  154.12), loss: 0.003991, time:  5.447 s\n",
      "Step 132700 (epoch  154.18), loss: 0.006158, time:  5.433 s\n",
      "Step 132750 (epoch  154.24), loss: 0.003931, time:  5.449 s\n",
      "Step 132800 (epoch  154.30), loss: 0.003267, time:  5.456 s, error: 0.016739\n",
      "Step 132850 (epoch  154.35), loss: 0.004044, time:  8.668 s\n",
      "Step 132900 (epoch  154.41), loss: 0.002614, time:  5.442 s\n",
      "Step 132950 (epoch  154.47), loss: 0.004638, time:  5.436 s\n",
      "Step 133000 (epoch  154.53), loss: 0.003382, time:  5.430 s\n",
      "Step 133050 (epoch  154.59), loss: 0.003789, time:  5.430 s\n",
      "Step 133100 (epoch  154.65), loss: 0.005719, time:  5.422 s\n",
      "Step 133150 (epoch  154.70), loss: 0.004645, time:  5.425 s\n",
      "Step 133200 (epoch  154.76), loss: 0.005954, time:  5.441 s, error: 0.017100\n",
      "Step 133250 (epoch  154.82), loss: 0.006183, time:  8.615 s\n",
      "Step 133300 (epoch  154.88), loss: 0.003398, time:  5.417 s\n",
      "Step 133350 (epoch  154.94), loss: 0.004773, time:  5.433 s\n",
      "Step 133400 (epoch  154.99), loss: 0.004336, time:  5.429 s\n",
      "Step 133450 (epoch  155.05), loss: 0.003206, time:  5.436 s\n",
      "Step 133500 (epoch  155.11), loss: 0.003043, time:  5.450 s\n",
      "Step 133550 (epoch  155.17), loss: 0.004349, time:  5.457 s\n",
      "Step 133600 (epoch  155.23), loss: 0.003816, time:  5.444 s, error: 0.017274\n",
      "Step 133650 (epoch  155.28), loss: 0.002958, time:  8.635 s\n",
      "Step 133700 (epoch  155.34), loss: 0.005038, time:  5.436 s\n",
      "Step 133750 (epoch  155.40), loss: 0.003850, time:  5.442 s\n",
      "Step 133800 (epoch  155.46), loss: 0.002782, time:  5.421 s\n",
      "Step 133850 (epoch  155.52), loss: 0.002572, time:  5.421 s\n",
      "Step 133900 (epoch  155.57), loss: 0.004143, time:  5.455 s\n",
      "Step 133950 (epoch  155.63), loss: 0.003876, time:  5.439 s\n",
      "Step 134000 (epoch  155.69), loss: 0.005397, time:  5.420 s, error: 0.016568\n",
      "\n",
      "Time since beginning  : 15650.320 s\n",
      "\n",
      "Step 134050 (epoch  155.75), loss: 0.003849, time:  8.698 s\n",
      "Step 134100 (epoch  155.81), loss: 0.003720, time:  5.428 s\n",
      "Step 134150 (epoch  155.87), loss: 0.004646, time:  5.433 s\n",
      "Step 134200 (epoch  155.92), loss: 0.004086, time:  5.433 s\n",
      "Step 134250 (epoch  155.98), loss: 0.004264, time:  5.428 s\n",
      "Step 134300 (epoch  156.04), loss: 0.004403, time:  5.437 s\n",
      "Step 134350 (epoch  156.10), loss: 0.003659, time:  5.426 s\n",
      "Step 134400 (epoch  156.16), loss: 0.004283, time:  5.426 s, error: 0.017350\n",
      "Step 134450 (epoch  156.21), loss: 0.003695, time:  8.599 s\n",
      "Step 134500 (epoch  156.27), loss: 0.005256, time:  5.428 s\n",
      "Step 134550 (epoch  156.33), loss: 0.003423, time:  5.426 s\n",
      "Step 134600 (epoch  156.39), loss: 0.002563, time:  5.448 s\n",
      "Step 134650 (epoch  156.45), loss: 0.002857, time:  5.446 s\n",
      "Step 134700 (epoch  156.50), loss: 0.003473, time:  5.426 s\n",
      "Step 134750 (epoch  156.56), loss: 0.004346, time:  5.427 s\n",
      "Step 134800 (epoch  156.62), loss: 0.002052, time:  5.426 s, error: 0.016698\n",
      "Step 134850 (epoch  156.68), loss: 0.002702, time:  8.628 s\n",
      "Step 134900 (epoch  156.74), loss: 0.003853, time:  5.442 s\n",
      "Step 134950 (epoch  156.79), loss: 0.004053, time:  5.450 s\n",
      "Step 135000 (epoch  156.85), loss: 0.004084, time:  5.444 s\n",
      "Step 135050 (epoch  156.91), loss: 0.003934, time:  5.435 s\n",
      "Step 135100 (epoch  156.97), loss: 0.004371, time:  5.421 s\n",
      "Step 135150 (epoch  157.03), loss: 0.004031, time:  5.428 s\n",
      "Step 135200 (epoch  157.09), loss: 0.003088, time:  5.438 s, error: 0.017098\n",
      "Step 135250 (epoch  157.14), loss: 0.003972, time:  8.599 s\n",
      "Step 135300 (epoch  157.20), loss: 0.001914, time:  5.440 s\n",
      "Step 135350 (epoch  157.26), loss: 0.004494, time:  5.456 s\n",
      "Step 135400 (epoch  157.32), loss: 0.004675, time:  5.440 s\n",
      "Step 135450 (epoch  157.38), loss: 0.004214, time:  5.444 s\n",
      "Step 135500 (epoch  157.43), loss: 0.003000, time:  5.448 s\n",
      "Step 135550 (epoch  157.49), loss: 0.003553, time:  5.445 s\n",
      "Step 135600 (epoch  157.55), loss: 0.003858, time:  5.436 s, error: 0.016982\n",
      "Step 135650 (epoch  157.61), loss: 0.003229, time:  8.654 s\n",
      "Step 135700 (epoch  157.67), loss: 0.004445, time:  5.427 s\n",
      "Step 135750 (epoch  157.72), loss: 0.004121, time:  5.443 s\n",
      "Step 135800 (epoch  157.78), loss: 0.003172, time:  5.444 s\n",
      "Step 135850 (epoch  157.84), loss: 0.004746, time:  5.441 s\n",
      "Step 135900 (epoch  157.90), loss: 0.004554, time:  5.420 s\n",
      "Step 135950 (epoch  157.96), loss: 0.003065, time:  5.438 s\n",
      "Step 136000 (epoch  158.01), loss: 0.002410, time:  5.455 s, error: 0.016744\n",
      "\n",
      "Time since beginning  : 15883.821 s\n",
      "\n",
      "Step 136050 (epoch  158.07), loss: 0.003109, time:  8.756 s\n",
      "Step 136100 (epoch  158.13), loss: 0.003641, time:  5.460 s\n",
      "Step 136150 (epoch  158.19), loss: 0.004381, time:  5.426 s\n",
      "Step 136200 (epoch  158.25), loss: 0.004390, time:  5.424 s\n",
      "Step 136250 (epoch  158.31), loss: 0.002822, time:  5.430 s\n",
      "Step 136300 (epoch  158.36), loss: 0.006341, time:  5.419 s\n",
      "Step 136350 (epoch  158.42), loss: 0.003556, time:  5.444 s\n",
      "Step 136400 (epoch  158.48), loss: 0.002868, time:  5.421 s, error: 0.017475\n",
      "Step 136450 (epoch  158.54), loss: 0.004154, time:  8.637 s\n",
      "Step 136500 (epoch  158.60), loss: 0.005217, time:  5.452 s\n",
      "Step 136550 (epoch  158.65), loss: 0.004476, time:  5.428 s\n",
      "Step 136600 (epoch  158.71), loss: 0.003649, time:  5.425 s\n",
      "Step 136650 (epoch  158.77), loss: 0.004481, time:  5.436 s\n",
      "Step 136700 (epoch  158.83), loss: 0.003725, time:  5.416 s\n",
      "Step 136750 (epoch  158.89), loss: 0.003608, time:  5.448 s\n",
      "Step 136800 (epoch  158.94), loss: 0.003759, time:  5.443 s, error: 0.016683\n",
      "Step 136850 (epoch  159.00), loss: 0.005134, time:  8.738 s\n",
      "Step 136900 (epoch  159.06), loss: 0.002541, time:  5.448 s\n",
      "Step 136950 (epoch  159.12), loss: 0.003307, time:  5.419 s\n",
      "Step 137000 (epoch  159.18), loss: 0.002994, time:  5.438 s\n",
      "Step 137050 (epoch  159.23), loss: 0.005158, time:  5.450 s\n",
      "Step 137100 (epoch  159.29), loss: 0.002639, time:  5.442 s\n",
      "Step 137150 (epoch  159.35), loss: 0.004112, time:  5.431 s\n",
      "Step 137200 (epoch  159.41), loss: 0.004276, time:  5.442 s, error: 0.016533\n",
      "Step 137250 (epoch  159.47), loss: 0.003948, time:  8.622 s\n",
      "Step 137300 (epoch  159.53), loss: 0.003642, time:  5.425 s\n",
      "Step 137350 (epoch  159.58), loss: 0.003111, time:  5.424 s\n",
      "Step 137400 (epoch  159.64), loss: 0.004492, time:  5.432 s\n",
      "Step 137450 (epoch  159.70), loss: 0.004650, time:  5.417 s\n",
      "Step 137500 (epoch  159.76), loss: 0.003277, time:  5.442 s\n",
      "Step 137550 (epoch  159.82), loss: 0.003507, time:  5.449 s\n",
      "Step 137600 (epoch  159.87), loss: 0.005258, time:  5.440 s, error: 0.016852\n",
      "Step 137650 (epoch  159.93), loss: 0.003941, time:  8.622 s\n",
      "Step 137700 (epoch  159.99), loss: 0.003925, time:  5.429 s\n",
      "Step 137750 (epoch  160.05), loss: 0.003907, time:  5.435 s\n",
      "Step 137800 (epoch  160.11), loss: 0.004322, time:  5.433 s\n",
      "Step 137850 (epoch  160.16), loss: 0.003962, time:  5.447 s\n",
      "Step 137900 (epoch  160.22), loss: 0.002043, time:  5.449 s\n",
      "Step 137950 (epoch  160.28), loss: 0.003406, time:  5.445 s\n",
      "Step 138000 (epoch  160.34), loss: 0.004015, time:  5.451 s, error: 0.016712\n",
      "\n",
      "Time since beginning  : 16117.399 s\n",
      "\n",
      "Step 138050 (epoch  160.40), loss: 0.003599, time:  8.687 s\n",
      "Step 138100 (epoch  160.45), loss: 0.002347, time:  5.428 s\n",
      "Step 138150 (epoch  160.51), loss: 0.005170, time:  5.440 s\n",
      "Step 138200 (epoch  160.57), loss: 0.003213, time:  5.437 s\n",
      "Step 138250 (epoch  160.63), loss: 0.004684, time:  5.440 s\n",
      "Step 138300 (epoch  160.69), loss: 0.005373, time:  5.451 s\n",
      "Step 138350 (epoch  160.74), loss: 0.003940, time:  5.435 s\n",
      "Step 138400 (epoch  160.80), loss: 0.004287, time:  5.441 s, error: 0.017492\n",
      "Step 138450 (epoch  160.86), loss: 0.003540, time:  8.634 s\n",
      "Step 138500 (epoch  160.92), loss: 0.003244, time:  5.421 s\n",
      "Step 138550 (epoch  160.98), loss: 0.002675, time:  5.437 s\n",
      "Step 138600 (epoch  161.04), loss: 0.003718, time:  5.440 s\n",
      "Step 138650 (epoch  161.09), loss: 0.002818, time:  5.459 s\n",
      "Step 138700 (epoch  161.15), loss: 0.002980, time:  5.426 s\n",
      "Step 138750 (epoch  161.21), loss: 0.003307, time:  5.435 s\n",
      "Step 138800 (epoch  161.27), loss: 0.002859, time:  5.437 s, error: 0.017369\n",
      "Step 138850 (epoch  161.33), loss: 0.003544, time:  8.622 s\n",
      "Step 138900 (epoch  161.38), loss: 0.003165, time:  5.453 s\n",
      "Step 138950 (epoch  161.44), loss: 0.005871, time:  5.439 s\n",
      "Step 139000 (epoch  161.50), loss: 0.003536, time:  5.438 s\n",
      "Step 139050 (epoch  161.56), loss: 0.004638, time:  5.438 s\n",
      "Step 139100 (epoch  161.62), loss: 0.002465, time:  5.427 s\n",
      "Step 139150 (epoch  161.67), loss: 0.003443, time:  5.436 s\n",
      "Step 139200 (epoch  161.73), loss: 0.004700, time:  5.443 s, error: 0.016757\n",
      "Step 139250 (epoch  161.79), loss: 0.004927, time:  8.630 s\n",
      "Step 139300 (epoch  161.85), loss: 0.002845, time:  5.433 s\n",
      "Step 139350 (epoch  161.91), loss: 0.004175, time:  5.437 s\n",
      "Step 139400 (epoch  161.96), loss: 0.003324, time:  5.438 s\n",
      "Step 139450 (epoch  162.02), loss: 0.003117, time:  5.448 s\n",
      "Step 139500 (epoch  162.08), loss: 0.004399, time:  5.446 s\n",
      "Step 139550 (epoch  162.14), loss: 0.003791, time:  5.432 s\n",
      "Step 139600 (epoch  162.20), loss: 0.004268, time:  5.431 s, error: 0.016689\n",
      "Step 139650 (epoch  162.26), loss: 0.002991, time:  8.636 s\n",
      "Step 139700 (epoch  162.31), loss: 0.003009, time:  5.443 s\n",
      "Step 139750 (epoch  162.37), loss: 0.004295, time:  5.456 s\n",
      "Step 139800 (epoch  162.43), loss: 0.003022, time:  5.431 s\n",
      "Step 139850 (epoch  162.49), loss: 0.003299, time:  5.434 s\n",
      "Step 139900 (epoch  162.55), loss: 0.002799, time:  5.427 s\n",
      "Step 139950 (epoch  162.60), loss: 0.002662, time:  5.433 s\n",
      "Step 140000 (epoch  162.66), loss: 0.004023, time:  5.439 s, error: 0.016487\n",
      "\n",
      "Time since beginning  : 16350.979 s\n",
      "\n",
      "Step 140050 (epoch  162.72), loss: 0.003703, time:  8.742 s\n",
      "Step 140100 (epoch  162.78), loss: 0.002918, time:  5.442 s\n",
      "Step 140150 (epoch  162.84), loss: 0.004181, time:  5.440 s\n",
      "Step 140200 (epoch  162.89), loss: 0.003986, time:  5.438 s\n",
      "Step 140250 (epoch  162.95), loss: 0.002304, time:  5.443 s\n",
      "Step 140300 (epoch  163.01), loss: 0.002765, time:  5.430 s\n",
      "Step 140350 (epoch  163.07), loss: 0.002927, time:  5.414 s\n",
      "Step 140400 (epoch  163.13), loss: 0.002996, time:  5.437 s, error: 0.016769\n",
      "Step 140450 (epoch  163.18), loss: 0.002328, time:  8.639 s\n",
      "Step 140500 (epoch  163.24), loss: 0.003166, time:  5.448 s\n",
      "Step 140550 (epoch  163.30), loss: 0.003073, time:  5.420 s\n",
      "Step 140600 (epoch  163.36), loss: 0.003428, time:  5.421 s\n",
      "Step 140650 (epoch  163.42), loss: 0.002964, time:  5.431 s\n",
      "Step 140700 (epoch  163.48), loss: 0.003747, time:  5.427 s\n",
      "Step 140750 (epoch  163.53), loss: 0.005909, time:  5.425 s\n",
      "Step 140800 (epoch  163.59), loss: 0.004241, time:  5.428 s, error: 0.016686\n",
      "Step 140850 (epoch  163.65), loss: 0.003088, time:  8.624 s\n",
      "Step 140900 (epoch  163.71), loss: 0.003807, time:  5.427 s\n",
      "Step 140950 (epoch  163.77), loss: 0.004376, time:  5.429 s\n",
      "Step 141000 (epoch  163.82), loss: 0.003934, time:  5.446 s\n",
      "Step 141050 (epoch  163.88), loss: 0.004138, time:  5.433 s\n",
      "Step 141100 (epoch  163.94), loss: 0.004934, time:  5.433 s\n",
      "Step 141150 (epoch  164.00), loss: 0.003377, time:  5.437 s\n",
      "Step 141200 (epoch  164.06), loss: 0.005092, time:  5.425 s, error: 0.017064\n",
      "Step 141250 (epoch  164.11), loss: 0.003841, time:  8.749 s\n",
      "Step 141300 (epoch  164.17), loss: 0.004422, time:  5.417 s\n",
      "Step 141350 (epoch  164.23), loss: 0.004362, time:  5.427 s\n",
      "Step 141400 (epoch  164.29), loss: 0.003789, time:  5.431 s\n",
      "Step 141450 (epoch  164.35), loss: 0.006191, time:  5.427 s\n",
      "Step 141500 (epoch  164.40), loss: 0.003475, time:  5.428 s\n",
      "Step 141550 (epoch  164.46), loss: 0.005330, time:  5.429 s\n",
      "Step 141600 (epoch  164.52), loss: 0.002974, time:  5.440 s, error: 0.016662\n",
      "Step 141650 (epoch  164.58), loss: 0.004074, time:  8.608 s\n",
      "Step 141700 (epoch  164.64), loss: 0.003559, time:  5.430 s\n",
      "Step 141750 (epoch  164.70), loss: 0.003517, time:  5.419 s\n",
      "Step 141800 (epoch  164.75), loss: 0.003559, time:  5.421 s\n",
      "Step 141850 (epoch  164.81), loss: 0.004183, time:  5.434 s\n",
      "Step 141900 (epoch  164.87), loss: 0.003707, time:  5.430 s\n",
      "Step 141950 (epoch  164.93), loss: 0.004292, time:  5.447 s\n",
      "Step 142000 (epoch  164.99), loss: 0.003542, time:  5.427 s, error: 0.016372\n",
      "\n",
      "Time since beginning  : 16584.398 s\n",
      "\n",
      "Step 142050 (epoch  165.04), loss: 0.003577, time:  8.700 s\n",
      "Step 142100 (epoch  165.10), loss: 0.005670, time:  5.427 s\n",
      "Step 142150 (epoch  165.16), loss: 0.003862, time:  5.425 s\n",
      "Step 142200 (epoch  165.22), loss: 0.004979, time:  5.432 s\n",
      "Step 142250 (epoch  165.28), loss: 0.004848, time:  5.434 s\n",
      "Step 142300 (epoch  165.33), loss: 0.003723, time:  5.430 s\n",
      "Step 142350 (epoch  165.39), loss: 0.003383, time:  5.439 s\n",
      "Step 142400 (epoch  165.45), loss: 0.002530, time:  5.439 s, error: 0.016526\n",
      "Step 142450 (epoch  165.51), loss: 0.005042, time:  8.619 s\n",
      "Step 142500 (epoch  165.57), loss: 0.003381, time:  5.412 s\n",
      "Step 142550 (epoch  165.62), loss: 0.003123, time:  5.423 s\n",
      "Step 142600 (epoch  165.68), loss: 0.003774, time:  5.441 s\n",
      "Step 142650 (epoch  165.74), loss: 0.003575, time:  5.425 s\n",
      "Step 142700 (epoch  165.80), loss: 0.004357, time:  5.448 s\n",
      "Step 142750 (epoch  165.86), loss: 0.003287, time:  5.426 s\n",
      "Step 142800 (epoch  165.92), loss: 0.002365, time:  5.410 s, error: 0.016783\n",
      "Step 142850 (epoch  165.97), loss: 0.003148, time:  8.604 s\n",
      "Step 142900 (epoch  166.03), loss: 0.003226, time:  5.422 s\n",
      "Step 142950 (epoch  166.09), loss: 0.003386, time:  5.430 s\n",
      "Step 143000 (epoch  166.15), loss: 0.004126, time:  5.441 s\n",
      "Step 143050 (epoch  166.21), loss: 0.003671, time:  5.451 s\n",
      "Step 143100 (epoch  166.26), loss: 0.003792, time:  5.430 s\n",
      "Step 143150 (epoch  166.32), loss: 0.003671, time:  5.428 s\n",
      "Step 143200 (epoch  166.38), loss: 0.004318, time:  5.436 s, error: 0.016770\n",
      "Step 143250 (epoch  166.44), loss: 0.003190, time:  8.633 s\n",
      "Step 143300 (epoch  166.50), loss: 0.003151, time:  5.433 s\n",
      "Step 143350 (epoch  166.55), loss: 0.004427, time:  5.428 s\n",
      "Step 143400 (epoch  166.61), loss: 0.002892, time:  5.447 s\n",
      "Step 143450 (epoch  166.67), loss: 0.003800, time:  5.426 s\n",
      "Step 143500 (epoch  166.73), loss: 0.003548, time:  5.458 s\n",
      "Step 143550 (epoch  166.79), loss: 0.004002, time:  5.429 s\n",
      "Step 143600 (epoch  166.84), loss: 0.003985, time:  5.428 s, error: 0.016739\n",
      "Step 143650 (epoch  166.90), loss: 0.004589, time:  8.650 s\n",
      "Step 143700 (epoch  166.96), loss: 0.005589, time:  5.435 s\n",
      "Step 143750 (epoch  167.02), loss: 0.003895, time:  5.434 s\n",
      "Step 143800 (epoch  167.08), loss: 0.003885, time:  5.442 s\n",
      "Step 143850 (epoch  167.14), loss: 0.002991, time:  5.442 s\n",
      "Step 143900 (epoch  167.19), loss: 0.004355, time:  5.425 s\n",
      "Step 143950 (epoch  167.25), loss: 0.004122, time:  5.423 s\n",
      "Step 144000 (epoch  167.31), loss: 0.004068, time:  5.430 s, error: 0.016736\n",
      "\n",
      "Time since beginning  : 16817.751 s\n",
      "\n",
      "Step 144050 (epoch  167.37), loss: 0.004003, time:  8.732 s\n",
      "Step 144100 (epoch  167.43), loss: 0.003054, time:  5.426 s\n",
      "Step 144150 (epoch  167.48), loss: 0.003200, time:  5.450 s\n",
      "Step 144200 (epoch  167.54), loss: 0.004481, time:  5.423 s\n",
      "Step 144250 (epoch  167.60), loss: 0.004838, time:  5.433 s\n",
      "Step 144300 (epoch  167.66), loss: 0.003556, time:  5.447 s\n",
      "Step 144350 (epoch  167.72), loss: 0.003202, time:  5.435 s\n",
      "Step 144400 (epoch  167.77), loss: 0.003199, time:  5.425 s, error: 0.016564\n",
      "Step 144450 (epoch  167.83), loss: 0.003612, time:  8.630 s\n",
      "Step 144500 (epoch  167.89), loss: 0.003774, time:  5.438 s\n",
      "Step 144550 (epoch  167.95), loss: 0.004616, time:  5.423 s\n",
      "Step 144600 (epoch  168.01), loss: 0.003102, time:  5.422 s\n",
      "Step 144650 (epoch  168.06), loss: 0.005694, time:  5.444 s\n",
      "Step 144700 (epoch  168.12), loss: 0.002979, time:  5.430 s\n",
      "Step 144750 (epoch  168.18), loss: 0.003425, time:  5.433 s\n",
      "Step 144800 (epoch  168.24), loss: 0.004441, time:  5.443 s, error: 0.017224\n",
      "Step 144850 (epoch  168.30), loss: 0.002635, time:  8.619 s\n",
      "Step 144900 (epoch  168.36), loss: 0.004030, time:  5.443 s\n",
      "Step 144950 (epoch  168.41), loss: 0.002713, time:  5.448 s\n",
      "Step 145000 (epoch  168.47), loss: 0.004089, time:  5.431 s\n",
      "Step 145050 (epoch  168.53), loss: 0.005115, time:  5.429 s\n",
      "Step 145100 (epoch  168.59), loss: 0.003779, time:  5.421 s\n",
      "Step 145150 (epoch  168.65), loss: 0.003040, time:  5.427 s\n",
      "Step 145200 (epoch  168.70), loss: 0.002505, time:  5.423 s, error: 0.016809\n",
      "Step 145250 (epoch  168.76), loss: 0.002285, time:  8.656 s\n",
      "Step 145300 (epoch  168.82), loss: 0.003014, time:  5.445 s\n",
      "Step 145350 (epoch  168.88), loss: 0.005928, time:  5.431 s\n",
      "Step 145400 (epoch  168.94), loss: 0.004592, time:  5.425 s\n",
      "Step 145450 (epoch  168.99), loss: 0.003971, time:  5.423 s\n",
      "Step 145500 (epoch  169.05), loss: 0.002491, time:  5.428 s\n",
      "Step 145550 (epoch  169.11), loss: 0.003442, time:  5.440 s\n",
      "Step 145600 (epoch  169.17), loss: 0.004046, time:  5.437 s, error: 0.016881\n",
      "Step 145650 (epoch  169.23), loss: 0.003160, time:  8.724 s\n",
      "Step 145700 (epoch  169.28), loss: 0.002676, time:  5.450 s\n",
      "Step 145750 (epoch  169.34), loss: 0.004651, time:  5.426 s\n",
      "Step 145800 (epoch  169.40), loss: 0.003572, time:  5.423 s\n",
      "Step 145850 (epoch  169.46), loss: 0.003162, time:  5.431 s\n",
      "Step 145900 (epoch  169.52), loss: 0.004408, time:  5.428 s\n",
      "Step 145950 (epoch  169.58), loss: 0.004821, time:  5.429 s\n",
      "Step 146000 (epoch  169.63), loss: 0.004153, time:  5.440 s, error: 0.016549\n",
      "\n",
      "Time since beginning  : 17051.244 s\n",
      "\n",
      "Step 146050 (epoch  169.69), loss: 0.004371, time:  8.700 s\n",
      "Step 146100 (epoch  169.75), loss: 0.002898, time:  5.429 s\n",
      "Step 146150 (epoch  169.81), loss: 0.002888, time:  5.416 s\n",
      "Step 146200 (epoch  169.87), loss: 0.002176, time:  5.436 s\n",
      "Step 146250 (epoch  169.92), loss: 0.005786, time:  5.440 s\n",
      "Step 146300 (epoch  169.98), loss: 0.003291, time:  5.426 s\n",
      "Step 146350 (epoch  170.04), loss: 0.003902, time:  5.451 s\n",
      "Step 146400 (epoch  170.10), loss: 0.004278, time:  5.442 s, error: 0.016912\n",
      "Step 146450 (epoch  170.16), loss: 0.003132, time:  8.599 s\n",
      "Step 146500 (epoch  170.21), loss: 0.004237, time:  5.433 s\n",
      "Step 146550 (epoch  170.27), loss: 0.002916, time:  5.433 s\n",
      "Step 146600 (epoch  170.33), loss: 0.003179, time:  5.441 s\n",
      "Step 146650 (epoch  170.39), loss: 0.002986, time:  5.425 s\n",
      "Step 146700 (epoch  170.45), loss: 0.002193, time:  5.435 s\n",
      "Step 146750 (epoch  170.50), loss: 0.003703, time:  5.441 s\n",
      "Step 146800 (epoch  170.56), loss: 0.004999, time:  5.435 s, error: 0.017331\n",
      "Step 146850 (epoch  170.62), loss: 0.004047, time:  8.632 s\n",
      "Step 146900 (epoch  170.68), loss: 0.002925, time:  5.438 s\n",
      "Step 146950 (epoch  170.74), loss: 0.004295, time:  5.426 s\n",
      "Step 147000 (epoch  170.80), loss: 0.004146, time:  5.429 s\n",
      "Step 147050 (epoch  170.85), loss: 0.003293, time:  5.420 s\n",
      "Step 147100 (epoch  170.91), loss: 0.003632, time:  5.449 s\n",
      "Step 147150 (epoch  170.97), loss: 0.002823, time:  5.431 s\n",
      "Step 147200 (epoch  171.03), loss: 0.004314, time:  5.429 s, error: 0.016838\n",
      "Step 147250 (epoch  171.09), loss: 0.002629, time:  8.638 s\n",
      "Step 147300 (epoch  171.14), loss: 0.003684, time:  5.450 s\n",
      "Step 147350 (epoch  171.20), loss: 0.002528, time:  5.459 s\n",
      "Step 147400 (epoch  171.26), loss: 0.003792, time:  5.453 s\n",
      "Step 147450 (epoch  171.32), loss: 0.002883, time:  5.463 s\n",
      "Step 147500 (epoch  171.38), loss: 0.003674, time:  5.451 s\n",
      "Step 147550 (epoch  171.43), loss: 0.003931, time:  5.456 s\n",
      "Step 147600 (epoch  171.49), loss: 0.002922, time:  5.456 s, error: 0.016558\n",
      "Step 147650 (epoch  171.55), loss: 0.003667, time:  8.678 s\n",
      "Step 147700 (epoch  171.61), loss: 0.003645, time:  5.448 s\n",
      "Step 147750 (epoch  171.67), loss: 0.003743, time:  5.454 s\n",
      "Step 147800 (epoch  171.72), loss: 0.002911, time:  5.453 s\n",
      "Step 147850 (epoch  171.78), loss: 0.004631, time:  5.446 s\n",
      "Step 147900 (epoch  171.84), loss: 0.005030, time:  5.443 s\n",
      "Step 147950 (epoch  171.90), loss: 0.004230, time:  5.443 s\n",
      "Step 148000 (epoch  171.96), loss: 0.004016, time:  5.459 s, error: 0.016582\n",
      "\n",
      "Time since beginning  : 17284.978 s\n",
      "\n",
      "Step 148050 (epoch  172.02), loss: 0.003635, time:  8.757 s\n",
      "Step 148100 (epoch  172.07), loss: 0.004260, time:  5.435 s\n",
      "Step 148150 (epoch  172.13), loss: 0.003247, time:  5.442 s\n",
      "Step 148200 (epoch  172.19), loss: 0.002705, time:  5.457 s\n",
      "Step 148250 (epoch  172.25), loss: 0.002784, time:  5.440 s\n",
      "Step 148300 (epoch  172.31), loss: 0.003473, time:  5.432 s\n",
      "Step 148350 (epoch  172.36), loss: 0.003755, time:  5.452 s\n",
      "Step 148400 (epoch  172.42), loss: 0.003410, time:  5.461 s, error: 0.016462\n",
      "Step 148450 (epoch  172.48), loss: 0.003745, time:  8.663 s\n",
      "Step 148500 (epoch  172.54), loss: 0.004392, time:  5.442 s\n",
      "Step 148550 (epoch  172.60), loss: 0.003904, time:  5.439 s\n",
      "Step 148600 (epoch  172.65), loss: 0.002804, time:  5.454 s\n",
      "Step 148650 (epoch  172.71), loss: 0.003530, time:  5.446 s\n",
      "Step 148700 (epoch  172.77), loss: 0.004080, time:  5.445 s\n",
      "Step 148750 (epoch  172.83), loss: 0.004458, time:  5.435 s\n",
      "Step 148800 (epoch  172.89), loss: 0.003097, time:  5.451 s, error: 0.016654\n",
      "Step 148850 (epoch  172.94), loss: 0.004775, time:  8.653 s\n",
      "Step 148900 (epoch  173.00), loss: 0.002568, time:  5.468 s\n",
      "Step 148950 (epoch  173.06), loss: 0.003307, time:  5.456 s\n",
      "Step 149000 (epoch  173.12), loss: 0.002603, time:  5.458 s\n",
      "Step 149050 (epoch  173.18), loss: 0.002488, time:  5.447 s\n",
      "Step 149100 (epoch  173.24), loss: 0.005685, time:  5.443 s\n",
      "Step 149150 (epoch  173.29), loss: 0.004379, time:  5.454 s\n",
      "Step 149200 (epoch  173.35), loss: 0.003870, time:  5.444 s, error: 0.016817\n",
      "Step 149250 (epoch  173.41), loss: 0.001603, time:  8.661 s\n",
      "Step 149300 (epoch  173.47), loss: 0.004094, time:  5.454 s\n",
      "Step 149350 (epoch  173.53), loss: 0.004280, time:  5.447 s\n",
      "Step 149400 (epoch  173.58), loss: 0.004020, time:  5.454 s\n",
      "Step 149450 (epoch  173.64), loss: 0.002515, time:  5.442 s\n",
      "Step 149500 (epoch  173.70), loss: 0.004175, time:  5.454 s\n",
      "Step 149550 (epoch  173.76), loss: 0.003872, time:  5.438 s\n",
      "Step 149600 (epoch  173.82), loss: 0.004875, time:  5.438 s, error: 0.017563\n",
      "Step 149650 (epoch  173.87), loss: 0.009884, time:  8.714 s\n",
      "Step 149700 (epoch  173.93), loss: 0.003541, time:  5.439 s\n",
      "Step 149750 (epoch  173.99), loss: 0.003630, time:  5.451 s\n",
      "Step 149800 (epoch  174.05), loss: 0.003994, time:  5.444 s\n",
      "Step 149850 (epoch  174.11), loss: 0.002785, time:  5.451 s\n",
      "Step 149900 (epoch  174.16), loss: 0.003000, time:  5.455 s\n",
      "Step 149950 (epoch  174.22), loss: 0.006785, time:  5.453 s\n",
      "Step 150000 (epoch  174.28), loss: 0.002215, time:  5.463 s, error: 0.016546\n",
      "\n",
      "Time since beginning  : 17519.093 s\n",
      "\n",
      "Step 150050 (epoch  174.34), loss: 0.003941, time:  8.755 s\n",
      "Step 150100 (epoch  174.40), loss: 0.004542, time:  5.462 s\n",
      "Step 150150 (epoch  174.46), loss: 0.004565, time:  5.456 s\n",
      "Step 150200 (epoch  174.51), loss: 0.003407, time:  5.463 s\n",
      "Step 150250 (epoch  174.57), loss: 0.003977, time:  5.465 s\n",
      "Step 150300 (epoch  174.63), loss: 0.003509, time:  5.466 s\n",
      "Step 150350 (epoch  174.69), loss: 0.003252, time:  5.459 s\n",
      "Step 150400 (epoch  174.75), loss: 0.003006, time:  5.462 s, error: 0.016545\n",
      "Step 150450 (epoch  174.80), loss: 0.004048, time:  8.718 s\n",
      "Step 150500 (epoch  174.86), loss: 0.004550, time:  5.467 s\n",
      "Step 150550 (epoch  174.92), loss: 0.004409, time:  5.465 s\n",
      "Step 150600 (epoch  174.98), loss: 0.003240, time:  5.458 s\n",
      "Step 150650 (epoch  175.04), loss: 0.002978, time:  5.459 s\n",
      "Step 150700 (epoch  175.09), loss: 0.003740, time:  5.464 s\n",
      "Step 150750 (epoch  175.15), loss: 0.005014, time:  5.466 s\n",
      "Step 150800 (epoch  175.21), loss: 0.003725, time:  5.457 s, error: 0.016734\n",
      "Step 150850 (epoch  175.27), loss: 0.002767, time:  8.760 s\n",
      "Step 150900 (epoch  175.33), loss: 0.002900, time:  5.465 s\n",
      "Step 150950 (epoch  175.38), loss: 0.003414, time:  5.445 s\n",
      "Step 151000 (epoch  175.44), loss: 0.003329, time:  5.440 s\n",
      "Step 151050 (epoch  175.50), loss: 0.003517, time:  5.423 s\n",
      "Step 151100 (epoch  175.56), loss: 0.003867, time:  5.443 s\n",
      "Step 151150 (epoch  175.62), loss: 0.004840, time:  5.452 s\n",
      "Step 151200 (epoch  175.68), loss: 0.003672, time:  5.437 s, error: 0.016440\n",
      "Step 151250 (epoch  175.73), loss: 0.004563, time:  8.612 s\n",
      "Step 151300 (epoch  175.79), loss: 0.005066, time:  5.452 s\n",
      "Step 151350 (epoch  175.85), loss: 0.004275, time:  5.433 s\n",
      "Step 151400 (epoch  175.91), loss: 0.003473, time:  5.441 s\n",
      "Step 151450 (epoch  175.97), loss: 0.002340, time:  5.450 s\n",
      "Step 151500 (epoch  176.02), loss: 0.003615, time:  5.454 s\n",
      "Step 151550 (epoch  176.08), loss: 0.003251, time:  5.418 s\n",
      "Step 151600 (epoch  176.14), loss: 0.003261, time:  5.431 s, error: 0.016836\n",
      "Step 151650 (epoch  176.20), loss: 0.003395, time:  8.638 s\n",
      "Step 151700 (epoch  176.26), loss: 0.003666, time:  5.441 s\n",
      "Step 151750 (epoch  176.31), loss: 0.002116, time:  5.436 s\n",
      "Step 151800 (epoch  176.37), loss: 0.002642, time:  5.426 s\n",
      "Step 151850 (epoch  176.43), loss: 0.004782, time:  5.445 s\n",
      "Step 151900 (epoch  176.49), loss: 0.002678, time:  5.430 s\n",
      "Step 151950 (epoch  176.55), loss: 0.004510, time:  5.419 s\n",
      "Step 152000 (epoch  176.60), loss: 0.003051, time:  5.433 s, error: 0.016483\n",
      "\n",
      "Time since beginning  : 17753.217 s\n",
      "\n",
      "Step 152050 (epoch  176.66), loss: 0.002887, time:  8.683 s\n",
      "Step 152100 (epoch  176.72), loss: 0.003645, time:  5.444 s\n",
      "Step 152150 (epoch  176.78), loss: 0.003776, time:  5.438 s\n",
      "Step 152200 (epoch  176.84), loss: 0.004011, time:  5.445 s\n",
      "Step 152250 (epoch  176.90), loss: 0.003777, time:  5.425 s\n",
      "Step 152300 (epoch  176.95), loss: 0.002764, time:  5.436 s\n",
      "Step 152350 (epoch  177.01), loss: 0.002785, time:  5.433 s\n",
      "Step 152400 (epoch  177.07), loss: 0.007269, time:  5.422 s, error: 0.016578\n",
      "Step 152450 (epoch  177.13), loss: 0.002477, time:  8.647 s\n",
      "Step 152500 (epoch  177.19), loss: 0.003548, time:  5.437 s\n",
      "Step 152550 (epoch  177.24), loss: 0.003875, time:  5.447 s\n",
      "Step 152600 (epoch  177.30), loss: 0.002273, time:  5.443 s\n",
      "Step 152650 (epoch  177.36), loss: 0.002974, time:  5.451 s\n",
      "Step 152700 (epoch  177.42), loss: 0.003258, time:  5.441 s\n",
      "Step 152750 (epoch  177.48), loss: 0.005063, time:  5.441 s\n",
      "Step 152800 (epoch  177.53), loss: 0.003209, time:  5.442 s, error: 0.016787\n",
      "Step 152850 (epoch  177.59), loss: 0.002855, time:  8.635 s\n",
      "Step 152900 (epoch  177.65), loss: 0.003789, time:  5.460 s\n",
      "Step 152950 (epoch  177.71), loss: 0.003621, time:  5.453 s\n",
      "Step 153000 (epoch  177.77), loss: 0.003605, time:  5.443 s\n",
      "Step 153050 (epoch  177.82), loss: 0.003441, time:  5.455 s\n",
      "Step 153100 (epoch  177.88), loss: 0.011798, time:  5.447 s\n",
      "Step 153150 (epoch  177.94), loss: 0.003618, time:  5.445 s\n",
      "Step 153200 (epoch  178.00), loss: 0.002097, time:  5.430 s, error: 0.016443\n",
      "Step 153250 (epoch  178.06), loss: 0.002618, time:  8.640 s\n",
      "Step 153300 (epoch  178.11), loss: 0.002709, time:  5.450 s\n",
      "Step 153350 (epoch  178.17), loss: 0.002416, time:  5.452 s\n",
      "Step 153400 (epoch  178.23), loss: 0.004134, time:  5.456 s\n",
      "Step 153450 (epoch  178.29), loss: 0.002927, time:  5.456 s\n",
      "Step 153500 (epoch  178.35), loss: 0.003402, time:  5.436 s\n",
      "Step 153550 (epoch  178.41), loss: 0.003573, time:  5.451 s\n",
      "Step 153600 (epoch  178.46), loss: 0.002973, time:  5.457 s, error: 0.016615\n",
      "Step 153650 (epoch  178.52), loss: 0.003886, time:  8.672 s\n",
      "Step 153700 (epoch  178.58), loss: 0.003745, time:  5.458 s\n",
      "Step 153750 (epoch  178.64), loss: 0.003014, time:  5.451 s\n",
      "Step 153800 (epoch  178.70), loss: 0.003149, time:  5.433 s\n",
      "Step 153850 (epoch  178.75), loss: 0.002922, time:  5.452 s\n",
      "Step 153900 (epoch  178.81), loss: 0.003203, time:  5.454 s\n",
      "Step 153950 (epoch  178.87), loss: 0.003987, time:  5.448 s\n",
      "Step 154000 (epoch  178.93), loss: 0.004224, time:  5.460 s, error: 0.016493\n",
      "\n",
      "Time since beginning  : 17987.232 s\n",
      "\n",
      "Step 154050 (epoch  178.99), loss: 0.003262, time:  8.857 s\n",
      "Step 154100 (epoch  179.04), loss: 0.003716, time:  5.433 s\n",
      "Step 154150 (epoch  179.10), loss: 0.003995, time:  5.436 s\n",
      "Step 154200 (epoch  179.16), loss: 0.004109, time:  5.456 s\n",
      "Step 154250 (epoch  179.22), loss: 0.006163, time:  5.458 s\n",
      "Step 154300 (epoch  179.28), loss: 0.003335, time:  5.443 s\n",
      "Step 154350 (epoch  179.33), loss: 0.004902, time:  5.450 s\n",
      "Step 154400 (epoch  179.39), loss: 0.004169, time:  5.461 s, error: 0.016577\n",
      "Step 154450 (epoch  179.45), loss: 0.002997, time:  8.616 s\n",
      "Step 154500 (epoch  179.51), loss: 0.003509, time:  5.450 s\n",
      "Step 154550 (epoch  179.57), loss: 0.005282, time:  5.457 s\n",
      "Step 154600 (epoch  179.63), loss: 0.002601, time:  5.464 s\n",
      "Step 154650 (epoch  179.68), loss: 0.003367, time:  5.452 s\n",
      "Step 154700 (epoch  179.74), loss: 0.002893, time:  5.458 s\n",
      "Step 154750 (epoch  179.80), loss: 0.005432, time:  5.469 s\n",
      "Step 154800 (epoch  179.86), loss: 0.002488, time:  5.463 s, error: 0.016895\n",
      "Step 154850 (epoch  179.92), loss: 0.004174, time:  8.762 s\n",
      "Step 154900 (epoch  179.97), loss: 0.004515, time:  5.462 s\n",
      "Step 154950 (epoch  180.03), loss: 0.002952, time:  5.462 s\n",
      "Step 155000 (epoch  180.09), loss: 0.003334, time:  5.451 s\n",
      "Step 155050 (epoch  180.15), loss: 0.003672, time:  5.460 s\n",
      "Step 155100 (epoch  180.21), loss: 0.004249, time:  5.467 s\n",
      "Step 155150 (epoch  180.26), loss: 0.003447, time:  5.466 s\n",
      "Step 155200 (epoch  180.32), loss: 0.003231, time:  5.460 s, error: 0.016470\n",
      "Step 155250 (epoch  180.38), loss: 0.002808, time:  8.743 s\n",
      "Step 155300 (epoch  180.44), loss: 0.002747, time:  5.420 s\n",
      "Step 155350 (epoch  180.50), loss: 0.004712, time:  5.435 s\n",
      "Step 155400 (epoch  180.55), loss: 0.004390, time:  5.421 s\n",
      "Step 155450 (epoch  180.61), loss: 0.004312, time:  5.432 s\n",
      "Step 155500 (epoch  180.67), loss: 0.002617, time:  5.447 s\n",
      "Step 155550 (epoch  180.73), loss: 0.005488, time:  5.429 s\n",
      "Step 155600 (epoch  180.79), loss: 0.003283, time:  5.445 s, error: 0.016421\n",
      "Step 155650 (epoch  180.85), loss: 0.002696, time:  8.636 s\n",
      "Step 155700 (epoch  180.90), loss: 0.004277, time:  5.455 s\n",
      "Step 155750 (epoch  180.96), loss: 0.002750, time:  5.448 s\n",
      "Step 155800 (epoch  181.02), loss: 0.002951, time:  5.427 s\n",
      "Step 155850 (epoch  181.08), loss: 0.004118, time:  5.443 s\n",
      "Step 155900 (epoch  181.14), loss: 0.002904, time:  5.425 s\n",
      "Step 155950 (epoch  181.19), loss: 0.003434, time:  5.430 s\n",
      "Step 156000 (epoch  181.25), loss: 0.003573, time:  5.430 s, error: 0.016815\n",
      "\n",
      "Time since beginning  : 18221.368 s\n",
      "\n",
      "Step 156050 (epoch  181.31), loss: 0.002662, time:  8.701 s\n",
      "Step 156100 (epoch  181.37), loss: 0.003147, time:  5.432 s\n",
      "Step 156150 (epoch  181.43), loss: 0.004343, time:  5.453 s\n",
      "Step 156200 (epoch  181.48), loss: 0.003543, time:  5.457 s\n",
      "Step 156250 (epoch  181.54), loss: 0.004341, time:  5.445 s\n",
      "Step 156300 (epoch  181.60), loss: 0.002667, time:  5.447 s\n",
      "Step 156350 (epoch  181.66), loss: 0.003160, time:  5.448 s\n",
      "Step 156400 (epoch  181.72), loss: 0.003453, time:  5.454 s, error: 0.016947\n",
      "Step 156450 (epoch  181.77), loss: 0.003952, time:  8.655 s\n",
      "Step 156500 (epoch  181.83), loss: 0.003709, time:  5.444 s\n",
      "Step 156550 (epoch  181.89), loss: 0.002870, time:  5.450 s\n",
      "Step 156600 (epoch  181.95), loss: 0.003342, time:  5.447 s\n",
      "Step 156650 (epoch  182.01), loss: 0.002976, time:  5.437 s\n",
      "Step 156700 (epoch  182.07), loss: 0.003264, time:  5.427 s\n",
      "Step 156750 (epoch  182.12), loss: 0.003597, time:  5.448 s\n",
      "Step 156800 (epoch  182.18), loss: 0.006291, time:  5.459 s, error: 0.016920\n",
      "Step 156850 (epoch  182.24), loss: 0.003728, time:  8.648 s\n",
      "Step 156900 (epoch  182.30), loss: 0.003073, time:  5.455 s\n",
      "Step 156950 (epoch  182.36), loss: 0.003855, time:  5.441 s\n",
      "Step 157000 (epoch  182.41), loss: 0.003135, time:  5.449 s\n",
      "Step 157050 (epoch  182.47), loss: 0.003758, time:  5.454 s\n",
      "Step 157100 (epoch  182.53), loss: 0.003385, time:  5.455 s\n",
      "Step 157150 (epoch  182.59), loss: 0.002615, time:  5.439 s\n",
      "Step 157200 (epoch  182.65), loss: 0.002953, time:  5.440 s, error: 0.016518\n",
      "Step 157250 (epoch  182.70), loss: 0.002888, time:  8.684 s\n",
      "Step 157300 (epoch  182.76), loss: 0.002791, time:  5.454 s\n",
      "Step 157350 (epoch  182.82), loss: 0.003147, time:  5.434 s\n",
      "Step 157400 (epoch  182.88), loss: 0.002658, time:  5.445 s\n",
      "Step 157450 (epoch  182.94), loss: 0.003679, time:  5.449 s\n",
      "Step 157500 (epoch  182.99), loss: 0.003302, time:  5.445 s\n",
      "Step 157550 (epoch  183.05), loss: 0.003799, time:  5.448 s\n",
      "Step 157600 (epoch  183.11), loss: 0.004618, time:  5.438 s, error: 0.017015\n",
      "Step 157650 (epoch  183.17), loss: 0.003037, time:  8.676 s\n",
      "Step 157700 (epoch  183.23), loss: 0.003742, time:  5.444 s\n",
      "Step 157750 (epoch  183.29), loss: 0.003584, time:  5.448 s\n",
      "Step 157800 (epoch  183.34), loss: 0.004512, time:  5.450 s\n",
      "Step 157850 (epoch  183.40), loss: 0.002169, time:  5.435 s\n",
      "Step 157900 (epoch  183.46), loss: 0.004179, time:  5.447 s\n",
      "Step 157950 (epoch  183.52), loss: 0.002786, time:  5.435 s\n",
      "Step 158000 (epoch  183.58), loss: 0.004589, time:  5.447 s, error: 0.017933\n",
      "\n",
      "Time since beginning  : 18455.424 s\n",
      "\n",
      "Step 158050 (epoch  183.63), loss: 0.004420, time:  8.809 s\n",
      "Step 158100 (epoch  183.69), loss: 0.002660, time:  5.461 s\n",
      "Step 158150 (epoch  183.75), loss: 0.004677, time:  5.459 s\n",
      "Step 158200 (epoch  183.81), loss: 0.004399, time:  5.450 s\n",
      "Step 158250 (epoch  183.87), loss: 0.004301, time:  5.450 s\n",
      "Step 158300 (epoch  183.92), loss: 0.003881, time:  5.443 s\n",
      "Step 158350 (epoch  183.98), loss: 0.003989, time:  5.456 s\n",
      "Step 158400 (epoch  184.04), loss: 0.002989, time:  5.441 s, error: 0.016877\n",
      "Step 158450 (epoch  184.10), loss: 0.002628, time:  8.730 s\n",
      "Step 158500 (epoch  184.16), loss: 0.003687, time:  5.448 s\n",
      "Step 158550 (epoch  184.21), loss: 0.005026, time:  5.446 s\n",
      "Step 158600 (epoch  184.27), loss: 0.002172, time:  5.442 s\n",
      "Step 158650 (epoch  184.33), loss: 0.004316, time:  5.451 s\n",
      "Step 158700 (epoch  184.39), loss: 0.002499, time:  5.457 s\n",
      "Step 158750 (epoch  184.45), loss: 0.002997, time:  5.453 s\n",
      "Step 158800 (epoch  184.51), loss: 0.003118, time:  5.455 s, error: 0.016545\n",
      "Step 158850 (epoch  184.56), loss: 0.004448, time:  8.668 s\n",
      "Step 158900 (epoch  184.62), loss: 0.003199, time:  5.439 s\n",
      "Step 158950 (epoch  184.68), loss: 0.004089, time:  5.441 s\n",
      "Step 159000 (epoch  184.74), loss: 0.002653, time:  5.454 s\n",
      "Step 159050 (epoch  184.80), loss: 0.003126, time:  5.456 s\n",
      "Step 159100 (epoch  184.85), loss: 0.003870, time:  5.468 s\n",
      "Step 159150 (epoch  184.91), loss: 0.004959, time:  5.448 s\n",
      "Step 159200 (epoch  184.97), loss: 0.003409, time:  5.452 s, error: 0.016785\n",
      "Step 159250 (epoch  185.03), loss: 0.003019, time:  8.711 s\n",
      "Step 159300 (epoch  185.09), loss: 0.003051, time:  5.461 s\n",
      "Step 159350 (epoch  185.14), loss: 0.005584, time:  5.455 s\n",
      "Step 159400 (epoch  185.20), loss: 0.003647, time:  5.440 s\n",
      "Step 159450 (epoch  185.26), loss: 0.002326, time:  5.437 s\n",
      "Step 159500 (epoch  185.32), loss: 0.003638, time:  5.448 s\n",
      "Step 159550 (epoch  185.38), loss: 0.003163, time:  5.425 s\n",
      "Step 159600 (epoch  185.43), loss: 0.004340, time:  5.425 s, error: 0.016620\n",
      "Step 159650 (epoch  185.49), loss: 0.004862, time:  8.637 s\n",
      "Step 159700 (epoch  185.55), loss: 0.004147, time:  5.444 s\n",
      "Step 159750 (epoch  185.61), loss: 0.008417, time:  5.422 s\n",
      "Step 159800 (epoch  185.67), loss: 0.002948, time:  5.422 s\n",
      "Step 159850 (epoch  185.73), loss: 0.005118, time:  5.441 s\n",
      "Step 159900 (epoch  185.78), loss: 0.006388, time:  5.434 s\n",
      "Step 159950 (epoch  185.84), loss: 0.003198, time:  5.424 s\n",
      "Step 160000 (epoch  185.90), loss: 0.004017, time:  5.438 s, error: 0.016691\n",
      "\n",
      "Time since beginning  : 18689.473 s\n",
      "\n",
      "Step 160050 (epoch  185.96), loss: 0.003923, time:  8.690 s\n",
      "Step 160100 (epoch  186.02), loss: 0.003294, time:  5.415 s\n",
      "Step 160150 (epoch  186.07), loss: 0.003004, time:  5.416 s\n",
      "Step 160200 (epoch  186.13), loss: 0.004164, time:  5.433 s\n",
      "Step 160250 (epoch  186.19), loss: 0.002494, time:  5.433 s\n",
      "Step 160300 (epoch  186.25), loss: 0.003340, time:  5.418 s\n",
      "Step 160350 (epoch  186.31), loss: 0.004654, time:  5.434 s\n",
      "Step 160400 (epoch  186.36), loss: 0.003447, time:  5.437 s, error: 0.016660\n",
      "Step 160450 (epoch  186.42), loss: 0.003035, time:  8.638 s\n",
      "Step 160500 (epoch  186.48), loss: 0.002144, time:  5.432 s\n",
      "Step 160550 (epoch  186.54), loss: 0.003589, time:  5.434 s\n",
      "Step 160600 (epoch  186.60), loss: 0.004206, time:  5.453 s\n",
      "Step 160650 (epoch  186.65), loss: 0.004395, time:  5.446 s\n",
      "Step 160700 (epoch  186.71), loss: 0.003113, time:  5.428 s\n",
      "Step 160750 (epoch  186.77), loss: 0.004197, time:  5.416 s\n",
      "Step 160800 (epoch  186.83), loss: 0.004461, time:  5.423 s, error: 0.017288\n",
      "Step 160850 (epoch  186.89), loss: 0.003631, time:  8.629 s\n",
      "Step 160900 (epoch  186.95), loss: 0.003114, time:  5.447 s\n",
      "Step 160950 (epoch  187.00), loss: 0.002813, time:  5.462 s\n",
      "Step 161000 (epoch  187.06), loss: 0.003077, time:  5.445 s\n",
      "Step 161050 (epoch  187.12), loss: 0.003141, time:  5.430 s\n",
      "Step 161100 (epoch  187.18), loss: 0.003485, time:  5.441 s\n",
      "Step 161150 (epoch  187.24), loss: 0.004010, time:  5.437 s\n",
      "Step 161200 (epoch  187.29), loss: 0.003006, time:  5.439 s, error: 0.016865\n",
      "Step 161250 (epoch  187.35), loss: 0.002896, time:  8.639 s\n",
      "Step 161300 (epoch  187.41), loss: 0.003129, time:  5.438 s\n",
      "Step 161350 (epoch  187.47), loss: 0.003838, time:  5.431 s\n",
      "Step 161400 (epoch  187.53), loss: 0.004169, time:  5.423 s\n",
      "Step 161450 (epoch  187.58), loss: 0.002445, time:  5.436 s\n",
      "Step 161500 (epoch  187.64), loss: 0.002591, time:  5.429 s\n",
      "Step 161550 (epoch  187.70), loss: 0.003348, time:  5.430 s\n",
      "Step 161600 (epoch  187.76), loss: 0.003873, time:  5.434 s, error: 0.016610\n",
      "Step 161650 (epoch  187.82), loss: 0.003855, time:  8.636 s\n",
      "Step 161700 (epoch  187.87), loss: 0.003093, time:  5.453 s\n",
      "Step 161750 (epoch  187.93), loss: 0.004538, time:  5.435 s\n",
      "Step 161800 (epoch  187.99), loss: 0.003053, time:  5.424 s\n",
      "Step 161850 (epoch  188.05), loss: 0.003298, time:  5.445 s\n",
      "Step 161900 (epoch  188.11), loss: 0.003434, time:  5.421 s\n",
      "Step 161950 (epoch  188.17), loss: 0.003818, time:  5.436 s\n",
      "Step 162000 (epoch  188.22), loss: 0.005210, time:  5.430 s, error: 0.016970\n",
      "\n",
      "Time since beginning  : 18922.897 s\n",
      "\n",
      "Step 162050 (epoch  188.28), loss: 0.004420, time:  8.725 s\n",
      "Step 162100 (epoch  188.34), loss: 0.005614, time:  5.451 s\n",
      "Step 162150 (epoch  188.40), loss: 0.002956, time:  5.432 s\n",
      "Step 162200 (epoch  188.46), loss: 0.003031, time:  5.445 s\n",
      "Step 162250 (epoch  188.51), loss: 0.003307, time:  5.426 s\n",
      "Step 162300 (epoch  188.57), loss: 0.003042, time:  5.425 s\n",
      "Step 162350 (epoch  188.63), loss: 0.004117, time:  5.432 s\n",
      "Step 162400 (epoch  188.69), loss: 0.003907, time:  5.437 s, error: 0.016493\n",
      "Step 162450 (epoch  188.75), loss: 0.003033, time:  8.733 s\n",
      "Step 162500 (epoch  188.80), loss: 0.004901, time:  5.457 s\n",
      "Step 162550 (epoch  188.86), loss: 0.003920, time:  5.457 s\n",
      "Step 162600 (epoch  188.92), loss: 0.002675, time:  5.436 s\n",
      "Step 162650 (epoch  188.98), loss: 0.002691, time:  5.421 s\n",
      "Step 162700 (epoch  189.04), loss: 0.003635, time:  5.427 s\n",
      "Step 162750 (epoch  189.09), loss: 0.003554, time:  5.426 s\n",
      "Step 162800 (epoch  189.15), loss: 0.004329, time:  5.438 s, error: 0.016626\n",
      "Step 162850 (epoch  189.21), loss: 0.003994, time:  8.630 s\n",
      "Step 162900 (epoch  189.27), loss: 0.002348, time:  5.451 s\n",
      "Step 162950 (epoch  189.33), loss: 0.005516, time:  5.427 s\n",
      "Step 163000 (epoch  189.39), loss: 0.003581, time:  5.427 s\n",
      "Step 163050 (epoch  189.44), loss: 0.002515, time:  5.425 s\n",
      "Step 163100 (epoch  189.50), loss: 0.002168, time:  5.415 s\n",
      "Step 163150 (epoch  189.56), loss: 0.004208, time:  5.435 s\n",
      "Step 163200 (epoch  189.62), loss: 0.004487, time:  5.443 s, error: 0.016521\n",
      "Step 163250 (epoch  189.68), loss: 0.004024, time:  8.621 s\n",
      "Step 163300 (epoch  189.73), loss: 0.003727, time:  5.426 s\n",
      "Step 163350 (epoch  189.79), loss: 0.003568, time:  5.424 s\n",
      "Step 163400 (epoch  189.85), loss: 0.003328, time:  5.428 s\n",
      "Step 163450 (epoch  189.91), loss: 0.004955, time:  5.440 s\n",
      "Step 163500 (epoch  189.97), loss: 0.003791, time:  5.417 s\n",
      "Step 163550 (epoch  190.02), loss: 0.002809, time:  5.460 s\n",
      "Step 163600 (epoch  190.08), loss: 0.003502, time:  5.442 s, error: 0.016887\n",
      "Step 163650 (epoch  190.14), loss: 0.003171, time:  8.635 s\n",
      "Step 163700 (epoch  190.20), loss: 0.003475, time:  5.434 s\n",
      "Step 163750 (epoch  190.26), loss: 0.002808, time:  5.435 s\n",
      "Step 163800 (epoch  190.31), loss: 0.002864, time:  5.443 s\n",
      "Step 163850 (epoch  190.37), loss: 0.005051, time:  5.432 s\n",
      "Step 163900 (epoch  190.43), loss: 0.003053, time:  5.449 s\n",
      "Step 163950 (epoch  190.49), loss: 0.003018, time:  5.429 s\n",
      "Step 164000 (epoch  190.55), loss: 0.004415, time:  5.440 s, error: 0.017248\n",
      "\n",
      "Time since beginning  : 19156.481 s\n",
      "\n",
      "Step 164050 (epoch  190.61), loss: 0.003903, time:  8.704 s\n",
      "Step 164100 (epoch  190.66), loss: 0.003813, time:  5.429 s\n",
      "Step 164150 (epoch  190.72), loss: 0.002454, time:  5.435 s\n",
      "Step 164200 (epoch  190.78), loss: 0.003410, time:  5.433 s\n",
      "Step 164250 (epoch  190.84), loss: 0.004851, time:  5.445 s\n",
      "Step 164300 (epoch  190.90), loss: 0.003724, time:  5.437 s\n",
      "Step 164350 (epoch  190.95), loss: 0.003386, time:  5.439 s\n",
      "Step 164400 (epoch  191.01), loss: 0.003177, time:  5.437 s, error: 0.016738\n",
      "Step 164450 (epoch  191.07), loss: 0.002908, time:  8.653 s\n",
      "Step 164500 (epoch  191.13), loss: 0.003119, time:  5.424 s\n",
      "Step 164550 (epoch  191.19), loss: 0.003205, time:  5.430 s\n",
      "Step 164600 (epoch  191.24), loss: 0.004129, time:  5.457 s\n",
      "Step 164650 (epoch  191.30), loss: 0.003824, time:  5.446 s\n",
      "Step 164700 (epoch  191.36), loss: 0.003511, time:  5.441 s\n",
      "Step 164750 (epoch  191.42), loss: 0.003110, time:  5.449 s\n",
      "Step 164800 (epoch  191.48), loss: 0.003579, time:  5.443 s, error: 0.016637\n",
      "Step 164850 (epoch  191.53), loss: 0.003667, time:  8.633 s\n",
      "Step 164900 (epoch  191.59), loss: 0.004046, time:  5.430 s\n",
      "Step 164950 (epoch  191.65), loss: 0.003964, time:  5.428 s\n",
      "Step 165000 (epoch  191.71), loss: 0.002486, time:  5.456 s\n",
      "Step 165050 (epoch  191.77), loss: 0.004316, time:  5.424 s\n",
      "Step 165100 (epoch  191.83), loss: 0.003039, time:  5.424 s\n",
      "Step 165150 (epoch  191.88), loss: 0.003916, time:  5.433 s\n",
      "Step 165200 (epoch  191.94), loss: 0.003633, time:  5.434 s, error: 0.016456\n",
      "Step 165250 (epoch  192.00), loss: 0.003182, time:  8.609 s\n",
      "Step 165300 (epoch  192.06), loss: 0.003237, time:  5.426 s\n",
      "Step 165350 (epoch  192.12), loss: 0.003039, time:  5.445 s\n",
      "Step 165400 (epoch  192.17), loss: 0.003485, time:  5.422 s\n",
      "Step 165450 (epoch  192.23), loss: 0.002643, time:  5.424 s\n",
      "Step 165500 (epoch  192.29), loss: 0.003053, time:  5.425 s\n",
      "Step 165550 (epoch  192.35), loss: 0.003785, time:  5.427 s\n",
      "Step 165600 (epoch  192.41), loss: 0.004455, time:  5.426 s, error: 0.016413\n",
      "Step 165650 (epoch  192.46), loss: 0.002888, time:  8.632 s\n",
      "Step 165700 (epoch  192.52), loss: 0.003687, time:  5.437 s\n",
      "Step 165750 (epoch  192.58), loss: 0.001938, time:  5.424 s\n",
      "Step 165800 (epoch  192.64), loss: 0.003030, time:  5.419 s\n",
      "Step 165850 (epoch  192.70), loss: 0.004791, time:  5.434 s\n",
      "Step 165900 (epoch  192.75), loss: 0.003269, time:  5.422 s\n",
      "Step 165950 (epoch  192.81), loss: 0.002825, time:  5.427 s\n",
      "Step 166000 (epoch  192.87), loss: 0.004378, time:  5.424 s, error: 0.017090\n",
      "\n",
      "Time since beginning  : 19389.859 s\n",
      "\n",
      "Step 166050 (epoch  192.93), loss: 0.002530, time:  8.695 s\n",
      "Step 166100 (epoch  192.99), loss: 0.002965, time:  5.446 s\n",
      "Step 166150 (epoch  193.05), loss: 0.004365, time:  5.439 s\n",
      "Step 166200 (epoch  193.10), loss: 0.003366, time:  5.430 s\n",
      "Step 166250 (epoch  193.16), loss: 0.005519, time:  5.436 s\n",
      "Step 166300 (epoch  193.22), loss: 0.002705, time:  5.421 s\n",
      "Step 166350 (epoch  193.28), loss: 0.003685, time:  5.443 s\n",
      "Step 166400 (epoch  193.34), loss: 0.004013, time:  5.421 s, error: 0.016677\n",
      "Step 166450 (epoch  193.39), loss: 0.004026, time:  8.619 s\n",
      "Step 166500 (epoch  193.45), loss: 0.002599, time:  5.427 s\n",
      "Step 166550 (epoch  193.51), loss: 0.002271, time:  5.433 s\n",
      "Step 166600 (epoch  193.57), loss: 0.004582, time:  5.435 s\n",
      "Step 166650 (epoch  193.63), loss: 0.003557, time:  5.417 s\n",
      "Step 166700 (epoch  193.68), loss: 0.003784, time:  5.423 s\n",
      "Step 166750 (epoch  193.74), loss: 0.002663, time:  5.441 s\n",
      "Step 166800 (epoch  193.80), loss: 0.002843, time:  5.442 s, error: 0.016815\n",
      "Step 166850 (epoch  193.86), loss: 0.004282, time:  8.777 s\n",
      "Step 166900 (epoch  193.92), loss: 0.003820, time:  5.425 s\n",
      "Step 166950 (epoch  193.97), loss: 0.003364, time:  5.435 s\n",
      "Step 167000 (epoch  194.03), loss: 0.002914, time:  5.422 s\n",
      "Step 167050 (epoch  194.09), loss: 0.002624, time:  5.441 s\n",
      "Step 167100 (epoch  194.15), loss: 0.002654, time:  5.426 s\n",
      "Step 167150 (epoch  194.21), loss: 0.003395, time:  5.419 s\n",
      "Step 167200 (epoch  194.26), loss: 0.002476, time:  5.455 s, error: 0.016515\n",
      "Step 167250 (epoch  194.32), loss: 0.003727, time:  8.625 s\n",
      "Step 167300 (epoch  194.38), loss: 0.002548, time:  5.426 s\n",
      "Step 167350 (epoch  194.44), loss: 0.003745, time:  5.423 s\n",
      "Step 167400 (epoch  194.50), loss: 0.004700, time:  5.443 s\n",
      "Step 167450 (epoch  194.56), loss: 0.004379, time:  5.440 s\n",
      "Step 167500 (epoch  194.61), loss: 0.003040, time:  5.416 s\n",
      "Step 167550 (epoch  194.67), loss: 0.003953, time:  5.447 s\n",
      "Step 167600 (epoch  194.73), loss: 0.003642, time:  5.427 s, error: 0.016647\n",
      "Step 167650 (epoch  194.79), loss: 0.004350, time:  8.653 s\n",
      "Step 167700 (epoch  194.85), loss: 0.004871, time:  5.430 s\n",
      "Step 167750 (epoch  194.90), loss: 0.005306, time:  5.424 s\n",
      "Step 167800 (epoch  194.96), loss: 0.003256, time:  5.432 s\n",
      "Step 167850 (epoch  195.02), loss: 0.005350, time:  5.422 s\n",
      "Step 167900 (epoch  195.08), loss: 0.003313, time:  5.441 s\n",
      "Step 167950 (epoch  195.14), loss: 0.004988, time:  5.442 s\n",
      "Step 168000 (epoch  195.19), loss: 0.004757, time:  5.433 s, error: 0.018663\n",
      "\n",
      "Time since beginning  : 19623.383 s\n",
      "\n",
      "Step 168050 (epoch  195.25), loss: 0.003705, time:  8.747 s\n",
      "Step 168100 (epoch  195.31), loss: 0.005751, time:  5.430 s\n",
      "Step 168150 (epoch  195.37), loss: 0.003648, time:  5.421 s\n",
      "Step 168200 (epoch  195.43), loss: 0.005028, time:  5.439 s\n",
      "Step 168250 (epoch  195.48), loss: 0.004263, time:  5.435 s\n",
      "Step 168300 (epoch  195.54), loss: 0.003449, time:  5.454 s\n",
      "Step 168350 (epoch  195.60), loss: 0.005037, time:  5.426 s\n",
      "Step 168400 (epoch  195.66), loss: 0.003418, time:  5.424 s, error: 0.016888\n",
      "Step 168450 (epoch  195.72), loss: 0.002087, time:  8.621 s\n",
      "Step 168500 (epoch  195.78), loss: 0.003088, time:  5.420 s\n",
      "Step 168550 (epoch  195.83), loss: 0.003481, time:  5.449 s\n",
      "Step 168600 (epoch  195.89), loss: 0.003912, time:  5.434 s\n",
      "Step 168650 (epoch  195.95), loss: 0.002774, time:  5.450 s\n",
      "Step 168700 (epoch  196.01), loss: 0.003659, time:  5.442 s\n",
      "Step 168750 (epoch  196.07), loss: 0.004961, time:  5.433 s\n",
      "Step 168800 (epoch  196.12), loss: 0.003109, time:  5.436 s, error: 0.016607\n",
      "Step 168850 (epoch  196.18), loss: 0.003119, time:  8.667 s\n",
      "Step 168900 (epoch  196.24), loss: 0.003145, time:  5.428 s\n",
      "Step 168950 (epoch  196.30), loss: 0.003170, time:  5.429 s\n",
      "Step 169000 (epoch  196.36), loss: 0.003513, time:  5.432 s\n",
      "Step 169050 (epoch  196.41), loss: 0.002388, time:  5.427 s\n",
      "Step 169100 (epoch  196.47), loss: 0.003042, time:  5.411 s\n",
      "Step 169150 (epoch  196.53), loss: 0.004445, time:  5.421 s\n",
      "Step 169200 (epoch  196.59), loss: 0.002071, time:  5.435 s, error: 0.016545\n",
      "Step 169250 (epoch  196.65), loss: 0.003000, time:  8.636 s\n",
      "Step 169300 (epoch  196.70), loss: 0.003929, time:  5.428 s\n",
      "Step 169350 (epoch  196.76), loss: 0.003847, time:  5.426 s\n",
      "Step 169400 (epoch  196.82), loss: 0.003761, time:  5.455 s\n",
      "Step 169450 (epoch  196.88), loss: 0.002059, time:  5.433 s\n",
      "Step 169500 (epoch  196.94), loss: 0.002478, time:  5.420 s\n",
      "Step 169550 (epoch  197.00), loss: 0.003853, time:  5.426 s\n",
      "Step 169600 (epoch  197.05), loss: 0.003735, time:  5.431 s, error: 0.016563\n",
      "Step 169650 (epoch  197.11), loss: 0.003728, time:  8.612 s\n",
      "Step 169700 (epoch  197.17), loss: 0.003872, time:  5.433 s\n",
      "Step 169750 (epoch  197.23), loss: 0.003028, time:  5.449 s\n",
      "Step 169800 (epoch  197.29), loss: 0.002601, time:  5.417 s\n",
      "Step 169850 (epoch  197.34), loss: 0.002054, time:  5.434 s\n",
      "Step 169900 (epoch  197.40), loss: 0.003957, time:  5.426 s\n",
      "Step 169950 (epoch  197.46), loss: 0.002440, time:  5.433 s\n",
      "Step 170000 (epoch  197.52), loss: 0.003948, time:  5.422 s, error: 0.016902\n",
      "\n",
      "Time since beginning  : 19856.730 s\n",
      "\n",
      "Step 170050 (epoch  197.58), loss: 0.002286, time:  8.682 s\n",
      "Step 170100 (epoch  197.63), loss: 0.003270, time:  5.460 s\n",
      "Step 170150 (epoch  197.69), loss: 0.002865, time:  5.434 s\n",
      "Step 170200 (epoch  197.75), loss: 0.003436, time:  5.414 s\n",
      "Step 170250 (epoch  197.81), loss: 0.003670, time:  5.427 s\n",
      "Step 170300 (epoch  197.87), loss: 0.004601, time:  5.429 s\n",
      "Step 170350 (epoch  197.92), loss: 0.003939, time:  5.419 s\n",
      "Step 170400 (epoch  197.98), loss: 0.003078, time:  5.428 s, error: 0.016450\n",
      "Step 170450 (epoch  198.04), loss: 0.003084, time:  8.713 s\n",
      "Step 170500 (epoch  198.10), loss: 0.003323, time:  5.447 s\n",
      "Step 170550 (epoch  198.16), loss: 0.005403, time:  5.441 s\n",
      "Step 170600 (epoch  198.22), loss: 0.002892, time:  5.454 s\n",
      "Step 170650 (epoch  198.27), loss: 0.003513, time:  5.443 s\n",
      "Step 170700 (epoch  198.33), loss: 0.003554, time:  5.435 s\n",
      "Step 170750 (epoch  198.39), loss: 0.002558, time:  5.421 s\n",
      "Step 170800 (epoch  198.45), loss: 0.003859, time:  5.447 s, error: 0.016713\n",
      "Step 170850 (epoch  198.51), loss: 0.003742, time:  8.646 s\n",
      "Step 170900 (epoch  198.56), loss: 0.004897, time:  5.424 s\n",
      "Step 170950 (epoch  198.62), loss: 0.003330, time:  5.420 s\n",
      "Step 171000 (epoch  198.68), loss: 0.002213, time:  5.427 s\n",
      "Step 171050 (epoch  198.74), loss: 0.002502, time:  5.426 s\n",
      "Step 171100 (epoch  198.80), loss: 0.003276, time:  5.429 s\n",
      "Step 171150 (epoch  198.85), loss: 0.003421, time:  5.423 s\n",
      "Step 171200 (epoch  198.91), loss: 0.005455, time:  5.428 s, error: 0.016511\n",
      "Step 171250 (epoch  198.97), loss: 0.002573, time:  8.754 s\n",
      "Step 171300 (epoch  199.03), loss: 0.005528, time:  5.439 s\n",
      "Step 171350 (epoch  199.09), loss: 0.002433, time:  5.433 s\n",
      "Step 171400 (epoch  199.14), loss: 0.004039, time:  5.435 s\n",
      "Step 171450 (epoch  199.20), loss: 0.004301, time:  5.441 s\n",
      "Step 171500 (epoch  199.26), loss: 0.001978, time:  5.432 s\n",
      "Step 171550 (epoch  199.32), loss: 0.003072, time:  5.420 s\n",
      "Step 171600 (epoch  199.38), loss: 0.002181, time:  5.449 s, error: 0.016439\n",
      "Step 171650 (epoch  199.44), loss: 0.003396, time:  8.624 s\n",
      "Step 171700 (epoch  199.49), loss: 0.004658, time:  5.422 s\n",
      "Step 171750 (epoch  199.55), loss: 0.003450, time:  5.440 s\n",
      "Step 171800 (epoch  199.61), loss: 0.002741, time:  5.431 s\n",
      "Step 171850 (epoch  199.67), loss: 0.003930, time:  5.426 s\n",
      "Step 171900 (epoch  199.73), loss: 0.002594, time:  5.452 s\n",
      "Step 171950 (epoch  199.78), loss: 0.003235, time:  5.462 s\n",
      "Step 172000 (epoch  199.84), loss: 0.005396, time:  5.463 s, error: 0.016954\n",
      "\n",
      "Time since beginning  : 20090.397 s\n",
      "\n",
      "Step 172050 (epoch  199.90), loss: 0.004705, time:  8.713 s\n",
      "Step 172100 (epoch  199.96), loss: 0.003708, time:  5.434 s\n",
      "Step 172150 (epoch  200.02), loss: 0.002613, time:  5.430 s\n",
      "Step 172200 (epoch  200.07), loss: 0.003210, time:  5.423 s\n",
      "Step 172250 (epoch  200.13), loss: 0.003432, time:  5.429 s\n",
      "Step 172300 (epoch  200.19), loss: 0.004292, time:  5.461 s\n",
      "Step 172350 (epoch  200.25), loss: 0.002562, time:  5.446 s\n",
      "Step 172400 (epoch  200.31), loss: 0.003426, time:  5.438 s, error: 0.016473\n",
      "Step 172450 (epoch  200.36), loss: 0.003868, time:  8.634 s\n",
      "Step 172500 (epoch  200.42), loss: 0.002865, time:  5.416 s\n",
      "Step 172550 (epoch  200.48), loss: 0.004136, time:  5.428 s\n",
      "Step 172600 (epoch  200.54), loss: 0.005036, time:  5.436 s\n",
      "Step 172650 (epoch  200.60), loss: 0.004667, time:  5.446 s\n",
      "Step 172700 (epoch  200.66), loss: 0.004004, time:  5.448 s\n",
      "Step 172750 (epoch  200.71), loss: 0.002231, time:  5.440 s\n",
      "Step 172800 (epoch  200.77), loss: 0.003323, time:  5.440 s, error: 0.016755\n",
      "Step 172850 (epoch  200.83), loss: 0.002462, time:  8.639 s\n",
      "Step 172900 (epoch  200.89), loss: 0.004825, time:  5.438 s\n",
      "Step 172950 (epoch  200.95), loss: 0.002443, time:  5.430 s\n",
      "Step 173000 (epoch  201.00), loss: 0.004117, time:  5.446 s\n",
      "Step 173050 (epoch  201.06), loss: 0.004856, time:  5.450 s\n",
      "Step 173100 (epoch  201.12), loss: 0.003264, time:  5.444 s\n",
      "Step 173150 (epoch  201.18), loss: 0.003204, time:  5.431 s\n",
      "Step 173200 (epoch  201.24), loss: 0.002664, time:  5.434 s, error: 0.016674\n",
      "Step 173250 (epoch  201.29), loss: 0.003122, time:  8.597 s\n",
      "Step 173300 (epoch  201.35), loss: 0.003491, time:  5.432 s\n",
      "Step 173350 (epoch  201.41), loss: 0.002035, time:  5.440 s\n",
      "Step 173400 (epoch  201.47), loss: 0.002840, time:  5.447 s\n",
      "Step 173450 (epoch  201.53), loss: 0.002723, time:  5.426 s\n",
      "Step 173500 (epoch  201.58), loss: 0.004432, time:  5.435 s\n",
      "Step 173550 (epoch  201.64), loss: 0.002376, time:  5.426 s\n",
      "Step 173600 (epoch  201.70), loss: 0.003617, time:  5.424 s, error: 0.016352\n",
      "Step 173650 (epoch  201.76), loss: 0.005011, time:  8.635 s\n",
      "Step 173700 (epoch  201.82), loss: 0.003388, time:  5.439 s\n",
      "Step 173750 (epoch  201.88), loss: 0.003121, time:  5.432 s\n",
      "Step 173800 (epoch  201.93), loss: 0.002583, time:  5.440 s\n",
      "Step 173850 (epoch  201.99), loss: 0.003600, time:  5.446 s\n",
      "Step 173900 (epoch  202.05), loss: 0.002941, time:  5.442 s\n",
      "Step 173950 (epoch  202.11), loss: 0.002615, time:  5.423 s\n",
      "Step 174000 (epoch  202.17), loss: 0.002416, time:  5.439 s, error: 0.017112\n",
      "\n",
      "Time since beginning  : 20323.881 s\n",
      "\n",
      "Step 174050 (epoch  202.22), loss: 0.003152, time:  8.706 s\n",
      "Step 174100 (epoch  202.28), loss: 0.003928, time:  5.437 s\n",
      "Step 174150 (epoch  202.34), loss: 0.003542, time:  5.451 s\n",
      "Step 174200 (epoch  202.40), loss: 0.003368, time:  5.443 s\n",
      "Step 174250 (epoch  202.46), loss: 0.002928, time:  5.444 s\n",
      "Step 174300 (epoch  202.51), loss: 0.003404, time:  5.438 s\n",
      "Step 174350 (epoch  202.57), loss: 0.003685, time:  5.434 s\n",
      "Step 174400 (epoch  202.63), loss: 0.003108, time:  5.431 s, error: 0.016426\n",
      "Step 174450 (epoch  202.69), loss: 0.001968, time:  8.637 s\n",
      "Step 174500 (epoch  202.75), loss: 0.004234, time:  5.453 s\n",
      "Step 174550 (epoch  202.80), loss: 0.004057, time:  5.434 s\n",
      "Step 174600 (epoch  202.86), loss: 0.002729, time:  5.433 s\n",
      "Step 174650 (epoch  202.92), loss: 0.003352, time:  5.421 s\n",
      "Step 174700 (epoch  202.98), loss: 0.003125, time:  5.426 s\n",
      "Step 174750 (epoch  203.04), loss: 0.003515, time:  5.437 s\n",
      "Step 174800 (epoch  203.10), loss: 0.003061, time:  5.424 s, error: 0.016936\n",
      "Step 174850 (epoch  203.15), loss: 0.003660, time:  8.639 s\n",
      "Step 174900 (epoch  203.21), loss: 0.003279, time:  5.427 s\n",
      "Step 174950 (epoch  203.27), loss: 0.003465, time:  5.436 s\n",
      "Step 175000 (epoch  203.33), loss: 0.003760, time:  5.447 s\n",
      "Step 175050 (epoch  203.39), loss: 0.002542, time:  5.420 s\n",
      "Step 175100 (epoch  203.44), loss: 0.002375, time:  5.428 s\n",
      "Step 175150 (epoch  203.50), loss: 0.003570, time:  5.428 s\n",
      "Step 175200 (epoch  203.56), loss: 0.004671, time:  5.445 s, error: 0.016717\n",
      "Step 175250 (epoch  203.62), loss: 0.002566, time:  8.706 s\n",
      "Step 175300 (epoch  203.68), loss: 0.003259, time:  5.448 s\n",
      "Step 175350 (epoch  203.73), loss: 0.003956, time:  5.437 s\n",
      "Step 175400 (epoch  203.79), loss: 0.004090, time:  5.442 s\n",
      "Step 175450 (epoch  203.85), loss: 0.002756, time:  5.429 s\n",
      "Step 175500 (epoch  203.91), loss: 0.004051, time:  5.417 s\n",
      "Step 175550 (epoch  203.97), loss: 0.002464, time:  5.439 s\n",
      "Step 175600 (epoch  204.02), loss: 0.003727, time:  5.443 s, error: 0.016823\n",
      "Step 175650 (epoch  204.08), loss: 0.002986, time:  8.697 s\n",
      "Step 175700 (epoch  204.14), loss: 0.001960, time:  5.431 s\n",
      "Step 175750 (epoch  204.20), loss: 0.004616, time:  5.412 s\n",
      "Step 175800 (epoch  204.26), loss: 0.003275, time:  5.430 s\n",
      "Step 175850 (epoch  204.32), loss: 0.003609, time:  5.424 s\n",
      "Step 175900 (epoch  204.37), loss: 0.001963, time:  5.445 s\n",
      "Step 175950 (epoch  204.43), loss: 0.004112, time:  5.447 s\n",
      "Step 176000 (epoch  204.49), loss: 0.003112, time:  5.455 s, error: 0.016426\n",
      "\n",
      "Time since beginning  : 20557.535 s\n",
      "\n",
      "Step 176050 (epoch  204.55), loss: 0.003725, time:  8.740 s\n",
      "Step 176100 (epoch  204.61), loss: 0.003176, time:  5.433 s\n",
      "Step 176150 (epoch  204.66), loss: 0.005393, time:  5.441 s\n",
      "Step 176200 (epoch  204.72), loss: 0.003306, time:  5.445 s\n",
      "Step 176250 (epoch  204.78), loss: 0.004700, time:  5.442 s\n",
      "Step 176300 (epoch  204.84), loss: 0.007321, time:  5.450 s\n",
      "Step 176350 (epoch  204.90), loss: 0.003653, time:  5.460 s\n",
      "Step 176400 (epoch  204.95), loss: 0.003326, time:  5.436 s, error: 0.016420\n",
      "Step 176450 (epoch  205.01), loss: 0.004768, time:  8.674 s\n",
      "Step 176500 (epoch  205.07), loss: 0.002994, time:  5.438 s\n",
      "Step 176550 (epoch  205.13), loss: 0.002574, time:  5.443 s\n",
      "Step 176600 (epoch  205.19), loss: 0.005001, time:  5.450 s\n",
      "Step 176650 (epoch  205.24), loss: 0.002558, time:  5.446 s\n",
      "Step 176700 (epoch  205.30), loss: 0.003252, time:  5.450 s\n",
      "Step 176750 (epoch  205.36), loss: 0.004416, time:  5.431 s\n",
      "Step 176800 (epoch  205.42), loss: 0.003516, time:  5.435 s, error: 0.016461\n",
      "Step 176850 (epoch  205.48), loss: 0.003211, time:  8.666 s\n",
      "Step 176900 (epoch  205.54), loss: 0.003163, time:  5.438 s\n",
      "Step 176950 (epoch  205.59), loss: 0.003400, time:  5.451 s\n",
      "Step 177000 (epoch  205.65), loss: 0.002979, time:  5.463 s\n",
      "Step 177050 (epoch  205.71), loss: 0.002646, time:  5.465 s\n",
      "Step 177100 (epoch  205.77), loss: 0.002940, time:  5.461 s\n",
      "Step 177150 (epoch  205.83), loss: 0.003109, time:  5.451 s\n",
      "Step 177200 (epoch  205.88), loss: 0.004073, time:  5.445 s, error: 0.017294\n",
      "Step 177250 (epoch  205.94), loss: 0.003006, time:  8.652 s\n",
      "Step 177300 (epoch  206.00), loss: 0.002739, time:  5.458 s\n",
      "Step 177350 (epoch  206.06), loss: 0.003163, time:  5.454 s\n",
      "Step 177400 (epoch  206.12), loss: 0.003357, time:  5.446 s\n",
      "Step 177450 (epoch  206.17), loss: 0.003370, time:  5.457 s\n",
      "Step 177500 (epoch  206.23), loss: 0.002765, time:  5.437 s\n",
      "Step 177550 (epoch  206.29), loss: 0.001997, time:  5.432 s\n",
      "Step 177600 (epoch  206.35), loss: 0.002649, time:  5.451 s, error: 0.016466\n",
      "Step 177650 (epoch  206.41), loss: 0.002975, time:  8.661 s\n",
      "Step 177700 (epoch  206.46), loss: 0.003367, time:  5.458 s\n",
      "Step 177750 (epoch  206.52), loss: 0.003776, time:  5.466 s\n",
      "Step 177800 (epoch  206.58), loss: 0.004272, time:  5.469 s\n",
      "Step 177850 (epoch  206.64), loss: 0.003228, time:  5.452 s\n",
      "Step 177900 (epoch  206.70), loss: 0.004207, time:  5.451 s\n",
      "Step 177950 (epoch  206.76), loss: 0.003670, time:  5.457 s\n",
      "Step 178000 (epoch  206.81), loss: 0.004114, time:  5.467 s, error: 0.016566\n",
      "\n",
      "Time since beginning  : 20791.744 s\n",
      "\n",
      "Step 178050 (epoch  206.87), loss: 0.003055, time:  8.860 s\n",
      "Step 178100 (epoch  206.93), loss: 0.001957, time:  5.457 s\n",
      "Step 178150 (epoch  206.99), loss: 0.002985, time:  5.471 s\n",
      "Step 178200 (epoch  207.05), loss: 0.003197, time:  5.456 s\n",
      "Step 178250 (epoch  207.10), loss: 0.002855, time:  5.468 s\n",
      "Step 178300 (epoch  207.16), loss: 0.002847, time:  5.467 s\n",
      "Step 178350 (epoch  207.22), loss: 0.002838, time:  5.463 s\n",
      "Step 178400 (epoch  207.28), loss: 0.002330, time:  5.462 s, error: 0.016597\n",
      "Step 178450 (epoch  207.34), loss: 0.003212, time:  8.662 s\n",
      "Step 178500 (epoch  207.39), loss: 0.003403, time:  5.444 s\n",
      "Step 178550 (epoch  207.45), loss: 0.002267, time:  5.447 s\n",
      "Step 178600 (epoch  207.51), loss: 0.003260, time:  5.440 s\n",
      "Step 178650 (epoch  207.57), loss: 0.002929, time:  5.446 s\n",
      "Step 178700 (epoch  207.63), loss: 0.002746, time:  5.452 s\n",
      "Step 178750 (epoch  207.68), loss: 0.002785, time:  5.428 s\n",
      "Step 178800 (epoch  207.74), loss: 0.002739, time:  5.438 s, error: 0.016477\n",
      "Step 178850 (epoch  207.80), loss: 0.003554, time:  8.640 s\n",
      "Step 178900 (epoch  207.86), loss: 0.003402, time:  5.451 s\n",
      "Step 178950 (epoch  207.92), loss: 0.002066, time:  5.432 s\n",
      "Step 179000 (epoch  207.98), loss: 0.002676, time:  5.448 s\n",
      "Step 179050 (epoch  208.03), loss: 0.003217, time:  5.454 s\n",
      "Step 179100 (epoch  208.09), loss: 0.002614, time:  5.436 s\n",
      "Step 179150 (epoch  208.15), loss: 0.003217, time:  5.423 s\n",
      "Step 179200 (epoch  208.21), loss: 0.003158, time:  5.436 s, error: 0.017575\n",
      "Step 179250 (epoch  208.27), loss: 0.002547, time:  8.654 s\n",
      "Step 179300 (epoch  208.32), loss: 0.003082, time:  5.427 s\n",
      "Step 179350 (epoch  208.38), loss: 0.003573, time:  5.423 s\n",
      "Step 179400 (epoch  208.44), loss: 0.003011, time:  5.441 s\n",
      "Step 179450 (epoch  208.50), loss: 0.002082, time:  5.441 s\n",
      "Step 179500 (epoch  208.56), loss: 0.003993, time:  5.444 s\n",
      "Step 179550 (epoch  208.61), loss: 0.003508, time:  5.454 s\n",
      "Step 179600 (epoch  208.67), loss: 0.003570, time:  5.428 s, error: 0.016572\n",
      "Step 179650 (epoch  208.73), loss: 0.002483, time:  8.787 s\n",
      "Step 179700 (epoch  208.79), loss: 0.002747, time:  5.428 s\n",
      "Step 179750 (epoch  208.85), loss: 0.010115, time:  5.417 s\n",
      "Step 179800 (epoch  208.90), loss: 0.003007, time:  5.440 s\n",
      "Step 179850 (epoch  208.96), loss: 0.002505, time:  5.441 s\n",
      "Step 179900 (epoch  209.02), loss: 0.002299, time:  5.449 s\n",
      "Step 179950 (epoch  209.08), loss: 0.002012, time:  5.429 s\n",
      "Step 180000 (epoch  209.14), loss: 0.003675, time:  5.452 s, error: 0.016656\n",
      "\n",
      "Time since beginning  : 21025.781 s\n",
      "\n",
      "Step 180050 (epoch  209.20), loss: 0.004366, time:  8.716 s\n",
      "Step 180100 (epoch  209.25), loss: 0.002648, time:  5.435 s\n",
      "Step 180150 (epoch  209.31), loss: 0.003240, time:  5.448 s\n",
      "Step 180200 (epoch  209.37), loss: 0.003041, time:  5.436 s\n",
      "Step 180250 (epoch  209.43), loss: 0.002927, time:  5.447 s\n",
      "Step 180300 (epoch  209.49), loss: 0.003338, time:  5.448 s\n",
      "Step 180350 (epoch  209.54), loss: 0.003896, time:  5.449 s\n",
      "Step 180400 (epoch  209.60), loss: 0.003508, time:  5.442 s, error: 0.016352\n",
      "Step 180450 (epoch  209.66), loss: 0.003322, time:  8.618 s\n",
      "Step 180500 (epoch  209.72), loss: 0.002393, time:  5.449 s\n",
      "Step 180550 (epoch  209.78), loss: 0.002351, time:  5.452 s\n",
      "Step 180600 (epoch  209.83), loss: 0.003647, time:  5.443 s\n",
      "Step 180650 (epoch  209.89), loss: 0.003366, time:  5.430 s\n",
      "Step 180700 (epoch  209.95), loss: 0.002924, time:  5.448 s\n",
      "Step 180750 (epoch  210.01), loss: 0.002774, time:  5.453 s\n",
      "Step 180800 (epoch  210.07), loss: 0.003514, time:  5.435 s, error: 0.016776\n",
      "Step 180850 (epoch  210.12), loss: 0.002740, time:  8.665 s\n",
      "Step 180900 (epoch  210.18), loss: 0.004267, time:  5.440 s\n",
      "Step 180950 (epoch  210.24), loss: 0.003433, time:  5.440 s\n",
      "Step 181000 (epoch  210.30), loss: 0.004438, time:  5.417 s\n",
      "Step 181050 (epoch  210.36), loss: 0.003331, time:  5.442 s\n",
      "Step 181100 (epoch  210.42), loss: 0.003192, time:  5.458 s\n",
      "Step 181150 (epoch  210.47), loss: 0.002928, time:  5.448 s\n",
      "Step 181200 (epoch  210.53), loss: 0.003992, time:  5.450 s, error: 0.016527\n",
      "Step 181250 (epoch  210.59), loss: 0.003050, time:  8.651 s\n",
      "Step 181300 (epoch  210.65), loss: 0.003148, time:  5.424 s\n",
      "Step 181350 (epoch  210.71), loss: 0.003176, time:  5.445 s\n",
      "Step 181400 (epoch  210.76), loss: 0.004107, time:  5.451 s\n",
      "Step 181450 (epoch  210.82), loss: 0.001861, time:  5.462 s\n",
      "Step 181500 (epoch  210.88), loss: 0.003550, time:  5.434 s\n",
      "Step 181550 (epoch  210.94), loss: 0.004393, time:  5.435 s\n",
      "Step 181600 (epoch  211.00), loss: 0.002828, time:  5.438 s, error: 0.016401\n",
      "Step 181650 (epoch  211.05), loss: 0.003874, time:  8.649 s\n",
      "Step 181700 (epoch  211.11), loss: 0.003099, time:  5.431 s\n",
      "Step 181750 (epoch  211.17), loss: 0.003334, time:  5.427 s\n",
      "Step 181800 (epoch  211.23), loss: 0.002790, time:  5.447 s\n",
      "Step 181850 (epoch  211.29), loss: 0.003133, time:  5.430 s\n",
      "Step 181900 (epoch  211.34), loss: 0.003027, time:  5.446 s\n",
      "Step 181950 (epoch  211.40), loss: 0.002325, time:  5.450 s\n",
      "Step 182000 (epoch  211.46), loss: 0.003192, time:  5.425 s, error: 0.016346\n",
      "\n",
      "Time since beginning  : 21259.509 s\n",
      "\n",
      "Step 182050 (epoch  211.52), loss: 0.003844, time:  8.718 s\n",
      "Step 182100 (epoch  211.58), loss: 0.003064, time:  5.452 s\n",
      "Step 182150 (epoch  211.63), loss: 0.002215, time:  5.461 s\n",
      "Step 182200 (epoch  211.69), loss: 0.003891, time:  5.461 s\n",
      "Step 182250 (epoch  211.75), loss: 0.002292, time:  5.459 s\n",
      "Step 182300 (epoch  211.81), loss: 0.003660, time:  5.452 s\n",
      "Step 182350 (epoch  211.87), loss: 0.003847, time:  5.471 s\n",
      "Step 182400 (epoch  211.93), loss: 0.003048, time:  5.463 s, error: 0.016396\n",
      "Step 182450 (epoch  211.98), loss: 0.002667, time:  8.778 s\n",
      "Step 182500 (epoch  212.04), loss: 0.003403, time:  5.447 s\n",
      "Step 182550 (epoch  212.10), loss: 0.003181, time:  5.469 s\n",
      "Step 182600 (epoch  212.16), loss: 0.003749, time:  5.451 s\n",
      "Step 182650 (epoch  212.22), loss: 0.003096, time:  5.441 s\n",
      "Step 182700 (epoch  212.27), loss: 0.002325, time:  5.440 s\n",
      "Step 182750 (epoch  212.33), loss: 0.002848, time:  5.433 s\n",
      "Step 182800 (epoch  212.39), loss: 0.004476, time:  5.443 s, error: 0.016468\n",
      "Step 182850 (epoch  212.45), loss: 0.003031, time:  8.630 s\n",
      "Step 182900 (epoch  212.51), loss: 0.003487, time:  5.440 s\n",
      "Step 182950 (epoch  212.56), loss: 0.002341, time:  5.451 s\n",
      "Step 183000 (epoch  212.62), loss: 0.002320, time:  5.440 s\n",
      "Step 183050 (epoch  212.68), loss: 0.002779, time:  5.437 s\n",
      "Step 183100 (epoch  212.74), loss: 0.003013, time:  5.447 s\n",
      "Step 183150 (epoch  212.80), loss: 0.003603, time:  5.442 s\n",
      "Step 183200 (epoch  212.85), loss: 0.004144, time:  5.463 s, error: 0.017532\n",
      "Step 183250 (epoch  212.91), loss: 0.002766, time:  8.778 s\n",
      "Step 183300 (epoch  212.97), loss: 0.002494, time:  5.452 s\n",
      "Step 183350 (epoch  213.03), loss: 0.002606, time:  5.430 s\n",
      "Step 183400 (epoch  213.09), loss: 0.003566, time:  5.448 s\n",
      "Step 183450 (epoch  213.15), loss: 0.005700, time:  5.450 s\n",
      "Step 183500 (epoch  213.20), loss: 0.002964, time:  5.447 s\n",
      "Step 183550 (epoch  213.26), loss: 0.002330, time:  5.433 s\n",
      "Step 183600 (epoch  213.32), loss: 0.002968, time:  5.445 s, error: 0.016331\n",
      "Step 183650 (epoch  213.38), loss: 0.002878, time:  8.729 s\n",
      "Step 183700 (epoch  213.44), loss: 0.002554, time:  5.447 s\n",
      "Step 183750 (epoch  213.49), loss: 0.001985, time:  5.432 s\n",
      "Step 183800 (epoch  213.55), loss: 0.003199, time:  5.424 s\n",
      "Step 183850 (epoch  213.61), loss: 0.002963, time:  5.450 s\n",
      "Step 183900 (epoch  213.67), loss: 0.003080, time:  5.455 s\n",
      "Step 183950 (epoch  213.73), loss: 0.001958, time:  5.443 s\n",
      "Step 184000 (epoch  213.78), loss: 0.003061, time:  5.449 s, error: 0.016491\n",
      "\n",
      "Time since beginning  : 21493.858 s\n",
      "\n",
      "Step 184050 (epoch  213.84), loss: 0.002890, time:  8.742 s\n",
      "Step 184100 (epoch  213.90), loss: 0.003646, time:  5.459 s\n",
      "Step 184150 (epoch  213.96), loss: 0.002844, time:  5.466 s\n",
      "Step 184200 (epoch  214.02), loss: 0.003578, time:  5.452 s\n",
      "Step 184250 (epoch  214.07), loss: 0.003071, time:  5.469 s\n",
      "Step 184300 (epoch  214.13), loss: 0.004082, time:  5.463 s\n",
      "Step 184350 (epoch  214.19), loss: 0.003770, time:  5.466 s\n",
      "Step 184400 (epoch  214.25), loss: 0.003080, time:  5.467 s, error: 0.016911\n",
      "Step 184450 (epoch  214.31), loss: 0.004172, time:  8.820 s\n",
      "Step 184500 (epoch  214.37), loss: 0.001936, time:  5.462 s\n",
      "Step 184550 (epoch  214.42), loss: 0.003410, time:  5.435 s\n",
      "Step 184600 (epoch  214.48), loss: 0.002971, time:  5.448 s\n",
      "Step 184650 (epoch  214.54), loss: 0.004776, time:  5.445 s\n",
      "Step 184700 (epoch  214.60), loss: 0.003952, time:  5.435 s\n",
      "Step 184750 (epoch  214.66), loss: 0.002876, time:  5.454 s\n",
      "Step 184800 (epoch  214.71), loss: 0.004722, time:  5.432 s, error: 0.016513\n",
      "Step 184850 (epoch  214.77), loss: 0.003781, time:  8.663 s\n",
      "Step 184900 (epoch  214.83), loss: 0.004452, time:  5.449 s\n",
      "Step 184950 (epoch  214.89), loss: 0.002906, time:  5.432 s\n",
      "Step 185000 (epoch  214.95), loss: 0.003460, time:  5.439 s\n",
      "Step 185050 (epoch  215.00), loss: 0.002821, time:  5.445 s\n",
      "Step 185100 (epoch  215.06), loss: 0.002080, time:  5.459 s\n",
      "Step 185150 (epoch  215.12), loss: 0.002535, time:  5.451 s\n",
      "Step 185200 (epoch  215.18), loss: 0.004757, time:  5.443 s, error: 0.017453\n",
      "Step 185250 (epoch  215.24), loss: 0.002113, time:  8.658 s\n",
      "Step 185300 (epoch  215.29), loss: 0.003729, time:  5.439 s\n",
      "Step 185350 (epoch  215.35), loss: 0.002346, time:  5.450 s\n",
      "Step 185400 (epoch  215.41), loss: 0.003162, time:  5.454 s\n",
      "Step 185450 (epoch  215.47), loss: 0.003480, time:  5.455 s\n",
      "Step 185500 (epoch  215.53), loss: 0.003229, time:  5.438 s\n",
      "Step 185550 (epoch  215.59), loss: 0.002316, time:  5.438 s\n",
      "Step 185600 (epoch  215.64), loss: 0.003107, time:  5.446 s, error: 0.016248\n",
      "Step 185650 (epoch  215.70), loss: 0.002737, time:  8.701 s\n",
      "Step 185700 (epoch  215.76), loss: 0.003019, time:  5.444 s\n",
      "Step 185750 (epoch  215.82), loss: 0.003414, time:  5.436 s\n",
      "Step 185800 (epoch  215.88), loss: 0.003758, time:  5.443 s\n",
      "Step 185850 (epoch  215.93), loss: 0.002779, time:  5.460 s\n",
      "Step 185900 (epoch  215.99), loss: 0.002770, time:  5.437 s\n",
      "Step 185950 (epoch  216.05), loss: 0.003125, time:  5.450 s\n",
      "Step 186000 (epoch  216.11), loss: 0.004131, time:  5.438 s, error: 0.017121\n",
      "\n",
      "Time since beginning  : 21728.113 s\n",
      "\n",
      "Step 186050 (epoch  216.17), loss: 0.003822, time:  8.721 s\n",
      "Step 186100 (epoch  216.22), loss: 0.002959, time:  5.437 s\n",
      "Step 186150 (epoch  216.28), loss: 0.002666, time:  5.445 s\n",
      "Step 186200 (epoch  216.34), loss: 0.001817, time:  5.456 s\n",
      "Step 186250 (epoch  216.40), loss: 0.003409, time:  5.434 s\n",
      "Step 186300 (epoch  216.46), loss: 0.003869, time:  5.434 s\n",
      "Step 186350 (epoch  216.51), loss: 0.004091, time:  5.452 s\n",
      "Step 186400 (epoch  216.57), loss: 0.007214, time:  5.443 s, error: 0.016605\n",
      "Step 186450 (epoch  216.63), loss: 0.002359, time:  8.661 s\n",
      "Step 186500 (epoch  216.69), loss: 0.004323, time:  5.444 s\n",
      "Step 186550 (epoch  216.75), loss: 0.004984, time:  5.460 s\n",
      "Step 186600 (epoch  216.81), loss: 0.002887, time:  5.459 s\n",
      "Step 186650 (epoch  216.86), loss: 0.002819, time:  5.458 s\n",
      "Step 186700 (epoch  216.92), loss: 0.003179, time:  5.435 s\n",
      "Step 186750 (epoch  216.98), loss: 0.002719, time:  5.432 s\n",
      "Step 186800 (epoch  217.04), loss: 0.002864, time:  5.450 s, error: 0.017298\n",
      "Step 186850 (epoch  217.10), loss: 0.003483, time:  8.679 s\n",
      "Step 186900 (epoch  217.15), loss: 0.002097, time:  5.452 s\n",
      "Step 186950 (epoch  217.21), loss: 0.002840, time:  5.443 s\n",
      "Step 187000 (epoch  217.27), loss: 0.004237, time:  5.430 s\n",
      "Step 187050 (epoch  217.33), loss: 0.003349, time:  5.438 s\n",
      "Step 187100 (epoch  217.39), loss: 0.002263, time:  5.455 s\n",
      "Step 187150 (epoch  217.44), loss: 0.001603, time:  5.449 s\n",
      "Step 187200 (epoch  217.50), loss: 0.003111, time:  5.443 s, error: 0.016983\n",
      "Step 187250 (epoch  217.56), loss: 0.003694, time:  8.675 s\n",
      "Step 187300 (epoch  217.62), loss: 0.004006, time:  5.452 s\n",
      "Step 187350 (epoch  217.68), loss: 0.002716, time:  5.439 s\n",
      "Step 187400 (epoch  217.73), loss: 0.004167, time:  5.440 s\n",
      "Step 187450 (epoch  217.79), loss: 0.004161, time:  5.446 s\n",
      "Step 187500 (epoch  217.85), loss: 0.003226, time:  5.433 s\n",
      "Step 187550 (epoch  217.91), loss: 0.003183, time:  5.442 s\n",
      "Step 187600 (epoch  217.97), loss: 0.002361, time:  5.440 s, error: 0.016408\n",
      "Step 187650 (epoch  218.03), loss: 0.002716, time:  8.678 s\n",
      "Step 187700 (epoch  218.08), loss: 0.001966, time:  5.449 s\n",
      "Step 187750 (epoch  218.14), loss: 0.003671, time:  5.442 s\n",
      "Step 187800 (epoch  218.20), loss: 0.003637, time:  5.453 s\n",
      "Step 187850 (epoch  218.26), loss: 0.002844, time:  5.441 s\n",
      "Step 187900 (epoch  218.32), loss: 0.002609, time:  5.448 s\n",
      "Step 187950 (epoch  218.37), loss: 0.003015, time:  5.433 s\n",
      "Step 188000 (epoch  218.43), loss: 0.003490, time:  5.432 s, error: 0.016361\n",
      "\n",
      "Time since beginning  : 21962.204 s\n",
      "\n",
      "Step 188050 (epoch  218.49), loss: 0.003156, time:  8.867 s\n",
      "Step 188100 (epoch  218.55), loss: 0.001990, time:  5.453 s\n",
      "Step 188150 (epoch  218.61), loss: 0.002970, time:  5.442 s\n",
      "Step 188200 (epoch  218.66), loss: 0.003095, time:  5.442 s\n",
      "Step 188250 (epoch  218.72), loss: 0.003427, time:  5.441 s\n",
      "Step 188300 (epoch  218.78), loss: 0.004153, time:  5.455 s\n",
      "Step 188350 (epoch  218.84), loss: 0.002958, time:  5.440 s\n",
      "Step 188400 (epoch  218.90), loss: 0.004072, time:  5.462 s, error: 0.016441\n",
      "Step 188450 (epoch  218.95), loss: 0.003240, time:  8.718 s\n",
      "Step 188500 (epoch  219.01), loss: 0.002661, time:  5.439 s\n",
      "Step 188550 (epoch  219.07), loss: 0.003408, time:  5.428 s\n",
      "Step 188600 (epoch  219.13), loss: 0.002203, time:  5.451 s\n",
      "Step 188650 (epoch  219.19), loss: 0.003916, time:  5.436 s\n",
      "Step 188700 (epoch  219.25), loss: 0.003690, time:  5.454 s\n",
      "Step 188750 (epoch  219.30), loss: 0.005198, time:  5.447 s\n",
      "Step 188800 (epoch  219.36), loss: 0.002780, time:  5.436 s, error: 0.016515\n",
      "Step 188850 (epoch  219.42), loss: 0.003064, time:  8.674 s\n",
      "Step 188900 (epoch  219.48), loss: 0.002508, time:  5.442 s\n",
      "Step 188950 (epoch  219.54), loss: 0.003244, time:  5.439 s\n",
      "Step 189000 (epoch  219.59), loss: 0.003908, time:  5.444 s\n",
      "Step 189050 (epoch  219.65), loss: 0.004487, time:  5.449 s\n",
      "Step 189100 (epoch  219.71), loss: 0.002196, time:  5.453 s\n",
      "Step 189150 (epoch  219.77), loss: 0.004154, time:  5.466 s\n",
      "Step 189200 (epoch  219.83), loss: 0.002358, time:  5.433 s, error: 0.016695\n",
      "Step 189250 (epoch  219.88), loss: 0.003108, time:  8.693 s\n",
      "Step 189300 (epoch  219.94), loss: 0.002113, time:  5.443 s\n",
      "Step 189350 (epoch  220.00), loss: 0.003177, time:  5.439 s\n",
      "Step 189400 (epoch  220.06), loss: 0.003645, time:  5.444 s\n",
      "Step 189450 (epoch  220.12), loss: 0.003163, time:  5.447 s\n",
      "Step 189500 (epoch  220.17), loss: 0.003560, time:  5.462 s\n",
      "Step 189550 (epoch  220.23), loss: 0.002347, time:  5.440 s\n",
      "Step 189600 (epoch  220.29), loss: 0.004671, time:  5.447 s, error: 0.016582\n",
      "Step 189650 (epoch  220.35), loss: 0.002969, time:  8.663 s\n",
      "Step 189700 (epoch  220.41), loss: 0.002270, time:  5.436 s\n",
      "Step 189750 (epoch  220.47), loss: 0.002437, time:  5.447 s\n",
      "Step 189800 (epoch  220.52), loss: 0.003593, time:  5.441 s\n",
      "Step 189850 (epoch  220.58), loss: 0.004493, time:  5.458 s\n",
      "Step 189900 (epoch  220.64), loss: 0.002892, time:  5.444 s\n",
      "Step 189950 (epoch  220.70), loss: 0.003552, time:  5.441 s\n",
      "Step 190000 (epoch  220.76), loss: 0.002993, time:  5.461 s, error: 0.016569\n",
      "\n",
      "Time since beginning  : 22196.296 s\n",
      "\n",
      "Step 190050 (epoch  220.81), loss: 0.002495, time:  8.745 s\n",
      "Step 190100 (epoch  220.87), loss: 0.003625, time:  5.441 s\n",
      "Step 190150 (epoch  220.93), loss: 0.003257, time:  5.445 s\n",
      "Step 190200 (epoch  220.99), loss: 0.002445, time:  5.463 s\n",
      "Step 190250 (epoch  221.05), loss: 0.003163, time:  5.451 s\n",
      "Step 190300 (epoch  221.10), loss: 0.002596, time:  5.454 s\n",
      "Step 190350 (epoch  221.16), loss: 0.002893, time:  5.452 s\n",
      "Step 190400 (epoch  221.22), loss: 0.002568, time:  5.454 s, error: 0.016487\n",
      "Step 190450 (epoch  221.28), loss: 0.002432, time:  8.698 s\n",
      "Step 190500 (epoch  221.34), loss: 0.003583, time:  5.455 s\n",
      "Step 190550 (epoch  221.39), loss: 0.002411, time:  5.454 s\n",
      "Step 190600 (epoch  221.45), loss: 0.002293, time:  5.447 s\n",
      "Step 190650 (epoch  221.51), loss: 0.002448, time:  5.437 s\n",
      "Step 190700 (epoch  221.57), loss: 0.003853, time:  5.443 s\n",
      "Step 190750 (epoch  221.63), loss: 0.003463, time:  5.445 s\n",
      "Step 190800 (epoch  221.69), loss: 0.002576, time:  5.444 s, error: 0.016200\n",
      "Step 190850 (epoch  221.74), loss: 0.002806, time:  8.661 s\n",
      "Step 190900 (epoch  221.80), loss: 0.003896, time:  5.442 s\n",
      "Step 190950 (epoch  221.86), loss: 0.003813, time:  5.440 s\n",
      "Step 191000 (epoch  221.92), loss: 0.002627, time:  5.439 s\n",
      "Step 191050 (epoch  221.98), loss: 0.003303, time:  5.439 s\n",
      "Step 191100 (epoch  222.03), loss: 0.002708, time:  5.440 s\n",
      "Step 191150 (epoch  222.09), loss: 0.002357, time:  5.448 s\n",
      "Step 191200 (epoch  222.15), loss: 0.002163, time:  5.431 s, error: 0.016486\n",
      "Step 191250 (epoch  222.21), loss: 0.003519, time:  8.660 s\n",
      "Step 191300 (epoch  222.27), loss: 0.003615, time:  5.452 s\n",
      "Step 191350 (epoch  222.32), loss: 0.004050, time:  5.441 s\n",
      "Step 191400 (epoch  222.38), loss: 0.002162, time:  5.435 s\n",
      "Step 191450 (epoch  222.44), loss: 0.003147, time:  5.450 s\n",
      "Step 191500 (epoch  222.50), loss: 0.003242, time:  5.455 s\n",
      "Step 191550 (epoch  222.56), loss: 0.003589, time:  5.454 s\n",
      "Step 191600 (epoch  222.61), loss: 0.003540, time:  5.445 s, error: 0.016390\n",
      "Step 191650 (epoch  222.67), loss: 0.001805, time:  8.658 s\n",
      "Step 191700 (epoch  222.73), loss: 0.003198, time:  5.442 s\n",
      "Step 191750 (epoch  222.79), loss: 0.003487, time:  5.438 s\n",
      "Step 191800 (epoch  222.85), loss: 0.003034, time:  5.448 s\n",
      "Step 191850 (epoch  222.91), loss: 0.002704, time:  5.446 s\n",
      "Step 191900 (epoch  222.96), loss: 0.002994, time:  5.434 s\n",
      "Step 191950 (epoch  223.02), loss: 0.002810, time:  5.435 s\n",
      "Step 192000 (epoch  223.08), loss: 0.002499, time:  5.423 s, error: 0.016807\n",
      "\n",
      "Time since beginning  : 22430.350 s\n",
      "\n",
      "Step 192050 (epoch  223.14), loss: 0.003694, time:  8.803 s\n",
      "Step 192100 (epoch  223.20), loss: 0.002898, time:  5.442 s\n",
      "Step 192150 (epoch  223.25), loss: 0.004029, time:  5.450 s\n",
      "Step 192200 (epoch  223.31), loss: 0.003234, time:  5.438 s\n",
      "Step 192250 (epoch  223.37), loss: 0.004446, time:  5.442 s\n",
      "Step 192300 (epoch  223.43), loss: 0.002640, time:  5.447 s\n",
      "Step 192350 (epoch  223.49), loss: 0.003278, time:  5.447 s\n",
      "Step 192400 (epoch  223.54), loss: 0.002618, time:  5.452 s, error: 0.016171\n",
      "Step 192450 (epoch  223.60), loss: 0.003177, time:  8.701 s\n",
      "Step 192500 (epoch  223.66), loss: 0.004816, time:  5.429 s\n",
      "Step 192550 (epoch  223.72), loss: 0.003058, time:  5.444 s\n",
      "Step 192600 (epoch  223.78), loss: 0.002390, time:  5.449 s\n",
      "Step 192650 (epoch  223.83), loss: 0.003147, time:  5.441 s\n",
      "Step 192700 (epoch  223.89), loss: 0.001903, time:  5.448 s\n",
      "Step 192750 (epoch  223.95), loss: 0.003058, time:  5.438 s\n",
      "Step 192800 (epoch  224.01), loss: 0.003325, time:  5.460 s, error: 0.016444\n",
      "Step 192850 (epoch  224.07), loss: 0.003193, time:  8.675 s\n",
      "Step 192900 (epoch  224.13), loss: 0.004521, time:  5.453 s\n",
      "Step 192950 (epoch  224.18), loss: 0.003124, time:  5.447 s\n",
      "Step 193000 (epoch  224.24), loss: 0.003394, time:  5.456 s\n",
      "Step 193050 (epoch  224.30), loss: 0.003492, time:  5.449 s\n",
      "Step 193100 (epoch  224.36), loss: 0.003093, time:  5.451 s\n",
      "Step 193150 (epoch  224.42), loss: 0.003335, time:  5.451 s\n",
      "Step 193200 (epoch  224.47), loss: 0.001975, time:  5.445 s, error: 0.016238\n",
      "Step 193250 (epoch  224.53), loss: 0.003493, time:  8.662 s\n",
      "Step 193300 (epoch  224.59), loss: 0.003182, time:  5.441 s\n",
      "Step 193350 (epoch  224.65), loss: 0.002400, time:  5.443 s\n",
      "Step 193400 (epoch  224.71), loss: 0.002405, time:  5.446 s\n",
      "Step 193450 (epoch  224.76), loss: 0.002177, time:  5.433 s\n",
      "Step 193500 (epoch  224.82), loss: 0.003593, time:  5.455 s\n",
      "Step 193550 (epoch  224.88), loss: 0.003253, time:  5.444 s\n",
      "Step 193600 (epoch  224.94), loss: 0.003481, time:  5.436 s, error: 0.016401\n",
      "Step 193650 (epoch  225.00), loss: 0.003024, time:  8.648 s\n",
      "Step 193700 (epoch  225.05), loss: 0.003781, time:  5.454 s\n",
      "Step 193750 (epoch  225.11), loss: 0.002464, time:  5.445 s\n",
      "Step 193800 (epoch  225.17), loss: 0.003657, time:  5.450 s\n",
      "Step 193850 (epoch  225.23), loss: 0.001922, time:  5.457 s\n",
      "Step 193900 (epoch  225.29), loss: 0.003551, time:  5.458 s\n",
      "Step 193950 (epoch  225.35), loss: 0.002403, time:  5.449 s\n",
      "Step 194000 (epoch  225.40), loss: 0.003714, time:  5.448 s, error: 0.017148\n",
      "\n",
      "Time since beginning  : 22664.418 s\n",
      "\n",
      "Step 194050 (epoch  225.46), loss: 0.004135, time:  8.775 s\n",
      "Step 194100 (epoch  225.52), loss: 0.004289, time:  5.450 s\n",
      "Step 194150 (epoch  225.58), loss: 0.003257, time:  5.442 s\n",
      "Step 194200 (epoch  225.64), loss: 0.003806, time:  5.458 s\n",
      "Step 194250 (epoch  225.69), loss: 0.003294, time:  5.451 s\n",
      "Step 194300 (epoch  225.75), loss: 0.003247, time:  5.439 s\n",
      "Step 194350 (epoch  225.81), loss: 0.003955, time:  5.444 s\n",
      "Step 194400 (epoch  225.87), loss: 0.003524, time:  5.435 s, error: 0.016738\n",
      "Step 194450 (epoch  225.93), loss: 0.002795, time:  8.631 s\n",
      "Step 194500 (epoch  225.98), loss: 0.003840, time:  5.432 s\n",
      "Step 194550 (epoch  226.04), loss: 0.003710, time:  5.423 s\n",
      "Step 194600 (epoch  226.10), loss: 0.003334, time:  5.449 s\n",
      "Step 194650 (epoch  226.16), loss: 0.002981, time:  5.437 s\n",
      "Step 194700 (epoch  226.22), loss: 0.003174, time:  5.429 s\n",
      "Step 194750 (epoch  226.27), loss: 0.005737, time:  5.432 s\n",
      "Step 194800 (epoch  226.33), loss: 0.003756, time:  5.433 s, error: 0.016552\n",
      "Step 194850 (epoch  226.39), loss: 0.005017, time:  8.617 s\n",
      "Step 194900 (epoch  226.45), loss: 0.002912, time:  5.427 s\n",
      "Step 194950 (epoch  226.51), loss: 0.003139, time:  5.449 s\n",
      "Step 195000 (epoch  226.57), loss: 0.002763, time:  5.441 s\n",
      "Step 195050 (epoch  226.62), loss: 0.002702, time:  5.435 s\n",
      "Step 195100 (epoch  226.68), loss: 0.002144, time:  5.446 s\n",
      "Step 195150 (epoch  226.74), loss: 0.002920, time:  5.430 s\n",
      "Step 195200 (epoch  226.80), loss: 0.003553, time:  5.429 s, error: 0.016478\n",
      "Step 195250 (epoch  226.86), loss: 0.002555, time:  8.613 s\n",
      "Step 195300 (epoch  226.91), loss: 0.002245, time:  5.437 s\n",
      "Step 195350 (epoch  226.97), loss: 0.003677, time:  5.444 s\n",
      "Step 195400 (epoch  227.03), loss: 0.004259, time:  5.439 s\n",
      "Step 195450 (epoch  227.09), loss: 0.002221, time:  5.435 s\n",
      "Step 195500 (epoch  227.15), loss: 0.003496, time:  5.433 s\n",
      "Step 195550 (epoch  227.20), loss: 0.002857, time:  5.427 s\n",
      "Step 195600 (epoch  227.26), loss: 0.003021, time:  5.429 s, error: 0.016453\n",
      "Step 195650 (epoch  227.32), loss: 0.003474, time:  8.657 s\n",
      "Step 195700 (epoch  227.38), loss: 0.002648, time:  5.436 s\n",
      "Step 195750 (epoch  227.44), loss: 0.002529, time:  5.443 s\n",
      "Step 195800 (epoch  227.49), loss: 0.003336, time:  5.421 s\n",
      "Step 195850 (epoch  227.55), loss: 0.002675, time:  5.430 s\n",
      "Step 195900 (epoch  227.61), loss: 0.002674, time:  5.434 s\n",
      "Step 195950 (epoch  227.67), loss: 0.003113, time:  5.429 s\n",
      "Step 196000 (epoch  227.73), loss: 0.003467, time:  5.428 s, error: 0.016820\n",
      "\n",
      "Time since beginning  : 22897.939 s\n",
      "\n",
      "Step 196050 (epoch  227.79), loss: 0.003090, time:  8.711 s\n",
      "Step 196100 (epoch  227.84), loss: 0.002966, time:  5.434 s\n",
      "Step 196150 (epoch  227.90), loss: 0.002059, time:  5.423 s\n",
      "Step 196200 (epoch  227.96), loss: 0.002654, time:  5.426 s\n",
      "Step 196250 (epoch  228.02), loss: 0.002853, time:  5.421 s\n",
      "Step 196300 (epoch  228.08), loss: 0.003877, time:  5.439 s\n",
      "Step 196350 (epoch  228.13), loss: 0.003687, time:  5.442 s\n",
      "Step 196400 (epoch  228.19), loss: 0.002572, time:  5.428 s, error: 0.016641\n",
      "Step 196450 (epoch  228.25), loss: 0.003148, time:  8.767 s\n",
      "Step 196500 (epoch  228.31), loss: 0.002648, time:  5.445 s\n",
      "Step 196550 (epoch  228.37), loss: 0.002655, time:  5.415 s\n",
      "Step 196600 (epoch  228.42), loss: 0.002597, time:  5.420 s\n",
      "Step 196650 (epoch  228.48), loss: 0.003160, time:  5.445 s\n",
      "Step 196700 (epoch  228.54), loss: 0.003011, time:  5.427 s\n",
      "Step 196750 (epoch  228.60), loss: 0.002922, time:  5.438 s\n",
      "Step 196800 (epoch  228.66), loss: 0.002318, time:  5.441 s, error: 0.016367\n",
      "Step 196850 (epoch  228.71), loss: 0.002136, time:  8.662 s\n",
      "Step 196900 (epoch  228.77), loss: 0.003111, time:  5.438 s\n",
      "Step 196950 (epoch  228.83), loss: 0.004294, time:  5.448 s\n",
      "Step 197000 (epoch  228.89), loss: 0.004203, time:  5.433 s\n",
      "Step 197050 (epoch  228.95), loss: 0.002208, time:  5.439 s\n",
      "Step 197100 (epoch  229.00), loss: 0.002654, time:  5.459 s\n",
      "Step 197150 (epoch  229.06), loss: 0.002585, time:  5.460 s\n",
      "Step 197200 (epoch  229.12), loss: 0.003830, time:  5.440 s, error: 0.016622\n",
      "Step 197250 (epoch  229.18), loss: 0.002263, time:  8.649 s\n",
      "Step 197300 (epoch  229.24), loss: 0.002896, time:  5.437 s\n",
      "Step 197350 (epoch  229.30), loss: 0.002719, time:  5.433 s\n",
      "Step 197400 (epoch  229.35), loss: 0.002443, time:  5.434 s\n",
      "Step 197450 (epoch  229.41), loss: 0.002924, time:  5.423 s\n",
      "Step 197500 (epoch  229.47), loss: 0.003045, time:  5.445 s\n",
      "Step 197550 (epoch  229.53), loss: 0.004928, time:  5.448 s\n",
      "Step 197600 (epoch  229.59), loss: 0.003209, time:  5.447 s, error: 0.016183\n",
      "Step 197650 (epoch  229.64), loss: 0.002067, time:  8.661 s\n",
      "Step 197700 (epoch  229.70), loss: 0.002419, time:  5.433 s\n",
      "Step 197750 (epoch  229.76), loss: 0.002405, time:  5.444 s\n",
      "Step 197800 (epoch  229.82), loss: 0.003178, time:  5.436 s\n",
      "Step 197850 (epoch  229.88), loss: 0.003931, time:  5.432 s\n",
      "Step 197900 (epoch  229.93), loss: 0.002624, time:  5.444 s\n",
      "Step 197950 (epoch  229.99), loss: 0.006124, time:  5.444 s\n",
      "Step 198000 (epoch  230.05), loss: 0.002273, time:  5.428 s, error: 0.016611\n",
      "\n",
      "Time since beginning  : 23131.674 s\n",
      "\n",
      "Step 198050 (epoch  230.11), loss: 0.003946, time:  8.687 s\n",
      "Step 198100 (epoch  230.17), loss: 0.002781, time:  5.440 s\n",
      "Step 198150 (epoch  230.22), loss: 0.001691, time:  5.436 s\n",
      "Step 198200 (epoch  230.28), loss: 0.002631, time:  5.426 s\n",
      "Step 198250 (epoch  230.34), loss: 0.002089, time:  5.442 s\n",
      "Step 198300 (epoch  230.40), loss: 0.003870, time:  5.430 s\n",
      "Step 198350 (epoch  230.46), loss: 0.003290, time:  5.434 s\n",
      "Step 198400 (epoch  230.52), loss: 0.003705, time:  5.434 s, error: 0.016319\n",
      "Step 198450 (epoch  230.57), loss: 0.003879, time:  8.641 s\n",
      "Step 198500 (epoch  230.63), loss: 0.003123, time:  5.437 s\n",
      "Step 198550 (epoch  230.69), loss: 0.002044, time:  5.428 s\n",
      "Step 198600 (epoch  230.75), loss: 0.002691, time:  5.435 s\n",
      "Step 198650 (epoch  230.81), loss: 0.005094, time:  5.437 s\n",
      "Step 198700 (epoch  230.86), loss: 0.003542, time:  5.432 s\n",
      "Step 198750 (epoch  230.92), loss: 0.003820, time:  5.428 s\n",
      "Step 198800 (epoch  230.98), loss: 0.002244, time:  5.444 s, error: 0.016446\n",
      "Step 198850 (epoch  231.04), loss: 0.003885, time:  8.609 s\n",
      "Step 198900 (epoch  231.10), loss: 0.003210, time:  5.438 s\n",
      "Step 198950 (epoch  231.15), loss: 0.003107, time:  5.434 s\n",
      "Step 199000 (epoch  231.21), loss: 0.003134, time:  5.463 s\n",
      "Step 199050 (epoch  231.27), loss: 0.003715, time:  5.445 s\n",
      "Step 199100 (epoch  231.33), loss: 0.003829, time:  5.435 s\n",
      "Step 199150 (epoch  231.39), loss: 0.002085, time:  5.419 s\n",
      "Step 199200 (epoch  231.44), loss: 0.003618, time:  5.415 s, error: 0.016347\n",
      "Step 199250 (epoch  231.50), loss: 0.003641, time:  8.619 s\n",
      "Step 199300 (epoch  231.56), loss: 0.003942, time:  5.431 s\n",
      "Step 199350 (epoch  231.62), loss: 0.003565, time:  5.455 s\n",
      "Step 199400 (epoch  231.68), loss: 0.002191, time:  5.420 s\n",
      "Step 199450 (epoch  231.74), loss: 0.002156, time:  5.423 s\n",
      "Step 199500 (epoch  231.79), loss: 0.002972, time:  5.432 s\n",
      "Step 199550 (epoch  231.85), loss: 0.002790, time:  5.444 s\n",
      "Step 199600 (epoch  231.91), loss: 0.002359, time:  5.435 s, error: 0.016311\n",
      "Step 199650 (epoch  231.97), loss: 0.003478, time:  8.678 s\n",
      "Step 199700 (epoch  232.03), loss: 0.004500, time:  5.460 s\n",
      "Step 199750 (epoch  232.08), loss: 0.002324, time:  5.441 s\n",
      "Step 199800 (epoch  232.14), loss: 0.002336, time:  5.451 s\n",
      "Step 199850 (epoch  232.20), loss: 0.002771, time:  5.426 s\n",
      "Step 199900 (epoch  232.26), loss: 0.003391, time:  5.427 s\n",
      "Step 199950 (epoch  232.32), loss: 0.002968, time:  5.430 s\n",
      "Step 200000 (epoch  232.37), loss: 0.001669, time:  5.466 s, error: 0.016308\n",
      "\n",
      "Time since beginning  : 23365.264 s\n",
      "\n",
      "Step 200050 (epoch  232.43), loss: 0.001708, time:  8.796 s\n",
      "Step 200100 (epoch  232.49), loss: 0.002317, time:  5.442 s\n",
      "Step 200150 (epoch  232.55), loss: 0.003569, time:  5.432 s\n",
      "Step 200200 (epoch  232.61), loss: 0.002488, time:  5.437 s\n",
      "Step 200250 (epoch  232.66), loss: 0.002907, time:  5.457 s\n",
      "Step 200300 (epoch  232.72), loss: 0.003994, time:  5.451 s\n",
      "Step 200350 (epoch  232.78), loss: 0.002951, time:  5.437 s\n",
      "Step 200400 (epoch  232.84), loss: 0.002862, time:  5.422 s, error: 0.016821\n",
      "Step 200450 (epoch  232.90), loss: 0.002578, time:  8.653 s\n",
      "Step 200500 (epoch  232.96), loss: 0.002908, time:  5.457 s\n",
      "Step 200550 (epoch  233.01), loss: 0.002318, time:  5.447 s\n",
      "Step 200600 (epoch  233.07), loss: 0.002818, time:  5.441 s\n",
      "Step 200650 (epoch  233.13), loss: 0.003473, time:  5.423 s\n",
      "Step 200700 (epoch  233.19), loss: 0.003700, time:  5.436 s\n",
      "Step 200750 (epoch  233.25), loss: 0.003393, time:  5.435 s\n",
      "Step 200800 (epoch  233.30), loss: 0.003478, time:  5.441 s, error: 0.016268\n",
      "Step 200850 (epoch  233.36), loss: 0.002780, time:  8.760 s\n",
      "Step 200900 (epoch  233.42), loss: 0.002329, time:  5.440 s\n",
      "Step 200950 (epoch  233.48), loss: 0.002604, time:  5.435 s\n",
      "Step 201000 (epoch  233.54), loss: 0.003348, time:  5.435 s\n",
      "Step 201050 (epoch  233.59), loss: 0.002897, time:  5.450 s\n",
      "Step 201100 (epoch  233.65), loss: 0.001770, time:  5.453 s\n",
      "Step 201150 (epoch  233.71), loss: 0.003299, time:  5.440 s\n",
      "Step 201200 (epoch  233.77), loss: 0.003664, time:  5.458 s, error: 0.016517\n",
      "Step 201250 (epoch  233.83), loss: 0.002398, time:  8.660 s\n",
      "Step 201300 (epoch  233.88), loss: 0.002554, time:  5.447 s\n",
      "Step 201350 (epoch  233.94), loss: 0.002383, time:  5.444 s\n",
      "Step 201400 (epoch  234.00), loss: 0.003175, time:  5.440 s\n",
      "Step 201450 (epoch  234.06), loss: 0.002470, time:  5.448 s\n",
      "Step 201500 (epoch  234.12), loss: 0.002600, time:  5.454 s\n",
      "Step 201550 (epoch  234.18), loss: 0.003191, time:  5.452 s\n",
      "Step 201600 (epoch  234.23), loss: 0.003698, time:  5.441 s, error: 0.016352\n",
      "Step 201650 (epoch  234.29), loss: 0.003627, time:  8.632 s\n",
      "Step 201700 (epoch  234.35), loss: 0.002738, time:  5.425 s\n",
      "Step 201750 (epoch  234.41), loss: 0.003295, time:  5.441 s\n",
      "Step 201800 (epoch  234.47), loss: 0.003252, time:  5.433 s\n",
      "Step 201850 (epoch  234.52), loss: 0.004129, time:  5.427 s\n",
      "Step 201900 (epoch  234.58), loss: 0.002996, time:  5.453 s\n",
      "Step 201950 (epoch  234.64), loss: 0.002873, time:  5.436 s\n",
      "Step 202000 (epoch  234.70), loss: 0.003414, time:  5.426 s, error: 0.016397\n",
      "\n",
      "Time since beginning  : 23599.119 s\n",
      "\n",
      "Step 202050 (epoch  234.76), loss: 0.003054, time:  8.702 s\n",
      "Step 202100 (epoch  234.81), loss: 0.002428, time:  5.439 s\n",
      "Step 202150 (epoch  234.87), loss: 0.003663, time:  5.430 s\n",
      "Step 202200 (epoch  234.93), loss: 0.002488, time:  5.431 s\n",
      "Step 202250 (epoch  234.99), loss: 0.003177, time:  5.443 s\n",
      "Step 202300 (epoch  235.05), loss: 0.002539, time:  5.445 s\n",
      "Step 202350 (epoch  235.10), loss: 0.002685, time:  5.439 s\n",
      "Step 202400 (epoch  235.16), loss: 0.004217, time:  5.434 s, error: 0.016381\n",
      "Step 202450 (epoch  235.22), loss: 0.002550, time:  8.625 s\n",
      "Step 202500 (epoch  235.28), loss: 0.003400, time:  5.449 s\n",
      "Step 202550 (epoch  235.34), loss: 0.003037, time:  5.428 s\n",
      "Step 202600 (epoch  235.40), loss: 0.003244, time:  5.453 s\n",
      "Step 202650 (epoch  235.45), loss: 0.002112, time:  5.456 s\n",
      "Step 202700 (epoch  235.51), loss: 0.004147, time:  5.426 s\n",
      "Step 202750 (epoch  235.57), loss: 0.002433, time:  5.443 s\n",
      "Step 202800 (epoch  235.63), loss: 0.005207, time:  5.450 s, error: 0.016819\n",
      "Step 202850 (epoch  235.69), loss: 0.002798, time:  8.656 s\n",
      "Step 202900 (epoch  235.74), loss: 0.003693, time:  5.428 s\n",
      "Step 202950 (epoch  235.80), loss: 0.004783, time:  5.434 s\n",
      "Step 203000 (epoch  235.86), loss: 0.003469, time:  5.453 s\n",
      "Step 203050 (epoch  235.92), loss: 0.002473, time:  5.420 s\n",
      "Step 203100 (epoch  235.98), loss: 0.003121, time:  5.431 s\n",
      "Step 203150 (epoch  236.03), loss: 0.002897, time:  5.435 s\n",
      "Step 203200 (epoch  236.09), loss: 0.002525, time:  5.434 s, error: 0.016483\n",
      "Step 203250 (epoch  236.15), loss: 0.003821, time:  8.632 s\n",
      "Step 203300 (epoch  236.21), loss: 0.002490, time:  5.445 s\n",
      "Step 203350 (epoch  236.27), loss: 0.002553, time:  5.440 s\n",
      "Step 203400 (epoch  236.32), loss: 0.003591, time:  5.431 s\n",
      "Step 203450 (epoch  236.38), loss: 0.002274, time:  5.439 s\n",
      "Step 203500 (epoch  236.44), loss: 0.004041, time:  5.422 s\n",
      "Step 203550 (epoch  236.50), loss: 0.002547, time:  5.437 s\n",
      "Step 203600 (epoch  236.56), loss: 0.002406, time:  5.441 s, error: 0.016317\n",
      "Step 203650 (epoch  236.62), loss: 0.002308, time:  8.670 s\n",
      "Step 203700 (epoch  236.67), loss: 0.002455, time:  5.427 s\n",
      "Step 203750 (epoch  236.73), loss: 0.002806, time:  5.448 s\n",
      "Step 203800 (epoch  236.79), loss: 0.002834, time:  5.422 s\n",
      "Step 203850 (epoch  236.85), loss: 0.002497, time:  5.430 s\n",
      "Step 203900 (epoch  236.91), loss: 0.003251, time:  5.431 s\n",
      "Step 203950 (epoch  236.96), loss: 0.002987, time:  5.430 s\n",
      "Step 204000 (epoch  237.02), loss: 0.003282, time:  5.431 s, error: 0.017214\n",
      "\n",
      "Time since beginning  : 23832.683 s\n",
      "\n",
      "Step 204050 (epoch  237.08), loss: 0.003169, time:  8.697 s\n",
      "Step 204100 (epoch  237.14), loss: 0.003273, time:  5.448 s\n",
      "Step 204150 (epoch  237.20), loss: 0.002908, time:  5.427 s\n",
      "Step 204200 (epoch  237.25), loss: 0.002080, time:  5.437 s\n",
      "Step 204250 (epoch  237.31), loss: 0.002808, time:  5.447 s\n",
      "Step 204300 (epoch  237.37), loss: 0.002738, time:  5.432 s\n",
      "Step 204350 (epoch  237.43), loss: 0.002430, time:  5.416 s\n",
      "Step 204400 (epoch  237.49), loss: 0.003705, time:  5.430 s, error: 0.016231\n",
      "Step 204450 (epoch  237.54), loss: 0.004307, time:  8.672 s\n",
      "Step 204500 (epoch  237.60), loss: 0.002639, time:  5.437 s\n",
      "Step 204550 (epoch  237.66), loss: 0.004054, time:  5.426 s\n",
      "Step 204600 (epoch  237.72), loss: 0.002979, time:  5.432 s\n",
      "Step 204650 (epoch  237.78), loss: 0.003793, time:  5.430 s\n",
      "Step 204700 (epoch  237.84), loss: 0.002880, time:  5.432 s\n",
      "Step 204750 (epoch  237.89), loss: 0.001595, time:  5.431 s\n",
      "Step 204800 (epoch  237.95), loss: 0.001877, time:  5.446 s, error: 0.016441\n",
      "Step 204850 (epoch  238.01), loss: 0.002535, time:  8.688 s\n",
      "Step 204900 (epoch  238.07), loss: 0.002179, time:  5.437 s\n",
      "Step 204950 (epoch  238.13), loss: 0.002797, time:  5.441 s\n",
      "Step 205000 (epoch  238.18), loss: 0.003402, time:  5.446 s\n",
      "Step 205050 (epoch  238.24), loss: 0.002441, time:  5.441 s\n",
      "Step 205100 (epoch  238.30), loss: 0.002748, time:  5.425 s\n",
      "Step 205150 (epoch  238.36), loss: 0.002791, time:  5.428 s\n",
      "Step 205200 (epoch  238.42), loss: 0.002373, time:  5.431 s, error: 0.016407\n",
      "Step 205250 (epoch  238.47), loss: 0.002485, time:  8.668 s\n",
      "Step 205300 (epoch  238.53), loss: 0.002829, time:  5.433 s\n",
      "Step 205350 (epoch  238.59), loss: 0.002611, time:  5.438 s\n",
      "Step 205400 (epoch  238.65), loss: 0.002242, time:  5.430 s\n",
      "Step 205450 (epoch  238.71), loss: 0.003193, time:  5.431 s\n",
      "Step 205500 (epoch  238.76), loss: 0.003630, time:  5.431 s\n",
      "Step 205550 (epoch  238.82), loss: 0.002957, time:  5.444 s\n",
      "Step 205600 (epoch  238.88), loss: 0.002145, time:  5.456 s, error: 0.016265\n",
      "Step 205650 (epoch  238.94), loss: 0.002368, time:  8.627 s\n",
      "Step 205700 (epoch  239.00), loss: 0.003146, time:  5.422 s\n",
      "Step 205750 (epoch  239.06), loss: 0.002725, time:  5.438 s\n",
      "Step 205800 (epoch  239.11), loss: 0.003022, time:  5.434 s\n",
      "Step 205850 (epoch  239.17), loss: 0.002853, time:  5.421 s\n",
      "Step 205900 (epoch  239.23), loss: 0.002886, time:  5.439 s\n",
      "Step 205950 (epoch  239.29), loss: 0.002842, time:  5.447 s\n",
      "Step 206000 (epoch  239.35), loss: 0.002390, time:  5.452 s, error: 0.016285\n",
      "\n",
      "Time since beginning  : 24066.305 s\n",
      "\n",
      "Step 206050 (epoch  239.40), loss: 0.002617, time:  8.750 s\n",
      "Step 206100 (epoch  239.46), loss: 0.001859, time:  5.437 s\n",
      "Step 206150 (epoch  239.52), loss: 0.003031, time:  5.445 s\n",
      "Step 206200 (epoch  239.58), loss: 0.004116, time:  5.432 s\n",
      "Step 206250 (epoch  239.64), loss: 0.002749, time:  5.439 s\n",
      "Step 206300 (epoch  239.69), loss: 0.002161, time:  5.451 s\n",
      "Step 206350 (epoch  239.75), loss: 0.002233, time:  5.437 s\n",
      "Step 206400 (epoch  239.81), loss: 0.009355, time:  5.457 s, error: 0.016530\n",
      "Step 206450 (epoch  239.87), loss: 0.002070, time:  8.691 s\n",
      "Step 206500 (epoch  239.93), loss: 0.002287, time:  5.428 s\n",
      "Step 206550 (epoch  239.98), loss: 0.002832, time:  5.448 s\n",
      "Step 206600 (epoch  240.04), loss: 0.001851, time:  5.443 s\n",
      "Step 206650 (epoch  240.10), loss: 0.002404, time:  5.454 s\n",
      "Step 206700 (epoch  240.16), loss: 0.003801, time:  5.458 s\n",
      "Step 206750 (epoch  240.22), loss: 0.002387, time:  5.434 s\n",
      "Step 206800 (epoch  240.28), loss: 0.003636, time:  5.423 s, error: 0.016560\n",
      "Step 206850 (epoch  240.33), loss: 0.002710, time:  8.639 s\n",
      "Step 206900 (epoch  240.39), loss: 0.002733, time:  5.431 s\n",
      "Step 206950 (epoch  240.45), loss: 0.002098, time:  5.423 s\n",
      "Step 207000 (epoch  240.51), loss: 0.003997, time:  5.439 s\n",
      "Step 207050 (epoch  240.57), loss: 0.003766, time:  5.461 s\n",
      "Step 207100 (epoch  240.62), loss: 0.002682, time:  5.443 s\n",
      "Step 207150 (epoch  240.68), loss: 0.002102, time:  5.436 s\n",
      "Step 207200 (epoch  240.74), loss: 0.002046, time:  5.441 s, error: 0.016532\n",
      "Step 207250 (epoch  240.80), loss: 0.003612, time:  8.660 s\n",
      "Step 207300 (epoch  240.86), loss: 0.003054, time:  5.439 s\n",
      "Step 207350 (epoch  240.91), loss: 0.002645, time:  5.424 s\n",
      "Step 207400 (epoch  240.97), loss: 0.001775, time:  5.445 s\n",
      "Step 207450 (epoch  241.03), loss: 0.004026, time:  5.447 s\n",
      "Step 207500 (epoch  241.09), loss: 0.002527, time:  5.421 s\n",
      "Step 207550 (epoch  241.15), loss: 0.003353, time:  5.440 s\n",
      "Step 207600 (epoch  241.20), loss: 0.003468, time:  5.439 s, error: 0.016332\n",
      "Step 207650 (epoch  241.26), loss: 0.003659, time:  8.669 s\n",
      "Step 207700 (epoch  241.32), loss: 0.002739, time:  5.457 s\n",
      "Step 207750 (epoch  241.38), loss: 0.002699, time:  5.463 s\n",
      "Step 207800 (epoch  241.44), loss: 0.003094, time:  5.437 s\n",
      "Step 207850 (epoch  241.50), loss: 0.003325, time:  5.432 s\n",
      "Step 207900 (epoch  241.55), loss: 0.002933, time:  5.436 s\n",
      "Step 207950 (epoch  241.61), loss: 0.002572, time:  5.432 s\n",
      "Step 208000 (epoch  241.67), loss: 0.002727, time:  5.440 s, error: 0.016227\n",
      "\n",
      "Time since beginning  : 24300.112 s\n",
      "\n",
      "Step 208050 (epoch  241.73), loss: 0.003057, time:  8.710 s\n",
      "Step 208100 (epoch  241.79), loss: 0.002881, time:  5.449 s\n",
      "Step 208150 (epoch  241.84), loss: 0.002842, time:  5.446 s\n",
      "Step 208200 (epoch  241.90), loss: 0.004277, time:  5.436 s\n",
      "Step 208250 (epoch  241.96), loss: 0.002739, time:  5.439 s\n",
      "Step 208300 (epoch  242.02), loss: 0.003399, time:  5.434 s\n",
      "Step 208350 (epoch  242.08), loss: 0.002084, time:  5.433 s\n",
      "Step 208400 (epoch  242.13), loss: 0.002493, time:  5.453 s, error: 0.016397\n",
      "Step 208450 (epoch  242.19), loss: 0.002109, time:  8.646 s\n",
      "Step 208500 (epoch  242.25), loss: 0.002849, time:  5.454 s\n",
      "Step 208550 (epoch  242.31), loss: 0.002663, time:  5.435 s\n",
      "Step 208600 (epoch  242.37), loss: 0.002140, time:  5.434 s\n",
      "Step 208650 (epoch  242.42), loss: 0.002904, time:  5.437 s\n",
      "Step 208700 (epoch  242.48), loss: 0.003498, time:  5.420 s\n",
      "Step 208750 (epoch  242.54), loss: 0.003049, time:  5.438 s\n",
      "Step 208800 (epoch  242.60), loss: 0.002268, time:  5.435 s, error: 0.016147\n",
      "Step 208850 (epoch  242.66), loss: 0.003246, time:  8.674 s\n",
      "Step 208900 (epoch  242.72), loss: 0.002756, time:  5.437 s\n",
      "Step 208950 (epoch  242.77), loss: 0.004615, time:  5.441 s\n",
      "Step 209000 (epoch  242.83), loss: 0.004009, time:  5.437 s\n",
      "Step 209050 (epoch  242.89), loss: 0.002635, time:  5.434 s\n",
      "Step 209100 (epoch  242.95), loss: 0.002223, time:  5.435 s\n",
      "Step 209150 (epoch  243.01), loss: 0.002765, time:  5.442 s\n",
      "Step 209200 (epoch  243.06), loss: 0.003338, time:  5.444 s, error: 0.016525\n",
      "Step 209250 (epoch  243.12), loss: 0.003012, time:  8.750 s\n",
      "Step 209300 (epoch  243.18), loss: 0.003203, time:  5.447 s\n",
      "Step 209350 (epoch  243.24), loss: 0.001762, time:  5.439 s\n",
      "Step 209400 (epoch  243.30), loss: 0.001618, time:  5.425 s\n",
      "Step 209450 (epoch  243.35), loss: 0.003959, time:  5.431 s\n",
      "Step 209500 (epoch  243.41), loss: 0.003345, time:  5.424 s\n",
      "Step 209550 (epoch  243.47), loss: 0.003058, time:  5.440 s\n",
      "Step 209600 (epoch  243.53), loss: 0.002618, time:  5.445 s, error: 0.016255\n",
      "Step 209650 (epoch  243.59), loss: 0.002278, time:  8.659 s\n",
      "Step 209700 (epoch  243.64), loss: 0.002321, time:  5.427 s\n",
      "Step 209750 (epoch  243.70), loss: 0.002679, time:  5.449 s\n",
      "Step 209800 (epoch  243.76), loss: 0.003322, time:  5.447 s\n",
      "Step 209850 (epoch  243.82), loss: 0.003448, time:  5.453 s\n",
      "Step 209900 (epoch  243.88), loss: 0.002512, time:  5.441 s\n",
      "Step 209950 (epoch  243.94), loss: 0.002444, time:  5.452 s\n",
      "Step 210000 (epoch  243.99), loss: 0.002663, time:  5.431 s, error: 0.016488\n",
      "\n",
      "Time since beginning  : 24533.917 s\n",
      "\n",
      "Step 210050 (epoch  244.05), loss: 0.002632, time:  8.732 s\n",
      "Step 210100 (epoch  244.11), loss: 0.004916, time:  5.434 s\n",
      "Step 210150 (epoch  244.17), loss: 0.002595, time:  5.437 s\n",
      "Step 210200 (epoch  244.23), loss: 0.002652, time:  5.433 s\n",
      "Step 210250 (epoch  244.28), loss: 0.002712, time:  5.424 s\n",
      "Step 210300 (epoch  244.34), loss: 0.002192, time:  5.428 s\n",
      "Step 210350 (epoch  244.40), loss: 0.002357, time:  5.446 s\n",
      "Step 210400 (epoch  244.46), loss: 0.001890, time:  5.432 s, error: 0.016173\n",
      "Step 210450 (epoch  244.52), loss: 0.002976, time:  8.658 s\n",
      "Step 210500 (epoch  244.57), loss: 0.002887, time:  5.456 s\n",
      "Step 210550 (epoch  244.63), loss: 0.002914, time:  5.430 s\n",
      "Step 210600 (epoch  244.69), loss: 0.001765, time:  5.441 s\n",
      "Step 210650 (epoch  244.75), loss: 0.003245, time:  5.433 s\n",
      "Step 210700 (epoch  244.81), loss: 0.002472, time:  5.463 s\n",
      "Step 210750 (epoch  244.86), loss: 0.002865, time:  5.436 s\n",
      "Step 210800 (epoch  244.92), loss: 0.002840, time:  5.420 s, error: 0.016837\n",
      "Step 210850 (epoch  244.98), loss: 0.003319, time:  8.625 s\n",
      "Step 210900 (epoch  245.04), loss: 0.002553, time:  5.420 s\n",
      "Step 210950 (epoch  245.10), loss: 0.003024, time:  5.443 s\n",
      "Step 211000 (epoch  245.15), loss: 0.003607, time:  5.451 s\n",
      "Step 211050 (epoch  245.21), loss: 0.003252, time:  5.448 s\n",
      "Step 211100 (epoch  245.27), loss: 0.003788, time:  5.437 s\n",
      "Step 211150 (epoch  245.33), loss: 0.001954, time:  5.418 s\n",
      "Step 211200 (epoch  245.39), loss: 0.002726, time:  5.445 s, error: 0.016291\n",
      "Step 211250 (epoch  245.45), loss: 0.002602, time:  8.620 s\n",
      "Step 211300 (epoch  245.50), loss: 0.004116, time:  5.437 s\n",
      "Step 211350 (epoch  245.56), loss: 0.003705, time:  5.437 s\n",
      "Step 211400 (epoch  245.62), loss: 0.002717, time:  5.437 s\n",
      "Step 211450 (epoch  245.68), loss: 0.004485, time:  5.438 s\n",
      "Step 211500 (epoch  245.74), loss: 0.002864, time:  5.421 s\n",
      "Step 211550 (epoch  245.79), loss: 0.002436, time:  5.436 s\n",
      "Step 211600 (epoch  245.85), loss: 0.002263, time:  5.435 s, error: 0.016308\n",
      "Step 211650 (epoch  245.91), loss: 0.003270, time:  8.629 s\n",
      "Step 211700 (epoch  245.97), loss: 0.002575, time:  5.430 s\n",
      "Step 211750 (epoch  246.03), loss: 0.003074, time:  5.420 s\n",
      "Step 211800 (epoch  246.08), loss: 0.002275, time:  5.448 s\n",
      "Step 211850 (epoch  246.14), loss: 0.003938, time:  5.443 s\n",
      "Step 211900 (epoch  246.20), loss: 0.001944, time:  5.452 s\n",
      "Step 211950 (epoch  246.26), loss: 0.003266, time:  5.442 s\n",
      "Step 212000 (epoch  246.32), loss: 0.002107, time:  5.436 s, error: 0.016373\n",
      "\n",
      "Time since beginning  : 24767.494 s\n",
      "\n",
      "Step 212050 (epoch  246.37), loss: 0.003381, time:  8.738 s\n",
      "Step 212100 (epoch  246.43), loss: 0.003476, time:  5.430 s\n",
      "Step 212150 (epoch  246.49), loss: 0.003248, time:  5.449 s\n",
      "Step 212200 (epoch  246.55), loss: 0.002987, time:  5.442 s\n",
      "Step 212250 (epoch  246.61), loss: 0.003104, time:  5.447 s\n",
      "Step 212300 (epoch  246.67), loss: 0.002225, time:  5.442 s\n",
      "Step 212350 (epoch  246.72), loss: 0.003026, time:  5.442 s\n",
      "Step 212400 (epoch  246.78), loss: 0.003498, time:  5.443 s, error: 0.016596\n",
      "Step 212450 (epoch  246.84), loss: 0.003658, time:  8.630 s\n",
      "Step 212500 (epoch  246.90), loss: 0.003002, time:  5.449 s\n",
      "Step 212550 (epoch  246.96), loss: 0.003143, time:  5.436 s\n",
      "Step 212600 (epoch  247.01), loss: 0.003404, time:  5.415 s\n",
      "Step 212650 (epoch  247.07), loss: 0.003072, time:  5.424 s\n",
      "Step 212700 (epoch  247.13), loss: 0.002258, time:  5.436 s\n",
      "Step 212750 (epoch  247.19), loss: 0.002706, time:  5.431 s\n",
      "Step 212800 (epoch  247.25), loss: 0.002380, time:  5.431 s, error: 0.016349\n",
      "Step 212850 (epoch  247.30), loss: 0.002183, time:  8.658 s\n",
      "Step 212900 (epoch  247.36), loss: 0.003410, time:  5.438 s\n",
      "Step 212950 (epoch  247.42), loss: 0.003344, time:  5.440 s\n",
      "Step 213000 (epoch  247.48), loss: 0.003270, time:  5.443 s\n",
      "Step 213050 (epoch  247.54), loss: 0.005609, time:  5.453 s\n",
      "Step 213100 (epoch  247.59), loss: 0.002664, time:  5.441 s\n",
      "Step 213150 (epoch  247.65), loss: 0.004118, time:  5.454 s\n",
      "Step 213200 (epoch  247.71), loss: 0.005221, time:  5.453 s, error: 0.016247\n",
      "Step 213250 (epoch  247.77), loss: 0.002162, time:  8.659 s\n",
      "Step 213300 (epoch  247.83), loss: 0.002924, time:  5.430 s\n",
      "Step 213350 (epoch  247.89), loss: 0.002510, time:  5.443 s\n",
      "Step 213400 (epoch  247.94), loss: 0.002802, time:  5.421 s\n",
      "Step 213450 (epoch  248.00), loss: 0.002763, time:  5.428 s\n",
      "Step 213500 (epoch  248.06), loss: 0.002596, time:  5.430 s\n",
      "Step 213550 (epoch  248.12), loss: 0.003316, time:  5.445 s\n",
      "Step 213600 (epoch  248.18), loss: 0.002469, time:  5.440 s, error: 0.016413\n",
      "Step 213650 (epoch  248.23), loss: 0.002631, time:  8.762 s\n",
      "Step 213700 (epoch  248.29), loss: 0.003910, time:  5.420 s\n",
      "Step 213750 (epoch  248.35), loss: 0.001948, time:  5.424 s\n",
      "Step 213800 (epoch  248.41), loss: 0.001672, time:  5.437 s\n",
      "Step 213850 (epoch  248.47), loss: 0.002311, time:  5.425 s\n",
      "Step 213900 (epoch  248.52), loss: 0.003330, time:  5.430 s\n",
      "Step 213950 (epoch  248.58), loss: 0.004079, time:  5.451 s\n",
      "Step 214000 (epoch  248.64), loss: 0.002621, time:  5.455 s, error: 0.016191\n",
      "\n",
      "Time since beginning  : 25001.203 s\n",
      "\n",
      "Step 214050 (epoch  248.70), loss: 0.003927, time:  8.717 s\n",
      "Step 214100 (epoch  248.76), loss: 0.003913, time:  5.454 s\n",
      "Step 214150 (epoch  248.81), loss: 0.003235, time:  5.443 s\n",
      "Step 214200 (epoch  248.87), loss: 0.002611, time:  5.447 s\n",
      "Step 214250 (epoch  248.93), loss: 0.002428, time:  5.457 s\n",
      "Step 214300 (epoch  248.99), loss: 0.002347, time:  5.447 s\n",
      "Step 214350 (epoch  249.05), loss: 0.002714, time:  5.447 s\n",
      "Step 214400 (epoch  249.11), loss: 0.003063, time:  5.421 s, error: 0.016314\n",
      "Step 214450 (epoch  249.16), loss: 0.003089, time:  8.669 s\n",
      "Step 214500 (epoch  249.22), loss: 0.002762, time:  5.444 s\n",
      "Step 214550 (epoch  249.28), loss: 0.003222, time:  5.451 s\n",
      "Step 214600 (epoch  249.34), loss: 0.002404, time:  5.437 s\n",
      "Step 214650 (epoch  249.40), loss: 0.003235, time:  5.451 s\n",
      "Step 214700 (epoch  249.45), loss: 0.003163, time:  5.452 s\n",
      "Step 214750 (epoch  249.51), loss: 0.003098, time:  5.444 s\n",
      "Step 214800 (epoch  249.57), loss: 0.003026, time:  5.435 s, error: 0.016173\n",
      "Step 214850 (epoch  249.63), loss: 0.002929, time:  8.663 s\n",
      "Step 214900 (epoch  249.69), loss: 0.002746, time:  5.436 s\n",
      "Step 214950 (epoch  249.74), loss: 0.003622, time:  5.437 s\n",
      "Step 215000 (epoch  249.80), loss: 0.003110, time:  5.428 s\n",
      "Step 215050 (epoch  249.86), loss: 0.004170, time:  5.444 s\n",
      "Step 215100 (epoch  249.92), loss: 0.003438, time:  5.455 s\n",
      "Step 215150 (epoch  249.98), loss: 0.002419, time:  5.443 s\n",
      "Step 215200 (epoch  250.03), loss: 0.003065, time:  5.442 s, error: 0.016566\n",
      "Step 215250 (epoch  250.09), loss: 0.002476, time:  8.653 s\n",
      "Step 215300 (epoch  250.15), loss: 0.003256, time:  5.426 s\n",
      "Step 215350 (epoch  250.21), loss: 0.003579, time:  5.440 s\n",
      "Step 215400 (epoch  250.27), loss: 0.004162, time:  5.455 s\n",
      "Step 215450 (epoch  250.33), loss: 0.003006, time:  5.454 s\n",
      "Step 215500 (epoch  250.38), loss: 0.002631, time:  5.443 s\n",
      "Step 215550 (epoch  250.44), loss: 0.002468, time:  5.451 s\n",
      "Step 215600 (epoch  250.50), loss: 0.002860, time:  5.443 s, error: 0.017756\n",
      "Step 215650 (epoch  250.56), loss: 0.003409, time:  8.630 s\n",
      "Step 215700 (epoch  250.62), loss: 0.004028, time:  5.432 s\n",
      "Step 215750 (epoch  250.67), loss: 0.002336, time:  5.434 s\n",
      "Step 215800 (epoch  250.73), loss: 0.003125, time:  5.456 s\n",
      "Step 215850 (epoch  250.79), loss: 0.002559, time:  5.449 s\n",
      "Step 215900 (epoch  250.85), loss: 0.003158, time:  5.448 s\n",
      "Step 215950 (epoch  250.91), loss: 0.002297, time:  5.439 s\n",
      "Step 216000 (epoch  250.96), loss: 0.002273, time:  5.438 s, error: 0.016432\n",
      "\n",
      "Time since beginning  : 25235.079 s\n",
      "\n",
      "Step 216050 (epoch  251.02), loss: 0.003273, time:  8.714 s\n",
      "Step 216100 (epoch  251.08), loss: 0.002165, time:  5.447 s\n",
      "Step 216150 (epoch  251.14), loss: 0.003590, time:  5.458 s\n",
      "Step 216200 (epoch  251.20), loss: 0.002073, time:  5.465 s\n",
      "Step 216250 (epoch  251.25), loss: 0.004074, time:  5.431 s\n",
      "Step 216300 (epoch  251.31), loss: 0.002490, time:  5.420 s\n",
      "Step 216350 (epoch  251.37), loss: 0.001947, time:  5.433 s\n",
      "Step 216400 (epoch  251.43), loss: 0.003322, time:  5.431 s, error: 0.016957\n",
      "Step 216450 (epoch  251.49), loss: 0.002734, time:  8.623 s\n",
      "Step 216500 (epoch  251.55), loss: 0.003717, time:  5.428 s\n",
      "Step 216550 (epoch  251.60), loss: 0.002786, time:  5.446 s\n",
      "Step 216600 (epoch  251.66), loss: 0.003374, time:  5.425 s\n",
      "Step 216650 (epoch  251.72), loss: 0.003021, time:  5.431 s\n",
      "Step 216700 (epoch  251.78), loss: 0.002504, time:  5.433 s\n",
      "Step 216750 (epoch  251.84), loss: 0.002748, time:  5.440 s\n",
      "Step 216800 (epoch  251.89), loss: 0.002774, time:  5.440 s, error: 0.016334\n",
      "Step 216850 (epoch  251.95), loss: 0.002227, time:  8.631 s\n",
      "Step 216900 (epoch  252.01), loss: 0.003927, time:  5.461 s\n",
      "Step 216950 (epoch  252.07), loss: 0.002570, time:  5.446 s\n",
      "Step 217000 (epoch  252.13), loss: 0.002283, time:  5.436 s\n",
      "Step 217050 (epoch  252.18), loss: 0.002527, time:  5.429 s\n",
      "Step 217100 (epoch  252.24), loss: 0.002223, time:  5.443 s\n",
      "Step 217150 (epoch  252.30), loss: 0.003256, time:  5.453 s\n",
      "Step 217200 (epoch  252.36), loss: 0.002367, time:  5.444 s, error: 0.016299\n",
      "Step 217250 (epoch  252.42), loss: 0.001860, time:  8.648 s\n",
      "Step 217300 (epoch  252.47), loss: 0.002522, time:  5.432 s\n",
      "Step 217350 (epoch  252.53), loss: 0.004356, time:  5.451 s\n",
      "Step 217400 (epoch  252.59), loss: 0.002849, time:  5.453 s\n",
      "Step 217450 (epoch  252.65), loss: 0.002251, time:  5.456 s\n",
      "Step 217500 (epoch  252.71), loss: 0.002948, time:  5.452 s\n",
      "Step 217550 (epoch  252.77), loss: 0.003582, time:  5.452 s\n",
      "Step 217600 (epoch  252.82), loss: 0.003011, time:  5.440 s, error: 0.016371\n",
      "Step 217650 (epoch  252.88), loss: 0.002291, time:  8.730 s\n",
      "Step 217700 (epoch  252.94), loss: 0.003380, time:  5.432 s\n",
      "Step 217750 (epoch  253.00), loss: 0.003670, time:  5.437 s\n",
      "Step 217800 (epoch  253.06), loss: 0.002424, time:  5.457 s\n",
      "Step 217850 (epoch  253.11), loss: 0.002364, time:  5.435 s\n",
      "Step 217900 (epoch  253.17), loss: 0.003891, time:  5.426 s\n",
      "Step 217950 (epoch  253.23), loss: 0.003775, time:  5.430 s\n",
      "Step 218000 (epoch  253.29), loss: 0.002630, time:  5.449 s, error: 0.016219\n",
      "\n",
      "Time since beginning  : 25468.920 s\n",
      "\n",
      "Step 218050 (epoch  253.35), loss: 0.001989, time:  8.792 s\n",
      "Step 218100 (epoch  253.40), loss: 0.003882, time:  5.449 s\n",
      "Step 218150 (epoch  253.46), loss: 0.002976, time:  5.437 s\n",
      "Step 218200 (epoch  253.52), loss: 0.003814, time:  5.457 s\n",
      "Step 218250 (epoch  253.58), loss: 0.003366, time:  5.451 s\n",
      "Step 218300 (epoch  253.64), loss: 0.001813, time:  5.443 s\n",
      "Step 218350 (epoch  253.69), loss: 0.002560, time:  5.451 s\n",
      "Step 218400 (epoch  253.75), loss: 0.002903, time:  5.466 s, error: 0.016746\n",
      "Step 218450 (epoch  253.81), loss: 0.002809, time:  8.656 s\n",
      "Step 218500 (epoch  253.87), loss: 0.002552, time:  5.458 s\n",
      "Step 218550 (epoch  253.93), loss: 0.002365, time:  5.448 s\n",
      "Step 218600 (epoch  253.99), loss: 0.003036, time:  5.450 s\n",
      "Step 218650 (epoch  254.04), loss: 0.002952, time:  5.451 s\n",
      "Step 218700 (epoch  254.10), loss: 0.002970, time:  5.421 s\n",
      "Step 218750 (epoch  254.16), loss: 0.003731, time:  5.457 s\n",
      "Step 218800 (epoch  254.22), loss: 0.003802, time:  5.440 s, error: 0.016380\n",
      "Step 218850 (epoch  254.28), loss: 0.003270, time:  8.662 s\n",
      "Step 218900 (epoch  254.33), loss: 0.003887, time:  5.445 s\n",
      "Step 218950 (epoch  254.39), loss: 0.003179, time:  5.448 s\n",
      "Step 219000 (epoch  254.45), loss: 0.002717, time:  5.449 s\n",
      "Step 219050 (epoch  254.51), loss: 0.002217, time:  5.442 s\n",
      "Step 219100 (epoch  254.57), loss: 0.004091, time:  5.466 s\n",
      "Step 219150 (epoch  254.62), loss: 0.004332, time:  5.444 s\n",
      "Step 219200 (epoch  254.68), loss: 0.002705, time:  5.459 s, error: 0.016410\n",
      "Step 219250 (epoch  254.74), loss: 0.002445, time:  8.688 s\n",
      "Step 219300 (epoch  254.80), loss: 0.003301, time:  5.441 s\n",
      "Step 219350 (epoch  254.86), loss: 0.002098, time:  5.450 s\n",
      "Step 219400 (epoch  254.91), loss: 0.002872, time:  5.446 s\n",
      "Step 219450 (epoch  254.97), loss: 0.003147, time:  5.450 s\n",
      "Step 219500 (epoch  255.03), loss: 0.002831, time:  5.448 s\n",
      "Step 219550 (epoch  255.09), loss: 0.003668, time:  5.447 s\n",
      "Step 219600 (epoch  255.15), loss: 0.002950, time:  5.445 s, error: 0.016409\n",
      "Step 219650 (epoch  255.21), loss: 0.002694, time:  8.621 s\n",
      "Step 219700 (epoch  255.26), loss: 0.003183, time:  5.428 s\n",
      "Step 219750 (epoch  255.32), loss: 0.003264, time:  5.442 s\n",
      "Step 219800 (epoch  255.38), loss: 0.002952, time:  5.445 s\n",
      "Step 219850 (epoch  255.44), loss: 0.002279, time:  5.443 s\n",
      "Step 219900 (epoch  255.50), loss: 0.003536, time:  5.434 s\n",
      "Step 219950 (epoch  255.55), loss: 0.003334, time:  5.420 s\n",
      "Step 220000 (epoch  255.61), loss: 0.002415, time:  5.432 s, error: 0.016223\n",
      "\n",
      "Time since beginning  : 25702.885 s\n",
      "\n",
      "Step 220050 (epoch  255.67), loss: 0.002098, time:  8.712 s\n",
      "Step 220100 (epoch  255.73), loss: 0.001670, time:  5.453 s\n",
      "Step 220150 (epoch  255.79), loss: 0.002419, time:  5.424 s\n",
      "Step 220200 (epoch  255.84), loss: 0.003461, time:  5.440 s\n",
      "Step 220250 (epoch  255.90), loss: 0.002613, time:  5.439 s\n",
      "Step 220300 (epoch  255.96), loss: 0.003005, time:  5.428 s\n",
      "Step 220350 (epoch  256.02), loss: 0.003595, time:  5.419 s\n",
      "Step 220400 (epoch  256.08), loss: 0.002689, time:  5.423 s, error: 0.016402\n",
      "Step 220450 (epoch  256.13), loss: 0.002453, time:  8.611 s\n",
      "Step 220500 (epoch  256.19), loss: 0.002317, time:  5.431 s\n",
      "Step 220550 (epoch  256.25), loss: 0.003346, time:  5.458 s\n",
      "Step 220600 (epoch  256.31), loss: 0.002862, time:  5.437 s\n",
      "Step 220650 (epoch  256.37), loss: 0.003028, time:  5.430 s\n",
      "Step 220700 (epoch  256.43), loss: 0.004014, time:  5.450 s\n",
      "Step 220750 (epoch  256.48), loss: 0.004165, time:  5.425 s\n",
      "Step 220800 (epoch  256.54), loss: 0.002700, time:  5.439 s, error: 0.016104\n",
      "Step 220850 (epoch  256.60), loss: 0.003741, time:  8.654 s\n",
      "Step 220900 (epoch  256.66), loss: 0.002827, time:  5.458 s\n",
      "Step 220950 (epoch  256.72), loss: 0.002532, time:  5.443 s\n",
      "Step 221000 (epoch  256.77), loss: 0.004215, time:  5.431 s\n",
      "Step 221050 (epoch  256.83), loss: 0.002729, time:  5.432 s\n",
      "Step 221100 (epoch  256.89), loss: 0.002202, time:  5.433 s\n",
      "Step 221150 (epoch  256.95), loss: 0.003255, time:  5.434 s\n",
      "Step 221200 (epoch  257.01), loss: 0.003448, time:  5.426 s, error: 0.016536\n",
      "Step 221250 (epoch  257.06), loss: 0.003255, time:  8.638 s\n",
      "Step 221300 (epoch  257.12), loss: 0.002446, time:  5.441 s\n",
      "Step 221350 (epoch  257.18), loss: 0.002660, time:  5.429 s\n",
      "Step 221400 (epoch  257.24), loss: 0.004177, time:  5.429 s\n",
      "Step 221450 (epoch  257.30), loss: 0.002627, time:  5.430 s\n",
      "Step 221500 (epoch  257.35), loss: 0.003773, time:  5.420 s\n",
      "Step 221550 (epoch  257.41), loss: 0.002282, time:  5.422 s\n",
      "Step 221600 (epoch  257.47), loss: 0.003079, time:  5.440 s, error: 0.016501\n",
      "Step 221650 (epoch  257.53), loss: 0.002214, time:  8.654 s\n",
      "Step 221700 (epoch  257.59), loss: 0.002282, time:  5.419 s\n",
      "Step 221750 (epoch  257.65), loss: 0.002660, time:  5.428 s\n",
      "Step 221800 (epoch  257.70), loss: 0.002753, time:  5.417 s\n",
      "Step 221850 (epoch  257.76), loss: 0.003346, time:  5.426 s\n",
      "Step 221900 (epoch  257.82), loss: 0.003002, time:  5.437 s\n",
      "Step 221950 (epoch  257.88), loss: 0.002223, time:  5.436 s\n",
      "Step 222000 (epoch  257.94), loss: 0.003233, time:  5.424 s, error: 0.016437\n",
      "\n",
      "Time since beginning  : 25936.412 s\n",
      "\n",
      "Step 222050 (epoch  257.99), loss: 0.004173, time:  8.836 s\n",
      "Step 222100 (epoch  258.05), loss: 0.002432, time:  5.439 s\n",
      "Step 222150 (epoch  258.11), loss: 0.002645, time:  5.427 s\n",
      "Step 222200 (epoch  258.17), loss: 0.002912, time:  5.428 s\n",
      "Step 222250 (epoch  258.23), loss: 0.001850, time:  5.419 s\n",
      "Step 222300 (epoch  258.28), loss: 0.003364, time:  5.419 s\n",
      "Step 222350 (epoch  258.34), loss: 0.002708, time:  5.442 s\n",
      "Step 222400 (epoch  258.40), loss: 0.002759, time:  5.453 s, error: 0.016564\n",
      "Step 222450 (epoch  258.46), loss: 0.002231, time:  8.633 s\n",
      "Step 222500 (epoch  258.52), loss: 0.001449, time:  5.436 s\n",
      "Step 222550 (epoch  258.57), loss: 0.002622, time:  5.427 s\n",
      "Step 222600 (epoch  258.63), loss: 0.002638, time:  5.439 s\n",
      "Step 222650 (epoch  258.69), loss: 0.002496, time:  5.428 s\n",
      "Step 222700 (epoch  258.75), loss: 0.002098, time:  5.447 s\n",
      "Step 222750 (epoch  258.81), loss: 0.002658, time:  5.444 s\n",
      "Step 222800 (epoch  258.87), loss: 0.002668, time:  5.439 s, error: 0.016435\n",
      "Step 222850 (epoch  258.92), loss: 0.002060, time:  8.622 s\n",
      "Step 222900 (epoch  258.98), loss: 0.002753, time:  5.433 s\n",
      "Step 222950 (epoch  259.04), loss: 0.004226, time:  5.445 s\n",
      "Step 223000 (epoch  259.10), loss: 0.003610, time:  5.439 s\n",
      "Step 223050 (epoch  259.16), loss: 0.002886, time:  5.440 s\n",
      "Step 223100 (epoch  259.21), loss: 0.002932, time:  5.440 s\n",
      "Step 223150 (epoch  259.27), loss: 0.002123, time:  5.456 s\n",
      "Step 223200 (epoch  259.33), loss: 0.002653, time:  5.422 s, error: 0.016264\n",
      "Step 223250 (epoch  259.39), loss: 0.002699, time:  8.643 s\n",
      "Step 223300 (epoch  259.45), loss: 0.002796, time:  5.430 s\n",
      "Step 223350 (epoch  259.50), loss: 0.002636, time:  5.430 s\n",
      "Step 223400 (epoch  259.56), loss: 0.003240, time:  5.448 s\n",
      "Step 223450 (epoch  259.62), loss: 0.002897, time:  5.441 s\n",
      "Step 223500 (epoch  259.68), loss: 0.002369, time:  5.462 s\n",
      "Step 223550 (epoch  259.74), loss: 0.003045, time:  5.435 s\n",
      "Step 223600 (epoch  259.79), loss: 0.003478, time:  5.431 s, error: 0.016421\n",
      "Step 223650 (epoch  259.85), loss: 0.003322, time:  8.653 s\n",
      "Step 223700 (epoch  259.91), loss: 0.003038, time:  5.437 s\n",
      "Step 223750 (epoch  259.97), loss: 0.002278, time:  5.453 s\n",
      "Step 223800 (epoch  260.03), loss: 0.002560, time:  5.434 s\n",
      "Step 223850 (epoch  260.09), loss: 0.003266, time:  5.438 s\n",
      "Step 223900 (epoch  260.14), loss: 0.002273, time:  5.452 s\n",
      "Step 223950 (epoch  260.20), loss: 0.003011, time:  5.442 s\n",
      "Step 224000 (epoch  260.26), loss: 0.003120, time:  5.452 s, error: 0.016708\n",
      "\n",
      "Time since beginning  : 26170.036 s\n",
      "\n",
      "Step 224050 (epoch  260.32), loss: 0.002358, time:  8.715 s\n",
      "Step 224100 (epoch  260.38), loss: 0.002259, time:  5.425 s\n",
      "Step 224150 (epoch  260.43), loss: 0.003119, time:  5.443 s\n",
      "Step 224200 (epoch  260.49), loss: 0.004116, time:  5.467 s\n",
      "Step 224250 (epoch  260.55), loss: 0.002847, time:  5.450 s\n",
      "Step 224300 (epoch  260.61), loss: 0.001933, time:  5.435 s\n",
      "Step 224350 (epoch  260.67), loss: 0.002694, time:  5.455 s\n",
      "Step 224400 (epoch  260.72), loss: 0.002513, time:  5.430 s, error: 0.016429\n",
      "Step 224450 (epoch  260.78), loss: 0.002768, time:  8.651 s\n",
      "Step 224500 (epoch  260.84), loss: 0.003535, time:  5.443 s\n",
      "Step 224550 (epoch  260.90), loss: 0.003347, time:  5.435 s\n",
      "Step 224600 (epoch  260.96), loss: 0.006305, time:  5.456 s\n",
      "Step 224650 (epoch  261.01), loss: 0.001662, time:  5.439 s\n",
      "Step 224700 (epoch  261.07), loss: 0.004277, time:  5.430 s\n",
      "Step 224750 (epoch  261.13), loss: 0.002855, time:  5.429 s\n",
      "Step 224800 (epoch  261.19), loss: 0.001822, time:  5.444 s, error: 0.016392\n",
      "Step 224850 (epoch  261.25), loss: 0.002371, time:  8.649 s\n",
      "Step 224900 (epoch  261.31), loss: 0.002667, time:  5.430 s\n",
      "Step 224950 (epoch  261.36), loss: 0.003968, time:  5.452 s\n",
      "Step 225000 (epoch  261.42), loss: 0.003543, time:  5.438 s\n",
      "Step 225050 (epoch  261.48), loss: 0.002527, time:  5.424 s\n",
      "Step 225100 (epoch  261.54), loss: 0.003732, time:  5.440 s\n",
      "Step 225150 (epoch  261.60), loss: 0.002840, time:  5.433 s\n",
      "Step 225200 (epoch  261.65), loss: 0.002141, time:  5.453 s, error: 0.016119\n",
      "Step 225250 (epoch  261.71), loss: 0.002176, time:  8.639 s\n",
      "Step 225300 (epoch  261.77), loss: 0.005154, time:  5.449 s\n",
      "Step 225350 (epoch  261.83), loss: 0.003054, time:  5.438 s\n",
      "Step 225400 (epoch  261.89), loss: 0.003448, time:  5.434 s\n",
      "Step 225450 (epoch  261.94), loss: 0.002013, time:  5.420 s\n",
      "Step 225500 (epoch  262.00), loss: 0.004253, time:  5.440 s\n",
      "Step 225550 (epoch  262.06), loss: 0.002629, time:  5.414 s\n",
      "Step 225600 (epoch  262.12), loss: 0.002511, time:  5.430 s, error: 0.016342\n",
      "Step 225650 (epoch  262.18), loss: 0.002528, time:  8.659 s\n",
      "Step 225700 (epoch  262.23), loss: 0.003624, time:  5.438 s\n",
      "Step 225750 (epoch  262.29), loss: 0.003384, time:  5.448 s\n",
      "Step 225800 (epoch  262.35), loss: 0.002375, time:  5.435 s\n",
      "Step 225850 (epoch  262.41), loss: 0.003417, time:  5.437 s\n",
      "Step 225900 (epoch  262.47), loss: 0.002686, time:  5.427 s\n",
      "Step 225950 (epoch  262.52), loss: 0.003789, time:  5.428 s\n",
      "Step 226000 (epoch  262.58), loss: 0.003386, time:  5.417 s, error: 0.016448\n",
      "\n",
      "Time since beginning  : 26403.654 s\n",
      "\n",
      "Step 226050 (epoch  262.64), loss: 0.003422, time:  8.733 s\n",
      "Step 226100 (epoch  262.70), loss: 0.001800, time:  5.440 s\n",
      "Step 226150 (epoch  262.76), loss: 0.002571, time:  5.439 s\n",
      "Step 226200 (epoch  262.82), loss: 0.003271, time:  5.444 s\n",
      "Step 226250 (epoch  262.87), loss: 0.001578, time:  5.441 s\n",
      "Step 226300 (epoch  262.93), loss: 0.003399, time:  5.433 s\n",
      "Step 226350 (epoch  262.99), loss: 0.003746, time:  5.431 s\n",
      "Step 226400 (epoch  263.05), loss: 0.002214, time:  5.432 s, error: 0.016408\n",
      "Step 226450 (epoch  263.11), loss: 0.002639, time:  8.754 s\n",
      "Step 226500 (epoch  263.16), loss: 0.002612, time:  5.436 s\n",
      "Step 226550 (epoch  263.22), loss: 0.002711, time:  5.442 s\n",
      "Step 226600 (epoch  263.28), loss: 0.002881, time:  5.414 s\n",
      "Step 226650 (epoch  263.34), loss: 0.002035, time:  5.434 s\n",
      "Step 226700 (epoch  263.40), loss: 0.002473, time:  5.435 s\n",
      "Step 226750 (epoch  263.45), loss: 0.002347, time:  5.453 s\n",
      "Step 226800 (epoch  263.51), loss: 0.003449, time:  5.449 s, error: 0.016157\n",
      "Step 226850 (epoch  263.57), loss: 0.003836, time:  8.683 s\n",
      "Step 226900 (epoch  263.63), loss: 0.002757, time:  5.450 s\n",
      "Step 226950 (epoch  263.69), loss: 0.003513, time:  5.426 s\n"
     ]
    }
   ],
   "source": [
    "# Build an initialization operation.\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Register a model saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Load the previous checkpoint if existed.\n",
    "checkpoint_path = restore_latest_from_ckpt(save_path)\n",
    "load = False\n",
    "if load and checkpoint_path:\n",
    "  saver.restore(sess, checkpoint_path)\n",
    "  print(\"Restore from the lastest checkpoint file %s ...\" % checkpoint_path)\n",
    "  print(\"\")\n",
    "\n",
    "print(\"Initialized!\")\n",
    "print(\"\")\n",
    "print(\"Training Samples      :\", len(X_train))\n",
    "print(\"Batch Size            :\", batch_size)\n",
    "print(\"Number of Epochs      :\", num_epochs)\n",
    "print(\"Log Frequency         :\", log_frequency)\n",
    "print(\"Eval Frequency        :\", eval_frequency)\n",
    "print(\"\")\n",
    "\n",
    "# Save the graph\n",
    "saver.save(sess, save_path=chk_file, global_step=0)\n",
    "\n",
    "tic = time.time()\n",
    "tstart = time.time()\n",
    "\n",
    "# Keep the validation errors locally\n",
    "valid_hists = []\n",
    "\n",
    "# Loop through training steps.\n",
    "for step in range(int(num_epochs * len(X_train)) // batch_size):\n",
    "  \n",
    "  # Compute the offset of the current minibatch in the data.\n",
    "  # The dataset was already shuffled in assignment 1 so we do not need to\n",
    "  # randomize it.\n",
    "  offset = (step * batch_size) % (len(X_train) - batch_size)\n",
    "  batch_dataset = X_train[offset: (offset + batch_size), ...]\n",
    "  batch_targets = y_train[offset: (offset + batch_size), ...]\n",
    "  \n",
    "  # Build the feed dict to feed previous defined placeholders.\n",
    "  feed_dict = {\n",
    "    X_batch: batch_dataset, \n",
    "    y_batch: batch_targets, \n",
    "    keep_prob: 0.7, \n",
    "    dense_keep_prob: 0.5\n",
    "  }\n",
    "  \n",
    "  # Run the optimization session.\n",
    "  sess.run([optimizer], feed_dict=feed_dict)\n",
    "  \n",
    "  # Run the step decay function\n",
    "  if step > 0 and step % (len(X_train) // batch_size) == 0:\n",
    "    sess.run([global_epoch_op])\n",
    "  \n",
    "  # Save the training accuracy every 100 steps.\n",
    "  if step % log_frequency == 0:\n",
    "    summary, error = sess.run([merged, loss], feed_dict=feed_dict)\n",
    "    elapsed_time = time.time() - tic\n",
    "    tic = time.time()\n",
    "    eval_run = (step % eval_frequency == 0) and (step > 0)\n",
    "    if not eval_run:\n",
    "      print(\"Step %6d (epoch %7.2f), loss: %.6f, time: %6.3f s\" % (\n",
    "        step, float(step) * batch_size / len(X_train), error, elapsed_time))\n",
    "    writer.add_summary(summary, step)\n",
    "    # Every `eval_frequency` steps we shall take several extra operations,\n",
    "    # including printing the validation accuracy and updating the learning\n",
    "    # rate.\n",
    "    if eval_run:\n",
    "      lr = sess.run(learning_rate)\n",
    "      valid_error = root_mean_squred(y_test - eval_in_batches(X_test, batch_size))\n",
    "      valid_hists.append([step, valid_error])\n",
    "      print(\"Step %6d (epoch %7.2f), loss: %.6f, time: %6.3f s, error: %.6f\" % (\n",
    "        step, float(step) * batch_size / len(X_train), error, elapsed_time, valid_error))\n",
    "    sys.stdout.flush()\n",
    "  \n",
    "  # Save the trained model every 1000 steps.\n",
    "  if step % save_frequency == 0:\n",
    "    print(\"\")\n",
    "    print(\"Time since beginning  : %.3f s\" % (time.time() - tstart))\n",
    "    print(\"\")\n",
    "    saver.save(sess, save_path=chk_file, global_step=global_step)\n",
    "\n",
    "# Close the writer\n",
    "writer.close()\n",
    "\n",
    "# Finally the training is completed. Now let me see if this MBE model can\n",
    "# really estimate DFT energies.\n",
    "print(\"\")\n",
    "print(\"-> Test error         : %.6f\" % root_mean_squred(y_test - eval_in_batches(X_test, batch_size)))\n",
    "print(\"\")\n",
    "\n",
    "# Do not forget to save the model one last time!\n",
    "saver.save(sess, save_path=chk_file, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.3 Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this section we will display some figures to analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7faf70e764a8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtoAAAGrCAYAAAD+eDDKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBtoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faf556b6b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the smoothed validation errors\n",
    "xy = np.asarray(valid_hists)\n",
    "size = min(10, len(valid_hists))\n",
    "x, y = xy[:, 0], smooth(xy[:, 1], size)[:len(xy)]\n",
    "fig, ax = plt.subplots(1, 1, figsize=[12, 7])\n",
    "ax.plot(x, y, \"r-\")\n",
    "ax.set_ylabel(\"Validation RMSE\", fontsize=12)\n",
    "ax.set_xlabel(\"Steps\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Transform the estimates and targets back to energies\n",
    "y_train_est = eval_in_batches(X_train)\n",
    "y_test_est = eval_in_batches(X_test)\n",
    "energies_train = scaler.inverse_transform(y_train)\n",
    "energies_train_pred = scaler.inverse_transform(y_train_est)\n",
    "energies_test = scaler.inverse_transform(y_test)\n",
    "energies_test_pred = scaler.inverse_transform(y_test_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8EAAAH2CAYAAABQqI48AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3XdgFHX+//Fnkk0PKYQAIVIiJdQUsMCBAhZERRQpgoqi\ngHKKeKDAV+/uhyceHtUCJ0oRFREsKIIHYkERUMGThBAIRCC0UFJISCF95/dHZI9IepuQfT3+YmY+\nM/veN9l973t29jMOhmEYiIiIiIiIiNgBR7MDEBEREREREakraoJFRERERETEbqgJFhEREREREbuh\nJlhERERERETshppgERERERERsRtqgkVERERERMRuqAkWERERERERu2ExOwCRK0l0dDT//Oc/sVgs\nNGvWjNmzZ+Ps7Gx2WCIiInZDtVhEqsvBMAzD7CBErhSJiYl4e3vj5ubG/Pnz6dKlCwMHDjQ7LBER\nEbuhWiwi1aVvguuxgQMHEhAQAMCRI0cwDIO2bdsCkJSUxJdfflnpY44fP55x48Zx/fXX18r4itqx\nYwdz5szhwIEDXHvttRiGQU5ODgMHDmTMmDH19oxu06ZNbf92dnbG0bFyvyjIzc1lxowZHDlyhIKC\nAqZMmUKfPn1KHZ+cnMyLL75IcnIyGRkZjBkzhqFDh5a7DaB3795cffXVtuVz587RsmVL3nzzzTJj\n/Prrr1m0aBEHDhxg3rx53HXXXcW2Z2Zm0rdvX7y9vRkyZAiTJk0qtn39+vW8+OKL/Pjjj7i4uACX\n/387ODgU22flypVlxvRHlc1jWePLO1Z5ea4NM2fO5PPPP+f555/n3nvvrfT2mlaX+a7sY4mYqTbq\ndGlUj/+nurUYKvdeU1P19ujRo9x55528/fbbFfp/rE49LqkWg7n1uKyx+fn5rFq1im+//RbDMMjP\nz+cvf/kLvXr1ssX1zTffYBgGqampjBgxgtGjR1cq1qooq97WdS2Gmst3RY8VHR3NM888w5///Oc6\ne451xpB668EHH7T9e/r06cYzzzxT4rbKyMjIMKxWa62Nr4yff/7Z6NChg5Gfn28YhmGcO3fOePTR\nR43x48cbhYWFFTrGgw8+aKxdu7ZG4klLSzM6dOhghIeHG6GhoUa/fv2Mjz76qMSxJ0+eNEaMGGHk\n5eVV6jHmzp1rTJs2zTAMwzhy5Ihx7bXXGklJSaWOHz16tLFw4ULDMAwjISHBiIiIMCIjI8vdZhiG\nMWPGjGLHmjlzpvHpp59WKM6ff/7ZCA0NNYYPH37ZtpUrVxqhoaHGggULStx30qRJRlhYmPHtt99e\ndsxL/78vqsrfcmXzWNb48o5VXp6roiJ/t+WNqcm//fLUZb4r+1giZqqNOn1x3z++vutzPa7p96OK\n1uOq1mLDqNx7TU3V28mTJxthYWHGzz//XOE4q1qPS6vFF49pRj0ua+yJEyeM/v37G+np6YZhGMb2\n7duN8PBw48yZM4ZhGMZdd91lJCcnG4ZhGPHx8UanTp2MPXv2VDreP6puPa7LWmwYNZfvimz/6quv\njClTphhDhgyp0+dYVzQxVj02ZcqUKm0ri5eX12Vn/WpyfHX4+fnxr3/9i507d7J+/fo6ecxLxcbG\n4ufnR2RkJHv27GHKlCnMmDGDc+fOFRuXmZnJtGnTePnllyt1htxqtfLxxx8zbNgwAIKDg+nUqVOp\nz/XMmTPs3LnTdra5RYsW9O7dm08++aTMbRe98MILtn8XFhby3Xffceutt1Y43jvuuIOYmBiio6Nt\n6wzDYMeOHXTr1q3EfTIyMnBycqJ///5s2rSpQo8zderUCscElc9jWePLO1ZF8tzQ1WW+K/tYImar\njTpdGtXj4vW4qrUYKvdeU1P1Njo6Gg8PDxo3blypWKHy9bgqtRhqtx6XN9bT05NJkybRqFEjoOjb\ndRcXF3bv3g3AnDlz8Pf3B6BNmzZ4e3tz8uTJSsV7pavJfFfkWN26dWP+/Pl4enrW9lMzhS6Hrsci\nIiLK3Pbhhx/y1ltvERYWhqenJ3v37sXb25vrr7+eXbt2AeDu7s6LL75Is2bNWLZsGStWrGDkyJE8\n9dRTxfZv1KgRe/fupUmTJixatAhXV9dKjwfYu3cvM2bMwMXFhZCQEOLi4khNTWXq1KncfPPN5T7n\ngIAA+vTpw5dffsk999wDwKJFi0p8PvPnzyc2NpakpCQ+++wzxo4dS79+/UodX54DBw7QpUsX2/J1\n111HYWEh6enptqJVUFDA5MmTmThxYrFLnyrixIkTpKWlFduvXbt2xMTElDj+7NmzQNGHkYv8/f3Z\nu3dvmdtK8uOPPxIeHo6Xl1eF423RogU333wz7733HvPmzQNg+/bt9O7dm82bN5e4z7fffsuAAQOw\nWCxMmzaNvLy8YpdhXWrnzp3s2rWLp556qsIxQeXzWNb48o5V2Txfymq18o9//IO4uDgcHR1p06YN\nf/3rX1m8eHGJf7cXXzuurq5069YN4w/TNZS1/fjx47zwwgvk5eVhtVp59tln6d69O3l5eQwdOpRj\nx45x55138vLLL7NixQrefPNNBg8ezF//+tdyn0dd5ruyjyVitvLqNJT++qzMe8ShQ4ds9bhp06a1\nXovh8npcV7UYyq/H1anFULn3mpqqtwsXLmTmzJn8+OOPlY63svW4MrUY6qYelzfWz8/P9rkPsF0S\nffHzV8eOHW3bNm/ejIeHR4V/KlOT9bi8Wl3S671r167VrsUVyWFlxlbkWM2bN69QXFcqNcFXsPvu\nu4/ExETWrFnDhg0b8PX1Zf78+fj4+PDuu+/i4ODAp59+yrx585g7dy7jxo3j0KFDl+3/0Ucf8cUX\nX9CoUSMGDx7M119/zaBBgyo9Pi8vj4kTJzJ16lQGDRpEbGwsQ4cO5aWXXqpw0QUICgpi+/bttuXS\nns8zzzxDVFQUQ4YMKfY7hdLGl2f//v22opuenm6bbKN169a2MV988QXR0dG88cYbvPHGG4waNYo7\n7riDf/7znxw4cKDE43bs2JG//vWvJCcnA9jOcl7896U5vtTFN5+zZ8/aYkhOTiY9Pb3MbSXZsGED\ngwcPLjcHfzR69GgeffRRpk+fTkBAAOvWrePFF18stQnetm0bs2bNwsHBAScnJ3744QduueWWYmPG\njBmDg4MD6enpl22rjTyWNb68Y1U2z5fatm0bCQkJrF69GoAnn3ySc+fOlfh3W9JrZ/Xq1RXaXlBQ\nwOOPP87YsWMZNmwYBw4c4OGHH+bbb7/Fy8uL9957j759+/L0008DRf+n//3vf/nrX/9a7/Jd2ccS\nqe/Ken3++uuvFX6P6Nevn+11UFe1GIrX47qqxVB+PS6tFkPN15GaqLdbt26lXbt21WoqKlOPK1KL\noW7rcWXf33ft2kVQUBDXXnutbd3BgweZPHky2dnZvPrqq3h7e5e47x/VVD0ur1aX9XovqxZD3edb\n9VZNcIMQHh5uO1M2depUvvnmGx566CGsViuZmZnk5+eXuX9YWBg+Pj4AtG/fvtzLS0obHxUVRUpK\nCrfffjsAnTp1sk0QUhlWq7XYcmBgYKWeT2XHXxQbG8tXX33F+++/T1ZWFn369GHZsmXFLj+75557\nip2pvKiiZ/FK8seziBc1a9aMP/3pT7z77rv8/e9/58iRI/z0008EBgaWue2PsrOziYyM5OWXX650\nbNdddx1t27ZlzZo13H333QQEBJR6WUx6ejqenp62byIGDBjAxo0bLyus77zzDhaLxXbm+VK1kceq\njL+4rTJ5/iNvb2/i4uLYsWMHvXr1YsGCBaVeslfSa6dNmzYV2r5nzx5OnDjB3XffDRQVy2bNmvH9\n998zaNAg/Pz86NOnD59//jmPP/4427Zt44YbbgDqX75r6rFE6ouyXp9BQUEVfo8oSW3XYihej+uq\nFkP59bi0Wgw1/75W3XprtVpZtmwZCxcurHJcUPF6XNFaDPWjHpc0Njc3l1deeYWXX3652KRnISEh\nbNy4kf379zN+/HgWL15MaGhouY9RU/W4vFpdXj0urRaDufmu6rGudPpNcANw6Vmco0eP8pe//IWp\nU6eyatUqnn/+eXJycsrc/9JLdlxdXcstVKWNT0pKwtvbGycnJ9t2X1/fSj0XgISEBFq1alWl51OV\n5w9FZ/6OHDnC+vXr2b17N6+//jp79uyp0VkxL/6WJSMjw7YuIyPDtr4kCxYsIDs7m1GjRrFo0SKG\nDx9OixYtyt12qW+//ZZ+/foV+3+pjAcffJAPP/yQd999l1GjRpU67ptvviEyMpLRo0czevRo9u7d\ny3fffVdq/q+//vpKX3oFlc9jWeMrcqyK5vmPIiIimDlzJkuXLqV///4sX7681OJS3munrO0XL9V7\n9NFHbbnPy8sr9pzuuece1q1bB8CmTZts35hURF3muyqvEZH6rKzXZ2XeI0pS27UY/leP66oWQ/2s\nx9Wptxs2bKBPnz5V/j+4VEXqcWVrMdRNPa7oWMMw+Pvf/87DDz9M165dS3zczp0707dvX957770K\nxVlT9bi811Z59bg6tRhqNt+qt/omuMHZv38/np6etjNjBQUFdfbYAQEBpKenU1BQgMVS9KeVlpZW\nqWMkJiayY8cO/vGPfwCVfz5Vff5xcXG4uLjQsmVLAG677TYWLVrE5s2bbZMGlKUil7G0atUKX19f\n4uPjbW8yhw4dom/fvqUe18/Pr9gZ5eeff56ePXuWu+1SGzZsqFJxu2jw4MHMmzePhISEYpeG/9H2\n7dv59NNPbR9U8vLy6NWrF99//32Z92/ctWsX1113HVA7eSxrfEWOVdE8/1FGRgbXXXcdffv25fjx\n44wbN45mzZqVeHul8l47ZW1v3rw5zs7OxW5rceHChWJnz/v168ff//5322WNFy8hq2/5rsprRKQ+\nK+v1WZn3iMqoiVoMxetxXdViqJ/1uDr19tdffyUuLs72/puUlMSsWbNo0aIFixcvLvf5XKoi9biq\ntRhqtx5XdOy//vUvQkNDuf3228nLyyM5ORl3d3d27drFbbfdZhvn4eHB+fPny3w+F9VUPS7vtVVe\nPS6tFkPd51v1Vk1wg9O6dWvS09OJj48nODiYbdu21dljh4eH4+/vz8aNGxk8eDCxsbEcPXq0wvun\npaXx3HPPcd1119kuJSnv+Xh6epKdnc3Ro0f58MMPGTRoUJWef2xsLO3bty926XPfvn3ZsmVLhYpu\nRS5jcXR0ZPjw4axdu5ZrrrmGo0eP2u79d9E///lPwsLCGDRoEFB0eftzzz1H48aNOXToEL/++ivP\nPfdcudsuOnfuHKdPny71bGpFuLq6MmvWLIKCgkodc/78eZycnIqdqXdxcaFv375s2rSpzMK7cOFC\nW8GojTyWNb4ix6pInkvy9ddfk56ezpgxY2jVqhXNmjWzXVr4x7/byZMnX/baOXz4sO1YJb22Lm4P\nCwsjMDCQr776igEDBlBQUMCTTz7J9OnTbROJuLi4cPvtt/Pcc8/ZTjDVx3xX5LFEriRlvT73799f\n4feI6dOnV/gxq1uL4fJ6HBsbWye1GOpnPa5OvX3xxReLLd900008//zzVbrfc3n1uDq1GGq+Hle0\nNly0ZMkSCgsLGTJkCFlZWZw+fZpNmzYxZMgQ3njjDW688Ubc3d05d+4c33zzDePGjSs3Rqi5elxW\nLYby63Fptbim8v3HnKvelk1N8BVgzpw5bNu2DcMwmDNnDtOmTQOKzjh+9tln5ObmMm3aNObMmUOX\nLl1sP8oPCQkhICCApKQkpk2bRocOHdi2bRuurq40b94cNzc32/4ffPABTk5Otu1t2rTh7NmzlRp/\n1113sXDhQl544QVWr15Nt27dCA0NLfGWDhdv1g5FEzMYhkF2djYDBw7kkUcesZ01K+v5zJkzh6FD\nhzJv3jw+++wznn322XLHL1iwgAsXLvC3v/2tWDyxsbGEhIQUW3fDDTfw/vvvk5uba/ttTXU99dRT\nzJgxgxEjRlBQUMD8+fMJCAiwbT9x4kSxS6yaN2/O6NGj8fb2xsPDg3//+9+2y9/L2nZRWZfblJaL\ni/83GRkZuLu7M27cuGKTqUybNo3Y2FhOnjyJp6cno0aN4qGHHiIzM5Pvv/+efv36AfD9998THR3N\n2bNnGT16tO3So8mTJ1f7Nh+VzWNZ48s7Vll5Li2HUFQs//Wvf7FlyxYuXLhASEiI7eTOH/9uXVxc\nbK+dDz74gPbt2xMWFsaSJUvw8fHh5ptvLnP7m2++yT/+8Q/ee+89rFYr9957b7GZNAGGDBnC5s2b\nufHGG+t1vsvbLlIflVannZycSn19uri4VPg9YtmyZbZ6a7FYaqwWQ8XqcV3VYqif9bi69RaKbpE0\nd+5c2zfBt956KxMnTiwzF5Wpx87Oznz99ddl1uIXXniBQYMG2X6fXNv1uDK1IT4+nvnz5wMU+yZ1\n4sSJBAQE0L9/f8aMGYOzszPp6ekMHTqU+++/3zaurupxebW6vHpcnVpcXg7/mPPq1tuYmBhmz55t\nm0F7y5YtLFq0qEpx10cOhj39AlpqXVpaWrHfR9x5551Mmzat3lxeMXbsWEaMGFHskhp7pVxUn3Io\nIvWRavGVQ7moGcqjVJYmxpIaNXXqVNvN7GNiYkhKSiIsLMzkqIr88MMPuLq6MmDAALNDMZ1yUX3K\noYjUV6rFVwblomYoj1IV+iZYatQ777zD+vXr8fDwIC8vj8mTJ9OrVy+zwwKKJidwdXWt8izJDYly\nUX3KoYjUV6rFVwblomYoj1IVaoJFRERERETEbuhyaBEREREREbEbaoJFRERERETEbqgJFhERERER\nEbtht/cJTkrKqJXj+vl5kJp6oVaO3ZApb1WjvFWN8lY1ylvZAgIalT9IylQbtVl/t9WnHFaP8ld9\nymH12WsOS6vN+ia4hlksmpmuKpS3qlHeqkZ5qxrlTa5E+rutPuWwepS/6lMOq085LE5NsIiIiIiI\niNgNNcEiIiIiIiJiN9QEi4iIiIiIiN1QEywiIiIiIiJ2w/TZoRcuXMiuXbtsyxMmTKB3794cOnSI\nefPm0aNHD44fP07z5s158sknL9v///2//0d8fLxt+W9/+xshISF1EruIiIiIiIhcWUxvggFWrlx5\n2bq8vDxGjBjBTTfdhNVqpWfPngwbNoxmzZoVGxcQEMCLL75YV6GKiIiIiIjIFaxeNMGLFy/GxcWF\nwsJCRo8ejbu7O507d6Zz584AJCUl4enpibe392X7ZmVlsXjxYpycnPDw8GDkyJFYLPXiaYmIiIiI\niEg9Uyfd4tixY0lOTr5s/aRJkxg4cCBBQUF4eHiwatUqZs6cyaxZs2xjVq1axfr16/nb3/6Gu7v7\nZce46667CAkJwWKxMGfOHN56660SL5sWERERERERcTAMwzA7iIsOHz7M+PHj2bJlS7H1ubm53HPP\nPbz66qtl/t73hx9+YOnSpSVeXv1HBQWFumm0iIhIPZKUlFHjxwwIaFQrx7UnymH1KH/VpxxWn73m\nMCCgUYnrTb9uePbs2UyfPh2AY8eO0apVKwC+/PJLunTpQsuWLXF1dcXf359Tp04REhLC6dOnadq0\nKU5OTqXuX57U1Au18nzs9Q+supS3qlHeqkZ5qxrlrWylFVoRERGpX0xvgi0WCy+99BL+/v7ExcUx\nY8YMAFxdXXnllVfo2LEjycnJhISEcOONNwIwefJknn32Wa655hrS0tKYN28ebm5uxMfH89xzz5n5\ndERERERERKQeq1eXQ9el2vo2Q9+UVI3yVjXKW9Uob1WjvJVN3wRXny6Hrp+Uw+pR/qpPOaw+e81h\nabXZsY7jEBERERERETGNmmARERERERGxG2qCRURERERExG6oCRYRERERERG7oSZYRESkGqyGwf6j\n58wOQ0RERCpITbCIiEgVFRRaeXNdDPPWRJkdioiIiFzCalhL3aYmWEREpAryC6wsXhfDfw8mmR2K\niIiIXCIzL4tvjm8tdbuaYBERkUrKLyjk35/tJfK3ZAD8GrmaHJGIiIgAnM/N4PXIJXx+eFOpYyx1\nGI+IiMgVLze/kEVro9l3NBWAJj5uTB0VYXJUIiIikpqTxqKo5Zy5cLbMcWqCRUREKignr4DXP4nm\nwPE0AJr6ujN1VAT+Pm4mRyYiImLfzmWnsjBqKYnZRVdpBXkGljpWTbCIiEgFZOcW8MrHezh08jwA\nzRt7MHVUhC6FFhERMVnihWQWRS0jJafobg1XebVgQuiYUserCRYRESlHVk4+Cz7cQ/zpdACCmnjy\n7KgIfDxdTI5MRETEvp3JOsuiqGWk5hadpG7dqCWPdXsYXzfvUvdREywiIlKGzOx85q+J4tjZDABa\nNvXimZHheHuoARYRETFTQuZp/h21nPN5RSepr/ZpzWPdHqaRi1eZ+6kJFhERKUV6Vh7z1kRyMikL\ngDbNGzHlvnC83J1NjkxERMS+Hc9I4I2o5WTkZwLQ3vdqxncbjaezZ7n7qgkWEREpQVpmLnNXR3I6\n5QIAbVt4M3lEOB5uKp0iIiJmij9/nMXRb5OVX1SjOzXuwNguD+Du7F6h/VXJRURE/uBceg5zV0dy\nNjUbgA5X+fD08DDcXVU2RUREzHQoLZ43o98hu6CoRnf178QjXe7HzVLxiSpVzUVERC6RfD6buasj\nSUrLAaBTaz8mDQ3F1cXJ5MhERETs24Fzv7F073vkFOYCEB7QlYc6j8TVqXLzdKgJFhER+V1i6gXm\nro4kJb2ouHYNbszEe7vh4qwGWERExEz7Uw6wNOZ98grzALimWTgPdByGSyUbYFATLCIiAsDplCzm\nrYkiNaOoAQ5r688TQ7ribFEDLCIiYqbopP28vW8V+dZ8AHo2v4ZRIfdicapaO6smWERE7F5CchZz\nV0eSnlV0drl7hwAm3N0Fi5OjyZGJiIjYt91n9/Du/g8pMAoA6NOiJ8PbD65yAwxqgkVExM6dSMxk\n3ppIMi4UnV2+rlNTxg3qrAZYRETEZLvO7Ob92I8pNAoB6HdVH+5tdydOjtW7SktNsIiI2JWY+BS2\nR58mKS0bD1cLh0+lk5NXVFx7dWnO2Ds74ejoYHKUIiIi9u2nU7/wwcG1WA0rALe06sfdbQfi6FD9\nk9RqgkVExG7ExKewdusRAHLzCjl6JgPDKNp2Q2ggDw/sqAZYRETEZD+c/ImP4tZhUFSkb29zM3cE\n31ojDTCoCRYRETuyPfo0ADl5BSSmZtsa4Ka+7jx8e0ccHdQAi4iImGnL8W2sPbTBtjz46oHc2rpf\njTXAoCZYRETsSFJaNjm5BSSm/a8BbuThjIebRQ2wiIiIyb469h2fH95kW7633SBuankDDjVco9UE\ni4iI3XCxOHE0NeP3i6vA29MFXy8Xmvq5mxqXiIiIvdsY/w3/if8KAAccGN5hMDcG/anGG2BQEywi\nInYi+nAyhxLO2xpgH08XfLxccHBwoE9ooKmxiYiI2LP1hzex+dh3ADg6ODKywxD+1OK6WmmAQU2w\niIjYgd1xSSxeF0OhtagFDgrwxMXiRICvG31CA+ka7G9yhCIiIvbHMAzW/vYF353cBoCTgxP3dxxG\nz8Aetfq4aoJFRKRB++VAIkvW77M1wMP7t+X261ubHJWIiIh9s1qtfPTb52xL+AkAi4OFhzrfR49m\nYbX+2GqCRUSkwfpp3xmWfbHfNgnWqJvbc+u1Lc0NSkRExM4VWgtZfXAtP53+LwDOjs6M6TyK8KZd\n6+Tx1QSLiEiDtD36NCs2xtp+Azx6QAf6d7/K1JhERETsXaG1kJWxH/HL2UgAXJ1ceLTLg3Rt0rHO\nYlATLCIiDc73UQm89+VBAByAMbd35IawFuYGJSIiYucKCgt4Z/9qIpP2AuDm5Mr4bg/RsXH7Oo1D\nTbCIiDQo3/56klVfxwHg4ADj7uxMr67NTY5KRETEvhUUFrAsZiV7U2IB8LC481i3h2jv17bOY1ET\nLCIiDcaXO4/z0XeHAHB0cOCxwZ25rlMzk6MSERGxb7mFeSzd+x6x54pOUns6e/Dn0EcI9jFnoko1\nwSIi0iB88eNRPv3hCABOjg5MuLsrPUICTI5KRETEvuUW5PJm9DvEpR0GoJGLF0+EPkorb/Pm6VAT\nLCIiVzTDMPh8ezzrdxwFwOLkwBNDuhHerom5gYmIiNi57IIcFu95m8PnjwLg4+LNk2GPEtTI3Hk6\n1ASLiMgVyzAM1m49wsafjwHgbHHkqaHd6Brsb3JkIiIi9u1CfjaLopZxLOMEAH6uvjwZNpZAL/N/\npqQmWERErkiGYfDhlkN89UtRcXVxduTpoaF0atPY5MhERETsW2ZeFgujlnIy8xQATdwaMzF8HAEe\n9eMqLTXBIiJyxbEaBh98HceW3QkAuLo4MXl4GB1a+pocmYiIiH1Lz01nYdQyTmWdAaCpRwATw8bh\n7+5ncmT/oyZYRESuKFbD4L0vD/DDntMAuLtamDIijLZBPiZHJiIiYt/Scs7zetQSzl5IAiDQsxkT\nw8bh61a/arSaYBERuWJYrQYrNsayI6bo7LKnm4VnRobTprm3yZGJiIjYt3PZqbwWtYTk7BQArvJq\nwRNhY/FxbWRyZJdTEywiIleEQquVZV/EsnP/WQC83J15dmQ4rZrVv+IqIiJiT5IupPB65Fucy00D\noFWjq3gybCxeLp4mR1YyNcEiIlLvFRRaeWv9Pn49WHR5lbeHM8+OiuCqAC+TIxMREbFvZ7ISWRi1\nhLTcdACu9mnNhNBH8HT2MDmy0qkJFhGRei2/wMridTFEHUoGwNfLhamjIgj0r59nl0VEROzFqcwz\nLIxaQnpeJgDtfdsyodvDuDm7mRxZ2dQEi4hIvZWXX8iiz/YSc+QcAI29XZk6KoJmfvX37LKIiIg9\nOJ6RwKKoZWTlZwHQya8947o9hJvF1eTIyqcmWERE6qXcvEJeXxtN7LFUAJr4uDFtVARNfN1NjkxE\nRMS+xZ8/zht7lnOhIBuArv6dGNf1QZydnE2OrGLUBIuISL2TnVvAa59EE3eiaIKNpn7uTBsVQWPv\n+n15lYiISEN3ODWeN6JXkFOYA0B4QFce6XI/Fscrp7W8ciIVERG7cCGngFc+juJwQtEEG4H+Hkwd\nFYGvV/1vAMFlAAAgAElEQVS/vEpERKQhO3juEG/tfYfcwjwAejQN46HO911RDTCoCRYRkXokKyef\nBR9GEX86A4CgAE+eHRmBj6eLyZGJiIjYt/0pB1my9z3yrfkA9Gzeg1Edh15xDTCoCRYRkXoi40Ie\n89dEcTyxaIbJVk29eGZkOI081ACLiIiYKTppH8tjVlFgFADQp0VP7utwD46OjiZHVjVqgkVExHTn\ns/KYtyaShKSiGSaDAxsx5b5wPN2ujAk2REREGqrIxL2s2PcBhUYhAP2u6s3QdnddsQ0wqAkWERGT\npWXmMnd1JKdTLgDQNsibycPD8XBTiRIRETHTrjO7WRn7EVbDCsCtrfpxd9vbcXBwMDmy6tEnDBER\nMc259Bzmro7kbGrRLRY6tPTl6WGhuLuqPImIiJjpx1O7+ODAWgwMAG5vcwuDrh5gclQ1Q58yRETE\nFMlp2cxZHUny+aJbLHRq7cekoaG4ujiZHJmIiIh9++HkT3wY95lt+e6rb2dAm/4mRlSz1ASLiEid\nS0y9wNzVkaSk5wLQ9erGTBzSDRdnNcD1wcKFC9m1a5dtecKECfTu3du2fPjwYYYNG8aCBQvo3//y\nD0VpaWnMnz+fli1bcvToUaZMmUKTJk3qJHYREameLce3sfbQBtvyve0GcXOrG02MqOapCRYRkTp1\nOiWLuasjScssusdgeLsm/PmerjhbrtwJNhqilStXlrg+JyeHZcuWERISUuq+CxYsoFevXtxxxx1s\n2bKF2bNnM3fu3NoKVUREashXx7bw+eEvAXDAgREh93BjUC+To6p5+sQhIiJ1JiEpk9kf/K8B7hES\nwBND1ADXR4sXL2b58uUsWbKE7Oxs2/pXXnmFJ554Amfn0mfu3rp1KxEREQB0796drVu31nq8IiJS\nPf858pWtAXZ0cOT+jkMbZAMM+iZYRETqyPGzGcxbE0Vmdj4A13Vqyvi7OuN0Bd9i4Uo2duxYkpOT\nL1s/adIkBg4cSFBQEB4eHqxatYqZM2cya9Ys1q1bR48ePWjZsmWZx05JScHT0xMALy8vzp8/T0FB\nARZL2R87/Pw8sFhq/pL4gIBGNX5Me6McVo/yV33KYfWVlkPDMPggeh0bj34DgJODI09c9xA3tLm+\nLsOrU2qCRUSk1h09k878NVFk5RQA8KeuzXn0jk44Ol7Zt1i4ki1fvrxC43r27Gkbu3PnToKDg1my\nZAmnTp1i8+bN5OfnM2BA8dlC/f39ycrKwtvbm8zMTHx8fMptgAFSUy9U/omUIyCgEUlJGTV+XHui\nHFaP8ld9ymH1lZZDwzBY+9sGvju5HQCLg4WHu4yko2fnBpHz0hp/NcEiIlKrDiecZ8FHe8jOLWqA\nbwwL5KGBHXG8wu8x2JDNnj2b6dOnA3Ds2DFatWoFwMsvv2wbs23bNm677TbbxFiJiYn4+Pjg6upK\n3759iYyMJDAwkN27d9O3b9+6fxIiIlImwzBYffBTdpzaCYCzozOPdrmf0IAuJkdW+9QEi4hIrYk7\nkcYrH+8hN68QgP7dg3jg1g5qgOs5i8XCSy+9hL+/P3FxccyYMaPY9hUrVpCQkMDGjRvx8fGhe/fu\nzJo1i1tuuYVBgwYxZcoU5s2bx9GjRzlx4oStoRYRkfrBalhZFfsxP5/5FQAXR2fGdxtNZ/+OJkdW\nNxwMwzDMDsIMtfX1vi7XqBrlrWqUt6pR3qqmsnmLPZbKa5/sIS/fCsCAa1ty303tcGigDbB+r1Z9\ntfG61Ou9+pTD6lH+qk85rL5Lc2g1rLyzbw2/JkYB4OrkyuOhYwjxa2tmiLVCl0OLiEidiYlPYeHa\nveQXFDXAt/dsxbC+bRtsAywiInIlKLQW8va+VUQlxQDgbnHjz6GP0ta3jbmB1TE1wSIiUqP2HErm\n35/tpaCw6EKjwb3bcHefYDXAIiIiJiqwFrB073vEpBwAwNPZgydCx9LGp+wZ/xsiNcEiIlJjdscl\nsXhdDIXWogb43huvZtCf2pgblIiIiJ3LK8xn8Z4VHEj9DYBGzl5MDB/HVY1amByZOdQEi4hIjdgV\ne5Yl6/dj/X2qiRH92zHw+lYmRyUiImLf8grzmLV1qa0B9nHx5qmI8QR6NjM5MvOoCRYRkWr7KeYM\ny/6zn4tTLd5/S3tuucb+Lq8SERGpT3Lyc1i0Zznx6ccA8HP1ZVLEeJp6BJgcmbnUBIuISLVs23OK\ndzYd4OKtBh4aGEK/8CBTYxIREbF3F/IvsDBqGcczTgLg79aYSRHjaeLub3Jk5lMTLCIiVfZdZAIr\nNx8EwAEYc0dHbgi1z98XiYiI1BeZeVm8HrWEhMzTAAR6NeXJbuPwc/c1ObL6wfQmeOHChezatcu2\nPGHCBHr37m1bPnz4MMOGDWPBggX079//sv3T0tKYP38+LVu25OjRo0yZMoUmTZrUSewiIvbs6/+e\nYPU3Rb8vcnCAcYM606tLc5OjEhERsW/puRm8FvUWZ7ISAQj0bMYLN02mINPR5MjqD9ObYICVK1eW\nuD4nJ4dly5YREhJS6r4LFiygV69e3HHHHWzZsoXZs2czd+7c2gpVRESATTuP8fF3hwFwcnTgscFd\nuLZjU5OjEhERsW9pOed5LfItErOTAQjyCmRS+Hj83H1IyswwObr6o16cDli8eDHLly9nyZIlZGdn\n29a/8sorPPHEEzg7O5e679atW4mIiACge/fubN26tdbjFRGxZxt2xBdrgJ+4p6saYBEREZOlZKey\nYPdiWwPcutFVPB3xOF4uXiZHVv/UyTfBY8eOJTk5+bL1kyZNYuDAgQQFBeHh4cGqVauYOXMms2bN\nYt26dfTo0YOWLcueXTQlJQVPT08AvLy8OH/+PAUFBVgsZT81Pz8PLBanqj+pMgQENKqV4zZ0ylvV\nKG9Vo7xVnmEYbP71JJ9tiwfA2eLI82Ou45pO9nuLBRERkfog6UIyr0UuITU3DYBg79Y8GfYo7s7u\nJkdWP9VJE7x8+fIKjevZs6dt7M6dOwkODmbJkiWcOnWKzZs3k5+fz4ABA4rt4+/vT1ZWFt7e3mRm\nZuLj41NuAwyQmnqh8k+kAgICGpGUpEsNKkt5qxrlrWqUt8ozDIONu06w9rtDALhYHHlqaCitm3go\nl7/TiRURETHDmaxEXo9cwvm8dADa+17NhNBHcLO4mhxZ/WX6b4Jnz57N9OnTATh27BitWrUC4OWX\nX7aN2bZtG7fddpttYqzExER8fHxwdXWlb9++REZGEhgYyO7du+nbt2/dPwkRkQbMMAzWfHuIr/97\nAgBXZyeeHhZKx9Z+JkcmIiJi305lnuH1yCVk5GcC0LFxex7vNgYXp9J/Tir1oAm2WCy89NJL+Pv7\nExcXx4wZM4ptX7FiBQkJCWzcuBEfHx+6d+/OrFmzuOWWWxg0aBBTpkxh3rx5HD16lBMnTtgaahER\nqT6rYbDqqzi+i0wAwM3Fickjwmh/lW6xICIiYqYTGQksjFxKVkHRFa5d/TsxvutoLE6mt3j1noNh\nGIbZQZihti7f02WWVaO8VY3yVjXKW8VYrQbvbT7AD3uK7jHo6WbhLyPCaNvCx+TI6iddDl19tfG6\n1Ou9+pTD6lH+qk85vFz8+eP8e88ysgtyAAgP6Mojne8vtQG21xyWVpt1mkBERC5TaLXy9n8O8NO+\nM0BRA/zShN74uNXOhIIiIiJSMYdSj7A4egU5hbkAXNMsnIc7j8TRoV7c+KfKYlPi+On0LyRnn6OJ\ne2N6BV5LJ/8OtfJYaoJFRKSYgkIry77Yz67YRAC83J15dmQ47Vr62uVZZBERkfri4LlDvBn9DnnW\nPAB6Bl7DAx2HNYgGeP2RTbblpOxk23JtNMJXdrZERKRGFRRaefPzfbYG2NvThen3R9CqmS71FRER\nMdP+5IMsjn7b1gD3aXE9D3YcfsU3wAA/nf6lUuurS98Ei4gIAPkFhbzxWQx7DqcA4OvlwtRREQT6\ne5ocmYiIiH3bk7SPt2Pep8AoBKB/yz4MbXcXDg4OJkdWM5Kzz5W8Pqfk9dV15Z82EBGRasvLL2Th\n2r22Btjf25X/e6C7GmARERGT7T4bzbKYlbYGeEDr/gxrP7jBNMAATdwbl7zereT11aUmWETEzuXm\nFfLaJ9HExBedbW3i48b0B7rT1M/D5MhERETs284zu3l73yqshhWAO4Nv5e62t5scVc3rFXhtpdZX\nly6HFhGxY9m5Bbz28R7iTp4HoJmfO1NHRdDY283kyEREROzbjoRdrD64FoOiO9re0/Z2bm3d3+So\nasfFya9+Ov0LyTnnaOKm2aFFRKQWXMgp4JWPozickA5AoL8HU0dF4OvlanJkIiIi9m3riR189Nvn\ntuVh7QfTv2UfEyOqfZ38O9Ra0/tHaoJFROxQZnY+Cz6M4uiZolseXRXgybMjI/D2dDE5MhEREfv2\nzfEf+OzQFwA44MB9He7hhqt6mRxVw6ImWETEzmRcyGP+miiOJ2YC0KqZF8+OjMDL3dnkyEREROzb\nl0e/ZcORzUBRA/xgpxH0DOxhclQNj5pgERE7cj4rj3mrI0lIzgIgONCbKfeF4emmBlhERMRMXxzZ\nzKaj3wLg6ODIw51Hck2zcJOjapjUBIuI2InUjFzmrYnkdMoFANoF+TB5RBjurioFIiIiZjEMg3WH\nN/LN8a0AODk48UiX+4lo2s3kyBouffIREbED59JzmLM6ksTUbABCWvry9PBQ3FxUBkRERMxiGAYf\nx33O1oQfAbA4WhjfdTRdm3QyObKGTZ9+REQauKS0bOaujiT5fA4Andv48dTQUFydnUyOTERExH5Z\nDStrDn7KjlO7AHBxdOHx0Ifp2Li9yZE1fGqCRUQasLOpF5i7OpJz6bkAdLvan4n3dsXZogZYRETE\nLFbDysrYj9h1ZjcArk4u/Dn0Udr7XW1yZPZBTbCISAN1OiWLOasjOZ+ZB0BE+yZMuLsrzhZHkyMT\nERGxX4XWQt7Zv4bdiXsAcHdy48mwsQT7tjY5MvuhJlhEpAE6mZTJvNWRpF/IB+CakAAeG9wFi5Ma\nYBEREbMUWgtZFvM+0cn7APC0eDAxfBytvK8yOTL7oiZYRKSBOX42g3lrosjMLmqAr+/cjHGDOuHk\nqAZYRETELPnWApbufY99KQcAaOTsxcTwcVzVqIXJkdkfNcEiIg1I/Ol0FnwYRVZOAQC9uzbnkTs6\n4ejoYHJkIiIi9iuvMJ83o1dwMPUQAN4ujXg64nGaezY1OTL7pCZYRKSBOJxwngUfRZGdWwjAjWEt\neGhgCI4OaoBFRETMkluYxxtRyzl0Ph4AP1dfJkU8RlOPJiZHZr/UBIuINAAHj6fy6ifR5OYVNcA3\ndQ/i/ls7qAEWERExUXZBDouilnI0/QQA/m6NeTriMfzdG5scmX1TEywiUk/ExKewPfo0SWnZBPi6\n0yc0kK7B/uXut//oOV5fG01evhWAAde25L6b2uGgBlhERMQ0F/Iv8HrkUk5kJgAQ4O7P0xGP4+fm\na3JkoiZYRKQeiIlPYe3WI7bls6nZtuWyGuGYIyks/HQv+QVFDfCdvVpz741XqwEWERExUWZeFq9F\nvsWprDMANPdoytMRj+Pt2sjkyARAU4WKiNQD26NPV2o9QNRvyby+NtrWAN/dJ1gNsIiIiMnS8zJY\nsHuxrQFu4dmcyd3/rAa4HtE3wSIi9UBSWnYp63NKXP/rwUTe/HwfhVYDgKF9r+bOXm1qKzwRERGp\ngLTc87y6+02SslMAaNXoKp4KH4eHs4fJkcml9E2wiEg9EODrXsp6t8vW7dx/lsXr/tcAj+jfTg2w\niIiIyVKyU5n/6xu2BjjYuxVPRzymBrgeUhMsIlIP9AkNrND6HXtPs2TDPqxGUQP8wK0dGHh9q1qP\nT0REREqXdCGFBbv/zbmcVADa+QYzKeIx3CyXn8wW8+lyaBGReuDi5FdFs0PnEODrdtns0D/sOcW7\nmw5gAA7A6IEh9AsPMidgERERAeBMViKvRb5Fel4GAB392jMhdAzOTs4mRyalURMsIlJPdA32L3Um\n6O92n2TlV3FAUQP8yB2dSv32WEREROpGQsZpXo9aQmZ+FgBd/DvyWLeHsDiqzarP9L8jIlLPff3L\nCVZ/+xsAjg4OjBvUiZ5dmpsclYiIiH07nn6ShVFLuVBQNLllWEBXxnZ5ACdHJ5Mjk/KoCRYRqcc2\n/XyMj78/DICTowOPD+7CNR2bmhyViIiIfYtPO8aiPcvJKSy6i0OPpuGM6TISRwdNuXQlUBMsIlJP\nbdgRz2fb4oGiBviJe7oS0SHA5KhERETs22+pR1gc/Ta5hXkA9Ay8hgc6DlMDfAVREywiUs8YhsFn\n2+L54sejAFicHJl4bzdC25b8e2ERERGpG7Hn4ngr+l3yrfkA3BDUk/s6DMHBwcHkyKQy1ASLiNQj\nhmHw8feH+XLncQBcLI48NTSULsGNTY5MRETEvsUkx7I0ZiUF1gIA+re8gaHtBqkBvgKpCRYRqScM\nw2D1t7/xzX9PAuDq7MRfhocS0srP5MhERETs256kGJbHrKLQKARgQOv+DL56oBrgK5SaYBGResBq\nGLz/VRzfRyYA4ObixOQRYbS/ytfkyEREROzbr2ejeGf/GqyGFYA7g2/ljuBbTY5KqkNNsIiIyaxW\ng3e+PMD26NMAuLtaeOa+cK5u4W1yZCIiIvZt5+lfWRn7EQYGAPe0vYNbW/czNyipNjXBIiImKrRa\nefs/sfy07ywAnm4Wnh0ZQevmjUyOTERExL5tT9jJmoNrf29/YXj7wfRr2cfUmKRmqAkWETFJQaGV\npRv288uBRAAaeTjz7MgIWjb1MjkyERER+/b9ie18/Nt62/KokKH0CbrexIikJqkJFhGpRTHxKWyP\nPk1SWjYBvu70CQ2ka7A/BYVW3vx8H7vjkgDw8XTh2VERBDXxNDliERER+/b1se9Zd3gjAI448ECn\n4fQMvMbkqKQmqQkWEaklMfEprN16xLZ8NjWbtVuPUFBo5fvIU0QfTgHAr5ErU0dF0Lyxh1mhioiI\nCLAx/hv+E/8VAI4OjozpPIoezcJMjkpqmppgEZFacnGiq0tZDYN3Nh0kPSsPAH9vN6beH0FTX/e6\nDk9ERER+ZxgG6498yVfHvgPAycGJR7s8QHjTriZHJrVBTbCISC1JSssutmy1GiSmZZObV3SPwQBf\nN6aOiqCJjxpgERERsxiGwaeHvmDLiW0AWBwsjO82mq5NOpkcmdQWR7MDEBFpqAIu+XbXajVITP1f\nA9yssQf/90APNcAiIiImshpWPoxbZ2uAXRydeSLsETXADZyaYBGRWtInNBAoaoDPpl4gN7+oAfb3\nduX/7o/Ar5GrmeGJiIjYNath5YMDn7At4ScAXJ1ceDJsLCGN25scmdQ2XQ4tIlJLugb7k51bwLtf\nHiQv3wpAEx83/vbQNXh7upgcnYiIiP2yGlbe2/8hv5yNBMDNyY2J4WMJ9mltcmRSF9QEi4jUkvQL\neWzYcYwLOQUAtG7WiGdGhuPl7mxyZCIiIvar0FrIin0fEJm0FwAPizuTwh+jpXeQyZFJXVETLCJS\nC85n5jJ3TRSnkrMACA705pn7wvBwUwMsIiJilnxrAcv3rmRvSiwAXs6ePB3xGC28Ak2OTOqSmmAR\nkRqWmpHLnNWRnD13AYB2V/kweXgY7q56yxURETFLfmE+b+19l9hzcQB4uzTi6YjHae7Z1OTIpK7p\nE5mISA1KOZ/D3NWRJP5+e6SQlr48PTwUNxe93YqIiJgltzCPN/esIC7tMAC+rj5M7j6BJu7+Jkcm\nZtCnMhGRGpKUls2cDyJJSc8BoEsbPyYODcXV2cnkyEREROxXTkEO/97zNkfOHwWgsZsfk7tPoLGb\nn7mBiWnUBIuI/C4mPoXt0adJSssmwNedPqGBdA2u2Bnis+cuMGd1JKkZuQCEtvXnySFdcbaoARYR\nETHLhfxsFkUt5VjGSQAC3Jvwl+6P4+vqY3JkYiY1wSIiFDXAa7cesS2fTc22LZfXCJ9KzmLumkjO\nZ+YBENG+CX++pysWJ92KXURExCyZ+VksjFzKycxTADT3aMpfuk+gkYuXyZGJ2fQJTUQE2B59ulLr\nLzqZmMnsD3bbGuBrOzZVAywiImKyjLxMXt39pq0BDvIKZHKPP6sBFkDfBIuIAEW/5y15fU6p+xw7\nk8H8D6PIzM4HoFeXZjx6ZyecHNUAi4iImCUt9zyv7X6LxOxkAFo1CuKp8MfwcHY3OTKpL/RJTUQE\nCPAtuTAG+LqVuD7+dDpzV0faGuA+3QIZe2dnNcAiIiImOpeTyiu/LrY1wMHerXk64nE1wFKMPq2J\niAB9QgMrvP7QyfPMXR3JhdwCAPqFt2DMHR1xdHSo1RhFRESkdMnZ51jw62KSc84B0N73ap6KGI+b\npeQT2mK/dDm0iAj/m/yqaHboHAJ83UqcHfrg8VRe/Tia3PxCAG7ucRX339IeBwc1wCIiImY5eyGJ\n13a/yfm8DAA6+rXn8dAxuDg5mxyZ1EdqgkVEftc12L/MmaD3Hz3H659Ek1dgBWDgda0Y3r+tGmBp\ncBYuXMiuXbtsyxMmTKB379625cOHDzNs2DAWLFhA//79K72/iEhNOpV5htci3yIzPwuALv4dGd/t\nIZwd1epIyfSXISJSAXuPpLDo073k/94AD/pTa4bccLUaYGmwVq5cWeL6nJwcli1bRkhISJX2FxGp\nSScyTrEwcglZBRcACA/oyqNdHsDJ0cnkyKQ+UxMsIlKOqN+SeWPdXgoKDQDuuSGYwb2DTY5KpHYt\nXrwYFxcXCgsLGT16NO7uRZPKvPLKKzzxxBM8//zzVdpfRKSmHEs/wcKopWQXFN3J4Zpm4TzU6T41\nwFIuNcEiImX474FE3lq/j0JrUQM8rF9b7ujZ2uSoRKpv7NixJCcnX7Z+0qRJDBw4kKCgIDw8PFi1\nahUzZ85k1qxZrFu3jh49etCyZcsyj13a/uXx8/PAYqn5D68BAY1q/Jj2RjmsHuWv+v6Yw4PJh1kY\ntYTsglwA+rXpxYTrHsTRQfP+lkZ/h/+jJlhEpBQ/7z/Dsg2xWI2iBnjkTe0YcF0rk6MSqRnLly+v\n0LiePXvaxu7cuZPg4GCWLFnCqVOn2Lx5M/n5+QwYMKDYPu3bty9x//Kkpl6oYPQVFxDQiKSkjBo/\nrj1RDqtH+au+P+YwLvUwi/e8TZ616DaFN7ToydDgu0lJzjIrxHrPXv8OS2v81QSLiJRgx97TvL0x\nlt/7Xx4c0IGbul9V6viY+JTfZ5bOJsDXvcSZpUWuFLNnz2b69OkAHDt2jFatik7+vPzyy7Yx27Zt\n47bbbrNNjJWYmIiPjw+urq6l7i8i9iU2JY6fTv9CcvY5mrg3plfgtXTy71DtY7659x0KrEW3Kezf\n8gaGthukOTqkUtQEi4j8wQ97TvHupgMYgAPw8O0duTGsRanjY+JTWLv1iG35bGq2bVmNsFyJLBYL\nL730Ev7+/sTFxTFjxoxi21esWEFCQgIbN27Ex8eH7t27M2vWLG655RYGDRpU7v4i0vDFpsSx/sgm\n23JSdrJtuaqN8N6k/SyNWUmhUXSbwgGt+zP46oFqgKXS1ASLiFzi219PsurrOAAcHODROzrRu1tg\nmftsjz5d6no1wXIleuaZZ8rc/sgjj/DII48UW/fqq69WeH8Rafh+Ov1Lqeur0gRHJu5lxb5VFBpF\nd2m4s82t3HH1rdWKUeyXmmARkd99tes4a7YcAsDRwYHxd3Xm+s7Nyt0vKS27lPU5NRqfiIjIlSI5\n+1zJ63NKXl+W7cd28XbM+1gp+o3S3VffzoA2l9+jXKSiNH2aiAiw8edjtgbYydGBCXd3qVADDBDg\nW/KtXwJ83WosPhERkStJE/fGJa93K3l9aX469QsLf37H1gAPaz9YDbBUm5pgEbFrhmGw9It9fPL9\nYaDoEuhBf2rNNR2bVvgYfUJLvly6tPUiIiINXa/Aayu1viTbEn7m/QMfY2DgAIwMuZf+LfvUUIRi\nz0y/HHrhwoXs2rXLtjxhwgR69+5tWz58+DDDhg1jwYIFthkoK7O/iEhpDMPgrfX72BWbaFsX4OtO\n1KEU2gb5VPj3vBfHFc0OnUOAr5tmhxYREbt28Xe/P53+heScczRxq9zs0FtObGPtbxsAcMCBBzsN\np2fgNbUWr9gX05tggJUrV5a4Picnh2XLlhESElKl/UVESmMYBh99d8jWADsAAX7uuLsWvS1WdlKr\nrsH+anpFREQu0cm/Q5Umwdp8dAvrj3wJgCMOTOz5CCEeHWs6PLFj9aIJXrx4MS4uLhQWFjJ69Gjc\n3Yt+X/fKK6/wxBNP8Pzzz1dpfxGRkhiGwQff/Ma3v54Eii6BburnjpvL/94SNamViIhI3TIMg//E\nf82mo98A4OTgyKNdHqRP62tJSsowOTppSMptgvPz89mxYwf79u0jJSUFq9VKkyZNCAkJ4cYbb8TV\n1bXcBxk7dizJycmXrZ80aRIDBw4kKCgIDw8PVq1axcyZM5k1axbr1q2jR48etGzZssxjl7Z/efz8\nPLBYnModVxUBAY1q5bgNnfJWNcpb5VitBv/+ZM//GmDA4uhIVnYBTo4OeLg5A9CiiZdyWwLlpH6o\nidosIlKfGIbB54c38fXx7wGwODgxvttDdG3SydzApEFyMAzDKG3j+vXrmTVrFn5+frRu3RpPT08A\nsrKySEhI4OzZs0yZMoWRI0fWSDCHDx9m/PjxbNmyheeee47g4GAAPvzwQ6699lpuuukmBgwYUKH9\ny1NbZ5MCAhrpTFUVKG9Vo7xVjtVqsGJTLDv2ngHA0QF8vVzJzM63jfFt5Iq7q4Whfa/W5c1/oL+3\nstXVCYK6rs11qTb+vvR3W33KYfUof+UzDINPflvP9yd3AODs6MyEbmPo6N8eUA5rgr3msLTaXOo3\nwR9//DExMTFs2LCBgICAEsekpqaybNkyli1bxrhx46oU2OzZs5k+fToAx44do1WrVgC8/PLLtjHb\ntkkPziIAACAASURBVG3jtttus02MlZiYiI+PD66urqXuLyJyqUKrleVfxPLz/rMAWJwcCPB1x8XZ\nCWeLIxnZ+RQUWikotKoBlnqrrmqziEhdsRpWPjz4GdtP7QTAxdGFJ8PH0s432OTIpCErtQkOCQlh\n+PDhZe7s5+fH1KlTiY6OrnoAFgsvvfQS/v7+xMXFMWPGjGLbV6xYQUJCAhs3bsTn/7N37/Fx1nXe\n/1/XNdeccj62DaUt5VColAIKaLEIuouAyHnV5WbxVkTk/om4qAvoeuPu3j7s8vO03ohVWNAVuuq6\nrMAWpIIgWqhY5FACwQhNW9qGkEzOkzldc133H5OZTg6TTJKZZJK8nzyUzGTmmm/mEfrte77f7+dT\nXc3b3/52vva1r/GXf/mXfPCDH5z0+SIidtLhjv9+hWdfTRXBqqnwUx7wZI5EBPwWgeGCWKZhKABL\nyZqtuVlEZDY4rsO9LT/nmTf/CEDA4+e6k65mdfWqOR6ZLHQ5t0Nfdtll/OQnP8Hn8832mGaFtkOX\nFr1v06P3bXIJ2+H7DzTz/J9TdQmqK3xs+v828qP/bqajJzLm8Utrg1x70brZHua8oN+3ic3GdmjN\nzVOn39uZ03s4M3r/xpd0kvzbKz/lj2+9CECZFeQzJ3+SlZWHj3ms3sOZW6zv4ZS3Q/f09HDDDTdQ\nXl7O+9//ft7znvcs2ElXRBamhJ3k9l80s+v1EAC1lX5uvPxkViytZOP6Ju57cveY52xc3zTbwxTJ\nm+ZmEVkIbMfm7uZ/58WuZgAqvOVcf/I1LK/QHCyzI2cI/tznPscHP/hBuru7eeSRR7j++uupqanh\nnHPOYePGjXi93tkcp4jIlMQSSb573y5e3tMDQEN1gL+7/GQaa1It1NJbnrfvaqezN0pjTYCN65u0\nFVpKmuZmEZnvEskEdzbfw8uhVwGo9FXwtydfy7LyJXM8MllMJqwOPVpnZyfbtm3jqaeeora2lnPP\nPZf3vOc9xRxf0Wg7dGnR+zY9et/GF43b/N//3MWr+3oBWFIT5O8uP5n66gCg92269L5NbK7aR2lu\nnph+b2dO7+HM6P07JJ6M84Nd/8arPX8GoMZfzWdP/hRLyhomfJ7ew5lbrO/hlLdDj6euro4VK1ZQ\nVVXFL3/5Sx588EGam5sLMkARkZlqbgvx5PMHaG7rIZZIArCsroy/u/xkaivVN1UWJs3NIjIfRO0Y\n3/zj7RwMp9oU+kwvFx557qQBWKQYcobgZ599llNOOQXHcfj973/Pww8/zKOPPko4HObUU0/li1/8\nImefffZsjlVEJKfmthA/f+J1OnqGiCccALyWyUVnHKEALAuG5mYRmY8idoRvPHs7bw6lujRYhoeG\nYD2/2b+dKl8la+vXzPEIZbHJGYJvueUWTjvtNH71q1/R19fHKaecwg033MD73/9+6urqZnOMIiKT\neuK5A3R0DxG3DwXgpXVBnm/t4p1rl83x6EQKQ3OziMw34cQQt71w56EAbFosDTbgMVNtCne071QI\nllmXMwTv2bOH+vp6rrvuOs455xzq61UsRkRKU384zstt3ZkA7LNMltSV4TENOnujczw6kcLR3Cwi\n88lAfJDbXriTA4PtAHhNiyVZARigK9o9V8OTRSxnCD733HP51re+NZtjERGZUHNbaLiac4TGmiAb\n1zdxeGMFX//J84cCsNdkaW0ZpmkA0FgTmMshixSU5mYRmS/6YgP83+d/kFkBLrOC1AVqMA1zxOMa\nAtrFIrMvZwgePckePHiQrVu3EolEuOaaa3jmmWc466yzij0+EVnEskOvzzLpHYwT8Kf+2OroifCz\nx18jHEnQOxgHwO/1sKQ2mAnAoL6/srBobhaRQmsJtbKjfSddkW4agnVsaDp1xtuTe6K9fOf5O+iM\ndAGwumolZ688i0f2/nrMYzc0nTqj1xKZDnPyh8Bjjz3GJZdcwgsvvMBjjz2Gx+Nh27Zt3HHHHcUe\nn4gsQs1tIW7d8hzf/a+XaG7rJhy1eaMzTM9AjGjMBsBOOnR0D2UC8HEra7jmwrU01ZdhGgZLa4Nc\nduaR6vsrC5bmZhGZqZZQKw/u/iWdkS5cHDojXTy4+5e0hFqnfc1QpJtvPbc5E4CPrl7NdSd9khOX\nrOPCI8+jMdiAYZg0Bhu48MjzdB5Y5kReLZLuuusutm7dSmNjI1deeSU+n49NmzbxN3/zN1xzzTXF\nHqOILALpVd+9HQMMDCVIJh1wIR5P0hm1STc07+yLUlvho3cwTtJJ3Xv86jquu/QE/F4PJx+zZO5+\nCJFZpLlZRGZqR/vOnPdPJ5y+NdTJd56/g95YHwDH1R7Dp9Z/DJ/HC8Da+jUKvVIS8grBHo+HxsbG\nYo9FRBap5rYQ9z25G4CBoQS27ZBIOhiA4458rOO4hPpjmdvVFT6uv+wEvJYHkcVEc7OIzFRXZPyi\nVNMpVvVmuIPvPH8H/fEBAI6vP45PnvBRvGZecUNkVuW1Hbq2tpbvfve7tLenKrv19vbyox/9SO0Y\nRKQgtu9qz3xtJ1MFrsYLwKMF/RYfO/dYBWBZlDQ3i8hMNQTH//NiqsWqDgy28+3nvp8JwCc1ruMa\nBWApYXmF4H/8x3/kueee473vfS87d+5kw4YN/O53v+Mf/uEfijw8EVkMOnsjma8tT+qPpeziVuPx\nmAbXXPg2TjxaK2GyOGluFpGZylWUairFqvb17+dfnvs+g4kwAKcsOYmrjr8CSwFYSlhev511dXXc\nfffddHR00NHRwbJly1iyROfuRKQwGmuCdPSkgnBl0EvPQAzTMDANcNyxy8Ee0+Dtaxo48aiG2R6q\nSMnQ3CwiM5U+n7ujfSdd0W4aAlOrDr27by+3v3AX0WQUgHctO4Ur1v7VmDZIIqUmZwh+4YUXOOmk\nk0bct3TpUpYuXTrmsS+++CInnnhi4UcnIovCxvVNmTPBAb9FLTAQSVAW9BAeSpAY3iINYBpQV+Xn\njBMPm6PRiswdzc0iUmjTLVb1557dbN51N7FkqkvDxsPeyUeOvUQBWOaFnL+lr7/+Ol/5ylcIhUI5\nn9zX18c3vvENnn322aIMTkQWh3Wr67nszCNZWhvENAxWLavk2ouO539ddDxG1q5oy2Nw9PIqrjzn\nWLU+kkVJc7OIlIJXu//M7S/+ayYAv/fwjfz1sZcqAMu8kXMl+LLLLsOyLM4//3xqa2tZsWIF5eXl\nGIZBOBzm4MGDdHR0cMMNN3D55ZfP5phFZAFat7p+RLB9dW8P3/rZi8Tt1CrwX55yOJf/xTEYxsRn\nhUUWMs3NIjLXmrtauPOlH2O7SQDOXnkWFx11nuZnmVcmPBN80UUX8YEPfICnn36al19+mVAohOu6\nHHnkkXzoQx9i48aNBAKB2RqriCwg6b7Anb0RGmuCbFzflAnBL7d1c9t9uzIB+Lx3ruSvzjpKE6wI\nmptFZO682NnMXc33knRT8/P5q8/mvCP+UvOzzDuTFsbyer2ceeaZnHnmmbMxHhFZBLL7AgN09EQy\ntx3H5bv/1ZxplXTB6Udw8RmrNcGKZNHcLCKz7Y8dL/Cjl3+CQ6pg5cVHfYCzV501t4MSmSbVLheR\nWZfdFzjbfz+1h90H+0kONwi+5IzVXPDu1bM5NBERERnlmfY/ck/Lf+AOB+C/OuZC3rti4xyPSmT6\nFIJFpKjG2/ac3Rc4LRxNsLc3mrn9obOO4rx3rZrNoYqIiMgoTx14hp/86T5cwAA+cuylnLH8XXM9\nLJEZUQgWkaLJte054DWJJg61PQpHEnT1HQrAf/0Xx/D+U1fM6lhFRERkpN/sf4qftz4AgIHB36z9\nEO9qOmWORyUyc3nVMX/wwQeLPQ4RWYBybXvO7ns0ODQyAF/5/jUKwCJ50NwsIsX02L4nMwHYxOBj\nx1+uACwLRl4rwV/72tfYt28fF154IStXriz2mERkgRhv23M0ZtPVG6GizMvAUIKhqJ353sfOO473\nnHjYbA5RZN7S3CwixfLLtl+ztW0bAB7D5Kp1f8NJjevmeFQihZNXCN64cSPvfe97uffee9m/fz8b\nN27kAx/4ADU1NcUen4jMYz7Lwxudg9hJB8tj4rNMhqI2lmUSTziZAGwY8Inz13L6uqY5HrHI/KG5\nWUQKzXVd/nv3NrbtfRwAy/DwyRM+yrqGtTO6bkuolR3tO+mKdNMQrGND06msrV9TiCGLTEteIfgb\n3/gGAMcffzzJZJJf//rXXHrppaxdu5YLL7yQ9773vfh8vqIOVETmj+a2EA89vZfXD/bhOC6GYWDb\nNkPRVOA1TYOegVjm8UceVqUALDJFmptFpJBc1+W/XtvK42/8DgCvafGp9R9jbd3MwmpLqJUHd/8y\nc7sz0pW5rSAscyWvELxlyxauuOIKdu3axf33389DDz1EbW0t69ato7u7m09/+tOcffbZfPjDHy72\neEWkxIyu/nz4kgr++KdO3gwN4Tgujgu4bubxrgvReDJzu7EmgOOMc2ERmZDmZhEpFMd1+HnrA/z2\nwA4A/B4f/2v9xzmm9qgZX3tH+86c9ysEy1zJKwRv3ryZe++9l+7ubj7wgQ9wxx13cOKJJ2a+f/nl\nl3PppZdqohVZZMar/tzc1o3PMonbSXAneDLQWBOkLGDRWBMYcc3RLZXWra4v1o8gMm9pbhaRQnBc\nh5+8eh9PD4fVgMfPp0+6miOrC9OmsCvSPf790fHvF5kNeYVgy7L43Oc+x1lnnYXX6x3z/fvvv19b\nrkQWieyQ2heO4/WYBPyH/iiJJ5JEovZk+RfLY1AWSD1v4/qmzLXHa6kEKAiLjKK5WURmKukkuafl\nP9jZ8TwAQSvIZ066mlVVhevS0BCsozPSNfb+QF3BXkNkqvIKwR/60Ic4++yzc37/4osv5uKLLy7Y\noESkNI0OqYORBLhQCwT8FtGYjeO4kwZgg9S26IDX5PzTj8gE3FwtlbbvalcIFhlFc7OIzKTgVNJJ\n8sOX/53nO18CoMJbzmdO+iSHVxa2S8OGplNHnAnOvl9kruQVgh966CGWL18+7vfq6+s58cQTqaqq\nKujARKT0ZIfUSMwmmXRxHJeuvij11QEGIwlM08BJuqmgO841LI+BaRjUVPqJJkYeBh6vpVLq/ui4\n94ssZpqbRRaO6YTZmRScSjg2dzXfw0tdLQBU+Sq5/uRraCpfOu3x5JJ+3o72nXRFu2kIqDq0zL28\nQnBNTQ1///d/z1FHHUV1dTW9vb3s3buXE088kZ6eHjo7O7nttts47bTTij1eEZlD6ZAaidn0DsQw\njNT9juvS3R8lmXTBAI9pkHTGRmCPCT6vh4qgl+DwFursVd7GmiAdPWODcPaZYRFJ0dwssjBMN8xO\nt+BUPJngjpf+jZbuVgBq/FV89uRPsaSscUbjmcja+jUKvVJS8grBa9as4ZZbbuG4447L3NfS0sID\nDzzAzTffTHNzM1/96lf56U9/WrSBisjcS4fUwUgCANMwwAOOSyb0egwDwwDHOFQU2gBqq/xUlo09\nn5i9yrtxfdOI7dbZ94vISJqbRRaG6YbZ6RSciiXjfP/FH9La+zoAdYFaPnvyp2gIHjqfq2rOshjk\nFYJbW1tHTLIAa9eu5atf/SoA69atG7coh4gsLBvXN7HlV61EY6nCVwapnr9ej0nScUgmXexxVoBr\nq/wARGP2iCJaMHKVN/tscGdvlMaagKpDi+SguVlkYZhu9eR8C06ltza/NdRFd7SHsD0EQGOwns+e\n/ClqAzUFGY/IfJJXCI7FYjz88MOcd955GIaB4zg8/PDDRCKpbYs9PT0MDQ0VdaAiUhpcwDAM3OFl\nXtdNVYTOVQyrImhRWeYjErPpGYhlimiljV7lXbe6XqFXJA+am0UWhulWT86n4FR6a7PjOrw11EXc\nSe3kqvVXc8Pb/xfV/rF1A1TNWRaDvELwP/zDP/CZz3yGm2++maqqKvr7+6mvr+e2226jr6+Pm266\niQsvvLDYYxWRObZ9VztBv0V9dYDegRiO65JMTlwNOr0wnD4DnEg6lBmGVnlFZkhzs8jCMN3qyfkU\nnNrRvpOkk+StSIjEcAD2mhYrKpePG4BnMh6R+SSvEHzCCSfw6KOP8sILL9DZ2UljYyMnnXRSZpvV\nHXfcUdRBikhpSBfGMgDDANuerBkS2MlDFaCDfgvTMPjf//OUYg1RZNHQ3CyyMKRD67Y9j3MgnOrC\nsLxiirUwckzHHeFO3op0kXBsALymlyVl9fTFByYdj6o5y0KWVwh+29vexsc//nH+7u/+rtjjEZES\n09wWYvuudvZ2DNDTH8NOOoxz7Dcny2OOuK1KzyKFoblZZGGJObFMgapYMjZpRebJqjj3xvrojIYy\nAdhnellS1oBpmJNubVY1Z1no8grB73jHO8adZPv7+9WDUGQBa24Lcd+TuzMtkZKOO6UAbAAVwZGF\neVTpWaQwNDeLzH/polXpdkXl3nIA+uP9JJIJNu/6IUdWr+KcVe8bE0onquK8pKyR//v8D4glYwD4\nTR+NZfWYRuqDaW1tlsXOnPwhcNZZZ/H73/9+zP3XXXddwQckIqVj+67Utqx0SyRnCgnYAE4/YRlH\nLKvENAyW1ga57MwjdQZYpEA0N4vMb+mV3M5IF7ZjYzs2PdFeuqPdxJMJXMBxHfb2v8HPWu+nJdQ6\n4vm5qji3D73Ft5/bnKnmHLQCmIZJd7QXv+nnwiPP0yqvLHp5rQRv2bKFzs5OysrKqKioAMB1XUKh\nUFEHJyJzI70F+oXXujCAuO2Am/PI0QiGkTr7u2JJBZ84/23FHqrIoqW5WaQw0quxXZFuGoKzd/41\neyXXMi1sx8Zxkzi4GBhA6gNlx3XojvZw98tbWFu3JrOK2x8fIJwIY5kW5d4yglaAhJOgK9Kd2QId\n8PhpCNRhGKnrxZxY0X8ukfkgrxBcVVXFP//zP4+4z3VdNm3aVJRBicjcSW+BBnAdl3hyCvufAY9p\n0FgT5PwNq4oxPBEZprlZZOYmO1dbTNkrueXeMvpi/eN82GyQdFMFJm3HpjPSxc9afwEYOK6D7dgk\nHJuoHcXn8RNNRg9d0yojaAUJRXuwHXs4LJezo33njH+2ufrgQKRQ8grB//RP/8T69evH3P/Nb36z\n4AMSkbmV3gIdidkkphiA07TtWaT4NDeLzNxE52qLHeqy+/EGrVTRyO5oL0k3OfwIF2c4FhsYWGbq\nr+3hxBBJN4nrupiGieM6OLgjAnCZFSRqx4glY5lzwLZj0xfrY//wqvB0zeUHByKFkteZ4PXr13Pw\n4EHuuOMOvvOd7xCJRPjNb37DUUcdVezxicgsam4L8XJbN+2hcKYdkoiUJs3NIjOX61xt+jxtMY0u\nThW0AtQFaimzgsP3HAqrLi5eM1Vo0nZsEsnUdmfTMDMhN63MClIfqAUjtZV6tHgyMaNxT/TBgch8\nkVcIfuyxx7jkkkt44YUXeOyxx/B4PGzbtk09CEUWkK079vD9B15mKGYTTzi401gENgzweT2Z1WQR\nKR7NzSIzl25JNOb+SVoIFcLa+jVceOR5NAYbMAyTxmADH1lzMcsrmvCZ3kwENjCwDA8JJ0HEjpF0\nHRxSW6GTThI7s3IMJgb1gVoMI3WqeLyp3OvxjnNv/ubygwORQskrBN91111s3bqV733ve9TU1ODz\n+di0aRO//e1viz0+EZkFzW0hHnlmH/FEMu/wa4z62iB1Hrgi6KWzN5rjWSJSKJqbRWYuV6ug2Woh\ntLZ+DVetu4IbT/kMV627grX1a4gnEywtb+TwysNoDNbjNS1MwyTuJOiL9Q3PuQYOLkkOrfQagN/j\nzxTB8nl8+ExvZhu1ZVpU+6tYUXHYjMY8lx8ciBRKXmeCPR4PjY2NxR6LiMyCrTv28JvnDzAYSVAR\n9LJ2VS27XgsxFLXzvoblSU2wznDfYMMAr+WhusJH0G/RWBMo0uhFJE1zs8jMpc+w7mjfSVe0m4bA\n3Bd5Gu+scDgxRCwZx/J4KfeWEU5EGLKHMs/xmT5c16HCV5G5L9Vz2M1cI22mAX9D06kjzgQX6roi\nsymvEFxbW8t3v/tdLrvsMgB6e3u5//77qavTJz4i88nWHXvY+tSezO3ewRhPvfTmlK9jGgaWZVIR\n9NI7GKOpvnzE9zeub5rhSEVkMpqbRQpjbf2akiroNDpkBq0AQStA1I4TsHwM2SMDsInJuvrjOLzy\nMA4Mto8I81D4gF+KHxwUgipeLy6G606++bG7u5svfOELPP3006knGQann346X//61+ftZNvZOVCU\n6zY2Vhbt2guZ3rfpmer79oXvPcXgUKoghp10cKZX/BnLY1BXFSDotwh4Taor/HT2RmmsCbBxfVPJ\nV4bW79v06H2bWGNj5ay+nubm/Oj3dub0Hs7MeO/fZIEr8/2skLmjfSd7+98Ycfa22lfF0TWruWrd\nFbP288yFYv8Ojq54nXbhkectmCC8WP87zjU357USXFdXx913301HRwcdHR0sW7aMJUuWFHSAIlJ8\ng5GZB2BIbYNOnwk+//QjSj70iixEmptFSl9LqJUtf36eA71v0RCsY3lFEy3dreztf2O4b28ZnRFn\nTIuh8Van/9TzGn9868XM7Rp/NVW+Cm1DLoC5bJUlcyOvEJy2dOlSli5dmrl9880388///M8FH5SI\nzExzW4jtu9rp7I3QWBPMrM5WBL30DsRmFIBNI1VtciCS4N3zYNVXZKHT3CxSmtKri5blwcVh38B+\nXup6JVO4KtW3tx9IbXmeKHA9fXAnj+17MnO7LlDL6qqVmQB8d/MWbeOdAVW8XnzyCsEvvPACt912\nGwcOHCCRSK0kua5LKBTSRCtSYprbQtz35O7M7Y6eSOb2sroyuvtj0762MerfL+8O8cENR0z7eiIy\nfZqbRUpP9jbnULSbRDIBBngMD0k3ieO62E4Cg1QLI9PwEE4MEbQCOQPXb/c/zc9a7wdS8+//OO5D\nnH7YqZnXy97G2xnpGrOqPN7YJgvL+T52oZyjzS5GNuJ+VbxesPIKwV/84he57LLLOPbYY/H7/UBq\not20aVNRByciU5erR+/2Xe20tffP6Nru8P95TAPbdmh7c4DmtpBWg0XmgOZmkdKSHUgjdoxwIlW8\nymN6cF2XuJMY8XgXSLpJ4sNtfscLXL/e91v+67WtQKoA1pVv+zCnLXt75vv5buOdaljO57HjPe5n\nrfdT46si7iTmVShWxevFJ68QvGzZMq6++uox92/evLngAxKRmensjYy5r28wxhsdgzj5NgHOwTRS\nlaFNM7UWbHlMtu9qVwgWmQOam0XyNxsrltmBNJwIY5AKuo7rYBqezPeMzH6qFDf1EfOYwPXInsf5\n792PAGAaJlcdfwUnLzlhxGPy3ca7be/jdEW6sR17+CxyOUHLP+4W7HyD9ejHRewYfbE+wolwZmU1\nV9AuNQu14rXkllcIvuiii9i2bRsbN26kvPxQK5SvfvWr3H777UUbnIhMXWNNkI6eVBCOxmxC/VHs\n5MzCb1p2AAaoDHrp7I0W5NoiMjWam0XyM5VV0JnIDqS2Y2MOb4F2XZfs3OsxTCAVjl3AMqxMFeJt\nex7nt/t30Bfvz4RjA4PzVv3FmAAM+W3jbQmlCnFlj60v1gdUj7sFO99gPfpx4UQ4c/1s86W4VKm1\nypLiyisE33zzzQCZg/yQ2nKVfVtEZt9zf3qLh373+ogCWBvXN3Hvr1rpGYiRsJ2CvI5pGHgtEzvp\ngJFaAa4Megn4LRprAgV5DRGZGs3NIvmZrcq/2YHUMi3iyTiQWum1HRsTE49p4jW92I6Nz+Oj3FvG\nysrD2Tewn39tvodocmzdjqAV4KXQK6yuXjVmO3J/fIA3w29lKk0HrdScnL2qvKN9J5ZpjQmn4USY\nlZXLJ/w5Rtw/arv26Melr2+ZI+OFiktJKcorBJ944ol861vfGnGf67p8/vOfL8qgRGRyzW0hHnxq\nTybodvREuPdXrTiOQ6gvRmHWftOGP402oKm+fMR3Nq5vKugriUh+NDeL5Gf0imXEjhJODPFmuJO7\nm7dMuu01363U2edKvaaXiJ3aKWUaJo7rAC6O45AYDosJxybpJgknhnhkz6/HnBlOG7IjdAx18qNX\nfkKZVUbCSeC4LgknTrm3nGp/NeFEmL5YPzW+as454n0jxtcV6abcW5apRJ1mO/a4Z17zPR87+nHp\noF3uLRvxOBWXklKUVwj+l3/5F5qaxv5F94477ij4gEQkP6MLYHX2RhiK2jkePTOmaeC6Lh7TJODz\nEE84NNYEMgH4+w80j2nHJCLFpblZJD/ZK5YRO5oJg4Zh0NLdyktdr7CqcsWY8Aiji11FaenuSj2+\nagX1gTr+1PMaA/FBXFzM4W3OScfGyfoo2s1RjyO9QvynntcyW59ziScTxJJxonYU0zAz261tp49q\nfzUNwVTQrPJXjvkZUj9/6gPzcGIocy54eXnTuGE+3/Oxox+3vKKJ3lhfZjU6TcWlpBTlDMH3358q\nxX766aePmWQPHjzIH/7wBwAuvvjiIg5PRHLp7I3g8ZiZr4sVgI3hYlgA1RU+qst9XHvROmDidkwK\nwiKFp7lZZOqWVzTR0t2K7dgkXWe4MJULroFrpMLngXD7uOeE01ups8MzwJ7+fbzW24aBgUMqYCbd\n5JTHlj4XPJXHm4Y5XHAriWlYhBNhglaqQvx4W4/TK7ZBKzAioJ5zxPtyvk6+52NHPy6zap4jPC+U\nlkoy/+UMwT/4wQ/41Kc+BaQm1rTDDjsMSH2q9e///u+aaEXmSGNNkO6BGJGYXZQAbJqAm1oFtiyT\niqCXoN8aUQhronZMCsEihae5WWRio0PW8oomdrTvJOEkMtuQ4VCFZscls7IKqSrK2c9/Y+AgAcuX\naXeUlr7WZCu4k5nqs9OPN7K+zj7rO97W49msfDxReJ6tAmUi+cgZghsaGjKT6Be/+EV+97vfccYZ\nZ7Bp0yYOO+wwLrnkEp544olZG6iIjLRxfRMPPrWHvsF4wa9tmgY1FT4sj0nQP/KPiexCWOO16Axw\nkwAAIABJREFUY0rdr4rRIsWguVkkt/FCVnOohUTSxsXFwMiEVjcrTiZdB4/pIWLHeDP8FtX+KsKJ\nIdrDb5J0HSq85SOCZup87+wwMDANE2PU6x46ZzyyEFWurcelUPl4tgqUieQjZwjOri65adMmrrzy\nSjZt2pTzMSIyu9atrqdzIM5rD7cU7JqGAVXlPo5dUcPG9U0jtjqnZRfCym7HlE0Vo0WKY7bm5ttu\nuy2ztRrg2muv5d3vfjf79+/n6quvprGxEYDjjz8+U6U62/79+/ne977HqlWrOHDgADfddNOINk4i\nxTBeyEokEzjDAXhCbrq3rzFi27MBDMQHMIxUWJ5NPtNLtb8aYLil0SGmYVLuLSfhJKj0VdAYbCj5\nrcX5tl4SmQ15FcYCBV6RUtPcFuKh7WND6nRZpkFddYCg3xpR3Gr7rnY6e6OZQljZ25zzCcoiUjzF\nnJvvueeece+/5ppruPTSSyd87le+8hU++9nPsn79eu655x7uvPNO/vZv/7YYwxTJyBWyJmLA8Bnb\nVJEqwzBG7FE2DRMHd9YDMEBDsJ5ybxlxJ0GNvwpc6EsMkEgm8Hm8HF5xWMkH32z5tl4SmQ05Q/Dr\nr7/OjTfemLm9e/fuEbcBXnrppeKNTEQymttCbN/Vzt6OAYYiNkMxm6RTuCZIBmB6DOykw2VnHpkJ\nuutW1094tjefoCwihTObc/PmzZvx+Xwkk0muvPJKgsEgAE888QTd3d0MDAxwwQUXcPTRR494XiKR\n4JlnnuGEE04A4O1vfztf/vKXFYKl6NIhK90CyXbs1BancaozGxj4PF48hme4rU85DYE69g68geM6\nmYJVBmQqPWdvp54NvbE+Ek6Cj6y5ZN4E3Ynk23pJZDZMuB3a4/Fkbp9xxhnjPkZEiitdgTkSs+nu\nj2InCz8BV1X4qKnwYxrGlAPsZEFZRAqnkHPzJz7xCbq6xq7KXH/99Zx77rksX76csrIytmzZwv/5\nP/+Hr33ta9TV1XH99ddzzDHH0NXVxYc//GHuv/9+qqqqMs/v6ekhEAhkxlFRUUEoFMprTLW1ZViW\nZ/IHTlFjY2XBr7nYzIf38Ny17+GuP/6UvvhA6g7DwCTVRSHdpigdZD2mh9pgaqvxQCxMwOfD6/Pg\nwohV39Ez7lSDcLpt0nTOEdtukr74AI8ffJL3HPeOKT+/1DQ2voPqmiBPtO3grcEullQ08N7VGzhx\n2dvyfH7p/w6WOr2Hh+QMwRdccAE33XTThE++/fbbCz4gERkpXYF5MJLAKeDqb7aailRrBZ3lFSlt\nhZyb77rrrrwe9653vSvz2LKyMo455hggVaSroaGBV199ldNOOy3z+NraWqLRKK7rYhgGg4OD1Nfn\n90FZT8/Q5A+aosbGSjo7Bwp+3cVkvryHh3lWUGlV0m8MEncSY/rzmoaJZVrUB2opt8roSwwwGB+k\n3FuObSfZ07OfhJPIef3prAJPt4iWyaEV7D09B+bF+5+PwzwruOLoFSPuy+dnmy+/g6Vssb6HuYK/\nmesJk02yAJ/+9KenPyIRyUu6ArOddChSBiYSS1W91FlekdI2W3Pzrbfemvl67969rFy5Ekj1Kf7T\nn/4EpLY9v/nmmyxfvhyA9vZ2kskkXq+Xd77znZlt2c899xxnnnnmjMckko94MkG5txxzuBSW4zqZ\nMFwXqGVpWQOXHv1B/vYd17Ki4jAagnWAS1+sP3UmeLICWkXgN/1j7kv1AXZxXJdYMsb/v/M27m7e\nQkuoddbHV8paQq3c3bxF749MWd6FsYpFFShFcmtuC9EXjjMYSWDbxUnApsGYs8AisrhZlsVXv/pV\n6uvraW1t5Stf+QoAS5cuZfPmzaxdu5a9e/fy2c9+NhOCb7jhBr7whS9wyimn8I//+I/cfvvtbN++\nnfb29nHnb5FiaAjW0dKdCkLZq7AGqerPQcufacmTLqSV7gHsuO6snvlNjcvAY5r48JJw7BGvn3ST\nAPg8Xlwc9dUdRX2HZSbmPASDKlCKjGfrjj088sw+4okkSccdr67HjJkGNNQEKQ94FYBFJOPzn//8\nuPdv2LCBDRs2jPu9n/70p5mvDz/88DGtm0Rmw4amU3mp6xVg9Hleg+hwH+CuSA8todZMIS3bsXFc\nNxM6Z1OVr5KIndrx5TFM7KwxuLhYhocq38jtnNl9dVtCrexo30lXpJuGYN28qhY9U+o7LDORczv0\nbNq8eTN33XUXd9xxB5HIoZ6jTzzxBP/6r//Kt7/9bV577bUxzxuvAuWTTz45a+MWKZbmthCPPLMP\n2059il2MAGwAleU+gn5LZ4FFRGRBWFu/hlWVK7BMCwOG/5cuZpWeTF0e3P1LllekjgBZpoUzHD5n\nczu0xzDxebxYZmpNyjRMTMxMMS0Dg3JvGUFr5Byd7qubXgntjHSNWCleLFuC1XdYZmJWVoJLsQKl\nSCl76Ok9RGN2asouQgD2egxqq1I9gUFngUVEZH7IZ+XznCPex4O7f0nEjmbO+qY3OsedBLaTpC/W\nz4HBdi488jy27Xmc1/raUoHZMEgWY+Idh+u6eE0vyyua2Nv/BgYGpmGkQrCRGkvEjuLz+EYE4XRf\n3cW+Eqq+wzITE4bgxx57jH379nHaaaexbt06Nm/ezL333ott22zYsIFbbrmFurrJf9FKsQJlsdow\ngMqPT5fet5Tn/vQWe94cLGgRLI9pcNTh1SyrK2fVYVXsPdjPm91hltWV8xenreTtxy4p3IvNE/p9\nmx69b3OvUHOzyHyT7xnQ9Nc72nfyhnGQzqGRQcnFpT8+yGt9e7hq3RWsrV/Dv/zx+xwItxO1o7Pa\nD7grEuLSoz8IwJZX/5NwIoxlWpR7U/Vt+mJ9hBNDI0Jwuq/uYl8JVd9hmYmcIfjWW2/lxz/+MTU1\nNXz729/m85//PNu2beOTn/wkHo+HRx99lK9//eszPvNz6623Zqpdjq5AuXbtWo499thxK1AuWbJk\nRAXK9evXT6kCZTHaMMDiLT8+U3rfUmeAf/P8AXoHYgUNwJbH4MKNq/nghiMO3Tlq5Xexvff6fZse\nvW8Tm40PCGZrbhYpRVNZ+VxbvyZz39/+5u+xHXvM84YSh/4umF49fjP81vDK8eywTIsd7Tu5at0V\nVPkqqfSNLO4aTwYZTIR5M9xJubeMM5a/K/NzpVdCI3aMcCKM7dhYppXZ5r3QZX/Y0RXtpiGwuM5E\ny8zkDMGPPvooDz/8MKtWreLZZ5/l2muv5de//jXV1anG4h/5yEe4/PLLZz4AVaCURai5LcT2Xe3s\n7RggYTvEEkkiURvTNAoagCvLvJx96oqRAVhE5q3ZmptFSsHorc9vDBwkYPnGPG6ilc9tex4f0/s3\nfe539Gqv3/RjO0mcWYrADi4GBl3RblpCrfTHB0atBLtE7Ah+j2+4lRO80PkSKysPZ239GjY0ncrP\nWu+nL9aXuabt2PTG+mgJtY4JgwuxiFb2hx0iU5EzBDc0NLBq1SoATjnlFI488sjMJAvg8/koKyub\n8QBUgVIWm+a2EPc9uZtozKZnIAZAfLgAlpMs3MR72tolXHvRuoJdT0Tm3mzNzSLFlE8YG2/rc1+s\nj7546ixtKiimikY1BOrGvea+gf08sufX44zAxWNYVHorRrxWxI5iGIwuK11U0WQMn+nlwd2/xDJT\nx/Rsx6Yv1pepeZPeGp2WXvleW7+GGl/ViFXg9HsyenVc7YRERsoZgn2+kZ+0BQJjq8em/+MUkfxt\n39UOwEAkkWrJUMil32GmAZ290YJfV0TmluZmme/yDWOjtz5H7Ci2a5N0HAwMbMcmnkxAAJZXNGWu\nEbFjtHS38lLXK8N9gg1MTBwO9Qx2AdMwOGP5uwD4rz9vpSPSOSctkpJukvZwB/FkHMu0CFoBEo6N\n7djYTpK6QC1Byz/iOdkr33EnkVklzvUYUBEtkdFyhuDnn3+es846K3M7FAqNuA3Q3b04Dt6LFFJn\nb6oNWDyRxC7gym82r+VR2yORBUhzs8x3+Yax0UWf+uMDOMNVm9OLtY6bxGd6OTCY+nC5LzZAf3xg\nzDbn8T4WOveIv2Bl5eH876c20R3rmfbPM1MGBvFkHGA4+NpU+6sIWgG6It2ZAJx97rfcW57Z7pxv\nheTFXkRLZLScIfjwww/nmmuuyflE13W58847izIokYWssSZIR0+kKK2P0oJ+j9oeiSxAmptlvss3\njI0Od4lk6lyvSaqFUNJN4gAHwm9yMNyBZXpIjFP8CsbubjZJ9eH9Wesv5jQAA1imB8u0sB0bx3Vw\nXIdQpJuAFaDGlzrqELFjI879WqYns/Kdb4VktRMSGSlnCL722mu56KKLJnyyx1OcFkMiC9nG9U3c\n9+Ru3CKlYMOApbVB1q3Or12YiMwfmptlvss3jI0Ody5kev06o7Ytu7g5A/B4XBy27v7VnB8dMADb\nSR4K9Vl/L7Adm4Rrc0rjSfzuwO8BRpz5BTJVpdNfT1QhWe2EREbKGYL/8z//k/vuu4/Pf/7znHji\nieM+ZrKJWETG19MfLWgV6DQDCPot4vYsVvUQkVmjuVnmu3zD2Ihev4MHC9q7NxWonVktgDWambVJ\n23XdEQHYNEyq/dUELT8HBtvHbZ0Eh1bP86mQrHZCIiPlDMEAP/7xj2drHCKLQnNbiB888DLhaP6f\nWE+FYUBF0KvzwCILmOZmmc+mEsbS4e7u5i24rktPtAd7nOJVhQzIs8XNGnHCsXFxMTGwTGv4THDq\nLHBXtLtgW5nVTkjkEDPXN/LZIvKjH/2okGMRWfD+4/E/FzUAV5b7CPotnQcWWaA0N8tCsLZ+DVet\nu4IbT/kMV627YtJgljpH7OIxx271T/f8NYb/mS/cEV+7mfvSRbHS0h8SjEdbmUWmL+dK8P79+/nO\nd74z4ZMfeOABPvaxjxV6TCILRnNbiO272mnd30v/YLwoW6ABLI9BfXWQVUsr2Li+SeeBRRYozc2y\nGCWcBKFINy65V309hodybxlDdoSEk5j9Qc6QQWobdHYABkaskmsrs0jh5AzBAwMDPPvssxM+eXBw\nsOADElko7nroFX7/ckdR+gBnW1ob5Ir3r1HwFVkENDfLYtMSaiUU7R53s7MxXCnaMi1WVi7nnFXv\n499fvW/OKz5PhwsELD+NwYYRQRfg7uYtdEVS26IvWH2Owq9IAeQMwccddxz33HPPhE++6aabCj4g\nkYVg6449PP3Sm0U/oVQWsBSARRYRzc2y2Oxo34nrungME8d1cEkVlfJ6vCwrX8oFq89JrZBGutm2\n53F6Yr1zPeRpMQCf6ctUe4bUBwDZRcQ6I12Z2wrCIjOTMwTnc+7o1ltvLehgRBaKR3e+UfQAbJpw\n00dPZUVdsMivJCKlQnOzLDZdke5MH13TOFTKxnVdfKZ3REg8EG6fdwWyDjFwRo19R/vOcR+5o32n\nQrDIDOUsjNXS0sJHP/pRXnzxxdkcj8iCMDBU3PNIpmlwzOE1vP3YJUV9HREpLZqbZbFpCNZR7i0D\nwHGdVP9cxybpOoQiPXRFunkz/BYdQ50M2ZE5Hu30GBh4DJNEMk5LqDVzf6og2Fjp1kgiMn05V4K/\n9KUvAdDUpCqzIlOxdceeol3bALyWSW2ln/M3rCra64hIadLcLItFS6iVbXsfZ9/AARLJOEbWSqkB\n+D1+emK9eAwTMLBLqBiW3/RR5a+kMxIa871DFaxdPIZnxOp2ubd8xCpvoVojichYOUPwnXfeyTXX\nXIPruuzceWg7xqmnqhy7yGhbd+zhN88foGcghlvEnVimabD6sCrO37BK54BFFiHNzTKftYRaM+d3\nG4K5Kxy3hFr5Wev99MX6ADAND7abai/oNS1MwyRqR3BxSY7TN3i2GRxqeWQAVf5KglaAMit1XGlZ\nZSMD0TDhxBC2Y1PuLcdxXRJOHNuxsUyLcm85Qcs/YpV3Q9OpI7Z7Z98vIjOTMwTX19dz8cUXA/Dz\nn/+cX/ziF1xyySWaaEWyNLeF+I/HX2N/Z7jor+Xzmlx36QkKvyKLmOZmma+mUuRpR/tOwolD86pp\nGBhuKmraThJIjuitO/dSYzOHq1Wn2xyVe8voi/UDELQCmfsvPPI8drTvnHSVV62RRIonr8JY1113\nHc888wzXXXfdrAxKpFSl+/529kZI2Ek6e2PEEsX/FNrvNTmiqUoBWGSR09ws89VUijx1RbqxHXvE\nfQbgAKUSe7O5WQHY5/Fl7g9aAWp81TRW1nGgt2NEiN03sJ+W7tasleAyglZgzCrv2vo1Cr0iRZAz\nBI82XkXKj3/84/zwhz8s6IBESlVzW4j7ntwNQCRm09UbKerWZ0h9+t1QEyDot3QGWETG0Nws88VU\nijylz8JmB2HTMHFKYOtzLqbhwXGTmSJeaecc8T7ec9w76OwcyNzXEmrlhc6XKPeWE06EsR2bcGJI\nq7wisyhnCH799de58cYbM7d379494jbAn//85+KNTKTEPPT0Xjp7I9hJh2TSxSlyADYMaKwNsmpp\nBRvXN2kVWEQ0N8u8NZUiTxuaTmXfwIHMmeD5wOfx4vOUs7Ly8DFbl1988xUeaflt5ix0fywViIOW\nn6Dlz1zjwGD7XA1fZNGZcDu0x+PJ3D7jjDNmZUAipai5LUTbm/2ZXVhOsRMwsGZFDTf9j7cX/XVE\nZP7Q3Czz1VSKPK2tX8NH1lzML17bysFwR8n3/vV7fDQE6/Cbw4E2a7gtoVYe3vcrbDu1it0Z6eLN\n8FtU+6syZ4TT1PpIZPbkDMEXXHABN91004RPvvXWWws+IJFStH1XO5bHxLYdHKf403FNhU/bn0Vk\nDM3NMl9lF3l6Y/AgiWQCr+nNnBUebxvwYKL4RScLIZ5M0B7uAMAYMLBMiyE7wr6B/UTsKAkngcfw\nZCpAW6ZFODE0JgRP1voo3+raIjK5nCF4skk238eILASdvREqgl66+6Iki7wK7DENrjp/rbY/i8gY\nmptlPksHts7dvyQwXEAqV5XoHe07GUwMzf4gpylVtRo8hont2PREewADx01iebzYjj28vbuacm/5\nuFu9J2p9NJXq2jOhoC2LxaSFsR544AEeeeQR3njjDQBWrlzJOeecw0UXXVT0wYmUisaaIE5PBMM0\ncIsYgk0Tjj68WgFYRCakuVnmq3yrRHdFunFcpyS3QvvNVIBPjy3pOjhuqna14zrDRbyczGNsx8Z1\nXQygP9bP0vJGavwrqPJVjjg/DHB385ZxA+hUqmtP12wFbZFSkDME27bNpz/9aZ555hk2btzIu9/9\nbhzH4cCBA9xyyy08/PDDfO973xtxNklkodq4vom7H2ohYTtFfZ3G6iDxRHFfQ0TmL83NMt9NVCU6\nexWyPz4w7uPmmgF4TM+IytUGxnCnYHCGQ68zHJANwHUP9TSOOwkidpQLjzxvRLCcLIBOpbr2dM1G\n0BYpFTlD8I9+9COSySSPPfYYDQ0NI7731ltv8aUvfYkf/vCHXH311UUfpMhsy+4H3FgTxGuZ9A7G\ni/Z6BuDzeQj4LRprApM+XkQWJ83NMt/lqhLtM72Z0BexYwzZQyW5CmyZFunYa7tJDIzhcRq443Qy\ndkltkXbdVCw2DZMaf/WYUDlZAJ1Kde3pmo2gLVIqzFzf2LZtG9/85jfHTLIAS5Ys4Rvf+AaPPPJI\nUQcnMhfS/YA7eiI4Lrzc1s1TL71ZtNczDfB4DGrKU9urNq5vKtpricj8prlZ5rvxzr1G7Cjt4Q4O\nDr7J/sF2OiNdJJKJORjd5JKOQ9JJgpGKwqZhZAXh1IfaI/+d+p5lWnhNi7pALXFn7M82WQDNdV54\nonPEU9UQHD9QFzJoi5SKnCvBfr+f6urqnE+sqakhENCKlSw823cd6tPXNxgjHLUnePTMeC2TyjIv\nPsvDSvUDFpFJaG6W+S77jGtXtBuf4SVCjKgdJZl1BtgpwVVgGD4HbLjDW5xdDMPEBDyml0TSxjI9\nWKZFubeM/vgAiWQC13WxPFamOvR4oXKyld7R71t2H+JCmUobK5H5bsI+wa7rZs4x5HqMyELT2RvJ\nfF3MLdA1FT5qKwP87/95StFeQ0QWFs3NUmqmU004O9C1dLcCpRt6R7NMT+a/P9Mw8RgeGspSQbUr\n0kNDsHbE4/ti/XgtL/X+Q/ePFyrzCaBr69cU9WzubARtkVKRMwTv3LmTt73tbbM5FpE519wWoi8c\npz8cx04Wb0KuqfBRXeHX+V8RmRLNzVJKpltNOPt56QJT6WrKpcwY/scyrcy4swtkLa9YRiwZG/Oc\npJOkK9LD8oplnLPqfeO+N6USQIsdtEVKRc4QfNxxx/GlL30p5xNd12XTpk1FGZTIXEifBY4lkkUL\nwIYBS2qCBPyp//R0/ldEpkJzs5SSqVYTTq8ap1d/y71luLgknOIdO5oqY/g0r4ub+dpjeHDcJC5k\ntjr3xfqBdKGslHNWvQ9I/fxvDB4knBiiyl9FZaAM206OCcijKYCKzJ6cIfjGG2/ktNNOm/DJN954\nY8EHJDJXtu9qp6N7iGg8WZTrlwUsAj4PZQEvjTUBnf8VkSnT3CylZCrVhNOrvxE7StSO4uAyZEfG\nefbcMTDwGB5cXBzXwQBMw4NpGEAqCJd7ywhaqV1c4cQQlb4KGoMNI1Zt19av4e7mLeOe8VW7IZHS\nkDMEn3766ZM+OZ/HiMwHzW0hnmvtLNoKcFnAorEmyGVnHqngKyLTprlZSslU2vZs2/M4HUOdxJOJ\nkmx9BKnVX5/Hi9e0iCZj2E6SpJvEdl08hoc1tUeTdJIcCKcKaC6vaMq5vVnthkRKW84QLLJYNLeF\nuPdXrUULwB7TYN3qOq38iojIgpJvNeFtex7n9b62eVH8ynVdoskYiaSNM9z318SgwltOKNoDuJlW\nQr2xPu5+eQsV3gpWVB42YjV4Nvr6isj0KQTLovcfj/+Zt3qKsyXL6zE5eU0D1160rijXFxERmSv5\nFHPatudxHmp7dF4EYIBIMjridqoXsEnCSZAY7u8btAJE7GjmXPBgYnBMUTC1GxIpbQrBsqht3bGH\n/Z1DRbm2aUJdlV/Fr0REZMGaqJhTS6iVbXseJ+kWp9bGbEifD7ZHFe8KJw793SH7e+kzv9kfEPTZ\nfTQGa0d8QDCd1lIiUjgKwbIoNLeF2L6rnc7eCI01QTaub+KRZ/bxyp6eoryeaRocc3g1529YpS3Q\nIiKyqKQDXnNXCzEnPtfDmbF0Vehs2cE3+3vZZ37TYbixsZLOzoHM/dNtLSUihaMQLAteuvURQDRm\n8+JrIf7Q8lZRXsswYOXSShXAEhGRRSk74KW3D893BlDuLR++ldrWnd0r+ND38jvzO9XWUiJSeArB\nsuBt35Wq4hiN2YT6oySLVAALYM2KGq3+iojIorWjfScRO0p/bGDenAOeTNBbxsrK5ZnzvDvadxJN\nxhmMD1LuLSdo+TOPzefMrypHi8w9hWBZ8Dp7I/QNxugbjBd1Oq4s83LT/3h7EV9BRESktL0xcJCe\naC9J15nroRSExzCpD9SM6QMMWed6cxQFy0WVo0XmnkKwLHgJ2yl6AAYoD3iL/AoiIiKlqyXUSk+s\nF3seF8IazTJSf1Ueb6vyREXBJqLK0SJzTyFYFry+cPEDsGkarFxaUeRXERERKU3b9jzOY/ueXDDn\ngAEsw4M7/DeIQm5Vzqe1lIgUl0KwLHixRPE/kTYNQ62QRERkUcr0Ap7nW6ANUn2B0x+cm4aZqfxc\n6K3K011FFpHCUAiWBS9hF3dS9ns9HLGsQsWwRERkQRuvty0w73sBG4DH8Ax/BUk3OfwVlHvLAG1V\nFlloFIJlQcnuB+yzPLzx1gBukfZC11T4qK5IVYQ8//QjivMiIiIiJWB0b9t9Awdo6W4l4dgltwU6\ntZo78eRvYmAYZioAmx6qfFUAhBNh4g4EPH7KvGWsqDhMW5VFFiCFYFkw0v2A+wZjqXPARQq/jTUB\nVjdV0dkbpbEmwMb1TVoFFhGRBS27t23EjtEX6wNKsxfwRAHYwMBnenFxWVW5gnOOeB9w6HxuuhXS\neKF3vJVwhWOR+UkhWBaM7bva6RuM0TsYL9prNFT7ufXa04t2fRERkVKU3ds2nAgD4BTr0+Yi8JoW\npmHiurCuYe2YADtZmB29Et4Z6crcVhAWmX8UgmXB6OyNMDBUvE+kLY/BR889rmjXFxERKVXZvW1t\nx8Zx3XlzDtjv8bG0rBGAxmADV627YsrXyF4JH32/QrDI/KMQLAtCc1uIvnCcpFOcT6UNA45aXq1t\nzyIisihl97a1TIuoHZ303G2pSGZVrc5V4Gqyrc7ZK+HZCtk6SURmj0KwzHtbd+zhkWf2EY3ZRXuN\noN/i/A2rinZ9ERGRUpbd2zaWjBO1o3kVoCoFruvSGGyY8KzvZFuds1fCsxW6dZKIzA6FYJnXmttC\nPPLMPiJRu2jTsNcyOfedK7UKLCIii1q6t+22PY/zcNuj2PNgO7RleKgL1k24BXq8rc4RO8qWV/+T\nKl8lDcE6llc0jRuC1TpJZH5SCJZ5bfuu9qIG4IZqPx899zgFYBEREeC25+/k1Z4/z/UwMkzDxMQg\n6TpjVqX9po8qfyUrKg6b8Brprc4RO0o4MUQ8GcdxHUzDpNJXTmeki85IFyc1nsCBwXa6ot00BFQd\nWmQ+UwiWeae5LcRDT+9lf+cg4WhxtkB7LZML3n0EH9xwRFGuLyIiMt9s2/N4yQRgv8dHwBOg3Cqj\nKxrC5/HhNb2EE+FMgPWYHmDy1dqGYB37BvbTF+sHwHEd3OF/R+woQSsAwIHB9mkV1RKR0qMQLPNK\nc1uILb9qJdQXxS5SESyf1+S6S0/Q6q+IiCxqLaFWtu19nDcGD5BI2iVTDdoyLdbVH2pzlC5qtX/w\nINFkFI/ryVoVNia93oamU2npbs3cTj/TNDyEE0OZEKwiWCILh0KwzCvbd7XT1Rch6Uz+2OmorvBR\nVxlQABYRkUWtJdTKz1rvJxTpxqFIk+40lFlB1tatGbEimz6rfHfzFvwe35jnTNbGaG1gyDAEAAAg\nAElEQVT9Giq8FQwmBrEdG9MwMTAwDQPbObTjTEWwRBYOhWCZV1r39xYtAHtMg5oKP401geK8gIiI\nyDyxo30n/bH+kgrAAEnHoT3cwed+82VsN4llWqysXM45q943ozZGKyoPyxS+itgx+mJ9QGrVOU1F\nsEQWDoVgKVnNbSG272qnszdCY02QwUiC3oF40V6vsswLwMb1TUV7DRERkfmgK9JN3EnM9TDGiDsx\n3hrqzPT+jSfj7O7bw53NPyZ7B7TrulimRbm3jJWVh0963ew+yEHLD1QTToSp8FVM2F5JROYnhWAp\nSc1tIe57cnfm9ouvdRFLFO/TaMtjsGZFDRvXN2krtIiILErps7VdkW764wMl2QM4XbAqe2xJ18VJ\nJrBMD7aTOrfsMUxsx6Yv1s/ypsk/3M7ug9wV7WZl5XIFX5EFTCFYStL2Xe2ZryMxu2gB2CB1Dviq\n89cq/IqIyKLVEmrNrIRCKmiWKmeccO7iknBsjOFCWI7r4vNYlHvLOTDYPubx40mfLRaRhU8hWEpS\nZ28k83VX1teFZBiwZkUN529YpQAsIiKL2o72nZmv+2ID9McH5nA0M2eaJg3BVCErVXUWkdEUgqUk\nNdYE+dMbvfQNFucMcHWFj09o9VdERAQgU1QqYsdKYiu0gTHuGLymRSKrYnMurnvouarqLCKjmXM9\nAJHxeC2zaAE46PcoAIuIiGRJr5r2x/vnPAADI8ZgYGBi4Pf4qAvUUuEtH7f7b/Z9hnHolqo6i8ho\nCsFSkna9FirKdcsDFsvqyhWARUREsmxoOpW+2ACxZPG6MEyHxzDxmhaWaVHlq+TwisP42NsuZ0Xl\nclZWLqfMCmIZHkwMPIaFz/Ti83jxmT4agw1ceOR5OucrImNoO7SUhOf+9BYP/e519nYM0DMQI16E\nQlg1FT6q1QdYREQkY9uex/ndgd/THx8g6SbnejhAauXXMjzYbjLV6siTanUUtAIjKjbvaN9JNBln\nMD5Iubd8uLVRisKviExEIVjmXHNbiAef2kN7V5hwdPJzPtPh93qorkhNjuoDLCIii11LqJX/em0r\n7eEOgJLYAp3m83ip8lUCEE5EaCyrpyFQNyIAZ1dyzrR2inaPeZyIyHgUgmXObd/VTnd/tGgB2DSg\nttLP0tqg+gCLiMii1BJqZduexzkQbifpOoBLIpmad0spAJuYuK5LX6yfan8Va+uO4ap1V0z4HLU2\nEpGpUgiWOffKnh4GI4mCX9cwIOi3OPedK/nghiMKfn0REZH5oCXUys9af0FfrB8A27FxKa3wm5Y9\npnBiaEpFrTIrwpFuGoJaERaR3BSCZU7d9dArRQnApgGnHLdEK78iIrLo7WjfSTgxlLntlGD4TUuH\nYMu0qPRVjN3ynCPgtoRaeXD3LzO3OyNdmdsKwiIymkKwzImtO/bw6M43GBgqfAAu83v47g1nFvy6\nIiIi81FXpBt7uLduqRS/Go8x/M+y8iUANAYbgPwC7o72neNec0f7ToVgERlDIVhm3V0PvcJTL71Z\nlGt7PSbXXryuKNcWEVksbrvtNv7whz9kbl977bW8+93vZv/+/Vx99dU0NjYCcPzxx3PzzTePef4t\nt9xCW1tb5vaXv/xljj322OIPXMbVEKyjM9JFPBkfPg9cWizDkxmX13Por6bprdATBdz0v1/qasEy\nPWOqRHdFu4s1bBGZxxSCZVY0t4XYvqudvR0DdHRHivIaBlBfHdD2ZxGRArjnnnvGvf+aa67h0ksv\nnfC5jY2N/NM//VMxhiXTsKHpVPYN7CdqR+d6KGMYGJiGicfwgAE1/moagw0jtjt3RcYPsvsHD2ZW\nhC3Tg+3Y9MX6gOpMEG4I1M3KzyEi88uch2B92rzwNbeFuO/J3QB09hQnAFseg7qqACuXVhTl+iIi\ni83mzZvx+Xwkk0muvPJKgsEgAE888QTd3d0MDAxwwQUXcPTRR495bjgcZvPmzXg8HsrKyvjrv/5r\nLGvO/8qxaK2tX8NR1avpjITmeigjeAwPlmmxrHzJhK2N0ivZo8WTCfweHwDl3rJM4a9wIpwJwVMp\nrCUii0dJzEj6tHlh276rHYBIzMYpQi0Ow4C6qgBBv6UewCIiefrEJz5BV9fYYHH99f+vvTsPj6o8\n9Af+fc85s2bfwJANKCaEVUCBCMqPuBBABDfso2AVFbW2uF0vVbxoixdZrNbrtQqI+ANptVcrpSLi\nLVRuxQhciUokECwJWwIkExKSySSZ5dw/JhkyZCFkzmQmM9/P87QPc+acd955Dbz5nvMuC5CXl4eU\nlBSYzWZs3LgRS5YswdKlSxEfH48FCxbg8ssvR2VlJWbPno1NmzYhOjraq4wZM2YgKysLiqJgxYoV\nWLVqFR599NGL1ikuzgxFkTX7ji2SkqI0LzMYfXfqAP5+5Cuctlaib0QiJg+8GiMvG4KPD3yGPaf3\nBbp6XgQEZEmGIsmYM2oWRl42pMNz87KvxR+//0ub40boPT8vUUoEZFlCbaMVDpcDqXHJmDwgp9Ny\ne1K4/Az6E9vQd2zD84IiBPNuc2irqLahpq4R1XVNfim/T5wZGX0juRI0EdElWLt2bZfOGz9+vOdc\ns9mMyy+/HACQmJiIxMREHDx4EGPHjvW6ZujQoV7Xr1mzpksh+OzZ+ouec6mSkqJQUVGrebnB5sLF\no05Un8KGfX9GzUAbPvzhU7iCbC6wLCRAVWFWTPis6H/QT05rc07rFaH1sg5QgSbV7nlqnF++1+sJ\nsV7okWDUI8mUiLsHzQaAoPhvHy4/g/7ENvRduLZhR8G/R9JiON1tBniX5UIuCL8F4GE/ScBLP5/o\nl7J7C/68dQ/brXvYbuFh+fLlWLhwIQDg6NGjSE9PBwBs2rQJ2dnZyMrKgt1ux6lTp5CSkgIAKC8v\nR58+fSDLcofXk/9cuHiUzdEAq70eawo3wO7SfieGSyFBAqB6tmYSAPSyHhE6M0yKsd3Fqy4M9Y3O\nRgDAzQOneg2Zbn1OCw6BJqKL6ZEQHC53m4HwvcvSkcISC46Wn/NL2bGRetx4ZWpYtzd/3rqH7dY9\nbLfOhdINAkVR8OKLLyIhIQHFxcV4/vnnAQB9+/bFm2++iezsbBw9ehSPPfaYJwQ/8cQT+Jd/+Rdc\neeWVqK6uxssvvwyj0YiSkhI888wzgfw6YaH14lE1jedQ21QHFef33Q0sFZKQIOAOwJKQkGg6v2BV\ne4tXdWXLo9bbI1U2VHU6r5iIqLWAjxvm3ebQ9fL7BThQelbzcmVJIDnBjNm5gzj8mYjID5566ql2\nj+fk5CAnJ6fd995//33Pn1966SW/1Is6lmiKx7Hak6huqIZddQS6Ol4USYEiKYjQRaCmsQaK5P3r\n54VPbossxSiqKobD5fBc17LQ1YVPjbMTMhl6ieiSBTwE825zaFq75YDmAVgI4InZIxl8iYiILpAS\nmYzvKgrhUJ2Brkq7HC5Hc5CNQaw+2mtub+sQe+Ew6Au3PeKWR0SkhYCHYN5tDi2FJRZs+aoUh47X\naFquEEBMpIEBmIiIqB0n68ohSzIczuAKwUbZAElIAESb/X/b0zIMuvWWR8D5bY8435eItBDwEEyh\n45P8Uvz1y1LYndqvQJkUa0L/y0Jnvh0REZFWiizFKKwsQqPLP4tQ+sKluhClj8KdmbO6NGy5ZW6z\nSTECAKz2ejhcDgCizaJYRETdxRBMmli75QB27T/ll7JjIvXcA5iIiKgdRZZibCj6ICgDsCLcc4Fj\n9dFdDq+JpnjPtkcmxegJw0mmRAZgItIMQzD5zJ8BODbKgMzUGO4BTERE1I5tpTtQ0xR8q7bH6KMR\nY3CP4GpSz2/R1Hrv30RT2znBOclXcdsjIvI7hmDySWGJBV/5IQBHm3X43YJrNC+XiIgoFBRZivHn\nHz9BmdU/N6F9VWe3Qi/rYFKMnsWsLlz0qsJW6XnNbY+IqCcxBFO3FZZY8O7Wg5rvQGg2KnhgxhCN\nSyUiIuqdiizF2Fa6A8dqT8ChOiEJCS7VBWeAV4I+v/Ov+zcBV6vfCJyq07OwVctT3K7s/Qtw2yMi\n8j+GYOqWwhILNn5ejKpzjZqWazLIeHjmUA59JiIigjsAf1D8Mc42VMOpuheeDHT4PU8gxhANl+pC\nbVMtBATUVkFYCIFYQ4wn0LYsenWhC/f+JSLyNynQFaDeaf1nh3D6rE3zch+ZNYwBmIiIqFl++V5Y\n7fVwNQdgVfPxV5dOFhL0kg4xhmhckzIedXYrXFA9dRMQUIQMWchocp2fD5xoan+PX+79S0Q9jSGY\nLklhiQW/eHUnKmsaNC87MYb7ABMREbVWaauCw+WAiuAIwACgqu56qFDxbcV+CAiv9yUhIAkJDpfD\nK+B2tLgVF70iop7G4dDUZf5cBTo+2oB78gb7pWwiIqLeqmXLoJYgHBwEYgwxzfv3up8MO1V46udS\nVcgCUCTFK+By0SsiChYMwdQln+SX+i0Aj83ug+nX/ARp8Sa/lE9ERNRb5SRfhWO1J9DgaASCJAYb\nFQNMigENDgGbo6F5rvL5BbJUqBBC4Pr0SW0CLhe9IqJgwOHQ1CWffFXql3JTkyLw8MxhGJ3Vxy/l\nExER9WbZCZm4M/MW6GV9oKvikRKZjJsHTkVaVD9Y7fWQhIAsZEgQEHD/OT0qFVP65wa6qkRE7eKT\nYOpQYYkFW746iiPl52B3uDQvX6dImJ07SPNyiYiIepsiS7F7mLCtCokm72HC2QmZ7qerF6y+3NNE\n82rQj49+2HNsf+UBAC3zgN2/VsYYYrwWxCIiCjYMwdSuwhIL3tlShHPWJrj80N8adDKmX53BhbCI\niCjotATSf9aUwmqvh6qqMCoGROujoJN0bUKqFp+3+chWz+sKW6XndXZCJoosxWh0arslYXfIQkKS\n6Xy/nZ2QiYzoNJysK4fD5YAiKYjQRcCkGLjiMxEFNYZgatf6zw6huq7JL2Vnpcdieg4DMBERBU5H\nT15bAmlNYy3ONdUCcM9xrbM7YLXXI1ofBRUur5Dq62efa6xt97z88r0AgDe/X9fNb6ktSUhtpiVP\nycj1CvAtuOIzEQUzhmBq45P8Ur9sgdQnzoQ5N2Yy/BIRUUB19uS1JXjW2a3tXltntyLGEOU591JD\ncHuffcp6BibFhAZHA+wuuydnHqs9iW/OfHdJ5fuTSTGiSfUe5swVn4moN2IIpjY2f1nil3IZgImI\nKBi0BN32jlfaqgAALrX9tTBaH69sqNLks4UQXk+dg03LYlf2C/b9bcEVn4mot2EIJo/CEgve/usB\nOJzad8CJMQYGYCIiCgotQbfN8YYqz768kpDaBOGWxalOWc9AkRSkRCb79Nk2RyOsdiuanPagDL8t\nJCFBEgIOl4PDnIkoJHCLJALgDsCvf/g9ztVrv5qjLAnckzdY83KJiIi6I9HU/qJNLUN5ASBSF9HB\n1SpcqgsOlwPVjTUoshRf8mfbHI04XV+BSpsFDY6GoA7AAgIu1QUhBDKi0/jEl4hCAp8EEwpLLPjP\nP++H3Q9PgM1GBXnj0vkUmIiIgkZO8lUdLubUeo7rP2tKUddUB4fqBACI5v93qS5E6MwwKcYO5wV3\ntPBWSmQyvqso9JQZrPHXPQRaci+G1bwP8JQM7vtLRKGBITjMFZZY8NHOI2iya7sPsCSA2CgD7p06\nmAGYiIiCysUWc2o9x/Wdwo2exatas7scANqfF9zZwlv/e/pbOJsDcDARzREfABJNCQBUWO31cLgc\n0DVvfcSnwEQUKhiCw1RhiQVffl+Obw5VwKnxRsBCAImxXAmaiIiCV1cXc6q0VcHmaIRTdbmHBcM9\nR9bRHIITjfEX3fLI5miA1V6PNYUbgmK/346I5ie+VrsVETqzZ9i4osiI08UFuHZERNphCA5Dn+SX\nYstXR9Fo1/5OtBBASmIEZucOYgAmIqJeTy/rUG495XlSqgJwqi7IkgwASIlMbnfLoxhDNEyKETZH\nA2oazwEAHK7gewIsQUAICQZZjwidGYBATWONp84mxQiA+/4SUWhhCA4zhSUWvwXgsdl9MHFEMsMv\nERGFjubBUpIQAGS4VCdUADpJh5sHTm13yyNFUmC118OkGHGuqRYOlwMqgm/7IwGBKH0kJCEjSt96\nIbAYWO1WWO02pEelIi/7WvST0wJWTyIirTEEh5k/7fjRLwE4Ky0GD88cpnm5REREgdTksiPGEO2Z\nH6uXjYjQmWHWmZGdkIm/HtnW5poIXQRqGmtgczSiyan9rgu+EgBkoUASAnaXA5E6o9f7JsUAk2KA\nEBLmDbsbSUlRqKiobb+wS9TRgmFERD2JIThMFJZYsP6zg6is0X4uUkykHtOv7q95uURERIHm3jfY\n5RkW7DlujG/1fqXXeybFgFhDGiptVZ7lpoQQcKrB8SS45am0IukQpY9EamS/Nt8BOP8dtdLZgmEM\nwkTUk7hPcBgoLLHgvc+L/RKAE2MMuH96NodAExFRSGqZC2tzNKLSVoVT1jOotFUhJTIZRZZinGuq\n9RyzORo8103JyEW0PgoJpnhIQoIrSAIw4B4GraouROgikBrZr8P5vlrPA25v6Hhnx4mI/IVPgsPA\nl9+X4+w5bQOwXifhpqv746ac/pqWS0REFEyyEzJxrPYE/nZsJxwuBxRJQYTOjPzy/0V++V6YFCNi\nDO45tDWN56ATOkTozfjrkW0411QLRZIhCzmotkVyz00WsNqtbfZGbm/LKK1U2tpuJwW0v80UEZE/\nMQSHuLVbDmBP0ZmLn3gJIkw6vP7YNZqWSUREFKxO1pV7tgtq0RLoTIrRM4fW5mhAdVMNFNm9crQi\nKahprIHD5QyyJbHc2zxF6iPb3RvZX9obOg5oP+yaiOhiGIJD2NotB7Br/ylNy4wwKhjan3sFEhFR\naGu9gFOFrRIRugiYFIPn/ZZ9glvYHA2wNJyFS3XheG0ZhBCQIEEIwAVXT1e/UwIC8cZYpEX269HP\nzUm+ymtOcOvjREQ9iSE4hOUXahuAdbJAYqwJE0cka1ouERFRMLlwASen6oTFZoEkZOhlHSJ0EVAk\nBU7ViUpbFZqcTXCqLs8WSCpUqKrqDr9B9ghYQEAv62BSjD0ePntq2DUR0cUwBIeotVsOwKVhx6tT\nJIy6PJH7ABMRUchrvVCTzdEAZ/NwZpfqhMMlUNNYA52kcx8XKlytAnCwEhAQQkARMtIjUzGlf25A\nwmdPDLsmIroYhuAQ9PL7BThQelaz8iYMvwz3Tx+iWXlERETBrGWlZ6u9Hg3NKz4LCLQ81lUkBUII\nxBljYbXXw37B0OhgopMU6CQdHC4HMqLTMCUjMOGXiCiYMASHmBfW7cGx03WalffknSP55JeIiMKK\nXtKhvPEcgNajmVXoJR36RiQBAE5ZKxBvjIVJMXpCczA9DZYgQScrUFUgOz6Tw46JiFphCA4hWgfg\nCcMvYwAmIqKwUmQpRnn9adhdDggAaN5OCHAvcOWeA2yHU3XiRG05hBBQVTUoArAECZIQ0Mt6ROjM\nMClGJJkSMW/Y3YGuGhFRUGEIDgGFJRas/+wQKmsaNCszOkLHIdBERBRWWhbEanI2QRYyXKqzOQKr\nEJDcWx2p7oWyBERQLXwlICBLkmfos9VeD4ArLxMRtYchuJf7JL8Un+0+hvoG7eYjSRLwwE0MwERE\nFF5aFsRSJAUOlwOScP+apEJt3utXhVN1Qmp++htMWuqok3StjoqA1YeIKJgxBPdihSUWzQOwTpYw\nY2J/DoMmIqKwU2mrAgBE6CJQ01gDl+oOvS1DnWUhwam64AyyANxaoine63V++V7OBSYiugBDcC/2\npx0/ahqADToJj946nAGYiIjCUqIpHhW2SpgUA+rtOlgd9V7vO1VXgGrWNaKdJ7+VDVUBqAkRUXCT\nAl0B6p61Ww7gRIVVs/IMOgkjByUyABMRUdjKSb4KNkcDTtdXtAnAvYEKFaesZzyrVQNAojH+IlcR\nEYUfPgnuhQpLLNi1/5Rm5UkSEBtpwMQRyZqVSURE1Bs1Oe1octoDXY0ua3n62/oZsMPlQE3zFk9c\nGIuIqC2G4F7olQ++06wsnSJhYL9oTM/J4FNgIiIKa9uO7oDNYQt0NS5KguRemRqAJCRIEBBCwKSY\nYHfZ4XA5oEgKYg0xnA9MRNQOhuBe5qGVf9esrKz0WCy8a7Rm5REREfVWRZZiHKk5CpfqCoo9fzun\nQhEKZCE1rwrtQJQuCjGGKK+zmly954k2EVFPYgjuReYt26FZWX3jTJiek6FZeURERL3VttId2HZ0\nB5yqM9BV6ZSAgAoVOkmHaEMUTIoRgHtVa3s7gZfzgYmI2scQ3AsUllg0HQIdH23A3TdmcvgzERGF\nvSJLMbaW/g12l3a7LWhNAJCFAr2sg07SI8YQ6fV+y5ZOF+J8YCKi9jEEBzktA7AsCaT2icRtkwYy\nABMRUVj47tQBfFb0P6i0VSHRFI+c5Ku85sluK90RlAHYs+CVENBJOgxLGIyc5KuQX74XFbZKr3NN\nigGxhjRE66NQ2VCFRGPb70lEROcxBAcxLQOwIgv8JCWGC2AREVHYKLIU49Njn8PhcA9zrrBVYvOR\nrQDgCYgnreUBq19HDJIBfSMSPa+FkDBv2N2e1y3fobUpGbkMvUREXcQQHKRefr8AB0rPalJWVloM\npl/dn+GXiIjCSn753g6PB2tgFBBeARjwntvbUu/88r186ktE1E0MwUFo7ZYDmgXgJ+8cyfBLRERh\nqdJWBVkRbY83VHn+HGuIQX0QbIskQYIkBCJ0EW3eu3Bub3ZCJkMvEZEPGIKD0K79pzQpp2+8mQGY\niIjCVqIpHmftbW8qt36yGqEzQxYSnKqrJ6vmRUAgxhCNa1LGIz0qlU95iYj8jCE4yGixDZIkgLS+\nUegbZ9KgRkRERL1TTvJV+PTY5+0eb1HTWAtXAAOwWTFh3tC7vYIuQy8RkX9Jga4AnadFABbNARgA\nJo5I9rk8IiKi3io7IRPjUkehtsmKU9YK1DZZcUXScE/ILLIUo85eBzWAdYw1xATw04mIwhOfBAeJ\n+zUIwADQ/7JoJMUaMXFEModCExFRWCuyFGP3iQJE6SMQpXfPtf22Yj/So1IBABsPfogmpz2QVYQi\nydh8ZCuO1Z7AybryDrdyIiIi7TAEB5iW2yBlpcdi4V2jNSmLiIiot2u9OrTN0Qir3QqHy4F3D/wR\nspBxrqkWagCfAxskfXPdGvC3YzuRaHLPVW5vKyciItIOQ3AArd1yQLNFsBRZQpM9cHOaiIiIgk3L\n6tA2RyPONpyFS3XBBRVNrsA+/W0RbXBPX7La6+FwOdq8H8xbORER9WacExwgn+SXahKAFVlAr0jQ\n6yQkxRp9rxgREVGIaHmyeq7pHJzNATgYCAgYZD1MirvfdrgcUKS2zyVab+VERETaYQgOgMISC/68\n84gmZUnCvf9hlEnHhbCIiIhaaVkF2h7geb+txeijoZMUROujPMcUSWl3f+DWWzkREZF2OBy6hxWW\nWPCqBnOAdbKAXicDAFL7RGJ6TgYXwiIiIrqAUTEEzRNgg2zAoNgBSIlMdi+C1bwX8BVJw/Ftxf42\n57feyomIiLTDENzDXvuv73zuis0GBf/5xLWa1IeIiCgUFVmKsfnIViiKDIOsR5PTHrBFsAQEZCHj\nwWFzO5zjmx6VivzyvZ5gzNWhiYj8hyG4B/3i1Z1w+rh2lRDAw7OGalMhIiKiEJVfvhc2RyNqbbUB\nCcCSkKCqKgQEdLKC9KjUTkNtdkImQy8RUQ9hCO4hC9/6CvWNTp/K0CkSZkzoz2HPREREF3G8tgxn\nG87Cqbp6NADrhA46WfEsytViSkZuj9WBiIg6xxDcAxa+9RUqqht8KkOWBH5523AGYCIioi6wu+xw\n9XAABoDkyD64Imm415xfDm0mIgouDMF+UlhiwZffl6PgcCXsDt/GQEsCGJOVxABMRETURTpJ1+ML\nYilCxs0DpzLwEhEFOYZgje07dAavf1CAyhrfnvy2plMkbn9ERER0CRQh9/jnTRtwAwMwEVEvwBCs\nocISC97deghV57QLwAAQF2XkU2AiIqJLUNVY3WOfJSAwbcANmNKf836JiHoDhmANffl9ueYBOCZS\nj/S+kZqWSUREFOoanNr2xx1RhIIB0ekMwEREvYgU6AqEkoLDlZqWp5MlxEYaOBSaiIgoCAkIxBlj\nGICJiHoZPgn20fkFsCpgd/i+AIeAey9gSZIwsF8Upl/NLZGIiIiCjSIpGBDlfgLMecBERL0LQ7AP\nCksseOPP+9Fo923159bMRgWpfSIxPSeD4ZeIiCgIyULGw8PvZfglIuqlGIJ98PZfD2gWgCUJmHXN\nQNyU01+T8oiIiEh7EgQGxmQwABMR9WJBEYJff/117Nmzx/P64YcfxoQJEwAAX3zxBYqLi9HY2Ijd\nu3dj3bp10Ol0XtcXFRVh48aNSE1NhcViwcKFC6Eo/v9q5+rtmpXFAExEROS7Iksx8sv3+qVsAUCW\nZNQ01uKdwo3ISb6KYZiIqBcKihAMABs2bGhz7Pjx49i+fTuWLFkCAJgyZQpk2XvfP1VV8fTTT2Pd\nunVISkrCsmXL8PHHH+OOO+7okXr7QgCIjtQjKy2WAZiIiMhHGw78CXtPF8CpOjUpT3Kv1AEhBCQh\nQRIC0fooGBU9KmyV2HxkKwAwCBMR9TJBE4LffPNN6PV6OJ1OzJ07FyaTCVu3boXJZMK7776L6upq\njBs3DpmZ3h3N8ePH0dDQgKSkJADA6NGjsXnz5qAPwbIkkNrHvfURV38mIqJg09tGaW0r3YGvT/2v\nZuXF6KMQY4jGzQOnIjshE+8UbkSFre0uEPnlexmCiYh6mR4Lwffffz8qK9t2HgsWLEBeXh5SUlJg\nNpuxceNGLFmyBEuXLsXJkydRWlqKZ555Bna7HbNmzcIbb7yBAQMGeK63WCyIiIjwvI6MjITFYrlo\nfeLizFAU+aLn+YvZqCDjsmhcNzYdo7P6BKwewSQpKSrQVeiV2G7dw3brHrZbeBxpje0AABXnSURB\nVOlNo7Q2H/lMs7IEBAbFDvQa7lxpq2r33MqG9o8TEVHw6rEQvHbt2i6dN378eM+5kZGRGDFiBIQQ\n0Ov1yMrKQkFBgVcITkhIgNVq9byuq6tDQsLFV1U+e7b+Er+BdmIj9Zg3Pduz+nNFRW3A6hIskpKi\n2A7dwHbrHrZb97DdOheKNwjCZZSWgIBOUqBICiJ0ZqRHpWLesLu9zkk0xbf7JDjRGN9T1SQiIo0E\nxXDo5cuXY+HChQCAo0ePIj09HQCQk5ODDz/80HNeWVkZ+vfvDwA4ceIEUlNTkZaWBqPRiIqKCiQl\nJWHfvn2YNGlSj3+HrkpNisDs3EHc/oiIiAKOo7QAo2xAv+i+Xsfysq9tc1MjL/ta/PH7v7S5vr1z\nQ1E4fEd/Yvv5jm3oO7bheUERghVFwYsvvoiEhAQUFxfj+eefBwBMnDgRBQUFeO2112Cz2ZCbm4vR\no0fD5XLhnnvuwfr165GamoqVK1fi1VdfRb9+/eB0OnHLLbf4vc5naxsRZdahtpMVoiVJwKSXue8v\nEREFpXAepWWQ9Xhw2D0A3PN6KxuqkGiMR07yVegnp7UZ9dBPTsO09Bu7dG6o4SgQ37D9fMc29F24\ntmFHwT8oQvBTTz3V4Xu//OUv2xyTJAk7duzwvM7OzsbSpUv9Urf2VJ1rwIo/FngCsCILSAKINOsx\nfeJATOZCV0RE1Mv1tlFaEgRcULt0rlkx4fr0SZ75vl1d2Co7IZOLYBERhYCgCMG9SUW1DSv/WIDK\nmgYAQHZGHBbcNgIGvXv4VrjeZSEiotDSm0Zp7a88AElIcF1kayRZyBgYnYEp/XMZZomIwphQVbVr\nt01DTHeC6umz9Vj5xwJUnWsEAAwbGI9f3DIcet35+UsMwd3Ddusetlv3sN26h+3WOc618l13fr4K\nzuzHOz9shEt1AQBmDMxDXv9cAECRpRj7zn6Lk9WnPcOXGX4vHf/u+4bt5zu2oe/CtQ2Dejh0b1Bu\nsWLlHwtQXdcEALhiUCIemTUMOkUKcM2IiIjC095TBVhf9IEnAN826Cbkpl/reT87IRPXDh4Tlr/4\nERFRxxiCu+BERR1efv9bnLO6A/CYrCQ8dPNQKDIDMBERUSDkl+3FxoMfQm2eB3xn5ixcm3p1gGtF\nRES9AUPwRRw7XYuX3/8WdTb3IljjhvTFAzdlQ5YYgImIiALhHyfz8f6hjwG49/i9a/DtuLrfVQGu\nFRER9RYMwZ0oKT+HVz74FtYGBwDg6mGXYd60bEiSCHDNiIiIwtOO4//AR4f/CsAdgO8ZcifGXjY6\nwLUiIqLehCG4A/88WYNX/vQtbI3ulSavHdkP9+RlQRIMwERERIHweenf8ZcjWwEAkpBw39C7MLrP\niADXioiIehuG4HYUH6/Gq//1HRqb3AE4d3QK7rohkwGYiIgoAFRVxacl/41PS/8GAFCEjAeGz8Xw\nxCEBrhkREfVGDMEXKCqtwmsffY8mu3ulyRuvSsOduYMgGICJiIh6nKqq+Ms/t+K/j30BANBJCuYP\n/xmGJGQFtmJERNRrMQS3UnjEgtf/vB92hzsATxufgdsmDWQAJiIiCgBVVfHh4c344sQuAIBe0uGR\nkfchM25QgGtGRES9GUNws29/rMTvP94Ph9O91cLMiQNw84T+DMBEREQB4FJd+ODQx/iybDcAwCgb\n8POR9+Mnsf0DWzEiIur1GIIBfHOoAm/9pRBOlzsA3zZpIKbn9A9spYiIiMKUS3XhvaL/wu5T3wAA\nTIoJv7jifvSPTg9wzYiIKBSEfQjeU3QaqzcfgEt1B+A7cwdhylh2skRERIHgdDnx/w+8j2/OfAcA\niNCZ8csrHkRaVEqAa0ZERKEirENwfuEpvL3lAJrzL+6+IRPXjUkNbKWIiIjClMPlwDs//AHfVRQC\nAKJ0kVgwaj76RV4W4JoREVEoCdsQ/I/vyvDu1oNozr+4Jy8L/+8K3mUmIiIKlDX716PQchAAEKOP\nxmOj5qNvRJ8A14qIiEJN2IbgdVvdnawAcO+0wbhmRL/AVoiIiCjMtQTgOEMsHhv1EJLMCQGuERER\nhaKwDcEAIATwwE1DkDOUw6yIiIiCQaIpAQuumI8EU1ygq0JERCEqbEOwLAnMv3korhrMYVZERETB\nYHDc5Zg7ZDZiDTGBrgoREYWwsA3BKx65GnFRhkBXg4iIiJr9ctSDga4CERGFASnQFQgUBmAiIiIi\nIqLwE7YhmIiIiIiIiMIPQzARERERERGFDYZgIiIiIiIiChsMwURERERERBQ2GIKJiIiIiIgobDAE\nExERERERUdhgCCYiIiIiIqKwwRBMREREREREYYMhmIiIiIiIiMIGQzARERERERGFDYZgIiIiIiIi\nChsMwURERERERBQ2GIKJiIiIiIgobDAEExERERERUdhgCCYiIiIiIqKwwRBMREREREREYYMhmIiI\niIiIiMKGUFVVDXQliIiIiIiIiHoCnwQTERERERFR2GAIJiIiIiIiorDBEExERERERERhgyGYiIiI\niIiIwgZDMBEREREREYUNhmAiIiIiIiIKG0qgK9Abvf7669izZ4/n9cMPP4wJEyYAAL744gsUFxej\nsbERu3fvxrp166DT6byuLyoqwsaNG5GamgqLxYKFCxdCUUL/P0VH7XbixAk88MADSEpKAgAMHToU\nv/rVr9pcv3jxYpSUlHheP/fcc8jKyvJ/xQPM13Y7ceIEfv/73yMjIwMnT57EwoULERER0WP1D5TO\n/p4CwD//+U/cfvvteOWVVzB58uRLvj5U+dpu1dXV+O1vf4u0tDSUlpbiySefRGJiYo/UncIX+2Xf\nsH/2Hftq37Hf9h378K4Ln3/hNbZhw4Y2x44fP47t27djyZIlAIApU6ZAlmWvc1RVxdNPP41169Yh\nKSkJy5Ytw8cff4w77rijR+odaO21GwDMnz8ft956a6fXJiUl4Te/+Y0/qhX0fGm3559/Ho899hhG\njBiBDRs2YM2aNXj88cf9Uc2g01G7NTQ04O23377oL2kdXR/qfGm3V155BTk5OZg2bRp27NiB5cuX\nY+XKlf6qKpEH+2XfsH/2Hftq37Hf9h378K5hCO6mN998E3q9Hk6nE3PnzoXJZMLWrVthMpnw7rvv\norq6GuPGjUNmZqbXdcePH0dDQ4PnjuDo0aOxefPmsOls22s3APj73/+Oqqoq1NbWYsaMGRg0aFCb\na61WK958803Isgyz2Yyf/vSnYXOnvrvtZrfbsXv3bgwfPhyA++ftueeeC5uOtaN2e/XVV/Hzn/8c\nzz77bLeuD3W+tNvOnTvxyCOPAHD/vLX3xIPIH9gv+4b9s+/YV/uO/bbv2Id3Tfj9C9VF999/Pyor\nK9scX7BgAfLy8pCSkgKz2YyNGzdiyZIlWLp0KU6ePInS0lI888wzsNvtmDVrFt544w0MGDDAc73F\nYvEa3hIZGQmLxdIj36kndKfd4uPjsWDBAlx++eWorKzE7NmzsWnTJkRHR3uVMWPGDGRlZUFRFKxY\nsQKrVq3Co48+2lNfza/81W5nz56F0WiEEAIAf96WLl2KTZs2YcyYMUhLS+u07I6uDwX+bLfW/8ZF\nRkaipqYGDocjLH8hJm2xX/YN+2ffsa/2Hftt37EP10bofSONrF27tkvnjR8/3nNuZGQkRowYASEE\n9Ho9srKyUFBQ4NXZJiQkwGq1el7X1dUhISFB28oHUHfazWw24/LLLwcAJCYmIjExEQcPHsTYsWO9\nrhk6dKjX9WvWrAmZTtZf7RYXF4eGhgaoqgohBH/eAOzevRsDBgzA6tWrUVZWhm3btsFut+PGG2/0\nuqalbS+8PhT4s91a/o2Ljo5GXV0dYmJiQrLzpJ7Hftk37J99x77ad+y3fcc+XBtcHbobli9f7vnz\n0aNHkZ6eDgDIycnB8ePHPe+VlZWhf//+ANwLHgBAWloajEYjKioqAAD79u3DpEmTeqjmgdVRu23a\ntAmHDh0C4B4SdOrUKaSkpAAAysvL4XQ6O70+1PnSbjqdDuPGjcP+/fsB8OcNAF566SXMnz8f8+fP\nR79+/TBlyhRPJ3DmzBk0NjZ2en2o87XdJk2ahIKCAgDh9fNGgcV+2Tfsn33Hvtp37Ld9xz686+QX\nXnjhhUBXorfZvXs3duzYgR9++AH5+flYuHAh4uLikJ6ejsOHD+Prr7/GF198gSFDhmDGjBlwuVy4\n5ZZbcN111yEmJgajRo3CqlWrUFRUBJvNhgceeACSFPr3Izpqt7Nnz+Ldd9/F0aNHsWnTJtxyyy0Y\nN24cAODBBx/EwIED0a9fP2zZsgXffvst9u3bh4MHD+Lpp5+G2WwO8LfyP1/bbcyYMXjnnXdQXFyM\no0eP4rHHHoNerw/wt/K/jtqtxbp16/Dll1/CarUiMTERycnJWLRoEQAgMzPzoteHKl/bbdSoUfjg\ngw9w8OBBFBQUhM3fUwos9su+Yf/sO/bVvmO/7Tv24V0nVFVVA10JIiIiIiIiop4QPrc5iYiIiIiI\nKOwxBBMREREREVHYYAgmIiIiIiKisMEQTERERERERGGDIZiIiIiIiIjCBkMwkUZ27dqFmTNnIisr\nC3PmzMHdd9+N2267DWvWrIHdbvect3jxYkyYMAGzZ8+Gy+XyHP/ggw+Ql5eHvLw8/OEPf0BBQQFm\nz56NrKwsfPHFF57zmpqaMHfuXFx55ZWYO3cuGhoa2tTlqaeewpVXXonJkydj7ty5nv/l5uZi9+7d\nfm0HrbhcLqxevRrHjh3r9Lyf/vSnyMrKwsyZM3HkyBHP8XXr1uHqq6/G3LlzsX37duzYscPfVSYi\noiDDvllb7JspZKhEpJmvv/5azczMVO12u6qqqlpVVaXOmzdPffDBB1Wn0+k5b+HChWp2dra6du1a\nr+s/+ugj9aOPPvK8Pn78uJqdna1ee+21am1trde5c+bM6bQuc+bMUV955RWvY//xH/+hfv311936\nbj3tjTfeUN96660unTt58mR11apVbY7fd999qqqqqtPpVB966CF1z549mtaRiIiCH/tm7bBvplDB\nJ8FEfhQXF4dly5Zh9+7d2Lx5s9d7999/P1577TUcPXq00zKuv/566PV6LFu2zOf63HzzzcjKyvK5\nHH87ffo0NmzYgHvvvbdL5+fl5eHTTz/1OlZYWIghQ4YAACRJwiOPPIIXXnhB45oSEVFvw765e9g3\nUyhhCCbys6SkJEycOBGfffaZ1/E777wTY8aMwaJFi6CqaofXm81mvPTSS/joo4+Qn5/f7XrMnTsX\nGRkZiI2Nxffff4+ZM2ciNzcXb7/9NubMmYMZM2agpKTEc/6xY8cwb948zJkzB3fddRf27dsHANi+\nfTvy8vIwZ84cLF++HLNnz0Zubi4AYOfOnZgxYwbmzJmDV199Fbm5uZg5cyZWr16N8ePHIzc319MO\nd911FyZOnIhdu3a1qevf/vY3DBkyBAaD4aL1AYCpU6eiqKjIq/6ffvoppk2b5nk9bNgwlJWV4dCh\nQ91uQyIiCg3sm9k3U3hjCCbqASkpKe3On/n3f/93FBUV4Q9/+EOn17fMMVq0aBHq6+u7/LmbN2/2\nzDkqKiryHB8xYgSeffZZnDlzBldccQXee+89jBkzBuvWrQMAOBwOPPTQQ5g2bRree+89LF68GI88\n8gjq6upw3XXXYf78+di/fz9uv/12/OlPf8KUKVNQVVWFxx9/HL/5zW/w3nvvYcSIESgrK8Ozzz6L\n+fPn46GHHkL//v2Rl5cHAPjZz36GJ554AhMmTGhT7x9++AGpqame153VBwCGDx+OtLQ0rzvOBw8e\n9NxtBgBZlpGSkoIDBw50uf2IiCh0sW9m30zhiyGYqAe0XmSjteTkZPzqV7/Cyy+/jJMnT3ZaxpNP\nPgmdToff/va3Xf7cm2++GRs2bMCGDRuQnZ3d5n2z2Ywrr7wSAJCVlYUTJ04AAL777jscP34cM2fO\nBAAMHjwYffv29VoEZMCAAfjJT34CAFi4cCF27tyJhIQEjBo1CgBw3XXXwWw2e86fMWMG9uzZg9On\nTwMAtm3bhilTprRbb4vFgoiICM/rrtQnLy8PW7du9Zw/fPjwNuVGRESgsrKykxYjIqJwwb6ZfTOF\nLyXQFSAKBydPnkR6enq7791xxx34/PPP8W//9m+46aabOizDaDRi2bJlmDNnDqZOnXrJddiwYUOb\nY5GRkZ4/GwwGz0qZLZ3hvHnzPO83NTWhtrbW8zoqKsqrrIqKCsTFxXkdi42N9fw5MTEROTk52Lx5\nM2677TbIsuz1+a2pqgohhOd1V+ozdepUrFmzBocPH8bWrVsxa9asNuUKITod3kZEROGDfTP7Zgpf\nDMFEfnbmzBns2rULv/71rzs858UXX8RNN92EpqYm3HrrrR2eN2rUKNxzzz1YtGiRVyfWVT/++CPi\n4+MRHx/f6XmXXXYZdDqdV+dcX18PSep48EhSUhKqqqq8jlVXV3u9njVrFt566y0YjUZMnz69w7IS\nEhJgtVovqT5Dhw5FRkYGtmzZgsOHD2Pw4MFtyrVarUhISOjwc4mIKDywbz6PfTOFIw6HJvKj6upq\nPPPMMxg7dqxnuFB7+vbti2effRZ79+69aJmPP/44ZFlGRUXFJddn69atOHz48EXPGzlyJJKTk/H5\n558DcM/7efTRR1FaWtrhNZMmTUJVVRW++eYbAO5FOhobG73Ouf7661FWVoYPP/wQEydO7LCsrKws\nlJWVXXJ98vLysH79eowcObJNmaqqoqysrFeswElERP7Dvpl9MxGfBBNpZNeuXVixYgUA4N5774Wq\nqrDZbMjLy8N9993nuTO6ePFi/OMf/8CRI0fw3HPPYcSIEQCAW265Bdu2bfOUV1BQgJdeegknT57E\nCy+84NlCwGAwYNmyZXjqqac6rMuSJUtw+PBhVFZWeq3K+OOPP2Ls2LH48ccfsXTpUlRUVGDx4sW4\n/fbbsXr1alRWVmLFihX413/9V7z11lv49a9/jfXr18PlcuHWW2/F4MGDkZ+f7zl33rx5eOeddwAA\n8fHx+N3vfocXXngBsbGxmDhxIvr06eM1dMpgMCAvLw8mkwmK0vE/PzfccANWr14Nu90OnU4HWZY7\nrE9r06ZNw6pVq9odknbw4EHExcVh2LBhHX4uERGFFvbN7JuJ2iNUDsInIo1UV1d7DQUbNWoUPvzw\nQ88iHQCwcuVKTJkyxfMLRkdWrFiBlJQU3H333ZrU7YknnsCsWbMwadIkTcojIiLqDdg3E7XF4dBE\npJlHH33UM8zq888/R0JCAjIyMgAAmzZtgt1uR1FR0UU7WcDdMZaVleH48eM+12v79u245ppr2MkS\nEVHYYd9M1BafBBORZlauXImvv/4aRqMRQggsWrTIs/3D5MmTERcXh1/84hfIzc0NcE2JiIjCA/tm\norYYgomIiIiIiChscDg0ERERERERhQ2GYCIiIiIiIgobDMFEREREREQUNhiCiYiIiIiIKGwwBBMR\nEREREVHYYAgmIiIiIiKisPF/BaVMMQ54omkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faf556b7eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the regression figure using ``seaborn.regplot``.\n",
    "fig, axes = plt.subplots(1, 2, figsize=[16, 8], sharey=False)\n",
    "\n",
    "def plot_regression(y_true, y_pred, iax, title):\n",
    "  r2 = r2_score(y_true, y_pred)\n",
    "  mae = mean_abs_error(y_true, y_pred)\n",
    "  diff = y_true - y_pred\n",
    "  stddev = np.var(diff)\n",
    "  ax = sns.regplot(x=y_true.flatten(), y=y_pred.flatten(), ax=axes[iax])\n",
    "  ax.set_ylabel(\"DFTB Energy (eV)\", fontsize=12)\n",
    "  ax.set_xlabel(\"DNN Energy (eV)\", fontsize=12)\n",
    "  ax.set_title(\"%s, $R^2$=%.3f, MAE=%.3f, stddev=%.3f\" % (title, r2, mae, stddev), fontsize=12)\n",
    "\n",
    "# Plot the training result\n",
    "plot_regression(energies_train, energies_train_pred, 0, \"Training Data\")\n",
    "\n",
    "# Plot the testing result\n",
    "plot_regression(energies_test, energies_test_pred, 1, \"Testing Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def transform_single(coords, l=4.0, sort=False):\n",
    "  \"\"\"\n",
    "  Transform the cartesian coordinates of the given molecule to input features.\n",
    "  \"\"\"\n",
    "  ntotal, n = coords.shape[:2]\n",
    "  cnkv = comb(n, 4, exact=True)\n",
    "  ck2v = comb(4, 2, exact=True)\n",
    "  cnkl = list(combinations(range(NUM_SITES), 4))\n",
    "  # Using mapping indices can increase the speed 30 times!\n",
    "  indices = np.zeros((ck2v, cnkv), dtype=int)\n",
    "  for i in range(cnkv):\n",
    "    for j, (vi, vj) in enumerate(combinations(cnkl[i], 2)):\n",
    "      indices[j, i] = vi * n + vj\n",
    "  features = np.zeros((ntotal, 1, cnkv, ck2v), dtype=np.float32)\n",
    "  for j in range(ntotal):\n",
    "    dists = pairwise_distances(coords[j]).flatten()\n",
    "    dists = exponential(dists, l=l)\n",
    "    for k in range(ck2v):\n",
    "      features[j, 0, :, k] = dists[indices[k]]\n",
    "    if sort:\n",
    "      features.sort(axis=-1)\n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = 100\n",
    "energy = energies[index]\n",
    "lmat = get_pyykko_bonds_matrix(species, flatten=True)\n",
    "rnd_coords = np.zeros((2, NUM_SITES, 3))\n",
    "\n",
    "rnd_coords[0] = coordinates[index][:]\n",
    "rnd_coords[1] = coordinates[index][:]\n",
    "np.random.shuffle(rnd_coords[1])\n",
    "\n",
    "rnd_features = transform_single(rnd_coords, l=lmat, sort=SORTED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.64338684   7.7286458   10.90121651]\n",
      " [ 10.24809837  10.92098141   7.77294302]\n",
      " [ 10.56604481   7.83291388  12.26269245]\n",
      " [  8.98704243  12.0750351    7.68494606]\n",
      " [ 11.77380371  11.68876362   8.39426231]\n",
      " [ 10.42732525  11.07882118  11.33800411]\n",
      " [ 11.41146946   8.09746456  10.98413944]\n",
      " [ 10.48521709  12.57310677   8.17328739]\n",
      " [ 11.52296448  10.22123909   8.79117203]\n",
      " [ 10.86783886  11.84549332   9.87564278]\n",
      " [ 10.50129795   9.4663229   11.77277565]\n",
      " [  9.1055851    8.52883244  12.27386093]\n",
      " [  9.95213604   9.43879604   8.47302246]\n",
      " [  9.38856125  12.08164024  10.82298183]\n",
      " [  8.40141296   8.94904995  10.91091442]\n",
      " [  9.29124546   8.37332344   9.44932461]\n",
      " [  9.21163368  12.47386551   9.31007004]\n",
      " [  8.23175526  11.13358688   8.67973614]\n",
      " [ 11.10936165   8.80452156   9.55805016]\n",
      " [  8.26894283   9.71574306   9.44312668]]\n",
      "[[  8.40141296   8.94904995  10.91091442]\n",
      " [ 10.48521709  12.57310677   8.17328739]\n",
      " [ 11.41146946   8.09746456  10.98413944]\n",
      " [ 11.77380371  11.68876362   8.39426231]\n",
      " [ 11.10936165   8.80452156   9.55805016]\n",
      " [  8.98704243  12.0750351    7.68494606]\n",
      " [  9.1055851    8.52883244  12.27386093]\n",
      " [ 11.52296448  10.22123909   8.79117203]\n",
      " [  9.38856125  12.08164024  10.82298183]\n",
      " [  9.64338684   7.7286458   10.90121651]\n",
      " [  9.29124546   8.37332344   9.44932461]\n",
      " [ 10.56604481   7.83291388  12.26269245]\n",
      " [ 10.42732525  11.07882118  11.33800411]\n",
      " [ 10.50129795   9.4663229   11.77277565]\n",
      " [ 10.24809837  10.92098141   7.77294302]\n",
      " [  9.95213604   9.43879604   8.47302246]\n",
      " [  8.26894283   9.71574306   9.44312668]\n",
      " [  8.23175526  11.13358688   8.67973614]\n",
      " [  9.21163368  12.47386551   9.31007004]\n",
      " [ 10.86783886  11.84549332   9.87564278]]\n"
     ]
    }
   ],
   "source": [
    "print(rnd_coords[0])\n",
    "print(rnd_coords[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4845\n"
     ]
    }
   ],
   "source": [
    "v1 = rnd_features[0, 0, ...]\n",
    "v2 = rnd_features[1, 0, ...]\n",
    "dists = pairwise_distances(v1, v2)\n",
    "zeros = np.where(dists <= 0.001)\n",
    "print(len(zeros[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8889.23 8889.23\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(rnd_features[0]), np.sum(rnd_features[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0014534]\n",
      " [ 0.2138443]]\n"
     ]
    }
   ],
   "source": [
    "print(scaler.inverse_transform(eval_in_batches(rnd_features, eval_size=2)) - energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv = graph.get_operation_by_name(\"Conv2/Conv2D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = sess.run([conv], feed_dict={X_batch: rnd_features, keep_prob: 1.0, dense_keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
