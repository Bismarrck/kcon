{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Xin Chen's implementation of the MBE-NN network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Author: Xin Chen\n",
    "* Email: Bismarrck@me.com\n",
    "\n",
    "This jupyter notebook is used to repeat the work of http://doi.org/10.1021/acs.jctc.6b00994. \n",
    "\n",
    "The test cluster is $\\textrm{C}_{9}\\textrm{H}_{7}\\textrm{N}$. The reference energies are calculated with DFTB.\n",
    "\n",
    "<img src=\"./C9H7N.png\" alt=\"network\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The structure of the deep convolutional neural network for the $Pt_{13}$ cluster is as follows:\n",
    "\n",
    "<img src=\"./convnet.jpg\" alt=\"network\" style=\"width: 800px;\"/>\n",
    "\n",
    "The input features are transformed interatomic distances. The output node represents the estimated DFT energies.\n",
    "The detailed explanantion of this convolutionary neural network will be given in the following section **Inference**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this section we shall import python modules and declare global constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport models\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import h5py\n",
    "import sys\n",
    "import time\n",
    "import hashlib\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from fnmatch import fnmatch\n",
    "from itertools import repeat\n",
    "from os.path import isfile, isdir, join, basename, splitext\n",
    "from os import makedirs, listdir\n",
    "from scipy.misc import comb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import pairwise_distances, r2_score\n",
    "from itertools import combinations\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework.ops import GraphKeys\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.contrib.learn import ModeKeys\n",
    "sns.set(font=\"serif\")\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The patch size is always 1x1\n",
    "PATCH_SIZE = 1\n",
    "\n",
    "# The HDF5 database file.\n",
    "HDF5_DATABASE_FILE = \"B20.hdf5\"\n",
    "\n",
    "# The dimension of the cluster.\n",
    "NUM_SITES = 20\n",
    "\n",
    "# The total number of structures in the XYZ file should be 209660.\n",
    "XYZ_FILE = \"../datasets/B20pbe.xyz\"\n",
    "TOTAL_SIZE = 209660\n",
    "\n",
    "# Setup the size.\n",
    "LOAD_SIZE = 200000\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# Setup the random seed.\n",
    "SEED = 235\n",
    "\n",
    "# Setup the precision of floats. \n",
    "TF_TYPE = tf.float32\n",
    "NP_TYPE = np.float32\n",
    "\n",
    "# Cuda\n",
    "CUDA_ON = True\n",
    "\n",
    "# Set this to True to make input features zero-centered.\n",
    "ZERO_CENTER = False\n",
    "\n",
    "# The fixed constants: C(N,k) and C(k,2) where k is 4.\n",
    "CNK = comb(NUM_SITES, 4, exact=True)\n",
    "CK2 = comb(4, 2, exact=True)\n",
    "\n",
    "# The pyykko radius (+1) for each element.\n",
    "pyykko = {\n",
    "  'Ac': 1.86, 'Ag': 1.28, 'Al': 1.26, 'Am': 1.66, 'Ar': 0.96, 'As': 1.21,\n",
    "  'At': 1.47, 'Au': 1.24, 'B': 0.85, 'Ba': 1.96, 'Be': 1.02, 'Bh': 1.41,\n",
    "  'Bi': 1.51, 'Bk': 1.68, 'Br': 1.14, 'C': 0.75, 'Ca': 1.71, 'Cd': 1.36,\n",
    "  'Ce': 1.63, 'Cf': 1.68, 'Cl': 0.99, 'Cm': 1.66, 'Co': 1.11, 'Cr': 1.22,\n",
    "  'Cs': 2.32, 'Cu': 1.12, 'Db': 1.49, 'Ds': 1.28, 'Dy': 1.67, 'Er': 1.65,\n",
    "  'Es': 1.65, 'Eu': 1.68, 'F': 0.64, 'Fe': 1.16, 'Fm': 1.67, 'Fr': 2.23,\n",
    "  'Ga': 1.24, 'Gd': 1.69, 'Ge': 1.21, 'H': 0.32, 'He': 0.46, 'Hf': 1.52,\n",
    "  'Hg': 1.33, 'Ho': 1.66, 'Hs': 1.34, 'I': 1.33, 'In': 1.42, 'Ir': 1.22,\n",
    "  'K': 1.96, 'Kr': 1.17, 'La': 1.8, 'Li': 1.33, 'Lu': 1.62, 'Md': 1.73,\n",
    "  'Mg': 1.39, 'Mn': 1.19, 'Mo': 1.38, 'Mt': 1.29, 'N': 0.71, 'Na': 1.55,\n",
    "  'Nb': 1.47, 'Nd': 1.74, 'Ne': 0.67, 'Ni': 1.1, 'No': 1.76, 'Np': 1.71,\n",
    "  'O': 0.63, 'Os': 1.29, 'P': 1.11, 'Pa': 1.69, 'Pb': 1.44, 'Pd': 1.2,\n",
    "  'Pm': 1.73, 'Po': 1.45, 'Pr': 1.76, 'Pt': 1.23, 'Pu': 1.72, 'Ra': 2.01,\n",
    "  'Rb': 2.1, 'Re': 1.31, 'Rf': 1.57, 'Rh': 1.25, 'Rn': 1.42, 'Ru': 1.25,\n",
    "  'S': 1.03, 'Sb': 1.4, 'Sc': 1.48, 'Se': 1.16, 'Sg': 1.43, 'Si': 1.16,\n",
    "  'Sm': 1.72, 'Sn': 1.4, 'Sr': 1.85, 'Ta': 1.46, 'Tb': 1.68, 'Tc': 1.28,\n",
    "  'Te': 1.36, 'Th': 1.75, 'Ti': 1.36, 'Tl': 1.44, 'Tm': 1.64, 'U': 1.7,\n",
    "  'V': 1.34, 'W': 1.37, 'X': 0.32, 'Xe': 1.31, 'Y': 1.63, 'Yb': 1.7,\n",
    "  'Zn': 1.18, 'Zr': 1.54\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Some global helper functions are declared here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def brange(start, stop, batchsize):\n",
    "  \"\"\"\n",
    "  Range from `start` to `stop` given a batch size and return the start and stop of each batch.\n",
    "  \n",
    "  Args:\n",
    "    start: int, the start number of a sequence.\n",
    "    stop: int, the end number of a sequence.\n",
    "    batchsize: int, the size of each batch.\n",
    "  \n",
    "  \"\"\"\n",
    "  istart = start\n",
    "  while istart < stop:\n",
    "    istop = min(istart + batchsize, stop)\n",
    "    yield istart, istop\n",
    "    istart = istop\n",
    "\n",
    "\n",
    "def exponential(x, l=4.0):\n",
    "  \"\"\"\n",
    "  Exponentially scale the input value(s).  \n",
    "  \"\"\"\n",
    "  return np.exp(-x / l)\n",
    "\n",
    "\n",
    "def md5(filename):\n",
    "  \"\"\" \n",
    "  Return the md5 checksum of the given file.\n",
    "\n",
    "  Args:\n",
    "    filename: a file.\n",
    "\n",
    "  Returns:\n",
    "    checksum: the MD5 checksum of the file.\n",
    "\n",
    "  \"\"\"\n",
    "  hash_md5 = hashlib.md5()\n",
    "  with open(filename, \"rb\") as f:\n",
    "    for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "      hash_md5.update(chunk)\n",
    "  return hash_md5.hexdigest()\n",
    "\n",
    "\n",
    "def root_mean_squred(x):\n",
    "  \"\"\"\n",
    "  Return the root mean squared of the given array.\n",
    "  \"\"\"\n",
    "  return np.sqrt(np.mean(np.square(x)))\n",
    "\n",
    "\n",
    "def mean_abs_error(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  Return the mean absolute error.\n",
    "  \"\"\"\n",
    "  return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "\n",
    "def get_time_id():\n",
    "  \"\"\"\n",
    "  Return a string as the ID of this run.\n",
    "  \"\"\"\n",
    "  return datetime.strftime(datetime.fromtimestamp(time.time()), \"%m%d%H%M\")\n",
    "\n",
    "\n",
    "def smooth(x, window_len=11, window='hanning'):\n",
    "  \"\"\"\n",
    "  smooth the data using a window with requested size.\n",
    "\n",
    "  This method is based on the convolution of a scaled window with the signal.\n",
    "  The signal is prepared by introducing reflected copies of the signal\n",
    "  (with the window size) in both ends so that transient parts are minimized\n",
    "  in the begining and end part of the output signal.\n",
    "\n",
    "  Args:\n",
    "    x: the input signal\n",
    "    window_len: the dimension of the smoothing window; should be an odd integer\n",
    "    window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman' \n",
    "      flat window will produce a moving average smoothing.\n",
    "\n",
    "  output:\n",
    "    the smoothed signal\n",
    "  \"\"\"\n",
    "\n",
    "  if x.ndim != 1:\n",
    "    raise ValueError(\"smooth only accepts 1 dimension arrays.\")\n",
    "\n",
    "  if x.size < window_len:\n",
    "    raise ValueError(\"Input vector needs to be bigger than window size.\")\n",
    "\n",
    "  if window_len < 3:\n",
    "    return x\n",
    "\n",
    "  if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "    raise ValueError(\"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\")\n",
    "\n",
    "  s = np.r_[x[window_len-1:0:-1],x,x[-1:-window_len:-1]]\n",
    "  if window == 'flat':\n",
    "    w = np.ones(window_len,'d')\n",
    "  else:\n",
    "    w = eval('np.'+ window +'(window_len)')\n",
    "\n",
    "  return np.convolve(w / w.sum(), s, mode='valid')\n",
    "\n",
    "\n",
    "def get_pyykko_bonds_matrix(species, factor=1.5, flatten=True):\n",
    "  \"\"\"\n",
    "  Return the pyykko-bonds matrix given a list of atomic symbols.\n",
    "  \n",
    "  Args:\n",
    "    species: List[str], a list of atomic symbols.\n",
    "    factor: a float, the scaling factor.\n",
    "    flatten: a bool. The bonds matrix will be flatten to a 1D array if True.\n",
    "\n",
    "  Returns:\n",
    "    bonds: the bonds matrix (or vector if `flatten` is True).\n",
    "  \n",
    "  \"\"\"\n",
    "  rr = np.asarray([pyykko[specie] for specie in species])[:, np.newaxis]\n",
    "  lmat = factor * (rr + rr.T)\n",
    "  if flatten:\n",
    "    return lmat.flatten()\n",
    "  else:\n",
    "    return lmat\n",
    "\n",
    "\n",
    "def plot_pyykko_bonds_matrix(bonds, species):\n",
    "  \"\"\"\n",
    "  Plot the heatmap of the given pyykko bonds matrix.\n",
    "  \"\"\"\n",
    "  if len(bonds.shape) == 1:\n",
    "    bonds = bonds.reshape((len(species), len(species)))\n",
    "  sns.heatmap(bonds, xticklabels=species, yticklabels=species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This convnet is a slightly different from normal convolutionary neural networks. Suppose N is the batch size:\n",
    "\n",
    "* The shape of the input tensor is: $[N, 1, C_{N}^{k}, C_{k}^{2}]$\n",
    "* The **k** defines the many-body expansion. In this paper, k is selected to be 4.\n",
    "* The patch (kernel) is always 1x1.\n",
    "* The padding scheme is 'SAME' and the strides are 1 in both directions.\n",
    "\n",
    "The ``MBE-NN-M`` model and its variants are implemented in **``model.py``**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4. Preparing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.1 Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It is known that Gaussian coordinates are not suitable to direcly used as input of NN. Therefor, two transformations are adapted here on the input sample data in Cartesian coordiantes $\\{r_{j}\\}$:\n",
    "\n",
    "1. Transform each set of combination indices $\\{j\\}$ ($k=4$) to interatomic distances $\\{d_{i,j}\\}$ ($C_{k}^{2}=6$). \n",
    "2. Dump these interatomic distances with the exponential function: $d_{i,j}' = e^{-d_{i,j}/L}$. $L$ is a fixed parameter and 4.0 was used in this paper.\n",
    "\n",
    "The following figure demonstrates the workflow of these transformations:\n",
    "\n",
    "<img src=\"./input_transform.png\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
    "\n",
    "The shape of the final input matrix should be: [1, $C_{N}^{k}$, $C_{k}^{2}$]\n",
    "\n",
    "* The width is 1\n",
    "* The height is $C_{N}^{k}$\n",
    "* The depth is $C_{k}^{2}$\n",
    "\n",
    "So that we can use it in a convolutionary neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def transform_coords(coords, chunksize, mapping, l=4.0, verbose=True):\n",
    "  \"\"\"\n",
    "  Transform the cartesian coordinates to input features.\n",
    "\n",
    "  Args:\n",
    "    coords: a 3D array with shape [M,N,3] representing the cartesian coordinates.\n",
    "    chunksize: the transformed array is too large. So save it piece by piece.\n",
    "    mapping: a `h5py.Dataset`, which is a symbolic to the real data on disk.\n",
    "    l: the exponential parameter.\n",
    "    verbose: print the transformation progress if True.\n",
    "\n",
    "  \"\"\"\n",
    "  ntotal, n = coords.shape[:2]\n",
    "  cnkv = comb(n, 4, exact=True)\n",
    "  ck2v = comb(4, 2, exact=True)\n",
    "  cnkl = list(combinations(range(NUM_SITES), 4))\n",
    "  # Using mapping indices can increase the speed 30 times!\n",
    "  indices = np.zeros((ck2v, cnkv), dtype=int)\n",
    "  for i in range(cnkv):\n",
    "    for j, (vi, vj) in enumerate(combinations(cnkl[i], 2)):\n",
    "      indices[j, i] = vi * n + vj\n",
    "  dataset = np.zeros((chunksize, 1, cnkv, ck2v), dtype=NP_TYPE)\n",
    "  tic = time.time()\n",
    "  if verbose:\n",
    "    print(\"Transform the cartesian coordinates ...\\n\")\n",
    "  for i, inext in brange(0, ntotal, chunksize):\n",
    "    for j in range(i, inext):\n",
    "      dists = pairwise_distances(coords[j]).flatten()\n",
    "      dists = exponential(dists, l=l)\n",
    "      for k in range(ck2v):\n",
    "        dataset[j - i, 0, :, k] = dists[indices[k]]\n",
    "      del dists\n",
    "    batch_size = inext - i\n",
    "    mapping[i: inext, ...] = dataset[:batch_size, ...]\n",
    "    if verbose:\n",
    "      sys.stdout.write(\"\\rProgress: %7d  /  %7d\" % (inext, ntotal))\n",
    "    dataset.fill(0.0)\n",
    "  del indices\n",
    "  del dataset\n",
    "  if verbose:\n",
    "    print(\"\")\n",
    "    print(\"Total time: %.3f s\\n\" % (time.time() - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.2 Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Extract the XYZ coordinates and atomic symbols from the raw file. The raw file is not a standard XYZ file, so we need to write a helper function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_xyz(filename, verbose=True, xyz_format='grendel'):\n",
    "  \"\"\"\n",
    "  Extract symbols, coordiantes and forces (for later usage) from the file.\n",
    "  \n",
    "  Args:\n",
    "    filename: a str, the file to parse.\n",
    "    verbose: a bool.\n",
    "    xyz_format: a string representing the format of the given xyz file.\n",
    "\n",
    "  Returns\n",
    "    species: Array[NUM_SITES], an array of the atomic symbols.\n",
    "    energies: Array[N,], a 1D array of the atomic energies.\n",
    "    coordinates: Array[N, 17, 3], a 3D array of the atomic coordinates.\n",
    "    forces: Array[N, 17, 3], a 3D array of the atomic forces.\n",
    "  \n",
    "  \"\"\"\n",
    "  group = \"raw\"\n",
    "  hdb = h5py.File(HDF5_DATABASE_FILE)\n",
    "  if group not in hdb:\n",
    "    hdb.create_group(group)\n",
    "  \n",
    "  try:\n",
    "    energies = hdb[group][\"energies\"][:]\n",
    "    coordinates = hdb[group][\"coordinates\"][:]\n",
    "    forces = hdb[group][\"forces\"][:]\n",
    "    species = hdb[group][\"species\"][:]\n",
    "\n",
    "  except Exception:\n",
    "    energies = np.zeros((TOTAL_SIZE,), dtype=NP_TYPE)\n",
    "    coordinates = np.zeros((TOTAL_SIZE, NUM_SITES, 3), dtype=NP_TYPE)\n",
    "    forces = np.zeros((TOTAL_SIZE, NUM_SITES, 3), dtype=NP_TYPE)\n",
    "    species = []\n",
    "    parse_species = True\n",
    "    parse_forces = False\n",
    "    stage = 0\n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    if xyz_format.lower() == 'grendel':\n",
    "      energy_patt = re.compile(r\".*energy=([\\d.-]+).*\")\n",
    "      string_patt = re.compile(r\"([A-Za-z]{1,2})\\s+([\\d.-]+)\\s+([\\d.-]+)\\s+([\\d.-]+)\\s+\"\n",
    "                              \"\\d+\\s+\\d.\\d+\\s+\\d+\\s+([\\d.-]+)\\s+([\\d.-]+)\\s+([\\d.-]+)\")\n",
    "      parse_forces = True\n",
    "    elif xyz_format.lower() == 'cp2k':\n",
    "      energy_patt = re.compile(r\"i\\s=\\s+\\d+,\\sE\\s=\\s+([\\w.-]+)\")\n",
    "      string_patt = re.compile(r\"([A-Za-z]+)\\s+([\\w.-]+)\\s+([\\w.-]+)\\s+([\\w.-]+)\")\n",
    "    else:\n",
    "      raise ValueError(\"The file format of %s is not supported!\" % xyz_format)\n",
    "    \n",
    "    tic = time.time()\n",
    "    if verbose:\n",
    "      sys.stdout.write(\"Extract cartesian coordinates ...\\n\")\n",
    "    with open(filename) as f:\n",
    "      for line in f:\n",
    "        if i == TOTAL_SIZE:\n",
    "          break\n",
    "        l = line.strip()\n",
    "        if l == \"\":\n",
    "          continue\n",
    "        if stage == 0:\n",
    "          if l.isdigit():\n",
    "            n = int(l)\n",
    "            if n != NUM_SITES:\n",
    "              raise ValueError(\"The parsed size %d != NUM_SITES\" % n)\n",
    "            stage += 1\n",
    "        elif stage == 1:\n",
    "          m = energy_patt.search(l)\n",
    "          if m:\n",
    "            energies[i] = float(m.group(1))\n",
    "            stage += 1\n",
    "        elif stage == 2:\n",
    "          m = string_patt.search(l)\n",
    "          if m:\n",
    "            coordinates[i, j, :] = float(m.group(2)), float(m.group(3)), float(m.group(4))\n",
    "            if parse_forces:\n",
    "              forces[i, j, :] = float(m.group(5)), float(m.group(6)), float(m.group(7))\n",
    "            if parse_species:\n",
    "              species.append(m.group(1))\n",
    "              if len(species) == NUM_SITES:\n",
    "                species = np.asarray(species, dtype=object)\n",
    "                parse_species = False\n",
    "            j += 1\n",
    "            if j == NUM_SITES:\n",
    "              j = 0\n",
    "              stage = 0\n",
    "              i += 1\n",
    "              if verbose and i % 1000 == 0:\n",
    "                sys.stdout.write(\"\\rProgress: %7d  /  %7d\" % (i, TOTAL_SIZE))\n",
    "      if verbose:\n",
    "        print(\"\")\n",
    "        print(\"Total time: %.3f s\\n\" % (time.time() - tic))\n",
    "      hdb[group].create_dataset(\"energies\", data=energies)\n",
    "      hdb[group].create_dataset(\"coordinates\", data=coordinates)\n",
    "      hdb[group].create_dataset(\"forces\", data=forces)\n",
    "      hdb[group].create_dataset(\"species\", data=species, dtype=h5py.special_dtype(vlen=str))\n",
    "  \n",
    "  finally:\n",
    "    hdb.close()\n",
    "  \n",
    "  return species, energies, coordinates, forces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f1f72f786d8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3IAAAK9CAYAAABhHl40AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f72f4fd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "species, energies, coordinates, _ = extract_xyz(XYZ_FILE, xyz_format='cp2k')\n",
    "ax = sns.distplot(energies)\n",
    "ax.set_xlabel(\"Energy (eV)\", fontsize=14)\n",
    "ax.set_ylabel(\"Probablity\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.3 Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Build the dataset and split it into training, validation and testing datasets:\n",
    "\n",
    "1. Initialize a new hdf5 file if it can not be accessed.\n",
    "1. Extract symbols, energies, atomic coordinates and forces from the file.\n",
    "2. Transform and scale the coordinates to build the 4D array $[N, 1, C_{N}^{k}, C_{k}^{2}]$.\n",
    "\n",
    "**Warning: in this case ($\\textrm{C}_{9}\\textrm{H}_{7}\\textrm{N}$), all samples are considered unique!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def may_build_dataset(filename, l=None, verbose=True, **kwargs):\n",
    "  \"\"\"\n",
    "  Build the training, validation and testing dataset and targets from a XYZ file.\n",
    "\n",
    "  Args:\n",
    "    filename: str, a file with CP2K/XYZ format.\n",
    "    l: a float, a flatten vector or a 2D matrix, the exponential parameter(s). \n",
    "      If None, the pyykko bonds matrix will be used.\n",
    "    verbose: bool, print the building progress if True.\n",
    "    kwargs: additional arguments for extracting xyz files.\n",
    "\n",
    "  Returns:\n",
    "    features: a 4D array as the transformed input features.\n",
    "    targets: a 2D array as the scaled ([0, 1]) targets.\n",
    "    scaler: a ``sklearn.preprocessing.MinMaxScaler``.\n",
    "\n",
    "  \"\"\"\n",
    "  # Compute the MD5 checksum of the xyzfile\n",
    "  checksum = md5(filename)\n",
    "\n",
    "  if verbose:\n",
    "    print(\"-> Load the training, validation and testing datasets ...\\n\")\n",
    "\n",
    "  coords, energies, species = None, None, None\n",
    "  features, targets = None, None\n",
    "  backup_hdf5 = False\n",
    "  extract_coords = True\n",
    "  build_features = True\n",
    "\n",
    "  # If the HDF5 file is already existed, we try to load dataset and targets from\n",
    "  # the HDF5 file directly if the checksums are equal.\n",
    "  if isfile(HDF5_DATABASE_FILE):\n",
    "    with h5py.File(HDF5_DATABASE_FILE, \"r\") as hdb:\n",
    "      if hdb.attrs.get(\"checksum\", 0) == checksum:\n",
    "        # There are two main groups in this HDF5 file:\n",
    "        # 1. the first group is 'train' where training data and training targets\n",
    "        # are stored.\n",
    "        if \"train\" in hdb:\n",
    "          features = hdb[\"train\"][\"dataset\"][:LOAD_SIZE]\n",
    "          targets = hdb[\"train\"][\"targets\"][:LOAD_SIZE]\n",
    "          build_features = False\n",
    "          extract_coords = False\n",
    "        # 2. the second group is 'unique' where uniquified cartesian coordinates\n",
    "        # and their energies extracted from a CP2K/XYZ file are saved.\n",
    "        elif \"unique\" in hdb.keys():\n",
    "          coords = hdb[\"unique\"][\"coords\"][:]\n",
    "          energies = hdb[\"unique\"][\"energies\"][:]\n",
    "          species = hdb[\"unique\"][\"species\"][:]\n",
    "          extract_coords = False\n",
    "      # The checksum are not equal, so we backup the existed HDF5 databse by\n",
    "      # renaming it.\n",
    "      else:\n",
    "        backup_hdf5 = True\n",
    "    if backup_hdf5:\n",
    "      if verbose:\n",
    "        print(\"MD5 checksums mismatched. Build a new dataset.\\n\")\n",
    "      shutil.move(HDF5_DATABASE_FILE, HDF5_DATABASE_FILE + \".bak\")\n",
    "\n",
    "  # Extract the raw cartesian coordinates and energis (eV) from the CP2K/XYZ\n",
    "  # file and save these data into group 'raw'. All data are compressed with the\n",
    "  # lossless gzip filter.\n",
    "  if extract_coords:\n",
    "    species, energies, coords, _ = extract_xyz(\n",
    "      filename, \n",
    "      verbose=verbose, \n",
    "      **kwargs\n",
    "    )\n",
    "    # Remove the duplicates to reduce the dataset\n",
    "    # coords, energies = remove_duplicates(coords, energies, verbose=verbose)\n",
    "    with h5py.File(HDF5_DATABASE_FILE) as hdb:\n",
    "      # Delete the previous group `unique`. This should not happen, but it may\n",
    "      # be inserted manually for debugging.\n",
    "      group = \"unique\"\n",
    "      if group in hdb.keys():\n",
    "        del hdb[group]\n",
    "      hdb.attrs[\"checksum\"] = checksum\n",
    "      hdb.create_group(group)\n",
    "      hdb[group].create_dataset(\"coords\", data=coords, compression=\"gzip\")\n",
    "      hdb[group].create_dataset(\"energies\", data=energies, compression=\"gzip\")\n",
    "      hdb[group].create_dataset(\"species\", data=species, \n",
    "                                dtype=h5py.special_dtype(vlen=str))\n",
    "  elif verbose:\n",
    "    print(\"Use existed coordinates and energies.\\n\")\n",
    "\n",
    "  # Transform the cartesian coordinates to a 4D dataset. Permute this dataset\n",
    "  # several times and then we split it into three parts: training, validation\n",
    "  # and testing. Save these datasets and their targets into group 'cnn'.\n",
    "  if build_features:\n",
    "    # Allocate the disk space and then write transformed data piece by piece\n",
    "    # because my little computer only has 16GB memory.\n",
    "    if l is None:\n",
    "      l = get_pyykko_bonds_matrix(species, flatten=True)\n",
    "    shape = [len(energies), 1, comb(NUM_SITES, 4, True), comb(4, 2, True)]\n",
    "    hdb = h5py.File(HDF5_DATABASE_FILE)\n",
    "    try:\n",
    "      group = hdb.require_group(\"train\")\n",
    "      group.create_dataset(\"targets\", data=energies)\n",
    "      mapping = group.create_dataset(\n",
    "        \"dataset\", shape=shape, dtype=np.float32)\n",
    "      # Set the chunksize to 10000.\n",
    "      chunksize = 10000\n",
    "      transform_coords(\n",
    "        coords, \n",
    "        chunksize, \n",
    "        mapping, \n",
    "        l=l, \n",
    "        verbose=verbose\n",
    "      )\n",
    "    except Exception as excp:\n",
    "      del hdb[\"train\"]\n",
    "      raise excp\n",
    "    finally:\n",
    "      hdb.close()\n",
    "    # After the transformation we now load the whole dataset into memory.\n",
    "    with h5py.File(HDF5_DATABASE_FILE) as hdb:\n",
    "      features = hdb[\"train\"][\"dataset\"][:LOAD_SIZE]\n",
    "      targets = np.array(energies[:LOAD_SIZE], copy=False)\n",
    "    if verbose:\n",
    "      print(\"Dataset size (MB)     : %8.2f\", features.nbytes / 1024 / 1024)\n",
    "      print(\"Targets size (MB)     : %8.2f\", targets.nbytes / 1024 / 1024)\n",
    "      print(\"\")\n",
    "    del coords\n",
    "  elif verbose:\n",
    "    print(\"Use existed features and targets.\\n\")\n",
    "\n",
    "  # Determine the maximum and minimum energy. The energies should be scaled to\n",
    "  # [0, 1] during training.\n",
    "  scaler = MinMaxScaler()\n",
    "  targets = scaler.fit_transform(np.atleast_2d(targets).T)\n",
    "\n",
    "  if verbose:\n",
    "    print(\"-> Datasets and targets are loaded into memories.\")\n",
    "    print(\"\")\n",
    "  return features, targets, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Load the training, validation and testing datasets ...\n",
      "\n",
      "Use existed coordinates and energies.\n",
      "\n",
      "Use existed features and targets.\n",
      "\n",
      "-> Datasets and targets are loaded into memories.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features, targets, scaler = may_build_dataset(XYZ_FILE, xyz_format='cp2k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.4 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this section further data preprocessing procedures will be taken. See http://cs231n.github.io/neural-networks-2/#batchnorm for explanantions.\n",
    "\n",
    "Before preprocessing, let's first demonstrate the value distributions of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1f72f316d8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAAKqCAYAAABLtK0LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3WmMZHl57/nfWWKP3DNr6ayld7rbTdO03RfuhR7K22Xg\nSh40GPkFCJh5gQEhISwbSwbJyJKREALxAnkkpJkLGrUGjRHyjGyw6fG4GPCFdrM0i6uXqu6uJWvJ\nPSNjjxNxzryIOJFZXVlZuUTE2b4fCYmqysr4d8WS5znPZnie5wkAAAAAEBpm0AcAAAAAANyMQA0A\nAAAAQoZADQAAAABChkANAAAAAEKGQA0AAAAAQsYO6oGXl8tBPfRtTU3ltb5eC/oY2AXPUbjx/IQf\nz1G48fyEG89P+PEchRvPz63m5sZu+2dk1LaxbSvoI+AOeI7Cjecn/HiOwo3nJ9x4fsKP5yjceH72\nh0ANAAAAAEKGQA0AAAAAQoZADQAAAABChkANAAAAAEKGQA0AAAAAQoZADQAAAABChkANAAAAAEKG\nQA0AAAAAQoZADQAAAABChkANAAAAAEKGQA0AAAAAQoZADQAAAABChkANAAAAAEKGQA0AAAAAQoZA\nDQAAAABChkANAAAAAEKGQA0AAAAAQoZADQAAAABChkANAAAAAEKGQA0AAAAAQoZADQAAAABChkAN\nAAAAAEKGQA0AAAAAQoZADQAAAABChkANAAAAAEKGQA0AAAAAQoZADQAAAABChkANAAAAAEKGQA2A\nJMlpu3rlainoYwAAAEAEagB6/vmnC/rr//2n+smLS0EfBQAAIPEI1ABIkl691s2m/cOPLsnzvIBP\nAwAAkGwEagAkSQvLVUnSpcWyzl1aD/g0AAAAyUagBkAtp6PF9ZomCmlJ0j/++FLAJwIAAEg2AjUA\nurZaledJTzw4p4dPT+nfL67r0o1y0McCAABILAI1AFpY6pY9npgr6F1vPSVJ+u6zZNUAAACCQqAG\nQAvLFUnSiSNF/cbd0zp1pKjnXlzS0kY94JMBAAAkkx30AQAE72ovUJufLer7v7im08fGdHmpov/6\nnRf0lkeO7vn7vO/3HxrWEQEAABKFjBoALSxXNTOeUT7bvXdz+tiYirmULiyUVG+2Az4dAABA8hCo\nAQm3WWupVG1pfq7Y/z3TNPTI3VPquJ5eurwR4OkAAACSiUANSLirS73+tG2BmiTdf2JCmZSlFy+v\ny2m7QRwNAAAgsQjUgITzF12fOFK46fdty9RDpyfVclydXyCrBgAAMEoEakDC9Sc+vi6jJklvODUl\n2zJ07uK6XNcb9dEAAAASi0ANSLiF5aos09Cx6fwtf5ZNW7r/xIRqjbZeu74ZwOkAAACSiUANSDDX\n83R1paLjM3nZ1s4fB4/cPS3DkP79tTV5Hlk1AACAUSBQAxJseaOuluPqxJFbyx59xVxKdx8b00al\npau9fjYAAAAMF4EakGALS71BIjv0p2336L3TkrpZNQAAAAwfgRqQYFf7g0QKu37d1FhWsxNZLa3X\nGSoCAAAwAgRqQILtNvHx9Yr5lDxJtWZ7yKcCAAAAgRqQYFeWq8plbE2NZe74tYVsSpJUazjDPhYA\nAEDiEagBCdVyOlpar+nkXEGGYdzx6wtZW5JUbZBRAwAAGDYCNSChrq1W5XnS/C4TH7fLE6gBAACM\nDIEakFB7nfjo65c+1il9BAAAGDYCNSChFvY48dFXyJFRAwAAGBUCNSCh/EBtfnZvGbVMypJlGgwT\nAQAAGAECNSChFparmhnP9nvP7sQwDOWzNhk1AACAESBQAxJos9bSZrW157JHXyGbUqPVUcd1h3Qy\nAAAASARqQCJdXer1p+1x4qPPz77VyKoBAAAMFYEakEALy92Jj/P7zqj1BorUCdQAAACGiUANSKAr\nvUEiJ/c4mt/nj+ivMlAEAABgqPY2RUBSo9HQ+973Pr397W/Xn//5n9/0Z67r6stf/rIKhYKuXr2q\nP/zDP9Tjjz8+8MMCGIyryxVZpqGj0/l9/b18jtJHAACAUdhzoPaVr3xFjzzyyI5/9t3vfleVSkV/\n+qd/qo2NDf3RH/2RvvOd78iyrIEdFMBguJ6nqytVHZ8pyLb2l1QnowYAADAaewrU/u7v/k5PPPGE\nXnrpJdVqtVv+/OzZs3rb294mSZqcnFQ6ndb58+f10EMP3fZ7Tk3lZdvhC+Tm5saCPgLugOfocK6t\nVNRyXN1/cnLHf8uxYva2fzed7gZqTce97dfx/IQfz1G48fyEG89P+PEchRvPz97dMVC7cOGCXn31\nVf3Jn/yJXnrppR2/Zm1tTcXiVq9LsVjU2trart93ff3WgC9oc3NjWl4uB30M7ILnaO/OPn91x9+/\nvNj999usNPS3z7y4r+/peZ5sy9BmtaVypbHj1/D8hBvvoXDj+Qk3np/w4zkKN56fW+0WuN6x7umZ\nZ55ROp3W1772Nf30pz/VL3/5S33961+/6Wump6dVqVT6v65UKpqenj74iQEMzXq5KUmaGsvs++8a\nhqFCNkXpIwAAwJDdMaP2sY99rP//m82marWaPvzhD6tWq6nRaGh6elpnzpzRc889p/e85z3a2NhQ\nq9XSAw88MNSDAzgYP1CbPECgJnV3qZWqLTltVymbwbEAAADDsOdhIv/0T/+k5557To7j6O///u9V\nKpX00ksv6a/+6q/0rne9S+fOndNXv/pVXbt2TV/4whcYJAKE1Ea5qbRtKp/Z89v/JoVct0+t1nA0\nUTxYsAcAAIDd7flK7Z3vfKfe+c537vhnpmnqz/7szwZ2KADD0e64KtcczU3lZBjGgb5Hf+l1o02g\nBgAAMCTULQEJUqq05Olg/Wm+fH9EP7vUAAAAhoVADUiQ/iCRQ2TC/IxajYEiAAAAQ0OgBiTIYQeJ\nSDeXPgIAAGA4CNSABNmo+IFa+sDfo1/6WCejBgAAMCwEakCCrJebKmRtpe2DT2VN2abSKVM1MmoA\nAABDQ6AGJESj1Vaj1TnUIBGfv/Ta87wBnAwAAACvR6AGJMRGuSVJmhzASP181la748lpu4f+XgAA\nALgVgRqQEKVqtz9tonjw/jQfA0UAAACGi0ANSIiNSjejNogl1YX+LjUGigAAAAwDgRqQEKVqL1Ar\nHD6jlvd3qdXJqAEAAAwDgRqQEKVKS/msrZR9+Lc9GTUAAIDhIlADEqDldFRvtgeSTZOkQo4eNQAA\ngGEiUAMSwC97HMTER2lb6SOBGgAAwFAQqAEJUKoMrj9NkizTVDZtUfoIAAAwJARqQAL0B4kMYDS/\nr5C1VWu0WXoNAAAwBARqQAKUKoPboebLZ1PquJ6aTmdg3xMAAABdBGpAApSqLWVSlrJpe2Dfs7/0\nmhH9AAAAA0egBsRcp+OqUnMGmk2TpHyOEf0AAADDQqAGxNxmzZGnwQ0S8RWY/AgA+/LK1ZL+z3+5\nIKftBn0UABEwuDooAKE0jP40aVvpI4EaAOzJPz13RT95cUmWaei977gv6OMACDkyakDM9Sc+Fgaz\nQ82Xz1L6CAD7sbRekyR998eX9dr1zYBPAyDsCNSAmOvvUBt0j1rGliFKHwFgLzzP0/JGXbmMJdfz\n9L/9wwuUQALYFYEaEHOlaku2ZfRLFQfFNA3lMraqdTJqAHAnlbqjerOjN5yc0m+/eV5XV6r6v//1\ntaCPBSDECNSAGHM9T6VqSxOFtAzDGPj3z2dt1ZptuSy9BoBdLW3UJUlzkzm977fv0+xEVt/58SVK\nIAHcFoEaEGPVuiPX9TRRHGx/mq+QS8nzpEaTpdcAsJvlXqB2ZCqnbNrW//Tuh+V50v/6Dy/IafMZ\nCuBWBGpAjPX70wY8mt+3NfmR8kcA2M3y+lZGTZIePj2l335iXtdWqvq/fngxwJMBCCsCNSDGNoY0\nmt+XZ5caAOzJ0raMmu99Z7olkN999pJevUYJJICbEagBMbY1mn9YGTVG9APAXiyv12UY0uxEtv97\n2bSt/7lfAnmOEkgANyFQA2KsVGnJMKSx/JBLH+tk1ABgN0sbdU2PZWVbN196PXR6Sr/zxLyur9b0\ndz9kCiSALQRqQEx5vYmP4/m0THPwEx+lraXXNTJqAHBbLaejjUrrprLH7f6wVwL5j89e1ivXSiM+\nHYCwIlADYqre7Mhpu0PrT5OkXMaSaUhVetQA4LaWN24eJPJ620sg/4//5/wojwYgxAjUgJgqVXuD\nRIbUnyZJhmEon00RqAHALnYaJPJ6D52e0sOnp/TqtU2tl5ujOhqAECNQA2KqP5p/SDvUfPmsrXqz\nLddl6TUA7MQfzX/kNhk135vun5Uk/erV1aGfCUD4EagBMTXsiY8+f6BIrUlWDQB2snSH0kffm+6b\nkST94sLK0M8EIPwI1ICY8jNq40MO1PKM6AeAXe2l9FGSjk7ndXQ6r3MX1xnVD4BADYirUrWpQtZW\nyh7u27yfUWNEPwDsaHm9rmIupVzGvuPXvum+GTWdjl66vDGCkwEIMwI1IIZqDUf1ZkeTQ+5Pk6RC\njowaANyO63paKTXumE3z9csfX6FPDUi6O9/aARA511ZrkjTU0fy+vL/0msmPAHCLtc2GOq530yCR\ns89fve3Xd1xPKcvUs+cWdddsXoZx8x7MM4/PD+2sAMKFjBoQQ9dXqpKGP0hE2lb6SKAGALfY6yAR\nn2Uaums2r0rd6Q+FApBMZNSAGLo+woxaJmXJMg1KHwEkzm6ZMd/LV7q9Zmvlxp6+XpLm54q6tFjR\nwnJ1JCXsAMKJjBoQQ9dW/Yza8H/Ad5de22TUAGAH5Vr3JlYxn9rz35mfK0iSri5VhnImANFAoAbE\n0PXVqrJpS5m0NZLHK2RTarQ6ajmMkwaA7cq1bvniWG7vFQ65jK3ZiayWNup8rgIJRqAGxEzL6Whl\nozGS/jSf36e2UqqP7DEBIArKNUe2ZSiX2d+NsxNzBXmedLXXcwwgeQjUgJi5sVaTp9H0p/nyvRH9\nKxsEagDg8zxPlZqjYi51y/TGO5mfK0qSri4TqAFJRaAGxMwo+9N8/YwagRoA9DWdjpyOq7H8/m+c\nTY9nlMvYurpclet5QzgdgLAjUANi5vrK6CY++vxAbZlADQD6/EEiY/sYJOIzDEPzcwU1nY5WNxqD\nPhqACCBQA2Lmup9RG2XpY9YvfeRiAgB8B5n4uN2J3vTHhWWmPwJJRKAGxMz11ZqyaUv5zOjWJPoZ\ntVWGiQBAX+UAEx+3Oz5TkGkYWqBPDUgkAjUgRjquqxtrNR2fye+7cf0wUrYpyzS0tklGDQB8hyl9\nlLqfrUenc1ovN1WtO4M8GoAIIFADYmR5o6GO6+n4TGGkj2sYhnIZW+sEagDQV647MgypmDtYoCZJ\nJ44w/RFIKgI1IEau9/btHJ/Jj/yx81lbG+WmOq478scGgDAq11oqZFMyzYNXONCnBiQXgRoQI/5o\n/rtmR5tRk6RcxpbrSZtVynMAoN1xVW92Dlz26BvLpzVRSOv6ak3tDjfCgCQZ3bQBAEN3fbU7mv+u\nmYJK1dZIH9sfXrJRaWpqbHQ73BAuZ5+/euC/e+bx+QGeBAjWYfvTtpufK+jcxXUtrtUO/b0ARAcZ\nNSBGrq1UZVuGZiezI3/sXHYrUAOApCv3Jj4WD7Ds+vX8PjWmPwLJQqAGxMRGpalLN8q6+/i4LHP0\nb+18xpIklSqjzeQBQBhV/IzaIQaJ+I5M5pSyTS0sVeR53qG/H4BoIFADYuLfXliSJ+ktDx8N5PFz\nGTJqAOAr1wdX+miahuZnC6o22rq6QlYNSAoCNSAmnj13Q6Zh6MmHjgTy+HkCNQDo80sfxwZQ+ih1\n+9Qk6devrg3k+wEIPwI1IAYW12t67XpZj9w9pfHCYC4K9murR43SRwAo1xxl05ZS9mAutabHu0Oa\nVkr1gXw/AOFHoAbEwLPnFiVJb3kkmLJHSUpZprJpSxtlMmoAks11PVXqzqEWXb9eNt29GUYfMJAc\nBGpAxHmep2fPLSplm3riwbnAzmEYhqbGs9oY8VoAAAibasOR5w2mP82XTVsyDI189QqA4BCoARF3\nZami66s1vem+mf5Aj6BMj2dVrrZYygog0bZ2qA2uFN0wDGXTNn3AQIIQqAERF4ayR9/MeFaepE3u\n+AJIsMoAl11vl89Y2qy2GNEPJASBGhBhrufp2RcWlctYeuy+maCPo+mJ7qJtBooASLJy3Z/4ONhA\nLZux1Wq7arQ6A/2+AMKJQA2IsAsLJa1tNvXEg3NK2VbQx9HUWDdQK1GaAyDBhlH6KLGvEkgaAjUg\nwvyyx7c+cizgk3RtZdS4iACQXOWaI9sylE0P9gaaH6hRXg4kwx0nD7iuq49+9KN67LHH5DiOrly5\nos9//vPKZrP9r/n2t7+tb37zm8pkujs+3vve9+o973nP8E4NQO2Oq+deXNJ4PqWHTk8GfRxJ3R41\nSVqn9BFAQnmep3KtpWIuJcMwBvq9c73Aj8mPQDLsaUTc448/ro9//OOSpI997GP63ve+pz/4gz+4\n6Wu+/OUv68SJE4M/IYAdnbu4rkrd0e/+5glZZjiS42TUACRdo9VRu+MNvOxR2l76SKAGJMEdAzXT\nNPtBWrvd1uLiou65555bvu7pp5/W7Oys6vW6PvCBD2hyMhx3+IG4evbcDUnhmPbomxrrZtUJ1AAk\n1bAmPkpbgVqpymcskAR7Xrr0gx/8QF//+td15swZvfGNb7zpz5588kmdOXNG09PT+v73v69PfvKT\n+sY3vrHr95uayssOwfCD15ubGwv6CLgDniOp0Wrr+QsrOjKd11vfNL9jec1YMbvD3xyufDalXMZS\ntdHmeQqxYT43h3nd8Zrp4t8h3LY/Pzu93q+t1rtfN5Uf+Oewq+5nfbPt8TrZBf824cbzs3d7DtSe\neuopPfXUU/r0pz+tp59+Wu9///v7f3by5Mn+/3/rW9+qj33sY+p0OrKs2wdi6+u1Ax55eObmxrS8\nXA76GNgFz1HXv72wqHqzo995Yk4rK5Udv6ZcaYz4VF3jhYxWNuo8TyE17PfQYV53vGb4jAu71z8/\nO73el9erkqSUZQz8c9jtuJKkxdUqr5Pb4D0Ubjw/t9otcL1jY8uFCxd09uzZ/q9PnDihhYUFbWxs\nqFLpXiB+6UtfUrvdliRdvHhR8/PzuwZpAA4nTEuuX2+qmFa55qjdu6AAgCQpD7H00bZM5TKWSvSo\nAYlwx4xaOp3Wt771LZ07d07tdluvvPKKPvvZz+prX/uaJicn9ZGPfESzs7P63Oc+pxMnTujll1/W\nF7/4xVGcHUikWsPRr15d1fxcQSfmikEf5xaTxW6fWqnS0szE6MsvASBI5Zojw5AK2cEHalK3aoEe\nNSAZ7hionTp1Sl/96ldv+f1Pf/rT/f//oQ99aLCnAnBbP31pWe2Op7eGMJsmbQVqG5UmgRqAxKnU\nWypkUzLNwY7m900W0lpaq6ndcWVb4Zj4C2A4eIcDEfPjXtnjf3g4nIHaRLE7kprx0QCSxmm7qjc7\nQyl79E0U0/K0VWIJIL4I1IAI2ag09eLldd03P665yVzQx9nR9owaACRJpT68/jTfeKF7M4zyRyD+\n9jz1EcDBnX3+6oH+3pnH5yVJnufppcsb+s6zl+R50ltCmk2TpMl+Ro2LCADJ4gdqxdzwArXtfcAA\n4o1ADQixzVpL/+1XN/T9569qcb27m+f00TH9x0ePBXyy25tk6XWieZ6nWqOtwhAvVIGwarY6kqRs\neniXVxP9jBqBGhB3BGpAyHiepxtrNZ2/UtLT33tZHdeTbZn6j79xTO94/C49cGJixwXXYTFZ8AM1\nLiKS6LXrm/rhL2/oPz95Usdm8kEfBxipltMN1NKp4XWW9AM1boYBsUegBozYRqWplY2GWk5Hzbar\nltPp/c9V0+mo1myr1ujuJZwopvXgiUnde9e4MmlL11arurZaDfi/YHeZtNXb88NFRBKtlrrP+6vX\nNwnUkDjNXqCWSQ1vl+yEX/pIRg2IPQI1YISctqt/fPayWs7Oy6ANo/sD/t67xvXgyQnNTeZCnT27\nnclihoxaQvk9OgtLFXmeF8nXL3BQzd5n+1ADtX5Gjc9YIO4I1IAReuVaSS3H1b13jevU0aLSKUuZ\nlKm0bSmdsmRbRiwubCeLGV1frclpu0rZDJdNEj9Qa7Q6WtloaG4qnNNJgWHYKn0cXqBWzKdkGgYZ\nNSABCNSAEfE8Ty9eXJdpGPrNN8wpl4nv28+f/FiqNDUb0jUCGA4/UJOky0sVAjUkylbp4/BuUJmG\nofFCivH8QAJwqxsYkasrVW3WHN1z11isgzRp+y417vgmSa3hyGm7Ojadl20ZurJUCfpIwEi1nI5s\ny5BlDffyaqKQUanSkud5Q30cAMEiUANG5IWL65Kkh09PBXyS4Ztg6XUirZQakrpDcO6aLWiz2mKo\nDBKl6bhDLXv0TRTTarVdNXrrAADEE4EaMALr5aaur9Z0bDqv6fFs0McZOpZeJ5MfqBVzKZ08UpQk\nsmpIlKbTGeogER+71IBkIFADRuCFS71s2t3xz6ZJlD4m1fZAbX6uKMMgUENyuK4np+0OdYeab6LI\nLjUgCQjUgCHbrLX06rVNjeVTmp8rBH2ckZgco/QxiVZKdUndQC2btnRkMqfljYbqzXbAJwOGr9Ue\n/g4130SBXWpAEhCoAUP2/eevyXU9PXR6SmYMRu/vxWSBu71JtNrLqBVyKUnSyaOUPyI5/P2YI+lR\nK/jl5QRqQJwRqAFD1O64+n9/tqCUber++YmgjzMy6ZSlfMbmIiJhljcasi2jP5qcPjUkydZo/uEH\nan55OSP6gXgjUAOG6LkXl1SqtPTAiYnELX6eHMtQ+pggnudpdbOuYi7VX9o+lk9rspjuLz8H4mwU\nO9R8470etU1uhgGxlqwrR2CEPM/T9567IsOQ3nBqMujjjNxkMa1qo62Ww/joJKg126o3Oyr2yh59\np46OyXU9XVupBnQyYDT8z7qRlj7SowbEGoEaMCTnF0q6dKOsJx6Y01g+HfRxRq4/+ZELiURY2bi5\nP81H+SOSotnqZo1HUfqYSVnKZSyVyKgBsUagBgzJMz+5Ikn6/SdPBnySYPR7KCh/TITto/m3mx7P\nKJ+1tbBcket6QRwNGIlR9qhJ0ngho0161IBYI1ADhmBlo66fvbys08fG9MCJ5AwR2W6iyFSyJFnd\nNpp/O8MwdPJIUS3H1dJ6PYijASOxVfo4mkuriUJa5Zqjjkv/JxBXBGrAEPzzzxbkedJ//q2T/cEK\nSTPllz6WueObBLfLqEmUPyIZRp1Rmyym5UnarDojeTwAo0egBgxYvdnW//eLa5oopPXkw0eCPk5g\n+j1qlD4mwkpp5x41STo6nVfKNnV5sSzPo/wR8TTKPWqSNO7vq6T8EYgtAjVgwP7br2+o3uzod56Y\nl20l9y022S995CIiCVZKDWXS1o6jyS3T0PxcQdVGW+tkWBFTTacj0zBkW6OpotjqA6a8HIir5F5F\nAkPy/PllSdJ/96a7Aj5JsOhRSw5/h9rsRPa2pb6UPyLumk5HmbQ5snL3iX5Gjc9YIK4I1IAB6riu\nLlzd1PGZvCZ6dzuTKmVbKmRtMmoJ4O9Qmx3P3vZr5ucKMg0CNcRXy3FHVvYobQvU+IwFYotADRig\ny4sVNZ2O3nAyeQuudzI5liGjlgD+DrXZidxtvyZtWzo6ndfaZlOVOsMPEC+e56nldEY2SERS/2Yg\nGTUgvgjUgAF66fKGJOkBAjVJ3R6KerOtZqsT9FEwRP4gkZmJ22fUJOnU0W754wJZNcSM03blaXSD\nRCRKH4EksIM+ABAn5xe6gRoZta7+QJFqU0fT+YBPg2Hxd6jNTmRVadw+W3bySFHPnlvS5aWKHjo9\nNarjAUO3NZp/+Pe/zz5/VVI3i2cY0uXFcv/3dnPm8flhHw3AgJFRAwbE9TydXyhpZjyr6V16dZKE\nqWTJ4GfUZid3f93nsylNjWW0tFZjTD9ipdkbzT/K0kfDMJRN26o3qVgA4opADRiQ6ytVVeqOHjw5\nEfRRQoNdasnQD9R26VHz5bO2XE9qdwjUEB+tXkZtlKWPkpTLWKo329z4AGKKQA0YkJcXSpKkByl7\n7OuXPrI7K9b8HWqF7J2r6VN298dOq00WAPExytLH7XIZWx3Xk9NxR/q4AEaDQA0YkJevdPvTCNS2\nbGXUKH2Mq73sUNsu3QvUnDYXloiPZmAZte7NkQblj0AsEagBA+B5nl6+sqGxfErHphma4aP0Mf72\nskNtu5TdvZB1HAI1xEcrgB41Scqlu49Xa7ZH+rgARoNADRiAlVJD6+WmHjwxuaesQlJM+KWPBGqx\ntZcdatul+6WPBGqID38FycgDtV5GrU6gBsQSgRowAJQ97sy2TBVzKUofY2yvO9R8qX7pI6VaiI+t\nYSKj71GTKH0E4opADRgAArXbmyxmyKjF2PYdanuRIqOGGNoaJjL6qY8SpY9AXLHwGtij3RaK/uLC\nilKWqVeul/Tajc0Rnir8JsfSWliuqNFqK5vmIydu9rpDzecPW2CYCOKk6bgyjK0bEaOylVEjUAPi\niIwacEj1ZlubNUdzUzmZ9KfdgqXX8bafHWqSlLKY+oj4aTkdpW1r5D3K/s2veotADYgjAjXgkJbW\nu6VfR6f2dqGaNJMMFIm1/exQk6RUij1qiJ+m0xn5DjWpm8FLWabq9KgBsUSgBhzS4npNknRkmkBt\nJ35GbZ1ALXb8HWpze9yhJrFHDfHjeV43ozbi/jRfLmMx9RGIKQI14JAW1+oyTWPPwxSSpr9LrUzp\nY9z0d6jtsexR2j71kUAN8dDueHI9KZMOKlCz1Wh15LpeII8PYHgI1IBDaDkdrZebmpvIyjJ5O+2E\npdfx5e9Q2+tofklK9xZet1h4jZgIauKjrz9QpEX5IxA3XFkCh7C00e1PO0J/2m35PWqlKhm1uNka\nJLL3QM00DVmmQUYNsRHUDjUfS6+B+GJWNnAIS2u9QSLT+YBPEg7/+KOLKlcaN/2eX45z8frmbVcc\nnHl8fthHwxCs7HOHmi9lmwwTQWwEnVHL9napEagB8UNGDTiExfWaDElzk2TUbsc0DWXTFgtZY2i/\no/l9adt4S8JHAAAgAElEQVQko4bY8Mt4gxomks8woh+IKwI14IDaHVerpYamx7MjX3IaNbmMrXqz\nLc+j2T1OVkv771GTpFTKIlBDbASeUfN3qTGiH4gdri6BA1opNeR69KftRT5rq93x5HS4OI+TlVJd\n2X3sUPOlbFMd11OHKXWIga1ALageNUofgbgiUAMOaGmtuz/tKPvT7qjf7N7gjm9ceJ6nlVJDs/vY\noebb2qXG6wHRtzVMJNipjwRqQPwQqAEHtLjOxMe9ynMhETvVRluN1v52qPnYpYY4afZ61IIrfbRk\nGHy+AnFEoAYcgOt6Wt6oa6KQ7vcH4Pb8O761phPwSTAoB+1Pk9ilhngJOqNmGIayaZseNSCGCNSA\nA1grN9TueJQ97lG+18NUa3DHNy4OOppfIqOGeGm2gt2jJnX71BotBjYBcUOgBhzA4ppf9sj+tL3w\nSx8J1OLjIMuufX6PGrvUEAdNp6O0bcrcZ6/mIOUyDGwC4ohADTiApV5/2lH60/akn1GjhyI2DrpD\nTSKjhnhpOW5gZY8+v7y8QfkjECsEasA+eZ6nxfWaCllbhVwq6ONEQjZtyTTIqMXJYXrUUr2L2haB\nGmKg6XQCGyTiy6UZ0Q/EEYEasE+lSkstx9XRacoe98owDOUyNhm1GDnoDjVp+3h+AjVEW7vjquN6\ngfanSdsHNvEZC8QJgRqwT0sbvf60Scoe9yOftVVvtuXS7B55h9mhJm0vfaRMC9HWCng0v4/SRyCe\nCNSAfVrZ6JZ8zU3tv+QryfLZlDyPC4k4OMwONYkeNcRHszeaP5MOOlDrPj4ZNSBeCNSAfVreqMu2\nDE0UM0EfJVLylObExmH60yT2qCE+gt6h5tvKqPH5CsQJgRqwDy2no1K1pdmJXKCjmKNoa5caS6+j\n7jA71CQyaoiPfkYt4B61bLr7+VpvEagBcUKgBuyDP5J8bpKyx/1il1p8HGaHmiTZliHDYOojoq8Z\nkh61lG0qZZmqU1oOxAqBGrAPy71BIrMMEtk3dqnFx2F2qEndKaAp22SYCCIvLKWPUrdPjfH8QLwQ\nqAH74A8SOWgmIcm2Sh+5kIi6w/aoSVLKMsmoIfLCUvoodfvUGq2OXJfJukBcBP/JAkSE53laLtU1\nlk/1G7exd+z5iY/D7FDzpVMWPWqIvDBl1LL+QJEWmWogLgjUgD3arDpqOS7ZtAOyLVPplKk6GbVI\nO+wONV+39NGVx149RFhYetQkKddbEcBAESA+CNSAPfIn3c3Rn3Zg+YxN6WPEHXaHmq8/+bFDVg3R\n1QxRRs1/T7XJVAOxcce6Fdd19dGPflSPPfaYHMfRlStX9PnPf17Z7FZWodls6gtf+IKOHj2qixcv\n6iMf+YjuueeeoR4cGDUGiRxeIZvSRqUlp+32LyoQLddWqpKkI1OHex+kt43o9/eqAVHTcjqyLUOW\nGfy6FtvqBWodstRAXOzpSunxxx/XJz7xCX3qU59SvV7X9773vZv+/Bvf+IaOHz+uP/7jP9aHP/xh\nfeYznxnKYYEgLW80ZJmGpsZYdH1QOXapRd6lxbIk6fTRsUN9n35GjaXXiLCW44ai7FHaHqjxngLi\n4o6Bmmma+vjHPy5JarfbWlxcvCVbdvbsWb35zW+WJL3hDW/Qiy++qEqlMoTjAsFotjraKDc1M5EN\nxZ3TqMozUCTyLvcCtVPHDhuodS9umfyIKGs6nVCUPUqSbXd/NhGoAfGx55FdP/jBD/T1r39dZ86c\n0Rvf+Mab/mx1dVWFQqH/62KxqNXVVRWLxdt+v6mpvOwQlrvMzR3u4gPDF8Rz9KsLK/Ik3TVX1FiR\nYSK72e3fZ7rX1+TKuOnreN+N1mH+va+t1pROWXrjG47ueNNir++PsUJakmSnrP7f4XXQxb9DuPnP\nT7vjymm7ymdTofi5MFboVnvYtrXjeZL0ukrSf2sU8fzs3Z4DtaeeekpPPfWUPv3pT+vpp5/W+9//\n/v6fzczMqFqt9n9dqVQ0MzOz6/dbX68d4LjDNTc3puXlctDHwC6Ceo5+eu66JGkin1K50hj540fF\nWDG767+PaXR7J9Y26irPbH0d77vROcx7yGm7unyjrNPHxrS2unPVxF7fH27vrn+p3FC52A3aeB3w\ncyjstj8/m7WWJMk29/66HyanN9ikXGvteJ6kvK54D4Ubz8+tdgtc71j6eOHCBZ09e7b/6xMnTmhh\nYUEbGxv98sYzZ87o5z//uSTppZde0kMPPbRrNg2ImlevbUqS5iaDv2saZZQ+RtvVlYo6rnfo/jRJ\nSqe2hokAUVStd3ttQ1P6aPVKH3lPAbFxx4xaOp3Wt771LZ07d07tdluvvPKKPvvZz+prX/uaJicn\n9ZGPfEQf/OAH9YUvfEF/8zd/o8uXL+uv//qvR3F2YCQ8z9Mr1zaVz9rKZ1NBHyfS8v1hIgRq+3H2\n+asH/rtnHp8f2DkuL3Zvzp06evgbcX6PGoEaoqra+xwLyzCRFMNEgNi5Y6B26tQpffWrX73l9z/9\n6U/3/382m9Vf/uVfDvZkQEislhrarLZ0+pDDE9C9oDFNg0AtovyJj6cGkFHzpz4yTARR1c+opcMR\nqDGeH4gfFhkBd/CKX/Y4QdnjYRmG0V163WQ8/7DVGo48b7AXbJdvlGWZhk7MFe78xXfQ36PW66sB\noqbaWzOSSYXjUorx/ED8hOPTBQixV66WJLHoelDyWVuNZkeuy13fYXA9Tz97eVnfOvuqnj+/Mrjv\n63q6slzR8ZlCv2zxMFI2PWqItmo9XKWPjOcH4mfPUx+BpHrl2qYs09D0OIuuByGfseVJqrfaKtDz\nN1Atp6Mf/PK6ri53p/C+fKWkdsft32k/jBtrNbUcV6ePDWZQFHvUEHV+Ri08w0QofQTihowasAun\n3dHlxbJOHS0O5GIXDBQZlo1KU9/50SVdXa7qrtm87p+fUNPp6BcXVgfy/QfZnyZtK30kUENEhS2j\n5u81JKMGxAdXnsAuLi12x5Hfd9dE0EeJDQK1wbuyVNF3f3RZmzVHv3HPtH7nN0/o4bunJEn/+qvr\nA3mMy71AbRCj+SXJNA3ZliGnTY8aoilsPWqG0X1PEagB8UHpI7CLV3v9affOj6vR4oJyENilNjie\n5+lXr67p+fMrskxDTz12XPfcNS5JmhrLaGY8o1++sqpSpamJ4uFKd/3R/CePDG5HZso2KX1EZFVC\nVvoodcsf2aMGxEc4bgMBIXWhN/GRjNrgkFEbDKft6vvPX9Pz51dUyNr67996qh+k+e6bn5DrefrR\nvy8e6rE8z9OlG2Udncoplxnc/b2UbVH6iMiq1tu9zHB4LqVsy6RHDYiR8Hy6ACH06rWSxvMpzTKa\nf2Dyme4AkVqDEf2H8S8/v6rLixUdncrpv/yn05oZv/U1es/xcdmWoR/+6vqhRvWvlhqqNdsD60/z\npcmoIcKqDSc0ZY8+Sh+BeKH0EbiN9XJTa5tNPX7/rAzDCPo4sZHLUvp4WJWaoxurNR2Zyun3nzwp\n09z59ZlJW3rzA3N67sUlvXa9rHtfl3Hbzdnnr/b/v9+f1nHdm37/sFK2Kdf11HFdWWa4LniBO6nW\nndAMEvF1M2oEakBc8JMRuI1Xr3X70+6b3/vFLe7MMg1l0xalj4dwZanbL3bvXeO3DdJ8b3/suCTp\nh4cYKrK62ZQkTe+QtTsMdqkhqlzPU63RDlV/miTZtinXkzrsqQRigUANuI1X6E8bmlzGVr3ZPlQ5\nXpL5gdqJuTsP9viNu6c1NZbRs+cW1XIONhBnbbMhSQPfJZj2d6k5BGqIlnqzLU/hGc3vS/V3qfGe\nAuKAQA24jVevlmQY0t3HB9uXg+5AkXbHI5NyAI1WR4vrNc1OZPuDWXZjmob+06PHVG+29bPzywd6\nzLXNpvJZW9n0YKvlyaghqqp1f+JjuC6jbItdakCchOsTBgiJdsfVxRtlnZgrDvziFFKByY8HdnW5\nIs+TTh3d+5j8t72xW/74r7/cf/ljvdlWvdnW9Nhgs2kSgRqiq9oI17Jrnz+Bst2mWgGIAwI1YAcL\nyxW12q7um6fscRjYpXZwftnjySN7z/Qem87r/hMTOndxXUvrtX093tqQ+tOkrWxEi6XXiBg/oxba\nQI2MGhALBGrADl656venMUhkGHJZf0Q/gdp+tDuuri5XNVFIa6KY3tffffsbj8uT9C8/ubKvvzes\n/jSJjBqiK4zLriVKH4G4IVADduBPfNzPOHPsHRm1g7m+WlPH9XTyyN7LHn1PPnRE6ZSpf37uyr6G\nuGwFakPIqPnDRAjUEDHVul/6GK7LqK2MGqWPQByE6xMGCIlXrm2qkLV1dDof9FFiKd/vUWPp9X74\n+8xO7qM/zZfL2PqtNxzR9dWqXr6ysee/t1ZuKp0y+32Fg0RGDVFVDWtGzab0EYgTAjXgdUqVppbW\n67r3rgmZLLoeijzDRPbN9TwtLFWVy1ianThYduvtb9zfTrWW01G55mhmPDuUpe9+oHbQtQFAULYy\naiEL1OhRA2KFQA14nfML3bLHB08ySGRY0rYpyzQofdyH5fW6mk5HJ48UDxw0PXhqUken8/rJi8tq\ntO78b79W9geJDL4/Teq+DiQyaogeP6MWvkCt+9ngEKgBsUCgBryOXxb2wInJgE8SX4ZhKJ+1yajt\nw0GmPb6eaRj63SdPqel09NyLS3f8+n5/2tjg+9MkKdXrUSNQQ9T096ilw3UZlaJHDYiVcH3CACHw\n8sKGbMvUPccZJDJM+YytRqujjssFxZ14nqfLixWlLFPHZnKH+l6/+1snZWhvO9WGOZpf2lb6SKCG\niKk22jINox8YhcXWHjXeU0AchOsTBghYvdnWlaWK7j0+1r+IxHD4fWp1yh/vaKPSUqXuaH6uIMs8\n3OvyyHReD52e0ssLpf5wkttZ22zItgyNF1KHeszbsS1DhiE57FFDxFQbjgo5eyi9m4fBeH4gXrgS\nBba5cLUkz5MeOEnZ47Dl2aW2Z1cOMe1xJ+96yylJ0v/yd7++baDc7rgqVVuaGssM7WLUMAylbJPS\nR0ROte6okB3ODYzDYJgIEC8EasA2fn/agwRqQ8cutb27vFSRaUjzs4WBfL9H753Ru95ySovrdf3X\n77yw4161jUpTnje8skdf2rYofUSkeJ6naqOtQm7wKysOa2s8PyXlQBwQqAHbnL+yIcOQ7p9n4uOw\nsUttb9Y2G1rbbOrodH6gO5v+x3fcqwdPTuonLy3rmZ8s3Pq4peH2p/nIqCFq/N7acGbUKH0E4oRA\nDehx2q5evV7WySNF5TLhu1MaN+xS25ufn1+RJJ0aUNmjzzJNffR/+A2NF9L623+5oAu9tRS+tXJv\n4uOQRvP7/EBtp6weEEb+aP5wBmqUPgJxwtUo0PPa9U21Oy5j+UeE0se9+dnLy5Kkk0cOFqidff7q\nTb8eK2ZVrjT6v37rI0f1zHNX9JW//YX+y3863b9JsbbZlGlIk8XhBmrsUkPU+Muuw1j6aJl+Ro0b\nH0AckFEDes4v0J82Sn5AUCejdlvVhqOXLm9odiLbH74yaMdm8nr8wVnVmm398JfX5XqeXNfTermp\nybFM/8JvWFIEaogYP6NWDGFGzTAM2ZZBRg2ICQI1oOd8r/TrwRP0p42CaRrKZSxVCdRu65cXVuV6\n3oGzaXv16D3TOjFX0PXVmn55YVWlaksd1xvaouvt/KXXDBRBVPifWYVc+AI1qVv+yI0PIB4I1ABJ\nruvp/EJJR6ZymhhyqRe25DO2as02/Um38bPzvbLHAfenvZ5hGHrbY8dVzKX0y1dW9etXVyUNvz9N\novQR0VOt+z1q4St9lLpZakofgXgI56cMMESv79mRupP16s227prN7/jnGI5cNqXVzaaqjbaKIb07\nHRSn3dGvX13T0em8JgrpoT9eJmXpHY/fpe/++LJeu97d2zbsiY+SlEr5gRpLrxEN/WEiuZQaTvhe\nt7ZlqtFimi4QB2TUAElL63VJ0tGpfMAnSRZ/oMh6uRnwScLn3MV1NZ2O3vzA7NAWTr/ezERW/+Hh\nI/1fT40NP6Pm96hR+oio6A8TCWGPmiR61IAYIaMGSFrsBWpHpnIBnyRZ/NKh9XJz6H1YUfNvLyxK\nkp54YE4LK5WRPe4DJydUaTjqdLx+EDVM6V6PGqWPiIpKP6MWzkso2zLleVLH9YY+DAjAcIXzUwYY\nIc/ztLReUy5jaSwfzjukceXvUtuokFHbrlJ39NyLyzo6ldN98+MjDdQMw9ATD86N7PHSZNQQMVs9\nauH8ebF9l5plWgGfBsBhUPqIxKvUHdWbHR2Zyo+sxAxdOUofd/SjX99Qu+PqHY/Px/412R/PH8Je\nH2An1UZbhrZKt8PGtvxdatz8AKKOQA2Jt7hG2WNQ8lkCtdfzPE/f/8U12Zaht73xWNDHGTr2qCFq\nqg1H+awtM6Rlhf2MGu8pIPII1JB4W4NECNRGjdLHW51fKOnaSlVPPDinsfzwpz0GLc0eNURMte6E\ntuxR2l76yIh+IOoI1JB4i+s1pWxTkyOYcIebpW1LtmWQUdvm+89fkySdeXw+4JOMBhk1RE210e7f\nZAoj297qUQMQbQRqSLR6s61yzdGRyZzMmPcChVU+myJQ6+kOEVnS0amc3nBqMujjjASBGqKk0WrL\nabuh3vtIjxoQHwRqSLQlxvIHLp+xVak7LDxWsoaI+EzTkG0ZavH8IwIqta1l12FF6SMQHwRqSLTF\n9Zok6cg0gVpQ+gNFKq2ATxKspA0R2S5lm2TUEAnlWvdzqhiJHjXeU0DUEagh0ZbW6zJNQ7MT2aCP\nklj+iOuNhJc/Jm2IyHZp2yJQQyT4gVpYl11LW6WPvKeA6CNQQ2K12h2tbzY1O5GVZfJWCAoj+rv8\nISLvSMgQke1StqmWw0Ulwq9cC/eya0lKkVEDYoOrUyTW8npDnhjLHzQCtZuHiDyUkCEi26VsU67n\n0aeI0Kv4pY/0qAEYAQI1JNaS3582lQ/4JMnGLrVkDhHZLt2b/FhvEqgh3DarESh9tJn6CMRFeD9p\ngCFbXK/LkDQ3RX9akPKZ7p3pJGTUzj5/9Zbf8zxP33n2Unc9hOHt+DVxl0p1l17Xm22NF5LVn4do\nqUSg9JFhIkB8kFFDInVcVyulhibHMkrbVtDHSbRsxpJpGFpPaEZtaaOuUqWlU0eLyqaTee/M76mp\nNdsBnwTYXZnSRwAjRKCGRFopNeS6Hv1pIWAahiaK6cROfTx/pSRJevBk8nrTfOlU90dRg0ANIbc1\n9THMgRqlj0BcEKghkZbWeouup+lPC4PJYkYblaZcL1l3gJutji7eKGssn9LRBO/yS9l+Ro0eNYRb\nuebI0NZakTDyM9SM5weij0ANibS43g3UyKiFw/R4Ru2O1x99nRSvXtuU63p68ORkIoeI+FK98uNG\ni4wawq1SaymftWWa4X2/mqYhQ5Q+AnFAoIbE6bieltZrmiiklQvxXdEkmRnvDnRZ22wEfJLR8TxP\nLy9syDQM3Tc/HvRxApW26VFDNJRrrVAPEpEkwzBkWyalj0AMEKghcVZKdbU7no7NUPYYFtO9QG21\nlJxAbXWzkfghIr5Ufzw/gRrCy/O6Wf8wj+b32bZBoAbEAIEaEufGand/2nECtdCYGc9ISlZGbbHX\nJ3nyaDHgkwTPz6g16FFDiLXarpy2G+pBIr5uRo3SRyDqCNSQOH6gdpRF16HRz6htJmfy41KvT3Ju\nkj5Jv0eN0keEWbXe7aEthrz0URKlj0BMEKghUZpOR8sbDU2PZ5RJsz8tLJLWo+Z5npY36spn7VDv\nYxoVSh8RBZV6+Jdd+2yrW/roJWySLhA3BGpIlAsLJbmeR9ljyIzlU7ItU6sJCdTKNUeNVodsWo+/\nR63O1EeEWLXRfX1GokfNMuV5StzKEyBuCNSQKC9cWpckHZsuBHwSbGcYhmbGM4nJqC1v9Pb4EahJ\nkizTkGGQUUO4+aWPUelRk6R2m0ANiDICNSTKC5fWZRjSEfanhc70eFabNUctJ/4DJfxAbY7XoaRu\noJ6yTdUZJoIQqzSi1KPW3fNGnxoQbQRqSIxaw9HFG5uancj1e2IQHjMT3T619XL8B4osrddlW4am\nxzJBHyU00rZFRg2hFsmMGoEaEGlcrSIxXrqyIc9jLH9YzfQnP8a7/LHldLRRaWlmIivTNII+Tmh0\nM2oEagivaj1aPWqSGNEPRByBGhKj359GoBZK071danEP1JY3uv999KfdLG2barQ6cl0uLBFOkSp9\n7FWNOGTUgEgjUENivHhpXSnb1NxkNuijYAf9jFop7oEa/Wk78cuRGy361BBOUSp9TNGjBsQCgRoS\nYbPa0sJyVQ+cmJBl8rIPo61davHuUWPR9c7Sqe5eQ8ofEVbVuiPDkPIZSh8BjAZXrEiEFy93yx4f\nPj0V8ElwO1Nj8S99dF1PK6W6JoppZVIsXN+OpdcIu2qjrUI2FYne0q3x/GTUgCgjUEMi+P1pD5+e\nDvgkuJ10ytJ4PhXrXWrr5abaHY9s2g76gRpLrxFSlYajsUI66GPsCeP5gXggUEMivHBpXbmMpdPH\nikEfBbuYHs9qdbMpz4tnuQ6Lrm8vTUYNIeZ5nqp1R2P58PenSYznB+LijoXWly9f1le+8hU98sgj\nunHjhiYnJ/WJT3zipq/59re/rW9+85vKZLqlS+9973v1nve8ZzgnBvZptdTQ0npdj98/S39ayM2M\nZ3XxRlnlmqPxiNy53g+/P42F67fyM2o1AjWEUMtx1e54Kuaj8blEjxoQD3cM1DY2NvTud79bv/d7\nvydJeve7360zZ87o0UcfvenrvvzlL+vEiRPDOSVwCH7Z40P0p4Xe9LZdanEM1JY36sqkrMjclR+l\nlN3t2Ws0mfqI8Kn2RvOPRyVQs8moAXFwx0Dtscceu+nXrusql7v1bvDTTz+t2dlZ1et1feADH9Dk\n5OSu33dqKi/bDl8z/dzcWNBHwB3s9zl6bfFlSdLb3nxCc3NjGisynn+YDvLv6z+np+6akHRFbRmx\ney+ubNRVbbR19/FxjY8Fm1EL43tgYqwlSTJtK3bP/X4l/b8/jMqtbsBTzKduen7C+F6SJKcXnxmm\n2T9jkl5XSfpvjSKen73b14zZZ555Rm9/+9t133333fT7Tz75pM6cOaPp6Wl9//vf1yc/+Ul94xvf\n2PV7ra/X9n/aIZubG9PycjnoY2AX+32OPM/T8y8vayyfUs6SlpfLKlfiO6wiaGPF7IH+ff3nNNOr\nTH1tYUMPHI/XB/m/vbAoSZoeSwf6GjzoczRsbadb8ri8Vk305zA/h8Jp4dqGJGksn77p+Qnje0mS\nms1uBrDecPpnTMrrivdQuPH83Gq3wHXPDTs//vGP9eyzz+ov/uIvbvmzkydPanq6O03vrW99q557\n7jl1OpSvIHg31mpaLzf10KkpmUb4Ryon3cyEv0stnBc/h3FhoSSJRde3k071Fl5T+ogQqja6NxLG\nolL6yDARIBb2FKidPXtWP/zhD/WZz3xGy8vL+vnPf66NjQ1VKhVJ0pe+9CW1290PsYsXL2p+fl6W\nFb6yRiTPi5fYnxYl23vU4ubC1ZJMQ5odD2epVND8HjWGiSCMKvVuhioq/aWp3nh+hz1qQKTdsfTx\n17/+tT71qU/p0Ucf1Qc/+EHVajW9//3v1zPPPKPJyUl95CMf0ezsrD73uc/pxIkTevnll/XFL35x\nFGcH7qi/P+1uArUoGMunZFtm7DJqzVZHlxcrmh7PyrKYPLoTf+pjgz1qCCF/mEhUpj6apiFDTH0E\nou6Ogdqjjz6qn//857t+zYc+9KGBHQgYFNfz9OLlDU2PZ9hbFRGmYWh6PKPVzWbQRxmo165vyvU8\nxvLvIsUeNYRYtd59XUZlGq1hGLJtk9JHIOK4tYvYWliqqFJ39PCpKRn0p0XGzHhWm9WWnHZ8epXO\nX+31p3HD4LZMw1AmbVH6iFDySx+LESl9lCTbMgjUgIgjUENssT8tmqbHM5KktXJ8smqv9AI1Mmq7\ny6UthokglPzSx6gME5G6A0UI1IBoI1BDbL3AIJFImukN21grxaNPzfU8XVgoaW4yq1xmXxtREieX\nscmoIZQqdUeGpEI2Shk1kx41IOII1BBL7Y6rl65s6Oh0vj9JENGwNfkxHhm166s11Zpt3T8/EfRR\nQi+fsVVvtuV5XFwiXKqNtvJZW6YZnTJ6v/SR9xMQXQRqiKVLN8pqtjpk0yKon1GLyeTHCwvdRbn3\nn5gM+CThl83Y6rge5VoInWrdUSEXnWya1M2oeV43qw8gmgjUEEt+T9CDJ8hiRI3foxaXXWoXeq9F\nMmp35peG1uhTQ4h4nqdK3VExYoGaP0m13SZQA6KKQA2xdGmxu4z99LGxgE+C/ZqOW0bt6qZyGUvz\ns4WgjxJ6uXR36TUj+hEmTaejjutFqj9N6mbUJMkhQw1EFoEaYunyUlnplKmjU/mgj4J9yqQsFXOp\nWPSobdZaWlyr6d67JiLV2xIUP6NGoIYw8XeoFXLRGgZkW93PHEqJgegiUEPstJyOrq/UdPJIkYvj\niJoZz2ptsxH5Jni/BPcByh73JE+ghhDq71CLaEaNyY9AdEXr9hCwzdnnr+74+yululzPk22Zt/0a\nhNv0eEaXFsuq1J1I7S16vQsL3UDtPnol9yRLoIYQ8neoRXGYiERGDYgyMmqInbVeydz0WCbgk+Cg\ntiY/Rrv88cLVkgxDuvf4eNBHiYRcxu9RY5gIwqPa6JU+ZqN1b5vSRyD6CNQQO/4QCvanRdfWLrXo\nDhRxPU+XbpQ1P1tg0fUeUfqIMOqXPkY2o0bpIxBVBGqInbXNpgxDmhyLbslc0s1MRD9QW9tsqNV2\ndRfTHveM0keEUbUe8dLHNhk1IKoI1BArrudpvdzUZDEjy+TlHVX+LrUoj+hfXKtLko5NM3l0r/oZ\ntRaBGsKj36MWtWEiNj1qQNRxJYtY2ay21HE9+tMibrZf+hjdHrUbazVJBGr7wXh+hNFW6WO0Spjp\nUf80SLsAACAASURBVAOij0ANsdIfJEJ/WqSNFdKyLSPSGbUbq91A7SiB2p75C69rDBNBiGztUYtY\nRo0eNSDyCNQQK1uDRMioRZlpGJoey2q1FOFAbZ2M2n75GbUGGTWESLXhyDAUuaFAjOcHoo9ADbHi\nZ9SmCNQib3o8o1K1JSeijfA3VmuaKKYjd3EXpJRtyjINetQQKpW6o0I2JdMwgj7KvlD6CEQfgRpi\nw/M8rZUbGsunlLatoI+DQ/J3qa2Xo5dVazkdrW02dGyKbNp+GIahXMZmjxpCpdpoR26HmiSlKH0E\nIo9ADbFRbbTVclwGicTEdIQHiiyt1+VJOjZDoLZf2bTFMBGEhud5qtadyPWnSVulj1GtSgBAoIYY\nYdF1vPi71KI4UISJjweXz9gEagiNRqujjutFbtm1JNk2pY9A1BGoITa2Jj6SUYsD/3mM4tLr62tM\nfDyobMZWo9WR61GuheBt7VCLXumjaRgyDAI1IMoI1BAbZNTixe9Ri2JGbbEXqB0nUNs3f0R/s0Wf\nGoLXH80fsWXXUrfn07ZMetSACCNQQ2yslZvKZSym7MXE9Fh0e9RurNVkmYZmJ7lpsF+5LEuvER6V\nhr/sOnqBmtSd/EhGDYguAjXEQqPVVq3R7l/cI/oyaUvFXCpyGTXP83Rjtaa5yZwsk4/Y/cqlCdQQ\nHtV6r/QxsoGaSaAGRBhXEYgF+tPiaXo8o9XNhrwI9SuV645qzTaDRA7Iz4gzoh9hUG34pY/RrNSw\nLVPtdnQ+PwHcjEANsUB/WjzNjGfVctz+xVIU3FjtTXxkNP+B5DLdHjWWXiMMKvWolz52M2pRutkF\nYAuBGmKBjFo89XeplaJT/sho/sPZyqgRqCF40S99NORJcl0CNSCKCNQQC2vlplK2Gdm7nthZFCc/\nLhKoHQo9agiTKI/nl6SU3Vt6zeRHIJII1BB5TtvVZrWl6bGMDMMI+jgYoCjuUiOjdjhZv/SRHjWE\ngD+eP6o3AW2re5nHQBEgmqJ5iwjYZr3slz3SnxZVZ5+/uuPvL2/UJUnPX1iRbd96X+nM4/NDPddB\n3FirKZexNZaP5oVd0PKUPiJEKg1HhtFdxB5FttW9eUmgBkQTGTVE3tYgEfrT4sZfMhuVYSId19XS\nel3HpvNkdw8o65c+MkwEIVCtOypkUzIj+n4mowZEG4EaIm+NjFps5TKWTGOroT/sVkoNdVyPssdD\nYOE1wqRadyI7SETaFqgxoh+IJAI1RN7aZkOmaWiikA76KBgwwzCUz6Yik1HbGiSSC/gk0ZVLd3vU\nGvSoIWCe56naaKsY0UEiEqWPQNQRqCHSXNfTRrmlqWJGphnN0hTsrpC1VW+21YnAeOmtHWqFgE8S\nXf3x/JQ+ImCNVkcd14tHRo1ADYgkAjVE2kalKdfz6E+LMf8iqdYIf/kjEx8Pz7ZMpWyT0kcErr9D\nLRuHQC38N7oA3IpADZHGouv48/cXRaH80Q/UjkxR+ngYubTFeH4Ezv/MKeQiXPpok1EDooxADZG2\nVvYnPjJIJK78jFoUBorcWKtpZjyjTMoK+iiRlsvYZNQQuEovi1+McEYtRY8aEGkEaoi0tc2mDElT\nY2TU4ioqI/rrzbY2Ki3KHgcgm7HpUUPg+qWPMehRcyh9BCIpuvl8JJ7neVrbbGi8kO7/MEL8+GVH\nYcmo3W4592qpm91tu95tvwZ7k0tbajmuOq4ry+S9jWBsBWrRvVTaGs9PRg2IIn4CIrLKNUftDoNE\n4s7PqFVCEqjdzma1JUkaZ03EofUnP9KnhgBVeln8KJc+Mp4fiDYCNUTW2ib9aUmQsk3lMpbKtXAH\naqVeoMY+v8PzA7UGfWoIUJxKHwnUgGgiUENkMfExOcbzaVXqjjohvtjYrPUyankCtcPyA7UagRoC\nFK9AjR41IIoI1BBZ/YmPY2TU4s4vJwxzVm2z2pJlGpHuZwmLXKY7NbPRovQRwan2Sx+j+55mPD8Q\nbdH99EGidQeJNFXI2sqkGYUed36gtllraTKEEz49z9NmtaWxfEqGYQR9nMjb6lEjo4bR2GkA0NWV\nigxDevaFxf77eqyYVbnSGPXxDswyDRkGgRoQVWTUEEnVRluNVoex/AnhB2p+H1jY1JsdtTseg0QG\nJJcmUEPwWo6rtG1F/uaLbZmUPgIRRaCGSFreqEuSivno9g5g7/y+r82QBmqbDBIZqGyv9LFO6SMC\n1HQ6yqSif5nUDdTIqAFRFP1PICTSSm9nVTHCTd7Yu2I+JcOQNqvh7FErMZp/oPKUPiJgnud1A7UY\nlNbblkGgBkQUgRoiacXPqBGoJYJlGirmUirXwp1RI1AbjCyljwiY03HleVI6FYdAzVS7TekjEEUE\naoik5V5GbYzSx8QYL6TVaHXUdMJXDtcfzU+gNhBk1BC0VqubgcrEIFBL2d3SR88jWAOihkANkbSV\nUePCOCn8PrVyCPvUNqstZdNWLC7qwqDfo9YMX1COZPBvCMXhPW1bhjwx+RGIIgI1RNJyqaFMylLK\n5iWcFOOFbvZ0M2Tljx3XU6XmaIxF1wPjj+dvtMioIRhbgVr0f8b4S6/ZSwhET/Q/gZA4rudptVRX\nkcXCidLfpRaygSLlWkuemPg4SIznR9D8QC0uPWqSQlk2DmB3BGqInFKlpXbHY5BIwoR1RP/WIBFe\nj4NimoYyKYvSRwSmn1GLydRHSWo6lD4CUUOghshZKbFDLYnyWVuWaYSu9JGJj8ORy1hk1BCYlhOf\nYSJ+Rq1FRg2IHAI1RM7KBjvUksgwDI0X0tqstkI1vcwvxSRQG6xcxladHjUEpNmKYekjPWpA5BCo\nIXKWS0x8TKrxQlrtjheqkrjNWkuGxDCRAcum7VA9z0iWWA0TselRA6Iq+p9ASBwyask13it3DVOf\n2ma1pWI+Jcs0gj5KrOQzltodV06bvhqMXitm4/klAjUgigjUEDn9HjWmPibO1uTHcARqTaej/5+9\nOw+S867vff95+ul9umdfpNFIshZb8optbGPHJBHbCbjOSbjATSqHFDlJBXNDDFkq5FbBrYI6laVM\nDo7vPRS34kpuDjcFIffEuMiJTQIB5BOQsbFlGUv2SLZkbSPNTM/SPb1vz3P/6OmWZY00W3c/z9P9\nfhWUpZnRzFfq7fn09/f7/gqlKsseWyBcP/Sa5Y9wQLFclWGoI46ACbD0EfAs7z8DoeskkgX1x4Iy\nTe6+3aYR1FwyUKQxSIRlj01XP0uNgSJwQrFsKRQwZRje75Qznh/wLq504SmVqqWFdEHD/RGnS4ED\n3Dain4mPrVM/S63APjU4oFSudsQgEYmlj4CXEdTgKXPJvGxbGukLO10KHBAKmgoFTNcFNQ67br5I\nqHaRnKOjhjazbVvFcrUjBolIb+6osd8T8JrOeBZC15iZz0mShvvoqHWr3p6A0vmyLMv5Ef0pOmot\nU1/6WCCooc3KFUu23RmDRCTOUQO8jKAGT5leWA5q/XTUulVvNCjbljL5stOlaDFdVChgNro/aJ4I\nw0TgkPoSwc5Z+li71CswTATwHIIaPGVmIStJGqGj1rXcMvmxXLGUzpXVHw92xMABt7k0TISLS7RX\naXmJYMd01Py15yc6aoD3rDrf/OzZs3r00Ud10003aXp6Wv39/XrooYcu+5pisaiHH35YY2NjOn36\ntB588EHt2rWrZUWje828qaM2vZhzuBo4wS2TH1OZoiRpIB5ytI5OFQnWLpKZ+oh2axx2HeyQoMbU\nR8CzVg1qyWRSDzzwgN773vdKkh544AEdOHBAt9xyS+NrvvrVr2rr1q36+Mc/ruPHj+tzn/ucvv71\nr7euanStmYWcTJ+hwThLH7tVb487Dr1eTC8HtRhBrRUYzw+nXFr62BmLjghqgHet+ix02223NUKa\nJFmWpUjk8mVnBw8e1B133CFJ2rdvnyYnJ5XJZJpcKlALaoO9Ifl8LDXrVvHGiH5n96gtLnfU+umo\ntcSlA6+5uER7NTpqHbL00fQZMgyCGuBFq3bU3uy73/2u3vnOd2rPnj2XfXx+fl49PT2N38diMc3P\nzysWi131ew0MROX3u+9JcGQk7nQJuIpCqaJkuqi3XT+skZG44jG6am7UjtslFg0oky878nit//3S\nuVqnZ2JLr4IufC67Frc+dt58exqB2suTLaPrnpe77e/rBpc/JmrvYQ/0RlZ8rLj18XMtQb+pqtU9\n961u+Xt6FbfP2q05qP34xz/Ws88+q89+9rNXfG5oaEjZbLbx+0wmo6GhoWt+v0UX7i8aGYkrkUg7\nXQauYmqudh/riwaUSKSVzhQcrghvFY+F23K7xCMBXZzP6fxUsu37SNKZgmzb1lwyr3g0oGKhrKKc\nn0C5Vu26jTbizc+/9SWPi0v5rnpe5nXIGW9+TKSztW55tVK94rHi5sfPtZg+Q7l8uSvuWzyG3I3b\n50rXCq5rWoB98OBB/fCHP9TnPvc5JRIJvfjii0omk43ljQcOHNCLL74oSTp+/Lj2799/zW4asBFz\nybwkzlDDpYEiMw694ZMvVlUsVxkk0kKhoClDnKOG9uu0PWqS5DcNFVj6CHjOqs9CR48e1e///u/r\npZde0sc+9jF98pOf1BtvvKHHHnusMTDkYx/7mC5cuKCvfOUr+pu/+Rv9yZ/8ScsLR/eZS9XexeQM\nNfQu71Orn6vXbvVBIv0MEmkZn2EoHPIrx3h+tFlxeV9kOLiu3SGu5vf72KMGeNCqz0K33HJLo1t2\nNeFwWJ///OebVhSwksRyR40z1FCf/DjjUFBLMpq/LSIhUwUOvEab5UsV+U1DfrNzhlb5TZ9Kpaps\n2+bcR8BDOqevj453qaNGUOt29aWP0wt5R35+YzQ/Qa2lIkE/4/nRdvliRZGQv6MCjd80ZEsqVyyn\nSwGwDgQ1eMZcMq9gwFRvNOB0KXBYTzggn+HcHrXFdFGmz1CM+2JLRUJ+5Yu1LgDQDpZtq1CqdtSy\nR4mz1ACvIqjBM+ZSBY0NRjrqXU5sjM9nKB4Nano+1/aLeMuylcqU1B8Lycd9saXCIVOWbatEFwBt\nUixVZdu1ZbedhKAGeBNBDZ6QK5SVK1Y0Ntiz+hejK8R7gsoVK8rk2zsafylXkmXb6o8H2/pzu1G0\nfug1yx/RJvU9kZFQh3bUOEAe8BSCGjwhkaztTxsbjDpcCdyivgR2ps371Nif1j715WcENbRLfnnK\naOcFtVr3v1imOw14CUENnjCXql2ME9RQd2mgSHv3qSUJam1TX36WZ0Q/2qT+pkAkyNJHAM4jqMET\n6KjhrfocOvSajlr71LsaeUb0o00aQa3TOmp+ghrgRQQ1eAIdNbyVUx21xXRR4aDZcVPh3Ciy/G9c\nYOkj2qTevQ13WlBbXvpYIqgBnkJQgyfUz1AbG2KYCGpqYcls66HXuUJF2UKFblqb1LsaOYIa2iTf\nGCbSWUsfAwwTATyJoAZPSCTziob8ikU4two1hmFobDCqmcW8rDaN6J+ay0hi2WO71C+WC+xRQ5vU\n72ud1jFnjxrgTQQ1uJ5t25pPFTTcH3a6FLjMlsGoyhVLi0vFtvy884msJKk/RlBrhwjj+dFm+WJF\noYAp09dZZyQS1ABvIqjB9ZayJZUqlkb6Ik6XApcZG6jdJ9q1T+18go5aOzFMBO2WL1U6btmj9Obx\n/AQ1wEsIanC9xPL+NDpqeKsty8Nl2hbUZjMyJPXFOOy6HcJBxvOjfaqWpVLZ6rhBItKbD7zmHDXA\nSwhqcL25ZG3i4zAdNbxFfQpoOwaK2Lat84msenuCjYsetFaUpY9oo8Zh1x12hprE0kfAq7jagOvV\nO2ojdNTwFmMDyx21NpyltrBUVL5YUT/LHtsmzNJHtFGhQ89Qk6SAn/H8gBcR1OB6dNRwNdGwX709\nwbZ01Br701j22DZBv0+mz6CjhrbIdXBQo6MGeBNBDa5XP0NtuI+OGq60ZSCiuVRB5Upr917Ugxod\ntfYxDEPhoMl4frRF/X7WmcNEapd7Bc5RAzyFoAbXSyTz6usJKhjovBdPbN7YYFS2XbuftFJ9ND8T\nH9srEvJz4DXaor7EttPOUJMkn8+Q6TNY+gh4DEENrla1LC0sFZn4iKvaMtSeyY/nExmFgiaHrrdZ\nJORXgT1qaIN8By99lKRQwGTpI+AxBDW42uJSUZZtc4YarmpiJCZJeuPiUst+RqVqaXo+p4nhHhlG\nZx2E63aR5aWPlm07XQo6XL6Dlz5KUihIUAO8hqAGV+MMNaxm77Y++QxDk2cXW/YzLs7nVLVsTYzG\nWvYzsLJIyC9bUpG9NWixfLEiw6h1njpRraPGOWqAlxDU4GpMfMRqIiG/dm2N6/TFdMuWyJ2frQ0S\nqXfv0D4RzlJDmxRKVUWC/o7tmocCJm94AB5DUIOrNc5QY+IjrmHfjgFVLVuvn0+15PvXJz5OjPS0\n5Pvj6sIENbSBbdvKFysdu+xRqi19LJVZRgx4CUENrjaXWu6o9dNRw9Xt39kvSXq1Rcsfzy0HtW10\n1NqufuGcpxOAFipXLFUtu2MHiUhSOGjKllRm+SPgGQQ1uNpcqiCfYWiwl5HouLq92/pk+gxNnkm2\n5PtPJbIaiIeY+OiAyPKo9AIdNbRQfZBIuIODWv2IGwaKAN5BUIOrzSXzGuwNyfRxV8XVhYN+7dra\nqzPT6aYvkcvky1pMF9mf5pB6h4Oz1NBK9TPUOrqjthzUCgQ1wDO4+oVrlStVJTMlDbM/DWuwb0e/\nLNvWa+eb21WbYn+ao+pLHwssfUQLNc5QC3bwHrV6R43HEuAZBDW41lx9ND8TH7EG+3cOSJImzzY3\nqJ1PZCWJ0fwOaXTUCnTU0DqFxhlqndtRCwVZ+gh4DUENrjXHGWpYh0v71Jo7UOQco/kd1dij1qKj\nFwDpTR21Tp76GKhd8tFRA7yDoAbXqp+hNkJHDWsQCpjaPd6rMzPppnZfphIZmT5DW4eiTfueWDv2\nqKEdumGPWmj5TQ86aoB3ENTgWgk6alinfTsGZNvSiSbtU7NsW+cTWW0Zispv8nTphMYetSIXl2id\nxtTHYAcHNTpqgOdw5QHXqnfU2KOGtbpxR+08teNNOk9tLlVQsVxl2aODGgdes/QRLZQvVuQ3DQX8\nnXtZxB41wHs69xkJnje/VJDfNNQXCzpdCjxiz7Y++c3mnad26kJKEhMfnVTfo9bsYxeANyuUKh29\n7FGSwoH6fk+CGuAVBDW41sJSUQPxkHyG4XQp8IhgwNTu8T6dnUkrVyhv+vs998qsJOn260c2/b2w\nMQG/T37T11iaBjSbZdkqFKsdH9TqSx9LdNQAzyCowZXKFUupbElDvexPw/rs39EvW9Lxc5vrqqVz\nJb18al47xmLaNkxHzUmRkElHDS2Tzpdlq7PPUJMuDRPhwGvAOwhqcKXFTFGSNBAnqGF99u+onad2\nfJPnqf1kclZVy9Z9N29pRlnYhEjIzx41tExq+fUm3CUdNYaJAN5BUIMrLS7VJj4O9YUcrgRes2db\nr/ymb9PnqT1zbFqGId1z41iTKsNGRYJ+pj6iZZayJUmdPZpfYpgI4EWd/awE1zt4ZGrFj5+cqg1x\nmEsWLvuaeCysdKbQltrgTQG/qb3benX8bFKZfFmxSGDd32N2MaeTU0u6+boBDcR5s8BpkZCpYrmq\nqmXJ9PH+Ipor1QhqHb70MbAc1OioAZ7BKx5cqX5gcU+E9xKwfvt3DMiWdGKD+9R+fGxGknQvyx5d\nod7pYFodWqER1Dr4DDVJCtNRAzyHoAZXyi5P7IuG198NAfYtn6e2keWPtm3rmWPTCvp9uvMGpj26\nQf0Q4nyBfWpovlSmO5Y++k2fDINhIoCXENTgSlk6atiE3eN9Cvh9mtzAQJFTF5c0s5jXHTeMdPyF\nm1dEG4dec4GJ5ktla8NEOn3po2EYCgdNlXgcAZ5BUIMrZfNlBfw+Bf2d/cKJ1gj4fdq7rU/nExml\nc6V1/dkfH60te7zvZoaIuEV4+QKaEf1ohfowkXCHL32UamdN0lEDvIOgBlfKFirqCXf+iyZaZ//y\n8sf17FOrVC09++qM4tGAbrpusFWlYZ0aHTWCGloglS0pFDDl8xlOl9Jy4YDJHjXAQwhqcJ1Spapy\nxVIP+9OwCfuWz1ObPLP2oHbsjQVl8mXdc+OY/CZPj24Rbix9JKih+VKZUscve6wLBU2mPgIewpUI\nXCeXr12MRemoYRN2j/cq6Pdp8tzaB4o8c2xakjjk2mUijaWPXGCiucqVqnLFStfsRw0FakHNtm2n\nSwGwBgQ1uM6lQSJ01LBxftOnvRN9mkpktbSGfWr5YkUvvjansYGIdm2Nt6FCrFV9bHqBpY9osm6Z\n+FgXCpqyJZUrltOlAFgDghpcpz6anz1q2Kz9y8sfT6xh+uPhEwmVK5buu3mLDKPz96p4Sf0iOkdQ\nQ5OlGoNEumTp4/Kh1wwUAbyBoAbXaXTU2KOGTdq/sxbUXj27+vLH+rLHe5n26DqNA69Z+ogmqwe1\naJd01MLLQY0R/YA3dMczEzwlm1/uqHGGGlZx8MjUNT9vWbb8pqHDxxOaGOm5rFN24PZtjV8vpot6\n9fSi9mzr1ehAtGX1YmMiy90Ohomg2RodtS4JasEgHTXAS+iowXVyBYaJoDl8PkNjg1GlsiV974Xz\nSmaKK37ds6/MyBZDRNwqEmY8P1ojlemOw67r6h01RvQD3kBQg+tkC2WFg6ZMH3dPbN47bhzT1qGo\nLszl9D9+dFrPvTJzxXjqHx+blukzdPf+UYeqxLXU9w8R1NBs9cOuu2aYSD2osfQR8ITueGaCZ9i2\nrWyhooF4yOlS0CFi0YDee9eEzieyen5yVpNnkzp1cUl+06cDd4zr4nxOZ2czun3vsOLRoNPlYgWm\nz6dgwMd4fjRdfeljfbJopwsF6agBXtIdz0zwjEKpKsuymfiIpjIMQ9tHYxof7tHkmUX99OS8vvbd\nE/qnZ043wllvT2DVPW9wTiTkZ48ami6VLcn0GQoGumMFRyOo0VEDPIGrYbgKEx/RSqbP0M27BrV7\nvFdHXpvTa+dTSmVKCpg+TYzGnC4P1xAJ+pVbProDaJZUpqS+WLBrjuRgPD/gLQQ1uEpj4iMdNbRQ\nJOTXfbds0b4d/frpyXmNDUblN7vjHXWvioT8mksVnC4DHcS2baWyJW0f7XG6lLZhPD/gLVwNw1Ua\nEx8jdNTQeoO9YR24Y9vqXwjHRUKmKlVL5YqlgJ9Qjc3LFyuqVC319XTPnmjG8wPewqsdXCVboKMG\n4Er1YQ/sU0Oz1AeJ9PZ0zxAhxvMD3kJQg6uwRw3ASurj0wuM6EeTpDK1oNbXRUGN8fyAt9C2gKtk\n82X5jO45fBTAJdeaujm/VNuf9m8/vaihvvBlnztwO8tXsX71jlpfrIuCGuP5AU+howZXyRYqioYD\nXTOBC8Da1PellSuWw5WgUzSCGh01AC5FUINrWJatfLHC/jQAV2gEtSpBDc2RyhYlqauGiVzqqPE4\nAryAoAbXaEx8JKgBeIvgclArsWQLTbK0vEett4uWPgb9PhmSigzlATyBoAbXaEx8ZDQ/gLdg6SOa\nrbH0Mdo9Qc0wDAWDJuP5AY8gqME1Lk18pKMG4HIBf23JFkENzZLKlhQOmo3lgN0iHDBZ+gh4BEEN\nrnHpDDU6agAu11j6SFBDk6Sypa4aJFIXCpgsfQQ8gqAG16jvUeuJ0FEDcDmWPqKZLMtWOtelQS1I\nRw3wilWviBOJhB599FFNTk7q8ccfv+Lzzz77rP70T/9Uvb29kqSf//mf12/91m81v1J0vGyejhqA\nlQUC9aDG3hpsXjpXkm1LvbHumfhYV+uoVWXbNkfhAC63alB74YUX9J73vEevvvrqVb/ms5/9rN7x\njnc0tTB0n2yhIr9pNN45B4A6Ompopm48Q60uFDRl2bYqVVsBP0ENcLNVg9r73/9+Pfvss9f8mm99\n61s6evSoMpmMfvmXf1lbt25tWoHoHtlCWT0RDrsGcKWAyR41NE9XB7X6odflKm+MAi636c1Ae/fu\n1Sc/+UlNTEzotdde02/8xm/oqaeeks937Qf/wEBUfr/7Ji2NjMSdLqGrxGNhSVKpUlWpbGnLYKjx\nsdX+DNyJ28f9vHobBf0+VS37ivo77Xm70/4+bmS9sShJmtjSq5GR+LoeE159/NTvV33x2nLPnnhY\nIwNRJ0tqGR5D7sbts3abDmpDQ0ONX19//fVKp9O6ePGitm3bds0/t7iY2+yPbrqRkbgSibTTZXSV\ndKYgSUpmipKkYMDX+NhK4rHwNT8PZ3H7uJ+XbyO/36diqXpF/Z30vM3rUHucn05Jkny2rUQivebH\nhJcfP/X7lW3ZkqQL00syOnDPJ48hd+P2udK1guuGet65XE4LCwuSpMcee0zJZFKSlEwmVS6XNTw8\nvJFviy5Wn/gY4ww1AFcR9PtU6sALS7RfNy99DC8vfSxx6DXgeqteFT/33HP61re+pUQioa985Sv6\nzd/8TT3xxBM6fvy4/vN//s+amJjQn/zJn2jv3r16/fXX9fDDDysU6r4pStic+sTHKBMfAVxFwO9T\nOWsxrQ6btlQParHuC2r1A74LJYIa4HarBrV77rlH99xzz2Uf++hHP9r49QMPPKAHHnig+ZWhq2Q5\nQw3AKgJ+n2xbqlq2/CZBDRuXypRkSIpHu+/NwcYwEYIa4HqM+4ErZAucoQbg2gLLA6gY0Y/NSmVL\nikcDMlcZfNaJ6h21IksfAdfrvmcouFK9oxZljxqAq6iPEi+VCWrYnFS2qN6e7tymEVo+PJ6gBrgf\nQQ2ukMuXFQ6a8pvcJQGsLFg/9LrKBSY2rliuKl+sduX+NEkKBWpviLL0EXA/rorhONu2lS1U1EM3\nDcA11DtqLH3EZix18cRHSQoFa4+jAh01wPUIanBcsVxV1bKZ+AjgmoL++lhxgho2rptH80tSeLmj\nxnh+wP0IanBcNs/ERwCrCwbqe9S4wMTGpTLdHdTqjyPG8wPuR1CD45j4CGAtwsHamzl5LjCxkmin\nJQAAIABJREFUCQtLBUlSf7w7h4mEmfoIeAZBDY5rnKHGHjUA1xAN1y4w88WKw5XAyxKpvCRppD/i\ncCXO4Bw1wDsIanBcjo4agDWIhJY7agQ1bMJcstZR69qgRkcN8AyCGhxX36MWZY8agGsIBUwZBkEN\nm5NI5RUJ+bt2FUeQjhrgGQQ1OC5bKMswLr1bDgArMQxDkaBf+SIXmNgY27aVSOY10heWYRhOl+MI\nn2EoGPAxnh/wAIIaHJctVBQN+eXr0hdNAGsXCZnKFyuybdvpUuBBS7mySmWra5c91oUDJtNTAQ8g\nqMFRlmUrX6ioJ8L+NACri4T8qlo2h15jQ+aStUEiw/1hhytxVjBgMp4f8ACCGhyVL1ZkS4p26V4B\nAOtTXyKdY58aNiCR7O6Jj3XhIB01wAsIanAUZ6gBWI96UCuwTw0bQFCrCdFRAzyBoAZHcYYagPWI\nhGoT6+ioYSMSqdpo/uG+7l76GAqaqlq2KlWWEANuRlCDo7L55Y4ae9QArMGljhpBDes3l8zLEEGt\nceg1yx8BVyOowVF01ACsB3vUsBmJZEH98ZACftPpUhzVOPSa5Y+AqxHU4KhLQY2OGoDV1YMah15j\nvSpVSwvpgka6vJsm1cbzS3TUALcjqMFRuUJZfrN2+CYArCay3AnI0wnAOs0vFWTbDBKRauP5JTFQ\nBHA5ro7hqGy+op5wQAaHXQNYA9P0KRjw0VHDus0la4NECGq18fySGNEPuBxBDY4plqsqlqucoQZg\nXSIhP0EN65bgsOuGEB01wBMIanDMwlLt3U0mPgJYj0jQr1LZUtVitDjWjjPULmkME6GjBrgaQQ2O\nWUgXJTHxEcD61M9Sy3PoNdbh0hlqBLXGeH46aoCrEdTgmIXlF80oEx8BrAOTH7ERiWReAb9PfbGg\n06U4jnPUAG8gqMExiVRtGUqcpY8A1oGgho2YS+Y13BeWj+FVLH0EPIKgBsfMLi4HtShBDcDaEdSw\nXrlCWdlChf1pyxgmAngDQQ2OSSTz8vkMpj4CWBf2qGG9EvXR/OxPk8TSR8ArCGpwzOxiXvEIZ6gB\nWB86alivuVR94iOj+aVL56gxTARwN4IaHJFdXoYSY9kjgHWKEtSwTvWO2jBLHyWxRw3wCoIaHMH+\nNAAbFfD75PMZLH3EmnGG2uUYzw94A0ENjqi/aMajjEkGsD6GYSga8tNRw5rVpwwP97H0UWKPGuAV\nBDU4go4agM2IhEzlSxXZtu10KfCARLKgWCTQ2N/Y7Xw+QwG/j6AGuBxBDY5oBLUIHTUA6xcJ+WXb\ndASwOsu2NZ/Ks+zxLUIBU8Wy5XQZAK6BoAZHzCbzMiTFory7CWD9mPyItUqmi6pUbSY+vkUoYKpY\n4vEDuBlBDY5IJPMa7A3J9HEXBLB+kSBnqWFtGCSysnDQ5MBrwOW4SkbblcpVLaaLvGgC2DA6alir\nudTyYde85lwmyNJHwPUIami7xPKL5uhA1OFKAHhVPajlCGpYRb2jxsTHy4WDpipVS1WLsAa4FUEN\nbZdYHiQyOsC7mwA2ph7UCix9xCpY+riyS2epEdQAtyKooe1mF3OSpFFeNAFsEB01rFUiVZDPMDTY\nG3K6FFcJBTlLDXA7ghrabpZ3NwFsUnj5IrNAUMMqGF61Mg69BtyPZy20XT2osfQRwEb5fIbCQZOO\nGq6pVK4qlSnxxuAKLi19JKgBbkVQQ9slFvOKRQKNpUsAsBGRkJ+pj7gmJj5eHUsfAfcjqKGtqpal\nuVSBbhqATYuETFWqtgoc2ouruDRIhImPbxUK1C4BOUsNcC9aGmirhaWiqpZNUAOwafWufCpbUjjI\ny1m3O3hk6oqPvXpmUVJtyf1Kn+9m9cdMiY4a4Fp01NBWjf1pLEMBsEmNoJYpOVwJ3CqTK0uS4pGA\nw5W4T5COGuB6BDW0Vf0MNfYLANisSPBSRw1YSSZfC2qxKEHtreodNfaoAe5FUENbMfERQLNEwrUL\nzWSm6HAlcKtMvqyA6WtMOMQl9T1qBDXAvQhqaKt6R42ljwA2KxKqXXwv0VHDCmzbVjpXUiwakGEY\nTpfjOvXwytJHwL0IamirmcW8QgFTvT1Bp0sB4HH1pY901LCSYrmqStVWjP1pK2KYCOB+BDW0jW3b\nSiTzGumP8O4mgE1jmAiupTFIhP1pK2KYCOB+BDW0zVKurGK5yv40AE0R8PvkNw2GiWBF6fogETpq\nK6KjBrgfQQ1tw/40AM0WCfmVYukjVlDvqDHxcWUceA24H0ENbTObzEmSRuioAWiSaMivdK6sqmU5\nXQpcpt5R4wy1lQWXh4kw9RFwL4Ia2maWjhqAJguH/LIlLWXLTpcCl2l01AhqK/KbtaXDBDXAvQhq\naJv6GWp01AA0S7Q+UCTL8kdcLpMvKxLyyzS51LmaUMBUkaWPgGvx7IW2SSzmZfoMDfWGnC4FQIeo\nn6XG5Ee8mWXZyhbKdNNWEQqadNQAFyOooW1mk3kN9YVl+rjbAWiOxoh+Jj/iTbKFsmyb0fyrCQVM\nhokALsYVM9oiX6wonSuzPw1AU9WDGode480yjOZfk3DQZDw/4GIENbRFgv1pAFqgsfSRjhreJM1h\n12sSCpgqVSxZlu10KQBWQFBDWzDxEUArNJY+skcNb8LEx7UJMaIfcDWCGtqiPvGRoAagmUIBU6bP\nYOojLlNf+khH7dpCQYIa4GYENbRFo6PG0kcATWQYhnp7gnTUcJl0viyfz2h0XLEyOmqAuxHU0BaN\nPWp01AA0WV9PUMlMSbbNPhvUZHK10fyGYThdiqs1ghqTHwFXIqihLWYX8+qPBRVcflEAgGbpj4VU\nqVrKFytOlwIXKJWrKparirM/bVX1pY+M6AfciaCGlitXLC2kC+xPA9ASvT1BSVKS5Y+QtJiu7Vfs\nj4ccrsT96h01RvQD7rTq4u1EIqFHH31Uk5OTevzxx6/4vGVZeuSRR9TT06OpqSl95CMf0e23396S\nYuFNc6m8bJvR/ABaoz9WC2qpTFHjwz0OVwOnLSzVgtoAQW1VdNQAd1s1qL3wwgt6z3veo1dffXXF\nz3/7299WJpPRH/7hHyqZTOpXfuVX9NRTT8k0WeKGmgQTHwG0UN9yR42z1CBd6qgNEtRWxTARwN1W\nDWrvf//79eyzz1718wcPHtT9998vServ71cwGNRrr72m/fv3X/P7DgxE5fe7L8yNjMSdLqHj5CYT\nkqS9Owev+PeNx8Lr/n4b+TNoH24f9+u026gvVrsgr8joiOfwTvg7OKF+v07lSjJ9hraN9crna/4w\nEa8+fla6X40OxSRJgVCgo+53nfR36UTcPmu36bm1CwsLisVijd/HYjEtLCys+ucWF3Ob/dFNNzIS\nVyKRdrqMjnPq/KIkKWwaV/z7pjOFdX2veCy87j+D9uH2cb9OvI36o7WXsgszac8/h/M6tHHpTEGW\nZWs+VdBALKhsrvln63n58bPS/apYqHWh5xezHXO/4zHkbtw+V7pWcN30MJHBwUFlMpnG7zOZjAYH\nBzf7bdFBEouM5gfQOvWlj0kOve56S9mSLMvWQNybXa92Yzw/4G4bCmq5XK7RNTtw4ICOHDkiSUom\nkyqVSrr++uubVyE8bzaZVzTkV4xRyQBaoK+ntvSRQ6+xkGaQyHrUh4mwRw1wp1WD2nPPPadvfetb\nSiQS+spXvqJCoaAnnnhCjz76qCTpAx/4gHp6evTlL39ZX/ziF/Xwww8zSAQNlm0rkSww8RFAywT8\nPvWE/QwTQWOQyEAvQW0t6KgB7rbqHrV77rlH99xzz2Uf++hHP9r4tc/n02c+85nmV4aOkEwXVala\nTHwE0FJ9sZBSGZY+druFpdr+MSY+rk24Pp6fjhrgShx4jZaaXd6fNkpHDUAL9fUElS1UVK5wwdnN\nFtNF9YT9CgZY2bMWQTpqgKsR1NBSs5yhBqAN+mKcpdbt8sWKCqWqBnoZJLJWYfaoAa5GUENLNQ67\npqMGoIX6GSjS9Tjoev38pk+mzyCoAS5FUENLzTCaH0Ab0FFDfX8aEx/XJxQwWfoIuBRBDS2VWMzL\nb/rUzwsngBaqn6XGQJHu1eioMfFxXUJBk44a4FIENbSMbduaTeY10h+WzzCcLgdAB+uL1S7Okyx9\n7FqL6aL8psGZnetERw1wL4IaWiadLytfrDBIBEDL9bP0sauVK1WlsiUNxMMyeGNwXWodNcvpMgCs\ngKCGljk/m5EkTYzGHK4EQKdj6WN3m5rLyrZZ9rgRoUBt6aNl206XAuAtCGpombMztaC2YyzucCUA\nOl0k5FfA71OSjlpXOrf8esMgkfWrj+gvsU8NcB2/0wWgMxw8MnXFx34yOSNJujifXfHzANAshmGo\nryeoJYJaVzo3S1DbqMah12VL4aDDxQC4DB01tEx9Y3c8ysZuAK3XF6sFNZZwdZ+zsxkZIqhtRLge\n1EoVhysB8FYENbREpWqxsRtAWw31hlW1bM2lCk6XgjaybVvnZjOK9wTlN7msWa/QmzpqANyFZzS0\nRDJTZGM3gLbaubwf9ux02uFK0E7zSwXlixW6aRsUCtY7auxRA9yGoIaWWFji4FEA7VUfXHR2lqDW\nTer70wYJahsSCtQuBTn0GnAfghpaohHU4mGHKwHQLXaM1Y4COTOdcbgStFNj4iNvDG5IKFibK1eg\nowa4DkENLbGYLsgwLh1CCwCtFo8GNdgb0tkZOmrdhI7a5jCeH3AvghqazrJtLaaL6usJymRjN4A2\n2jkWVypbUpKDr7vGudmMYpGAIiFOHNqI4PLSxwJBDXAdrqLRdOlsWZWqrcFelj0CaK/6QJEzDBTp\nCvliRbPJvLaPxpgwvEHhQC3gMkwEcB+CGppuIV0bjc0gEQDt1hgowvLHrnA+UVv2uH005nAl3sUw\nEcC9CGpoOgaJAHDKzi3LHbUZBop0g/r+NILaxtWHidBRA9yHoIamW1zuqDGBC0C79ceCikcDdNS6\nBEFt8+ioAe5FUENT2bathaWiesJ+hQKm0+UA6DKGYWjnWFxzqYIy+bLT5aDFzs1mZPoMjQ/3OF2K\nZ9UPvGY8P+A+BDU0Vb5YVaFUZZAIAMfU96mdo6vW0SzL1vnZjMaHe+RnwvCG1d9UZTw/4D48s6Gp\nGCQCwGnsU+sOM4s5lSoWyx43qX6OGuP5AfchqKGpFpcHiQxw8CgAh+wcq124n6Gj1tHYn9YcftMn\nn2GwRw1wIYIammohvTzxkaWPABwy3B9RJGQyUKTDEdSawzAMhYI+pj4CLkRQQ1MtLBUUDPjUE/Y7\nXQqALuUzDO0YjWt6PqdCqeJ0OWgRglrzhAImHTXAhQhqaJpyxVI6V9ZgPCzDMJwuB0AX2zEWly3p\n/GzW6VLQIudmMxqIhxSPBp0uxfNCAZOOGuBCBDU0DYNEALjFzi3sU+tk6VxJi+ki3bQmCQVNhokA\nLkRQQ9MwSASAW9RH9BPUOhPLHpsrFDBVKlVl27bTpQB4E4IammZhiUEiANxh61BUAb9PZ6cJap2I\noNZcoaApW1KpYjldCoA3IaihaRbSBfl8hvp62C8AwFmmz6ftozFNzWVV5uKz4xDUmqt+6DUDRQB3\nIaihKSzLVjJd0kAsJJ+PQSIAnLdjLK6qZevCHANFOs1r55OKhEyNDUSdLqUjhOtBjYEigKsQ1NAU\nqWxRlm1rgEEiAFyCg6870+xiTolkQTfuHOSNwSYJBumoAW5EUENTNPanMUgEgEswUKQzHX1jQZJ0\ny65BhyvpHHTUAHciqKEpGCQCwG0mRnpk+gwGinSYY8tB7WaCWtOE6KgBrkRQQ1PUz1BjND8Atwj4\nTW0d6tG52Ywsi7HjnaBStfTqmUWNDkQ00h9xupyOEaKjBrgSQQ2bZtu2FpeK6o0GFPBzlwLgHju3\nxFSqWLq4kHO6FDTBqQtLKpSqLHtssnpHjUOvAXfhqhqbNp8qqFSxNMCyRwAus3N5nxrLHzvDUZY9\ntgTj+QF3Iqhh084un2czyMRHAC7DQJHOcuyNBZk+Q/t3DDhdSkdhmAjgTgQ1bNrZ5QugwTgdNQDu\nsn00JkOXnqfgXZl8WacvLmnPtj5FQn6ny+kojOcH3Imghk07O0NHDYA7RUJ+jQ5GdWYmI9tmoIiX\nvXJ6QbZY9tgKdNQAd+ItKWzaudm0IiGTdzgBOOLgkalrfj4SNDWzUNE/PXNa8Wjwss8duH1bCytD\nMx3j/LSWqb9+54oVhysB8GZ01LApmXxZ80tFDbDsEYBL1bv99fMe4T22bevY6QXFIoHGgBg0T3+s\n9gZGKlNyuBIAb0ZQw6acq+9PY9kjAJcaXJ5Iu7BUcLgSbNTF+ZwWloq66boB+XyG0+V0nEjIr6Df\np8UMb2YAbkJQw6acqe9P46BrAC5VD2rzdNQ8q77s8ebrWPbYCoZhqC8WVJKgBrgKQQ2bcm623lFj\n6SMAdwoHTfWE/VpYKjBQxKOOneb8tFbrj4W0lC3JsniMAG5BUMOmnJxaUiRkKh4NOF0KAFzVYG9Y\nhVJV+SJT7bymXLE0eXZR48M9vCnYQv2xkGxbWsqxTw1wC8b0YcNmF3OaTeZ1x/XDMgz2DABwr8He\nkM7NZrSwVFA0HHO6HFzDW6d4XpzPqlS21NcTXHXCJ65utX+7TL4sSfreC+c11Hd5IGY6KuAMOmrY\nsKP1Ucm7hxyuBACubYiBIp51YS4nSRof7nG4ks4WCdXOUsszoh9wDYIaNowzbQB4RX0y7VyKoOY1\nF+ay8hmGxgYjTpfS0aJhzlID3Iaghg2pVC29emZRowMRjfTz4gnA3SIhv/p6grown1OpzD41r8gX\nK1pMFzU6GJHf5JKllRqHXhcIaoBb8KyHDTk5lVKhVKWbBsATDMPQ7vFeWZatM9Npp8vBGl2cZ9lj\nu9SDGksfAfcgqGFDGvvTdrE/DYA37BrvlSSdurDkcCVYqwtzWUnS+FDU4Uo6X5SgBrgOQQ0bcuyN\nBZk+Q/t39jtdCgCsSSwS0NhgRDOLeWVyZafLwSps29aFuazCQVMD8ZDT5XS8gN8n02ewRw1wEYIa\n1i2dK+nMdFp7t/UpHOSEBwDesXu8T5J06iJdNbdLZooqlKoaH+7hCJg2MAxD0bCfjhrgIgQ1rNux\n0wuyJd2ym/1pALxl55aYTJ+hU1Mp2bbtdDm4hktj+Vn22C6RkF+FYlWWxWMDcAOCGtbtGPvTAHhU\n0G9q+2hMS7my5hnV72r1/Wlbhxgk0i6RkF+2pEKJyaiAGxDUsC62bevYGwuKRwPaPhZzuhwAWLfd\ny0NFTjJUxLUqVUszi3kNxEONaYRovfpAEfapAe5AUMO6TCWySmZKuvm6QfnYMwDAg8aHexQOmjp9\nMa1K1XK6HKxgZiEvy7IZy99mkTCTHwE3IahhXepj+W/m/DQAHuXzGbpua1zFclVHTy04XQ5WcPJC\nSpI0MUJQa6doyJQk5Tn0GnAFghrW5dgb85IIagC8rT798dCxaYcrwVvlixWdnU6rPxbU6EDE6XK6\nSoSlj4CrENSwZsVyVcfPpTQxElN/jDNtAHjXUG9IfT1BHXltTrkCZ6q5yWvnU7Js6YYd/YzlbzMO\nvQbchaCGNTtxLqlK1WIsPwDPMwxDu8d7Valaev54wulysKxqWTpxLqmA6dOe5a4n2oeOGuAuBDWs\n2aWx/AQ1AN63a3n646GjLH90i1qHs6Ld23oV8HOJ0m4Bv09+06CjBrjEmmbeHjp0SN/5znc0NDQk\nwzD00EMPXfb5b37zm/rGN76hUKi2HO7DH/6wPvjBDza/Wjjq6BsLCvp9un6CdzkBeF8sEtC+7f06\nfi6puWRew/3sh3La9w9PSZL27eh3uJLuZBiGIiE/QQ1wiVWDWj6f1+c//3k9+eSTCgaD+tSnPqVn\nnnlG991332Vf98gjj2hiYqJlhcJZC0sFXZjL6tbdQwr4TafLAYCmuO+WLTp+LqlnXpnRf/iZ65wu\np6tdmMvq1TOL2jIYZR+0gyIhv2YXa8cj+HzsEQSctOq6giNHjmh8fFzBYFCSdOedd+rgwYNXfN3X\nvvY1/fVf/7W+/OUvK5lMNr1QOOsoyx4BdKC79o0q4PfpmaPTsm3b6XK62g/oprlCfaBIoURXDXDa\nqh21+fl59fRcOsckFotpfn7+sq+5++67deDAAQ0ODurpp5/W7/7u7+qrX/3qNb/vwEBUfhd2ZkZG\n4k6X4EqvX1iSJP3s27ev+G8Uj4XbVks7fxbWj9vH/biNLtm5fUDvuHmLfvjSBSULVd2wY8Dpkrry\ndShXKOvQsWkN9YV10+5hV3dyOv3x0xcPSdNpGT6z8Xf12n3Sa/V2G26ftVs1qA0NDSmbzTZ+n8lk\nNDQ0dNnXbN++vfHre++9V7/927+tarUq07x6EFtczG2k3pYaGYkrkUg7XYbrWJatF4/ParA3pJBh\nr/hvlM4U2lJLPBZu28/C+nH7uB+30eUSibTuvH5YP3zpgp764SkNvO8GR+vp1tehHxw+r3yxol+4\nZ7uyuaLT5VxVNzx+/MsheS6ZUyRYW3jlpftktz6GvILb50rXCq6rLn28/fbbdeHCBZVKJUnS4cOH\ndeDAASWTSWUyGUnSl770JVUqtRb56dOntW3btmuGNHjLG9NLyhYqumXXIGfaAOg4t+waVDwa0LOv\nzKhStZwup+vYtq3vH56S6TP0c28bd7qcrtcY0V9g6SPgtFU7apFIRF/4whf0x3/8xxoYGNC+fft0\n33336Ytf/KL6+/v14IMPanh4WF/4whc0MTGhEydO6M///M/bUTva5Nip+v60oVW+EgC8x2/6dM+N\nY/reC+f10utzevu+UadL6ionziU1NZfVPTeOMkTEBTj0GnCPNY3nv//++3X//fdf9rE/+qM/avz6\n13/915tbFVzl6OkFGYZ043XO790AgFY4cPu4vn/4vP7h4EndtmeYM7za6HvLQ0TefSeTo92AQ68B\n9+CVCNeUK1R0ampJu7f2qicccLocAGiJbSMxvefOCc0s5vXPz511upyusZgu6sUTCU2M9HBGp0tE\nwrWtK3TUAOetqaOG7nDwyNQVHzsznZZl2+qJBFb8PAB0ig/+7G49NzmrJw+d1n03jXEAdhs8fWRK\nVcvWu++cYA+0SwRMn/ymQVADXICOGq7pzHRtMs/EaM8qXwkA3hYN+/Ur79qrUsXS333vNafL6XiV\nqqWnX7qgSMjUvTePOV0OlhmGoUjIzzARwAUIariqUrmqs7MZ9fYENdTb2efGAIAk3XvzmG7Y3q8X\nX5vTS6/POV1ORzt8IqFUpqT7b9mqcJAFPm4SDflVKFVlWRwCDziJoIarOj2dlmXZ2jPey5IUAF3B\nMAz92r+7QT7D0Nf/9YRK5arTJXWs7y8PEXnXndscrgRvVR8oUijRVQOcRFDDVZ2cSkmSdm/rdbgS\nAGifiZGY3nvXhBLJgr79LINFWuHY6QWdOJfUTdcNaOsQS+vdJhpm8iPgBqw1wIqWsiUlkgVtHYoy\n7RFAx7rakKShvrAiIb/+x6HT8vmkeDR42ecP3E4XaKOKpaq++u1J+QxD/+uBvU6XgxVcdug1wzgB\nx9BRw4pOXliSJO2hmwagCwX8Pt21f0SWZeu5V2dl2+zVaZYn/u2U5lIF/cI7tmvnlrjT5WAFkcah\n1yz9BZxEUMMVbNvWqamU/Kah7aO8iALoTtdtiWvLYFRTiazOzWacLqcjnLyQ0nefP6exgYh+6f5d\nTpeDq4g2ghpLHwEnEdRwhZmFvLKFinZuiSvg5y4CoDsZhqF7bhqVz5B+8uqsKlXL6ZI8rVK19N++\nPSnblv7TB/YrGDCdLglX0Vj6SFADHMVVOK5QHyKyZxsL0wF0t/5YSDdeN6hsoaKXT847XY6nPfXM\nGU0lsjpw+7j27RhwuhxcQ32YCB01wFkME8FlyhVLZ2bSikUCGhuIOF0OADjutj1DeuPiko69saAt\nQ1GmFK7BW4e0JNNF/dOh04qE/NoyFL3qEBe4Q8Dvk980OPQacBgdNVzm7Exalaqt3ZydBgCSahet\n99+6RZKhHxyeUiKZd7okT7FsW4eOTsuyaweKs+TRGyIhPx01wGEENVymPu1x9zjTHgGgbutQj37u\n9q2qVm1974XzOs9wkTU7fiapuVRB122Ja/tozOlysEbRkF+FUlWWxcRTwCkENTRk82VNz+c00h9R\nb09w9T8AAF1kx1hcP3PrFpXKlr7090c0s5hzuiTXy+TKevG1hIIBn+6+cdTpcrAOkfo+tRJdNcAp\nBDU0nOLsNAC4pj3b+nT3jaNKZUv6L393RAtLBadLci3btvXMsWlVqrbu3j/amCQIb2iM6GefGuAY\nghok1V5QT06lZPoMXccBpABwVTfuHND/8rO7NL9U0Jf+/oiWciWnS3Klk1NLujif0/hwlOX0HsSI\nfsB5BDVIqnXTlnJlbR+NsdEbAFbx73/mOv3CPdt1cT6nR/7+CNPx3mIqkdWPX5mR3zR0781bGE7l\nQRx6DTiPoAZJ0o+OTkvi7DQAWAvDMPTL79qrn3vbVp2dyej//IeXVCxXnS7LFV4+Na8fvDglQ9KB\nO7YpFgk4XRI2oL5HLVfkfg04haAGlStVPffKjCIhU1uHo06XAwCeYBiGPvYL+3X3/lG9dj6lL32D\nASMvn5rXf338ZRmS3nXnNo0Pc+acV7FHDXAeQQ068vq8csWKdo/3ysfyFABYM5/P0Mf/w02658ZR\nvT6V0uf/+jl957mzXTnSvBHSDEJaJ4iw9BFwHEEN+tHLFyVJe8ZZ9ggA6+U3ffrEL96sT/zizQoG\nTH3j+6/rz772gi7OZ50urW1+enJe//Xxn8owpE9/5DZCWgcI+H0KmD6GiQAOIqh1uVSmqKOnFrRz\nS1z98ZDT5QCAJxmGoXfcNKY//q136K79ozo5taTP/z8/0bd/fEZVy3K6vJb66cl5ffmbP5VhGPrd\nj9ymm68bdLokNEkkZNJRAxzEoSZd7umXLsiybf3cbVudLgUAPOPgkamrfu6m6wbUE/Z0CC4WAAAS\nuklEQVTr2Vdm9N8PntT3X5zSz9yyRQPLb4YduH1bu8psuXpI8xmGPv2R23QTIa2jRMJ+LS3kVala\n8pu8tw+0G4+6Lla1LD195ILCQVP33rzF6XIAoGPs3BLXL75zl3ZtjWs+VdCTh07r2VdmlO6QM9ds\n29a//fQCIa3D1QeKLGU7434LeA0dtS724ok5LaaLeved2xqbhgEAzREOmvrZt43ruq0ZPffKjI6f\nTerE2aTOzWb1gXfs0K6t3jwEej5V0Ff/eVJH31hQOGjqoQ/dSkjrUPVrg8VMUYO9YYerAboPV+dd\n7PuHz0uS3nXnhMOVAEDn2j4a07bhHp2eTuvYGwt6fnJWz0/Oav+Ofr3/HTt06+4hTxwIbdm2nj5y\nQf/fD15XsVTVLbsG9evv36+hPi7gO1W9o5bK0FEDnEBQ61JTc1lNnk1q/45+bWM6FwC0lM9naPd4\nr3ZtjWu0P6p/fvaMjp1e1OTZpLYN9+jf3b1dd+0fdbrMK9T34qVzJR06Oq2ZhbyCfp/uv3WLdo/3\n6uU35h2uEK1UP/Q6mSk6XAnQnQhqXeoHy920d9NNA4C2MQxDN+8a1M27BnV2Jq1/fu6snntlVn/z\n7Un97XdO6K4bR3Xb7kG9bc+wK5akW7atyTOLevHEnKqWrYnRmO69aUzRsPO1ofXq90GCGuAMnmm7\nUL5Y0aGj0xqIh3THDcNOlwMAXeXNEyNv2N6v8eEenZxK6cx0Wj8+Oq0fH52W6TO0baRH122Ja9tI\nTAG/r63TIpeyJR1+LaFv//is5lMFhQKmfubWUV23Je6JZZpojmgjqLH0EXACQa0LPXNsWoVSVe9/\nxw6ZPgZ/AoCTYpGA3rZ3WG/bO6xSVXrl1JzOTKd1diajszMZmT5DY4NRJZJ5bR+NaftITGOD0aaP\nS19MF3X4REIvHJ/V8XNJ2Xbt49dtievuG0dd0eFDe9FRA5zFs26XsW1bPzg8JdNn6OffNu50OQCA\nNxnqC+v264f1tr1DSmZKOj2d1pnptC7MZXVhLtv4Or9paHy4R9tHYpoYjWkgHlI8ElAsGlQsElAs\nElDAf2WQs21bpYqlfLGifLGiXLGik+dTev5EQifPp7SczbR3W5/evm9E5YqlWDTQpr893Cbg9ylg\n+pRM01EDnEBQ6zInziU1NZfVPTeOqi8WcrocAMAKDMPQQDxUW6J+/bAKpaquG4vrXCKjc7MZnZ/N\naGouq7Mzmat+j3DQVCwSUDjoV6FUC2aFUlVVy77iaw2jtgzzrv2juvOGkcbh3Nc62BvdIRL2K5Wl\nowY4gaDWZb53uPaiyxARAPCOcNDU9GJOAb9Pu8d7tXu8V5ZlK50rKZkpKV+sqFiuqlCqqliqNn6d\nK1aUypYUMH0KBHwaDPtrXRK/qaDfp4Dfp96eoLaPxhrL3F46Oefw3xZuEg35Nb2QU6VqNX25LYBr\nI6h1kcV0US+eSGhipEfXT/Q5XQ4AYBN8PkN9sRCrI9BSkZApqXaWGmfmAe3FWyNd5OkjU6patt59\n5wRTuwAAwKoaA0VY/gi0HUGtS1Sqlp5+6YIiIVP33jzmdDkAAMAD6mfmMVAEaD+CWpc4fCKhVKak\n+2/ZqnCQFa8AAGB1jOgHnMMVewdaaUrXvzx7VpLUEwkwxQsAAKxJ/dBrJj8C7UdHrQsspouaWcxr\n61BUfbGg0+UAAACPaHTUWPoItB1BrQscP7soSdq3o9/hSgAAgJew9BFwDkGtwy1lS3r9/JJ6wn5N\njMScLgcAAHhIwO9TJGQqmaGjBrQbQa3D/WRyVpZt6+37R+XzMZIfAACsT38sREcNcABBrYOdn81o\nKpHVlsGodo7RTQMAAOvXHwspky+rUrWcLgXoKgS1DlW1LP1kclaGId1z4ygHXAMAgA2pDyJLsfwR\naCuCWod65fSi0rmy9u8YUH885HQ5AADAo/pjteuIuVTe4UqA7kJQ60C5Qlkvn5xXOGjqbXuHnC4H\nAAB42A0TtanRh45OO1wJ0F0Iah3o+eMJVaq27rhhWMGA6XQ5AADAw27bM6TR/oieOTajpRzLH4F2\nIah1mBPnkjp9Ma2hvrD2butzuhwAAOBxPp+h99w1oUrV0tNHLjhdDtA1CGodxLJsfe27JyQxQAQA\nADTPO2/dqnDQ1A8On2f6I9AmBLUOcvDIlM7NZrRnW69G+iNOlwMAADpEJOTXz942rmSmpOcnZ50u\nB+gKBLUOkcmX9cT/PKVIyNSdN4w4XQ4AAOgw77lrQoak7z5/TrZtO10O0PEIah3im//zlLKFin7p\n/l2KhPxOlwMAADrMaH9Et18/rDcupnXywpLT5QAdj6DWAU5dWNLTL05pfLhH7377hNPlAACADvXe\nu7ZLkv71+XMOVwJ0PoKax718al7/5Rsvypb0H997vfwmNykAAGiN/Tv6NTES0/OTCS0sFZwuB+ho\nXNV72PdeOK9H//tLqlRt/W+/dLNuum7Q6ZIAAEAHMwxD77trQpZt6/uHp5wuB+hoBDUPqlqWvvad\nE/rad08oHgnof/+Pd+ieG8ecLgsAAHSBe28eUywS0NNHplQsV50uB+hYBDWPyRcr+r/+4WV97/B5\nbRvp0f/xsbu0h4OtAQBAmwT8pg7csU3ZQkXPHJt2uhygYxHUPGQumdef/u0LevnUvG7dPaTP/trb\nNcx5aQAAoM3edcc2mT5D//r8eUb1Ay1CUPOI16dS+uP/93lNzWX1nrdP6NMfuZUx/AAAwBED8ZDu\nvnFUF+ayeuX0otPlAB2JK32XOniktkF3LlXQ5JlFnb64JNuW7rlxVNtGevRvP73ocIUAAKCbve+u\n7frxsRl99/lzunkXA82AZiOouVClaumNi0uaPLOoRLI2+ravJ6i7bxzV+HCPw9UBAABIu7b2au+2\nPv305LymF3LaMhh1uiSgoxDUXGQpV9LTL07pBy9OKZkpSZK2jfToxp0D2joUlWEYDlcIAABwyfvu\n3q7Xp1L6l+fO6mO/sI9rFaCJCGoOs2xbx88m9cOfXtRPJmdVqVoKB03t39mv/TsG1NsTdLpEAACA\nFd15w7CGesN6+sgFnZxK6b13bde9N40pGDCdLg3wPIKaQxLJvH708kUdOjqtuVRteePYQETvefuE\n7r91q559dcbhCgEAAC7tm7+ad962RS+9Pq8zM2n9t29P6u/+9TXdsL1P/+kDN2ogHmpTlUDnIai1\nUaFU0QvHE/rhTy/q+LmkJCkUMHX/rVv0zlu36vrt/fKxZAAAAHhIXyykn7t9XNlCWcfPJvXauZRe\nPrWgP/q/D+nt+0b0vru2a/d4L8sigXVaU1A7dOiQvvOd72hoaEiGYeihhx667PPFYlEPP/ywxsbG\ndPr0aT344IPatWtXSwr2Csu2NbuY17efPaOFpaIWlgpKJPOqVGtnjYwNRrR3W592jMUV8Pt0cSGn\niws5h6sGAADYmJ5wQHfeMKLb9gzpjQtLOjub0XOvzuq5V2cVCfk12h/RyEBEo/0RjQ5ENNIf0Uh/\nWIPxsHw+QhzwVqsGtXw+r89//vN68sknFQwG9alPfUrPPPOM7rvvvsbXfPWrX9XWrVv18Y9/XMeP\nH9fnPvc5ff3rX29p4a2SLZRVtWxp+exGW5JsW3btP6palkplS6VKtfHfctlSsVJVoVjVVCKrM7Np\nnZvNqFiqXva949GAdm3t1Z5tvYpH2XsGAAA6j9/06frt/fqtf3+TJs8s6umXLuh8IqsL81mdmUlf\n8fWmz1Ak5Fc4aCoa8ity2f9NhYN++U1DAb9PAb+pgGnI7/cpYPoU8Ptk+nzy+SSfz9BgsqD0Ul4+\nn1H7v2HIMCRDtf9Kqq1eqv1PMgwZtf+o9ltj+eOXPr/82zd9fPljK2TLq3UN3/xhY6UPrvRnrvnZ\nTVrlm6/2szfaHQ1mikrnShv6s2txrbrCQVN+01tHSK8a1I4cOaLx8XEFg7Vgceedd+rgwYOXBbWD\nBw/qD/7gDyRJ+/bt0+TkpDKZjGKxWIvKbo1v/uA1/c0/vbLp7+MzDG0dimrHWFzlalWD8bAGe0Ns\nrAUAAF3j6ZcuSJL27xzQ/p0Dsm1b+WJF6Vy59v98WelcSdl8WaWKpVyhomSm2Fh9BDTT6EBEf/bg\nvZ5agrtqUJufn1dPz6Wzu2KxmObn59f0NdcKaiMj8Y3U21IfeldcH3rX9U6XAQAAAKDLrdr/Gxoa\nUjabbfw+k8loaGho3V8DAAAAAFibVYPa7bffrgsXLqhUqq0nPXz4sA4cOKBkMqlMJiNJOnDggF58\n8UVJ0vHjx7V//37PLXsEAAAAALcwbNtedSHwj370I/3Lv/yLBgYGFAgE9NBDD+mLX/yi+vv79eCD\nD6pQKOjhhx/WyMiIzp49q0984hNdP/URAAAAADZqTUENAAAAANA+3ppRCQAAAABdgKAGAAAAAC5D\nUAMAAAAAl1n1HLVOdOjQIX3nO9/R0NCQDMPQQw89dNnni8WiHn74YY2Njen06dN68MEHGY7SRqvd\nPpL01FNP6ZFHHtHnPvc5vetd73Kgyu622m302GOPaW5uTiMjIzp69Kg+/elPa8+ePQ5V231Wu32e\neuopfe9739P+/fv18ssv64Mf/KDe/e53O1Rtd1rL85wk/eM//qM+85nP6PDhw5edV4rWWu32+eY3\nv6lvfOMbCoVCkqQPf/jD+uAHP+hEqV1rtdvItm397d/+rSRpampKS0tL+rM/+zMnSu1Kq90+n/3s\nZ3Xu3LnG70+cOKHHH39cExMT7S7V3ewuk8vl7Pe+9712sVi0bdu2H3roIfvQoUOXfc1f/uVf2o89\n9pht27Y9OTlp/+qv/mrb6+xWa7l9zp49az/zzDP2r/3ar9nf//73nSizq63lNvqLv/gL27Is27Zt\n+8knn7Q/8YlPtL3ObrWW2+fxxx+3p6ambNu27WPHjtnve9/72l5nN1vLbWTbtv3666/bjzzyiH3D\nDTfYmUym3WV2rbU+hs6dO+dEebDXdhs98cQT9hNPPNH4/auvvtrWGrvZWm6fJ598svHrdDpt/87v\n/E5ba/SKrlv6eOTIEY2PjysYDEqS7rzzTh08ePCyrzl48KDuuOMOSdK+ffs0OTnZODMOrbWW22f7\n9u269957HagO0tpuo9/7vd+TYRiSJMuyFI1G211m11rL7fOhD31I4+PjkqQzZ87Q7WyztdxG+Xxe\nf/VXf6X/v537B0nmj+MA/v4hQcEtJYURFdISpBRFUNvNbeLgJkglUUM5GG05Bv2DCILAhKAtFPpD\nf4YgaOgoKogKdWnSFkvipLLM3/QIz/I8X4fn7uL7fk0KN7y5N9/j+znPm5iYMCGh3ET6AYCtrS1E\no1Gsrq4in88bnFJuIh3t7u4in89jc3MTS0tL/EXaQCL9DA0NVT5vb2/D6/UaGfHHkG5Qy+Vyvy1W\nRVGQy+WqPob+DZ5766umo2KxiEQigampKaPiSU+0n/f3d8zPz2NjYwMzMzNGRpSeSEfLy8sYHx+v\nbHTIOCL99Pf3Y3R0FMPDw3C73ZicnDQ6ptREOspkMtB1HX6/Hx6PByMjIyiVSkZHlVI1+4Tv72+c\nnZ1BVVWD0v0s0g1qdrsdhUKh8l3Xddjt9qqPoX+D5976RDsqFouIRCIIhUJoa2szMqLURPupra1F\nOBzGwsIC/H4/Pj8/jYwptb91lM1m8fr6ioODA6yvrwMAYrEYbm9vDc8qI5E11NraioaGBgDAwMAA\nLi4uOAQYSKQjRVHQ3d0NAHA6ndB1Hdls1tCcsqpmL3dycgJVVStP4dDvpBvUenp6kMlkUCwWAQBX\nV1dQVRX5fL7yeKOqqri+vgYAJJNJdHZ2QlEU0zLLRKQfMpdIR29vb5idnUUgEIDL5cLR0ZGZkaUi\n0k80GkW5XAYAOBwOvLy84OPjw7TMsvlbR83NzZibm0MwGEQwGAQABAIBuN1uM2NLQ2QNLS4u4uvr\nCwDw+PiIlpYW2Gw20zLLRqSjwcHByssqdF1HqVRCY2OjaZllUs1eLpFIwOPxmBHzR7BFIpGI2SGM\nVFNTg46ODsRiMdzc3KCpqQlerxcrKytIp9Po6+tDV1cXDg8PcX9/j9PTU0xPT6O+vt7s6FIQ6adc\nLmNtbQ2apqFQKKCurg7t7e1mR5eGSEehUAh3d3e4vLxEIpHA+fk5fD6f2dGlINKPpmnY399HKpVC\nPB6Hz+dDb2+v2dGlIdIRADw/PyMWi0HTNNhsNjidTt40NIBIP+l0GvF4HKlUCsfHxwiHw3A4HGZH\nl4ZIRy6XCzs7O0gmk9jb28PY2Bj/j2sQ0Wvcw8MDnp6e+PbuP/iv/Ou2KhEREREREVmCdI8+EhER\nERERWR0HNSIiIiIiIovhoEZERERERGQxHNSIiIiIiIgshoMaERERERGRxXBQIyIiIiIishgOakRE\nRERERBbzP4R+sGkEaAiBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f6aedd320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(features.flatten()[::1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then, before we start training, the dataset should also be splited. ``sklearn.model_selection.train_test_split`` can be used to achieve this. Remember to set the random state!\n",
    "\n",
    "Some important notes:\n",
    "\n",
    "1. **In practice: ** it is very important to zero-center the data.\n",
    "2. **Common pitfall: ** An important point to make about the preprocessing is that any preprocessing statistics (e.g. the data mean) must only be computed on the training data, and then applied to the validation / test data. E.g. computing the mean and subtracting it from every image across the entire dataset and then splitting the data into train/val/test splits would be a mistake. Instead, the mean must be computed only over the training data and then subtracted equally from all splits (train/val/test).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Split the datatset into training and testing datasets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  features, \n",
    "  targets, \n",
    "  random_state=SEED, \n",
    "  test_size=TEST_SIZE\n",
    ")\n",
    "\n",
    "# Further split the training dataset into training and validation datasets.\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, random_state=SEED, test_size=0.1)\n",
    "\n",
    "# Make the data zero-centered.\n",
    "if ZERO_CENTER:\n",
    "  mean = X_train.mean()\n",
    "  X_train -= mean\n",
    "  X_test -= mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Display the value distributions of ``X_train`` and ``X_test``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Only display these figures if TOTAL_SIZE is not too large!\n",
    "if TOTAL_SIZE < 10000:\n",
    "  fig, axes = plt.subplots(1, 2, figsize=[14, 7])\n",
    "\n",
    "  ax = sns.distplot(X_train.flatten()[::20], ax=axes[0])\n",
    "  ax.set_xlabel(\"Scaled Distance\", fontsize=12)\n",
    "  ax.set_ylabel(\"Relative Numbers\", fontsize=12)\n",
    "  ax.set_title(\"X_train\", fontsize=14)\n",
    "\n",
    "  ax = sns.distplot(X_test.flatten()[::2], ax=axes[1])\n",
    "  ax.set_xlabel(\"Scaled Distance\", fontsize=12)\n",
    "  ax.set_ylabel(\"Relative Numbers\", fontsize=12)\n",
    "  ax.set_title(\"X_test\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f1f6fd98710>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0kAAAG8CAYAAAALue7DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f6fdd30b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=[14, 7])\n",
    "\n",
    "ax = sns.distplot(y_train.flatten(), ax=axes[0])\n",
    "ax.set_xlabel(\"Scaled Energies\", fontsize=12)\n",
    "ax.set_ylabel(\"Relative Numbers\", fontsize=12)\n",
    "ax.set_title(\"y_train\", fontsize=14)\n",
    "\n",
    "ax = sns.distplot(y_test.flatten(), ax=axes[1])\n",
    "ax.set_xlabel(\"Scaled Energies\", fontsize=12)\n",
    "ax.set_ylabel(\"Relative Numbers\", fontsize=12)\n",
    "ax.set_title(\"y_test\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we can start training this network. The training settings defined in this paper are summerized here:\n",
    "\n",
    "* Root-mean-squared loss.\n",
    "* Mini-batch stochastic gradient descent with momentum\n",
    "     * Batch size: 50.\n",
    "     * Momentum factor: 0.7.\n",
    "* Step Decay Function: $s_{i} = s_{0}r/(r + i)$\n",
    "     * $s_0$ is the initial step length, which is 0.1 in this paper.\n",
    "     * r is a predefined factor and r = 60 in this paper.\n",
    "* Regularization factor is not mentioned in this paper.\n",
    "     * $\\lambda$ is set to None.\n",
    "* The exponentia ldecay rate is 0.9 in this implementaion.\n",
    "* The total number of epochs for the reference dataset is 1400.\n",
    "\n",
    "These parameters are declared in the following section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "momentum_factor = 0.8\n",
    "start_learning_rate = 0.04\n",
    "decay_rate = 0.90\n",
    "decay_factor = 60\n",
    "rlambda = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Invoke the interactive session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "sess = tf.InteractiveSession(graph=graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we can begin the training. Initialize a new ``graph`` and use it as the default graph. Then we get the infered network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1/Tanh            : [     50,      1,   4845,     40]\n",
      "Conv2/Tanh            : [     50,      1,   4845,     70]\n",
      "Conv3/Tanh            : [     50,      1,   4845,     60]\n",
      "Conv4/Tanh            : [     50,      1,   4845,      2]\n",
      "Switch                : [     50,      1,      2,   4845]\n",
      "Conv5/Softplus        : [     50,      1,      2,    200]\n",
      "Conv6/Softplus        : [     50,      1,      2,     20]\n",
      "Flatten/Reshape       : [     50,     40]\n",
      "Output                : [     50,      1]\n"
     ]
    }
   ],
   "source": [
    "X_batch = tf.placeholder(TF_TYPE, [batch_size, 1, CNK, CK2], name=\"X_batch\")\n",
    "y_batch = tf.placeholder(TF_TYPE, [batch_size, 1], name=\"y_batch\")\n",
    "keep_prob = tf.placeholder(TF_TYPE, name=\"conv_keep\")\n",
    "dense_keep_prob = tf.placeholder(TF_TYPE, name=\"fc_keep\")\n",
    "\n",
    "# y_pred = models.inference(\n",
    "#   X_batch, \n",
    "#   \"mbe-nn-m-fc\", \n",
    "#   conv_keep_prob=keep_prob, \n",
    "#   dense_keep_prob=dense_keep_prob, \n",
    "#   dropouts=(6,), \n",
    "#   dense_dims=[200, 20], \n",
    "#   dense_funcs=[tf.nn.relu, tf.nn.relu, tf.nn.tanh],\n",
    "#   conv_dims=[40, 70, 60, 2, 400, 200],\n",
    "#   verbose=True\n",
    "# )\n",
    "\n",
    "y_pred = models.inference(\n",
    "  X_batch,\n",
    "  \"mbe-nn-m\",\n",
    "  dims=[40, 70, 60, 2, 200, 20],\n",
    "  verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Output the total number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1/filter/kernel:0      240\n",
      "Conv1/biases/biases:0      40\n",
      "Conv2/filter/kernel:0      2800\n",
      "Conv2/biases/biases:0      70\n",
      "Conv3/filter/kernel:0      4200\n",
      "Conv3/biases/biases:0      60\n",
      "Conv4/filter/kernel:0      120\n",
      "Conv4/biases/biases:0      2\n",
      "Conv5/filter/kernel:0      969000\n",
      "Conv5/biases/biases:0      200\n",
      "Conv6/filter/kernel:0      4000\n",
      "Conv6/biases/biases:0      20\n",
      "\n",
      "Total number of parameters: 980752\n"
     ]
    }
   ],
   "source": [
    "models.get_number_of_trainable_parameters(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Add helper functions to visualize the graph. These codes are copied from the official example **deepdream**. Uncomment the last line can display the network graph inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def strip_consts(graph_def, max_const_size=32):\n",
    "  \"\"\"\n",
    "  Strip large constant values from graph_def.\n",
    "  \"\"\"\n",
    "  strip_def = tf.GraphDef()\n",
    "  for n0 in graph_def.node:\n",
    "    n = strip_def.node.add() \n",
    "    n.MergeFrom(n0)\n",
    "    if n.op == 'Const':\n",
    "      tensor = n.attr['value'].tensor\n",
    "      size = len(tensor.tensor_content)\n",
    "      if size > max_const_size:\n",
    "        tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "  return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "  \"\"\"\n",
    "  Visualize TensorFlow graph.\n",
    "  \"\"\"\n",
    "  if hasattr(graph_def, 'as_graph_def'):\n",
    "    graph_def = graph_def.as_graph_def()\n",
    "  strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "  code = \"\"\"\n",
    "    <script src=\"//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js\"></script>\n",
    "    <script>\n",
    "      function load() {{\n",
    "        document.getElementById(\"{id}\").pbtxt = {data};\n",
    "      }}\n",
    "    </script>\n",
    "    <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "    <div style=\"height:600px\">\n",
    "    <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "    </div>\n",
    "  \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "  iframe = \"\"\"\n",
    "    <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "  \"\"\".format(code.replace('\"', '&quot;'))\n",
    "  display(HTML(iframe))\n",
    "\n",
    "# Uncomment the following line to visualize the graph in this notebook. \n",
    "# show_graph(graph.as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Setup the total loss. Add L2 loss for the weights of dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "  \n",
    "  regvars = tf.get_collection(GraphKeys.REGULARIZATION_LOSSES)\n",
    "  if len(regvars) > 0 and rlambda is not None:\n",
    "    def _nameit(_w):\n",
    "      return \"%sL2\" % _w.name.split(\"/\")[0]\n",
    "\n",
    "    with tf.name_scope(\"regularizer\"):\n",
    "      l2 = tf.multiply(\n",
    "        tf.add_n([tf.nn.l2_loss(w, name=_nameit(w)) for w in regvars]), rlambda)\n",
    "      tf.summary.scalar(\"l2\", l2)\n",
    "  else:\n",
    "    l2 = tf.constant(0.0)\n",
    "\n",
    "  rms = tf.sqrt(tf.losses.mean_squared_error(y_pred, y_batch, scope=\"RMSE\"))\n",
    "  loss = tf.add(rms, l2)\n",
    "  tf.summary.scalar(\"rms\", rms)\n",
    "  tf.summary.scalar(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Setup the learning rate decay and the momentum optimizer. The learning rate $r_i$ computed by the inverse decay function in this paper is implemented here:\n",
    "$$r_{i} = \\frac{rs_{0}}{r + i}$$\n",
    "where $i$ is the epoch, $r$ is a constant decay factor and $s_0$ is the initial learning rate. $r$ is set to 60 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def inverse_decay(init_learning_rate, epoch, decay_factor, name=None):\n",
    "  \"\"\"\n",
    "  The inverse decay function.\n",
    "  \n",
    "  Args:\n",
    "    init_learning_rate: A scalar `float32` or `float64` `Tensor` or a\n",
    "      Python number.  The initial learning rate.\n",
    "    global_epoch: A scalar `int32` or `int64` `Tensor` or a Python number.\n",
    "      Global epoch to use for the decay computation.  Must not be negative.\n",
    "    decay_factor: A scalar `int32` or `int64` `Tensor` or a Python number.\n",
    "      Must be positive.  See the decay equation above.\n",
    "    name: String.  Optional name of the operation.  Defaults to\n",
    "      'ExponentialDecay'.\n",
    "\n",
    "  Returns:\n",
    "    A scalar `Tensor` of the same type as `learning_rate`.  The decayed\n",
    "    learning rate.\n",
    "  \n",
    "  \"\"\"\n",
    "  if epoch is None:\n",
    "    raise ValueError(\"global_step is required for inv_decay.\")\n",
    "  with ops.name_scope(name, \"InvDecay\", \n",
    "                      [init_learning_rate, epoch, decay_factor]) as name:\n",
    "    init_learning_rate = ops.convert_to_tensor(\n",
    "      init_learning_rate, name=\"init_learning_rate\")\n",
    "    dtype = init_learning_rate.dtype\n",
    "    epoch = math_ops.cast(epoch, dtype)\n",
    "    decay_factor = math_ops.cast(decay_factor, dtype)\n",
    "    top = math_ops.multiply(init_learning_rate, decay_factor)\n",
    "    return math_ops.div(top, math_ops.add(epoch, decay_factor), name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "global_epoch = tf.Variable(0, dtype=tf.int64, trainable=False, name=\"global_epoch\")\n",
    "global_epoch_op = tf.assign(global_epoch, global_epoch + 1)\n",
    "global_step = tf.Variable(0, dtype=tf.int64, trainable=False, name=\"global_step\")\n",
    "learning_rate = inverse_decay(start_learning_rate, global_epoch, decay_factor)\n",
    "\n",
    "# learning_rate = tf.train.exponential_decay(\n",
    "#   start_learning_rate, \n",
    "#   tf.multiply(global_step, batch_size), \n",
    "#   len(X_train), \n",
    "#   decay_rate,\n",
    "#   staircase=True,\n",
    "# )\n",
    "\n",
    "# Test the Adagrad optimzier\n",
    "# trainer = tf.train.AdagradDAOptimizer(\n",
    "#   learning_rate=learning_rate, \n",
    "#   initial_gradient_squared_accumulator_value=0.1,\n",
    "#   global_step=batch\n",
    "# )\n",
    "\n",
    "tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "\n",
    "# Use the Momentum optimizer\n",
    "trainer = tf.train.MomentumOptimizer(learning_rate, momentum_factor)\n",
    "optimizer = trainer.minimize(loss, global_step=global_step)\n",
    "\n",
    "# Merge all the summaries.\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(logdir=\"./events/%s\" % get_time_id(), graph=graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.2 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "These variabls control the key settings of a training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "log_frequency = 20\n",
    "eval_frequency = 200\n",
    "save_frequency = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Setup the saving path and the checkpoint file so that we can reuse this model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "save_path = \"./saves\"\n",
    "chk_file = join(save_path, \"c9h7n.ckpt\")\n",
    "if not isdir(save_path):\n",
    "  makedirs(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Small utility function to evaluate a dataset by feeding batches of data to ``{eval_dataset}`` and pulling the results \n",
    "from ``{eval_values}``. Saves memory and enables this to run on smaller GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def eval_in_batches(data):\n",
    "  \"\"\" Get all predictions for a dataset by running it in small batches. \"\"\"\n",
    "  size = data.shape[0]\n",
    "  if size < batch_size:\n",
    "    raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\n",
    "  eval_values = np.ndarray(shape=(size, 1), dtype=NP_TYPE)\n",
    "  for i, inext in brange(0, size, batch_size):\n",
    "    eval_values[i: inext] = sess.run(\n",
    "      y_pred,\n",
    "      feed_dict={X_batch: data[i: inext, ...], keep_prob: 1.0, dense_keep_prob: 1.0})\n",
    "  return eval_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This helper function is used to restore the latest checkpoint if existed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def restore_latest_from_ckpt(save_dir):\n",
    "  \"\"\"\n",
    "  Restore the latest checkpoint from the 'scratch' if possible.\n",
    "  \"\"\"\n",
    "  if not isdir(save_dir):\n",
    "    return None\n",
    "  ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "  if ckpt and ckpt.model_checkpoint_path:\n",
    "    return ckpt.model_checkpoint_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally we invoke a new session and start this training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\n",
      "\n",
      "Training Samples      : 160000\n",
      "Batch Size            : 50\n",
      "Number of Epochs      : 100\n",
      "Log Frequency         : 20\n",
      "Eval Frequency        : 200\n",
      "\n",
      "Step      0 (epoch    0.00), loss: 0.757168, time: 0.701 s\n",
      "\n",
      "Time since beginning  : 0.703 s\n",
      "\n",
      "Step     20 (epoch    0.01), loss: 0.061363, time: 2.530 s\n",
      "Step     40 (epoch    0.01), loss: 0.048773, time: 2.210 s\n",
      "Step     60 (epoch    0.02), loss: 0.054765, time: 2.222 s\n",
      "Step     80 (epoch    0.03), loss: 0.057087, time: 2.201 s\n",
      "Step    100 (epoch    0.03), loss: 0.073611, time: 2.203 s\n",
      "Step    120 (epoch    0.04), loss: 0.078144, time: 2.210 s\n",
      "Step    140 (epoch    0.04), loss: 0.088645, time: 2.198 s\n",
      "Step    160 (epoch    0.05), loss: 0.078884, time: 2.207 s\n",
      "Step    180 (epoch    0.06), loss: 0.068156, time: 2.200 s\n",
      "Step    200 (epoch    0.06), loss: 0.056265, time: 2.205 s, error: 0.067328\n",
      "Step    220 (epoch    0.07), loss: 0.052190, time: 14.236 s\n",
      "Step    240 (epoch    0.07), loss: 0.054995, time: 2.206 s\n",
      "Step    260 (epoch    0.08), loss: 0.073393, time: 2.209 s\n",
      "Step    280 (epoch    0.09), loss: 0.078408, time: 2.207 s\n",
      "Step    300 (epoch    0.09), loss: 0.076260, time: 2.200 s\n",
      "Step    320 (epoch    0.10), loss: 0.082105, time: 2.201 s\n",
      "Step    340 (epoch    0.11), loss: 0.068125, time: 2.195 s\n",
      "Step    360 (epoch    0.11), loss: 0.054942, time: 2.220 s\n",
      "Step    380 (epoch    0.12), loss: 0.061835, time: 2.220 s\n",
      "Step    400 (epoch    0.12), loss: 0.070999, time: 2.203 s, error: 0.067308\n",
      "Step    420 (epoch    0.13), loss: 0.085194, time: 14.270 s\n",
      "Step    440 (epoch    0.14), loss: 0.046522, time: 2.221 s\n",
      "Step    460 (epoch    0.14), loss: 0.083144, time: 2.218 s\n",
      "Step    480 (epoch    0.15), loss: 0.056122, time: 2.215 s\n",
      "Step    500 (epoch    0.16), loss: 0.091009, time: 2.204 s\n",
      "Step    520 (epoch    0.16), loss: 0.064049, time: 2.200 s\n",
      "Step    540 (epoch    0.17), loss: 0.074713, time: 2.216 s\n",
      "Step    560 (epoch    0.17), loss: 0.057049, time: 2.191 s\n",
      "Step    580 (epoch    0.18), loss: 0.057027, time: 2.207 s\n",
      "Step    600 (epoch    0.19), loss: 0.057193, time: 2.198 s, error: 0.067264\n",
      "Step    620 (epoch    0.19), loss: 0.042874, time: 14.143 s\n",
      "Step    640 (epoch    0.20), loss: 0.058968, time: 2.213 s\n",
      "Step    660 (epoch    0.21), loss: 0.068337, time: 2.225 s\n",
      "Step    680 (epoch    0.21), loss: 0.059081, time: 2.225 s\n",
      "Step    700 (epoch    0.22), loss: 0.050113, time: 2.206 s\n",
      "Step    720 (epoch    0.23), loss: 0.063606, time: 2.221 s\n",
      "Step    740 (epoch    0.23), loss: 0.058405, time: 2.217 s\n",
      "Step    760 (epoch    0.24), loss: 0.044355, time: 2.221 s\n",
      "Step    780 (epoch    0.24), loss: 0.071361, time: 2.221 s\n",
      "Step    800 (epoch    0.25), loss: 0.067666, time: 2.210 s, error: 0.067361\n",
      "Step    820 (epoch    0.26), loss: 0.052310, time: 14.157 s\n",
      "Step    840 (epoch    0.26), loss: 0.055208, time: 2.219 s\n",
      "Step    860 (epoch    0.27), loss: 0.067572, time: 2.212 s\n",
      "Step    880 (epoch    0.28), loss: 0.065319, time: 2.217 s\n",
      "Step    900 (epoch    0.28), loss: 0.060640, time: 2.222 s\n",
      "Step    920 (epoch    0.29), loss: 0.069024, time: 2.203 s\n",
      "Step    940 (epoch    0.29), loss: 0.047671, time: 2.227 s\n",
      "Step    960 (epoch    0.30), loss: 0.044618, time: 2.223 s\n",
      "Step    980 (epoch    0.31), loss: 0.062245, time: 2.213 s\n",
      "Step   1000 (epoch    0.31), loss: 0.081897, time: 2.216 s, error: 0.067221\n",
      "Step   1020 (epoch    0.32), loss: 0.062772, time: 14.144 s\n",
      "Step   1040 (epoch    0.33), loss: 0.064612, time: 2.216 s\n",
      "Step   1060 (epoch    0.33), loss: 0.063177, time: 2.222 s\n",
      "Step   1080 (epoch    0.34), loss: 0.062509, time: 2.218 s\n",
      "Step   1100 (epoch    0.34), loss: 0.092706, time: 2.216 s\n",
      "Step   1120 (epoch    0.35), loss: 0.092574, time: 2.215 s\n",
      "Step   1140 (epoch    0.36), loss: 0.082897, time: 2.216 s\n",
      "Step   1160 (epoch    0.36), loss: 0.093298, time: 2.213 s\n",
      "Step   1180 (epoch    0.37), loss: 0.087739, time: 2.206 s\n",
      "Step   1200 (epoch    0.38), loss: 0.050239, time: 2.215 s, error: 0.067178\n",
      "Step   1220 (epoch    0.38), loss: 0.079422, time: 14.252 s\n",
      "Step   1240 (epoch    0.39), loss: 0.072109, time: 2.219 s\n",
      "Step   1260 (epoch    0.39), loss: 0.084567, time: 2.218 s\n",
      "Step   1280 (epoch    0.40), loss: 0.038484, time: 2.202 s\n",
      "Step   1300 (epoch    0.41), loss: 0.074232, time: 2.217 s\n",
      "Step   1320 (epoch    0.41), loss: 0.061515, time: 2.221 s\n",
      "Step   1340 (epoch    0.42), loss: 0.089722, time: 2.222 s\n",
      "Step   1360 (epoch    0.42), loss: 0.060657, time: 2.222 s\n",
      "Step   1380 (epoch    0.43), loss: 0.076616, time: 2.224 s\n",
      "Step   1400 (epoch    0.44), loss: 0.052782, time: 2.223 s, error: 0.067146\n",
      "Step   1420 (epoch    0.44), loss: 0.046951, time: 14.379 s\n",
      "Step   1440 (epoch    0.45), loss: 0.052549, time: 2.220 s\n",
      "Step   1460 (epoch    0.46), loss: 0.058829, time: 2.217 s\n",
      "Step   1480 (epoch    0.46), loss: 0.068180, time: 2.218 s\n",
      "Step   1500 (epoch    0.47), loss: 0.044319, time: 2.225 s\n",
      "Step   1520 (epoch    0.47), loss: 0.102227, time: 2.211 s\n",
      "Step   1540 (epoch    0.48), loss: 0.054404, time: 2.212 s\n",
      "Step   1560 (epoch    0.49), loss: 0.040708, time: 2.205 s\n",
      "Step   1580 (epoch    0.49), loss: 0.054148, time: 2.223 s\n",
      "Step   1600 (epoch    0.50), loss: 0.050657, time: 2.222 s, error: 0.067286\n",
      "Step   1620 (epoch    0.51), loss: 0.047085, time: 14.239 s\n",
      "Step   1640 (epoch    0.51), loss: 0.076184, time: 2.218 s\n",
      "Step   1660 (epoch    0.52), loss: 0.055601, time: 2.217 s\n",
      "Step   1680 (epoch    0.53), loss: 0.072531, time: 2.222 s\n",
      "Step   1700 (epoch    0.53), loss: 0.059088, time: 2.225 s\n",
      "Step   1720 (epoch    0.54), loss: 0.064346, time: 2.223 s\n",
      "Step   1740 (epoch    0.54), loss: 0.076463, time: 2.217 s\n",
      "Step   1760 (epoch    0.55), loss: 0.076238, time: 2.213 s\n",
      "Step   1780 (epoch    0.56), loss: 0.065448, time: 2.206 s\n",
      "Step   1800 (epoch    0.56), loss: 0.061509, time: 2.214 s, error: 0.066725\n",
      "Step   1820 (epoch    0.57), loss: 0.064220, time: 14.188 s\n",
      "Step   1840 (epoch    0.57), loss: 0.046035, time: 2.223 s\n",
      "Step   1860 (epoch    0.58), loss: 0.051064, time: 2.221 s\n",
      "Step   1880 (epoch    0.59), loss: 0.057518, time: 2.222 s\n",
      "Step   1900 (epoch    0.59), loss: 0.054927, time: 2.228 s\n",
      "Step   1920 (epoch    0.60), loss: 0.037249, time: 2.223 s\n",
      "Step   1940 (epoch    0.61), loss: 0.074951, time: 2.227 s\n",
      "Step   1960 (epoch    0.61), loss: 0.068068, time: 2.218 s\n",
      "Step   1980 (epoch    0.62), loss: 0.072028, time: 2.221 s\n",
      "Step   2000 (epoch    0.62), loss: 0.058241, time: 2.215 s, error: 0.066382\n",
      "\n",
      "Time since beginning  : 342.533 s\n",
      "\n",
      "Step   2020 (epoch    0.63), loss: 0.084159, time: 14.266 s\n",
      "Step   2040 (epoch    0.64), loss: 0.067999, time: 2.224 s\n",
      "Step   2060 (epoch    0.64), loss: 0.082226, time: 2.215 s\n",
      "Step   2080 (epoch    0.65), loss: 0.050750, time: 2.224 s\n",
      "Step   2100 (epoch    0.66), loss: 0.061275, time: 2.214 s\n",
      "Step   2120 (epoch    0.66), loss: 0.047379, time: 2.221 s\n",
      "Step   2140 (epoch    0.67), loss: 0.051503, time: 2.224 s\n",
      "Step   2160 (epoch    0.68), loss: 0.061632, time: 2.226 s\n",
      "Step   2180 (epoch    0.68), loss: 0.069892, time: 2.223 s\n",
      "Step   2200 (epoch    0.69), loss: 0.081968, time: 2.219 s, error: 0.065971\n",
      "Step   2220 (epoch    0.69), loss: 0.058598, time: 14.199 s\n",
      "Step   2240 (epoch    0.70), loss: 0.068476, time: 2.218 s\n",
      "Step   2260 (epoch    0.71), loss: 0.060659, time: 2.223 s\n",
      "Step   2280 (epoch    0.71), loss: 0.041490, time: 2.223 s\n",
      "Step   2300 (epoch    0.72), loss: 0.055950, time: 2.222 s\n",
      "Step   2320 (epoch    0.72), loss: 0.069249, time: 2.221 s\n",
      "Step   2340 (epoch    0.73), loss: 0.140809, time: 2.223 s\n",
      "Step   2360 (epoch    0.74), loss: 0.068211, time: 2.220 s\n",
      "Step   2380 (epoch    0.74), loss: 0.064022, time: 2.213 s\n",
      "Step   2400 (epoch    0.75), loss: 0.043627, time: 2.220 s, error: 0.064920\n",
      "Step   2420 (epoch    0.76), loss: 0.052627, time: 14.461 s\n",
      "Step   2440 (epoch    0.76), loss: 0.075562, time: 2.225 s\n",
      "Step   2460 (epoch    0.77), loss: 0.076619, time: 2.227 s\n",
      "Step   2480 (epoch    0.78), loss: 0.042608, time: 2.218 s\n",
      "Step   2500 (epoch    0.78), loss: 0.067352, time: 2.223 s\n",
      "Step   2520 (epoch    0.79), loss: 0.048801, time: 2.215 s\n",
      "Step   2540 (epoch    0.79), loss: 0.080608, time: 2.224 s\n",
      "Step   2560 (epoch    0.80), loss: 0.072251, time: 2.217 s\n",
      "Step   2580 (epoch    0.81), loss: 0.068520, time: 2.222 s\n",
      "Step   2600 (epoch    0.81), loss: 0.051534, time: 2.215 s, error: 0.064739\n",
      "Step   2620 (epoch    0.82), loss: 0.052417, time: 14.349 s\n",
      "Step   2640 (epoch    0.82), loss: 0.039865, time: 2.226 s\n",
      "Step   2660 (epoch    0.83), loss: 0.052131, time: 2.225 s\n",
      "Step   2680 (epoch    0.84), loss: 0.081268, time: 2.225 s\n",
      "Step   2700 (epoch    0.84), loss: 0.088525, time: 2.221 s\n",
      "Step   2720 (epoch    0.85), loss: 0.040803, time: 2.215 s\n",
      "Step   2740 (epoch    0.86), loss: 0.044393, time: 2.216 s\n",
      "Step   2760 (epoch    0.86), loss: 0.069317, time: 2.224 s\n",
      "Step   2780 (epoch    0.87), loss: 0.047815, time: 2.217 s\n",
      "Step   2800 (epoch    0.88), loss: 0.061691, time: 2.214 s, error: 0.062846\n",
      "Step   2820 (epoch    0.88), loss: 0.032319, time: 14.323 s\n",
      "Step   2840 (epoch    0.89), loss: 0.067447, time: 2.226 s\n",
      "Step   2860 (epoch    0.89), loss: 0.065184, time: 2.231 s\n",
      "Step   2880 (epoch    0.90), loss: 0.097053, time: 2.227 s\n",
      "Step   2900 (epoch    0.91), loss: 0.051896, time: 2.218 s\n",
      "Step   2920 (epoch    0.91), loss: 0.040865, time: 2.220 s\n",
      "Step   2940 (epoch    0.92), loss: 0.057040, time: 2.209 s\n",
      "Step   2960 (epoch    0.93), loss: 0.079698, time: 2.223 s\n",
      "Step   2980 (epoch    0.93), loss: 0.070743, time: 2.203 s\n",
      "Step   3000 (epoch    0.94), loss: 0.072896, time: 2.224 s, error: 0.061300\n",
      "Step   3020 (epoch    0.94), loss: 0.078922, time: 14.336 s\n",
      "Step   3040 (epoch    0.95), loss: 0.040201, time: 2.226 s\n",
      "Step   3060 (epoch    0.96), loss: 0.060029, time: 2.214 s\n",
      "Step   3080 (epoch    0.96), loss: 0.065058, time: 2.225 s\n",
      "Step   3100 (epoch    0.97), loss: 0.035029, time: 2.227 s\n",
      "Step   3120 (epoch    0.97), loss: 0.039139, time: 2.227 s\n",
      "Step   3140 (epoch    0.98), loss: 0.049737, time: 2.217 s\n",
      "Step   3160 (epoch    0.99), loss: 0.056546, time: 2.214 s\n",
      "Step   3180 (epoch    0.99), loss: 0.061306, time: 2.224 s\n",
      "Step   3200 (epoch    1.00), loss: 0.036280, time: 2.234 s, error: 0.061267\n",
      "Step   3220 (epoch    1.01), loss: 0.052827, time: 14.344 s\n",
      "Step   3240 (epoch    1.01), loss: 0.046361, time: 2.225 s\n",
      "Step   3260 (epoch    1.02), loss: 0.060754, time: 2.216 s\n",
      "Step   3280 (epoch    1.02), loss: 0.051129, time: 2.215 s\n",
      "Step   3300 (epoch    1.03), loss: 0.041376, time: 2.204 s\n",
      "Step   3320 (epoch    1.04), loss: 0.058811, time: 2.211 s\n",
      "Step   3340 (epoch    1.04), loss: 0.056428, time: 2.206 s\n",
      "Step   3360 (epoch    1.05), loss: 0.053273, time: 2.217 s\n",
      "Step   3380 (epoch    1.06), loss: 0.054490, time: 2.228 s\n",
      "Step   3400 (epoch    1.06), loss: 0.042471, time: 2.219 s, error: 0.059068\n",
      "Step   3420 (epoch    1.07), loss: 0.044715, time: 14.175 s\n",
      "Step   3440 (epoch    1.07), loss: 0.068041, time: 2.219 s\n",
      "Step   3460 (epoch    1.08), loss: 0.046309, time: 2.220 s\n",
      "Step   3480 (epoch    1.09), loss: 0.050994, time: 2.221 s\n",
      "Step   3500 (epoch    1.09), loss: 0.072625, time: 2.208 s\n",
      "Step   3520 (epoch    1.10), loss: 0.043842, time: 2.215 s\n",
      "Step   3540 (epoch    1.11), loss: 0.059907, time: 2.218 s\n",
      "Step   3560 (epoch    1.11), loss: 0.079793, time: 2.222 s\n",
      "Step   3580 (epoch    1.12), loss: 0.063809, time: 2.216 s\n",
      "Step   3600 (epoch    1.12), loss: 0.071271, time: 2.223 s, error: 0.067455\n",
      "Step   3620 (epoch    1.13), loss: 0.082508, time: 14.329 s\n",
      "Step   3640 (epoch    1.14), loss: 0.104361, time: 2.211 s\n",
      "Step   3660 (epoch    1.14), loss: 0.073905, time: 2.220 s\n",
      "Step   3680 (epoch    1.15), loss: 0.053706, time: 2.223 s\n",
      "Step   3700 (epoch    1.16), loss: 0.073107, time: 2.206 s\n",
      "Step   3720 (epoch    1.16), loss: 0.037220, time: 2.221 s\n",
      "Step   3740 (epoch    1.17), loss: 0.051084, time: 2.216 s\n",
      "Step   3760 (epoch    1.18), loss: 0.060100, time: 2.210 s\n",
      "Step   3780 (epoch    1.18), loss: 0.070862, time: 2.215 s\n",
      "Step   3800 (epoch    1.19), loss: 0.044322, time: 2.217 s, error: 0.058926\n",
      "Step   3820 (epoch    1.19), loss: 0.052256, time: 14.177 s\n",
      "Step   3840 (epoch    1.20), loss: 0.049789, time: 2.226 s\n",
      "Step   3860 (epoch    1.21), loss: 0.057977, time: 2.219 s\n",
      "Step   3880 (epoch    1.21), loss: 0.046455, time: 2.220 s\n",
      "Step   3900 (epoch    1.22), loss: 0.052128, time: 2.220 s\n",
      "Step   3920 (epoch    1.23), loss: 0.044067, time: 2.206 s\n",
      "Step   3940 (epoch    1.23), loss: 0.095812, time: 2.207 s\n",
      "Step   3960 (epoch    1.24), loss: 0.060535, time: 2.210 s\n",
      "Step   3980 (epoch    1.24), loss: 0.050608, time: 2.225 s\n",
      "Step   4000 (epoch    1.25), loss: 0.063777, time: 2.222 s, error: 0.058373\n",
      "\n",
      "Time since beginning  : 685.282 s\n",
      "\n",
      "Step   4020 (epoch    1.26), loss: 0.045464, time: 14.323 s\n",
      "Step   4040 (epoch    1.26), loss: 0.068228, time: 2.211 s\n",
      "Step   4060 (epoch    1.27), loss: 0.051150, time: 2.208 s\n",
      "Step   4080 (epoch    1.27), loss: 0.045876, time: 2.220 s\n",
      "Step   4100 (epoch    1.28), loss: 0.058496, time: 2.218 s\n",
      "Step   4120 (epoch    1.29), loss: 0.096190, time: 2.218 s\n",
      "Step   4140 (epoch    1.29), loss: 0.061176, time: 2.211 s\n",
      "Step   4160 (epoch    1.30), loss: 0.034318, time: 2.206 s\n",
      "Step   4180 (epoch    1.31), loss: 0.035694, time: 2.206 s\n",
      "Step   4200 (epoch    1.31), loss: 0.078279, time: 2.218 s, error: 0.065863\n",
      "Step   4220 (epoch    1.32), loss: 0.061601, time: 14.190 s\n",
      "Step   4240 (epoch    1.32), loss: 0.040386, time: 2.223 s\n",
      "Step   4260 (epoch    1.33), loss: 0.070756, time: 2.216 s\n",
      "Step   4280 (epoch    1.34), loss: 0.050236, time: 2.222 s\n",
      "Step   4300 (epoch    1.34), loss: 0.047718, time: 2.217 s\n",
      "Step   4320 (epoch    1.35), loss: 0.057450, time: 2.212 s\n",
      "Step   4340 (epoch    1.36), loss: 0.033111, time: 2.213 s\n",
      "Step   4360 (epoch    1.36), loss: 0.058541, time: 2.225 s\n",
      "Step   4380 (epoch    1.37), loss: 0.042599, time: 2.223 s\n",
      "Step   4400 (epoch    1.38), loss: 0.033464, time: 2.214 s, error: 0.051413\n",
      "Step   4420 (epoch    1.38), loss: 0.057016, time: 14.307 s\n",
      "Step   4440 (epoch    1.39), loss: 0.064219, time: 2.223 s\n",
      "Step   4460 (epoch    1.39), loss: 0.039823, time: 2.225 s\n",
      "Step   4480 (epoch    1.40), loss: 0.069832, time: 2.219 s\n",
      "Step   4500 (epoch    1.41), loss: 0.064527, time: 2.212 s\n",
      "Step   4520 (epoch    1.41), loss: 0.054691, time: 2.202 s\n",
      "Step   4540 (epoch    1.42), loss: 0.056076, time: 2.223 s\n",
      "Step   4560 (epoch    1.43), loss: 0.070251, time: 2.219 s\n",
      "Step   4580 (epoch    1.43), loss: 0.046185, time: 2.223 s\n",
      "Step   4600 (epoch    1.44), loss: 0.042409, time: 2.221 s, error: 0.058890\n",
      "Step   4620 (epoch    1.44), loss: 0.061042, time: 14.334 s\n",
      "Step   4640 (epoch    1.45), loss: 0.036428, time: 2.223 s\n",
      "Step   4660 (epoch    1.46), loss: 0.048217, time: 2.226 s\n",
      "Step   4680 (epoch    1.46), loss: 0.047642, time: 2.214 s\n",
      "Step   4700 (epoch    1.47), loss: 0.048558, time: 2.227 s\n",
      "Step   4720 (epoch    1.48), loss: 0.037078, time: 2.216 s\n",
      "Step   4740 (epoch    1.48), loss: 0.044679, time: 2.222 s\n",
      "Step   4760 (epoch    1.49), loss: 0.068532, time: 2.209 s\n",
      "Step   4780 (epoch    1.49), loss: 0.035114, time: 2.220 s\n",
      "Step   4800 (epoch    1.50), loss: 0.043176, time: 2.223 s, error: 0.059455\n",
      "Step   4820 (epoch    1.51), loss: 0.047747, time: 14.302 s\n",
      "Step   4840 (epoch    1.51), loss: 0.058731, time: 2.219 s\n",
      "Step   4860 (epoch    1.52), loss: 0.069041, time: 2.223 s\n",
      "Step   4880 (epoch    1.52), loss: 0.038971, time: 2.220 s\n",
      "Step   4900 (epoch    1.53), loss: 0.056298, time: 2.225 s\n",
      "Step   4920 (epoch    1.54), loss: 0.073147, time: 2.214 s\n",
      "Step   4940 (epoch    1.54), loss: 0.049994, time: 2.225 s\n",
      "Step   4960 (epoch    1.55), loss: 0.038203, time: 2.213 s\n",
      "Step   4980 (epoch    1.56), loss: 0.036191, time: 2.221 s\n",
      "Step   5000 (epoch    1.56), loss: 0.062944, time: 2.225 s, error: 0.063837\n",
      "Step   5020 (epoch    1.57), loss: 0.068983, time: 14.245 s\n",
      "Step   5040 (epoch    1.57), loss: 0.028436, time: 2.218 s\n",
      "Step   5060 (epoch    1.58), loss: 0.036204, time: 2.221 s\n",
      "Step   5080 (epoch    1.59), loss: 0.039278, time: 2.223 s\n",
      "Step   5100 (epoch    1.59), loss: 0.052847, time: 2.216 s\n",
      "Step   5120 (epoch    1.60), loss: 0.059282, time: 2.217 s\n",
      "Step   5140 (epoch    1.61), loss: 0.043060, time: 2.211 s\n",
      "Step   5160 (epoch    1.61), loss: 0.032738, time: 2.225 s\n",
      "Step   5180 (epoch    1.62), loss: 0.059072, time: 2.221 s\n",
      "Step   5200 (epoch    1.62), loss: 0.034571, time: 2.224 s, error: 0.051245\n",
      "Step   5220 (epoch    1.63), loss: 0.039398, time: 14.220 s\n",
      "Step   5240 (epoch    1.64), loss: 0.040150, time: 2.222 s\n",
      "Step   5260 (epoch    1.64), loss: 0.041357, time: 2.226 s\n",
      "Step   5280 (epoch    1.65), loss: 0.047655, time: 2.217 s\n",
      "Step   5300 (epoch    1.66), loss: 0.042585, time: 2.213 s\n",
      "Step   5320 (epoch    1.66), loss: 0.070783, time: 2.211 s\n",
      "Step   5340 (epoch    1.67), loss: 0.053897, time: 2.223 s\n",
      "Step   5360 (epoch    1.68), loss: 0.022028, time: 2.229 s\n",
      "Step   5380 (epoch    1.68), loss: 0.038266, time: 2.213 s\n",
      "Step   5400 (epoch    1.69), loss: 0.048134, time: 2.218 s, error: 0.049749\n",
      "Step   5420 (epoch    1.69), loss: 0.041958, time: 14.207 s\n",
      "Step   5440 (epoch    1.70), loss: 0.053317, time: 2.214 s\n",
      "Step   5460 (epoch    1.71), loss: 0.022503, time: 2.225 s\n",
      "Step   5480 (epoch    1.71), loss: 0.049138, time: 2.218 s\n",
      "Step   5500 (epoch    1.72), loss: 0.048006, time: 2.226 s\n",
      "Step   5520 (epoch    1.73), loss: 0.060054, time: 2.222 s\n",
      "Step   5540 (epoch    1.73), loss: 0.055048, time: 2.217 s\n",
      "Step   5560 (epoch    1.74), loss: 0.056176, time: 2.207 s\n",
      "Step   5580 (epoch    1.74), loss: 0.035133, time: 2.222 s\n",
      "Step   5600 (epoch    1.75), loss: 0.051122, time: 2.213 s, error: 0.046945\n",
      "Step   5620 (epoch    1.76), loss: 0.041252, time: 14.363 s\n",
      "Step   5640 (epoch    1.76), loss: 0.064395, time: 2.226 s\n",
      "Step   5660 (epoch    1.77), loss: 0.036423, time: 2.213 s\n",
      "Step   5680 (epoch    1.77), loss: 0.032592, time: 2.220 s\n",
      "Step   5700 (epoch    1.78), loss: 0.039059, time: 2.219 s\n",
      "Step   5720 (epoch    1.79), loss: 0.040126, time: 2.211 s\n",
      "Step   5740 (epoch    1.79), loss: 0.035844, time: 2.215 s\n",
      "Step   5760 (epoch    1.80), loss: 0.042653, time: 2.222 s\n",
      "Step   5780 (epoch    1.81), loss: 0.028113, time: 2.220 s\n",
      "Step   5800 (epoch    1.81), loss: 0.061179, time: 2.222 s, error: 0.049210\n",
      "Step   5820 (epoch    1.82), loss: 0.036583, time: 14.339 s\n",
      "Step   5840 (epoch    1.82), loss: 0.058890, time: 2.225 s\n",
      "Step   5860 (epoch    1.83), loss: 0.069390, time: 2.210 s\n",
      "Step   5880 (epoch    1.84), loss: 0.050751, time: 2.219 s\n",
      "Step   5900 (epoch    1.84), loss: 0.042101, time: 2.216 s\n",
      "Step   5920 (epoch    1.85), loss: 0.045702, time: 2.241 s\n",
      "Step   5940 (epoch    1.86), loss: 0.039794, time: 2.225 s\n",
      "Step   5960 (epoch    1.86), loss: 0.032011, time: 2.220 s\n",
      "Step   5980 (epoch    1.87), loss: 0.039344, time: 2.218 s\n",
      "Step   6000 (epoch    1.88), loss: 0.043190, time: 2.228 s, error: 0.048231\n",
      "\n",
      "Time since beginning  : 1027.825 s\n",
      "\n",
      "Step   6020 (epoch    1.88), loss: 0.042048, time: 14.342 s\n",
      "Step   6040 (epoch    1.89), loss: 0.040164, time: 2.225 s\n",
      "Step   6060 (epoch    1.89), loss: 0.037477, time: 2.232 s\n",
      "Step   6080 (epoch    1.90), loss: 0.050433, time: 2.212 s\n",
      "Step   6100 (epoch    1.91), loss: 0.044684, time: 2.217 s\n",
      "Step   6120 (epoch    1.91), loss: 0.038956, time: 2.217 s\n",
      "Step   6140 (epoch    1.92), loss: 0.052976, time: 2.223 s\n",
      "Step   6160 (epoch    1.93), loss: 0.047536, time: 2.216 s\n",
      "Step   6180 (epoch    1.93), loss: 0.045008, time: 2.214 s\n",
      "Step   6200 (epoch    1.94), loss: 0.062452, time: 2.225 s, error: 0.044972\n",
      "Step   6220 (epoch    1.94), loss: 0.054967, time: 14.215 s\n",
      "Step   6240 (epoch    1.95), loss: 0.030307, time: 2.213 s\n",
      "Step   6260 (epoch    1.96), loss: 0.033506, time: 2.226 s\n",
      "Step   6280 (epoch    1.96), loss: 0.040158, time: 2.218 s\n",
      "Step   6300 (epoch    1.97), loss: 0.033675, time: 2.223 s\n",
      "Step   6320 (epoch    1.98), loss: 0.059716, time: 2.221 s\n",
      "Step   6340 (epoch    1.98), loss: 0.026826, time: 2.231 s\n",
      "Step   6360 (epoch    1.99), loss: 0.043693, time: 2.216 s\n",
      "Step   6380 (epoch    1.99), loss: 0.043733, time: 2.214 s\n",
      "Step   6400 (epoch    2.00), loss: 0.037030, time: 2.221 s, error: 0.046326\n",
      "Step   6420 (epoch    2.01), loss: 0.046268, time: 14.317 s\n",
      "Step   6440 (epoch    2.01), loss: 0.044851, time: 2.222 s\n",
      "Step   6460 (epoch    2.02), loss: 0.051579, time: 2.207 s\n",
      "Step   6480 (epoch    2.02), loss: 0.038063, time: 2.223 s\n",
      "Step   6500 (epoch    2.03), loss: 0.035968, time: 2.215 s\n",
      "Step   6520 (epoch    2.04), loss: 0.042563, time: 2.221 s\n",
      "Step   6540 (epoch    2.04), loss: 0.030605, time: 2.217 s\n",
      "Step   6560 (epoch    2.05), loss: 0.047198, time: 2.222 s\n",
      "Step   6580 (epoch    2.06), loss: 0.043182, time: 2.217 s\n",
      "Step   6600 (epoch    2.06), loss: 0.049899, time: 2.230 s, error: 0.050576\n",
      "Step   6620 (epoch    2.07), loss: 0.038230, time: 14.253 s\n",
      "Step   6640 (epoch    2.08), loss: 0.043848, time: 2.220 s\n",
      "Step   6660 (epoch    2.08), loss: 0.075286, time: 2.220 s\n",
      "Step   6680 (epoch    2.09), loss: 0.035698, time: 2.216 s\n",
      "Step   6700 (epoch    2.09), loss: 0.046952, time: 2.223 s\n",
      "Step   6720 (epoch    2.10), loss: 0.028821, time: 2.219 s\n",
      "Step   6740 (epoch    2.11), loss: 0.045082, time: 2.207 s\n",
      "Step   6760 (epoch    2.11), loss: 0.048670, time: 2.209 s\n",
      "Step   6780 (epoch    2.12), loss: 0.040943, time: 2.218 s\n",
      "Step   6800 (epoch    2.12), loss: 0.029754, time: 2.223 s, error: 0.043178\n",
      "Step   6820 (epoch    2.13), loss: 0.045110, time: 14.437 s\n",
      "Step   6840 (epoch    2.14), loss: 0.050760, time: 2.229 s\n",
      "Step   6860 (epoch    2.14), loss: 0.031295, time: 2.223 s\n",
      "Step   6880 (epoch    2.15), loss: 0.043839, time: 2.227 s\n",
      "Step   6900 (epoch    2.16), loss: 0.035184, time: 2.229 s\n",
      "Step   6920 (epoch    2.16), loss: 0.030991, time: 2.217 s\n",
      "Step   6940 (epoch    2.17), loss: 0.031833, time: 2.224 s\n",
      "Step   6960 (epoch    2.17), loss: 0.039316, time: 2.214 s\n",
      "Step   6980 (epoch    2.18), loss: 0.047270, time: 2.222 s\n",
      "Step   7000 (epoch    2.19), loss: 0.042464, time: 2.219 s, error: 0.042565\n",
      "Step   7020 (epoch    2.19), loss: 0.027126, time: 14.240 s\n",
      "Step   7040 (epoch    2.20), loss: 0.045383, time: 2.229 s\n",
      "Step   7060 (epoch    2.21), loss: 0.042809, time: 2.220 s\n",
      "Step   7080 (epoch    2.21), loss: 0.032974, time: 2.223 s\n",
      "Step   7100 (epoch    2.22), loss: 0.037086, time: 2.224 s\n",
      "Step   7120 (epoch    2.23), loss: 0.049894, time: 2.214 s\n",
      "Step   7140 (epoch    2.23), loss: 0.054571, time: 2.209 s\n",
      "Step   7160 (epoch    2.24), loss: 0.039346, time: 2.217 s\n",
      "Step   7180 (epoch    2.24), loss: 0.041627, time: 2.218 s\n",
      "Step   7200 (epoch    2.25), loss: 0.043018, time: 2.217 s, error: 0.043296\n",
      "Step   7220 (epoch    2.26), loss: 0.052799, time: 14.149 s\n",
      "Step   7240 (epoch    2.26), loss: 0.035284, time: 2.218 s\n",
      "Step   7260 (epoch    2.27), loss: 0.035032, time: 2.217 s\n",
      "Step   7280 (epoch    2.27), loss: 0.045692, time: 2.210 s\n",
      "Step   7300 (epoch    2.28), loss: 0.034069, time: 2.227 s\n",
      "Step   7320 (epoch    2.29), loss: 0.028551, time: 2.215 s\n",
      "Step   7340 (epoch    2.29), loss: 0.040945, time: 2.218 s\n",
      "Step   7360 (epoch    2.30), loss: 0.046202, time: 2.224 s\n",
      "Step   7380 (epoch    2.31), loss: 0.044918, time: 2.218 s\n",
      "Step   7400 (epoch    2.31), loss: 0.037265, time: 2.225 s, error: 0.041552\n",
      "Step   7420 (epoch    2.32), loss: 0.063911, time: 14.227 s\n",
      "Step   7440 (epoch    2.33), loss: 0.042195, time: 2.216 s\n",
      "Step   7460 (epoch    2.33), loss: 0.029260, time: 2.219 s\n",
      "Step   7480 (epoch    2.34), loss: 0.055571, time: 2.216 s\n",
      "Step   7500 (epoch    2.34), loss: 0.035435, time: 2.223 s\n",
      "Step   7520 (epoch    2.35), loss: 0.037153, time: 2.222 s\n",
      "Step   7540 (epoch    2.36), loss: 0.048878, time: 2.210 s\n",
      "Step   7560 (epoch    2.36), loss: 0.039389, time: 2.213 s\n",
      "Step   7580 (epoch    2.37), loss: 0.028797, time: 2.230 s\n",
      "Step   7600 (epoch    2.38), loss: 0.040721, time: 2.217 s, error: 0.045885\n",
      "Step   7620 (epoch    2.38), loss: 0.033522, time: 14.215 s\n",
      "Step   7640 (epoch    2.39), loss: 0.034442, time: 2.212 s\n",
      "Step   7660 (epoch    2.39), loss: 0.024650, time: 2.223 s\n",
      "Step   7680 (epoch    2.40), loss: 0.032530, time: 2.217 s\n",
      "Step   7700 (epoch    2.41), loss: 0.037091, time: 2.225 s\n",
      "Step   7720 (epoch    2.41), loss: 0.042155, time: 2.217 s\n",
      "Step   7740 (epoch    2.42), loss: 0.066667, time: 2.227 s\n",
      "Step   7760 (epoch    2.42), loss: 0.031726, time: 2.225 s\n",
      "Step   7780 (epoch    2.43), loss: 0.038637, time: 2.225 s\n",
      "Step   7800 (epoch    2.44), loss: 0.043967, time: 2.215 s, error: 0.042644\n",
      "Step   7820 (epoch    2.44), loss: 0.027681, time: 14.402 s\n",
      "Step   7840 (epoch    2.45), loss: 0.035614, time: 2.219 s\n",
      "Step   7860 (epoch    2.46), loss: 0.054740, time: 2.225 s\n",
      "Step   7880 (epoch    2.46), loss: 0.026711, time: 2.222 s\n",
      "Step   7900 (epoch    2.47), loss: 0.053587, time: 2.217 s\n",
      "Step   7920 (epoch    2.48), loss: 0.028674, time: 2.218 s\n",
      "Step   7940 (epoch    2.48), loss: 0.023826, time: 2.225 s\n",
      "Step   7960 (epoch    2.49), loss: 0.041131, time: 2.224 s\n",
      "Step   7980 (epoch    2.49), loss: 0.062108, time: 2.222 s\n",
      "Step   8000 (epoch    2.50), loss: 0.040937, time: 2.223 s, error: 0.053505\n",
      "\n",
      "Time since beginning  : 1370.391 s\n",
      "\n",
      "Step   8020 (epoch    2.51), loss: 0.045594, time: 14.345 s\n",
      "Step   8040 (epoch    2.51), loss: 0.042961, time: 2.220 s\n",
      "Step   8060 (epoch    2.52), loss: 0.043508, time: 2.221 s\n",
      "Step   8080 (epoch    2.52), loss: 0.031653, time: 2.226 s\n",
      "Step   8100 (epoch    2.53), loss: 0.030120, time: 2.221 s\n",
      "Step   8120 (epoch    2.54), loss: 0.038216, time: 2.214 s\n",
      "Step   8140 (epoch    2.54), loss: 0.038625, time: 2.201 s\n",
      "Step   8160 (epoch    2.55), loss: 0.027061, time: 2.221 s\n",
      "Step   8180 (epoch    2.56), loss: 0.035648, time: 2.220 s\n",
      "Step   8200 (epoch    2.56), loss: 0.053773, time: 2.208 s, error: 0.039143\n",
      "Step   8220 (epoch    2.57), loss: 0.040881, time: 14.173 s\n",
      "Step   8240 (epoch    2.58), loss: 0.027899, time: 2.219 s\n",
      "Step   8260 (epoch    2.58), loss: 0.036980, time: 2.214 s\n",
      "Step   8280 (epoch    2.59), loss: 0.026468, time: 2.219 s\n",
      "Step   8300 (epoch    2.59), loss: 0.034608, time: 2.223 s\n",
      "Step   8320 (epoch    2.60), loss: 0.028210, time: 2.227 s\n",
      "Step   8340 (epoch    2.61), loss: 0.032929, time: 2.219 s\n",
      "Step   8360 (epoch    2.61), loss: 0.033387, time: 2.218 s\n",
      "Step   8380 (epoch    2.62), loss: 0.058264, time: 2.211 s\n",
      "Step   8400 (epoch    2.62), loss: 0.033629, time: 2.212 s, error: 0.043381\n",
      "Step   8420 (epoch    2.63), loss: 0.037689, time: 14.190 s\n",
      "Step   8440 (epoch    2.64), loss: 0.044357, time: 2.204 s\n",
      "Step   8460 (epoch    2.64), loss: 0.020308, time: 2.218 s\n",
      "Step   8480 (epoch    2.65), loss: 0.038975, time: 2.208 s\n",
      "Step   8500 (epoch    2.66), loss: 0.038451, time: 2.211 s\n",
      "Step   8520 (epoch    2.66), loss: 0.039044, time: 2.219 s\n",
      "Step   8540 (epoch    2.67), loss: 0.035991, time: 2.216 s\n",
      "Step   8560 (epoch    2.67), loss: 0.048526, time: 2.225 s\n",
      "Step   8580 (epoch    2.68), loss: 0.030475, time: 2.217 s\n",
      "Step   8600 (epoch    2.69), loss: 0.033989, time: 2.219 s, error: 0.037270\n",
      "Step   8620 (epoch    2.69), loss: 0.057150, time: 14.083 s\n",
      "Step   8640 (epoch    2.70), loss: 0.042354, time: 2.215 s\n",
      "Step   8660 (epoch    2.71), loss: 0.024303, time: 2.217 s\n",
      "Step   8680 (epoch    2.71), loss: 0.028789, time: 2.221 s\n",
      "Step   8700 (epoch    2.72), loss: 0.080195, time: 2.214 s\n",
      "Step   8720 (epoch    2.73), loss: 0.032743, time: 2.226 s\n",
      "Step   8740 (epoch    2.73), loss: 0.033282, time: 2.208 s\n",
      "Step   8760 (epoch    2.74), loss: 0.026933, time: 2.221 s\n",
      "Step   8780 (epoch    2.74), loss: 0.041259, time: 2.212 s\n",
      "Step   8800 (epoch    2.75), loss: 0.043239, time: 2.209 s, error: 0.040314\n",
      "Step   8820 (epoch    2.76), loss: 0.041534, time: 14.359 s\n",
      "Step   8840 (epoch    2.76), loss: 0.032365, time: 2.214 s\n",
      "Step   8860 (epoch    2.77), loss: 0.040087, time: 2.221 s\n",
      "Step   8880 (epoch    2.77), loss: 0.032145, time: 2.221 s\n",
      "Step   8900 (epoch    2.78), loss: 0.022299, time: 2.218 s\n",
      "Step   8920 (epoch    2.79), loss: 0.044468, time: 2.217 s\n",
      "Step   8940 (epoch    2.79), loss: 0.033970, time: 2.223 s\n",
      "Step   8960 (epoch    2.80), loss: 0.044072, time: 2.219 s\n",
      "Step   8980 (epoch    2.81), loss: 0.030797, time: 2.220 s\n",
      "Step   9000 (epoch    2.81), loss: 0.046345, time: 2.214 s, error: 0.039463\n",
      "Step   9020 (epoch    2.82), loss: 0.046978, time: 14.387 s\n",
      "Step   9040 (epoch    2.83), loss: 0.026170, time: 2.225 s\n",
      "Step   9060 (epoch    2.83), loss: 0.037725, time: 2.210 s\n",
      "Step   9080 (epoch    2.84), loss: 0.030300, time: 2.218 s\n",
      "Step   9100 (epoch    2.84), loss: 0.040177, time: 2.209 s\n",
      "Step   9120 (epoch    2.85), loss: 0.030349, time: 2.224 s\n",
      "Step   9140 (epoch    2.86), loss: 0.046314, time: 2.215 s\n",
      "Step   9160 (epoch    2.86), loss: 0.029176, time: 2.222 s\n",
      "Step   9180 (epoch    2.87), loss: 0.022989, time: 2.209 s\n",
      "Step   9200 (epoch    2.88), loss: 0.053571, time: 2.209 s, error: 0.046513\n",
      "Step   9220 (epoch    2.88), loss: 0.045540, time: 14.117 s\n",
      "Step   9240 (epoch    2.89), loss: 0.027923, time: 2.217 s\n",
      "Step   9260 (epoch    2.89), loss: 0.036396, time: 2.224 s\n",
      "Step   9280 (epoch    2.90), loss: 0.029393, time: 2.226 s\n",
      "Step   9300 (epoch    2.91), loss: 0.046968, time: 2.218 s\n",
      "Step   9320 (epoch    2.91), loss: 0.037029, time: 2.227 s\n",
      "Step   9340 (epoch    2.92), loss: 0.024124, time: 2.226 s\n",
      "Step   9360 (epoch    2.92), loss: 0.031696, time: 2.219 s\n",
      "Step   9380 (epoch    2.93), loss: 0.046820, time: 2.223 s\n",
      "Step   9400 (epoch    2.94), loss: 0.023126, time: 2.225 s, error: 0.035862\n",
      "Step   9420 (epoch    2.94), loss: 0.027280, time: 14.103 s\n",
      "Step   9440 (epoch    2.95), loss: 0.029279, time: 2.204 s\n",
      "Step   9460 (epoch    2.96), loss: 0.050591, time: 2.214 s\n",
      "Step   9480 (epoch    2.96), loss: 0.028931, time: 2.204 s\n",
      "Step   9500 (epoch    2.97), loss: 0.039162, time: 2.224 s\n",
      "Step   9520 (epoch    2.98), loss: 0.030829, time: 2.213 s\n",
      "Step   9540 (epoch    2.98), loss: 0.034938, time: 2.227 s\n",
      "Step   9560 (epoch    2.99), loss: 0.038762, time: 2.219 s\n",
      "Step   9580 (epoch    2.99), loss: 0.029769, time: 2.218 s\n",
      "Step   9600 (epoch    3.00), loss: 0.042913, time: 2.215 s, error: 0.037796\n",
      "Step   9620 (epoch    3.01), loss: 0.030955, time: 14.210 s\n",
      "Step   9640 (epoch    3.01), loss: 0.033010, time: 2.214 s\n",
      "Step   9660 (epoch    3.02), loss: 0.023087, time: 2.221 s\n",
      "Step   9680 (epoch    3.02), loss: 0.036955, time: 2.210 s\n",
      "Step   9700 (epoch    3.03), loss: 0.021658, time: 2.222 s\n",
      "Step   9720 (epoch    3.04), loss: 0.027468, time: 2.218 s\n",
      "Step   9740 (epoch    3.04), loss: 0.029319, time: 2.219 s\n",
      "Step   9760 (epoch    3.05), loss: 0.020069, time: 2.217 s\n",
      "Step   9780 (epoch    3.06), loss: 0.036209, time: 2.211 s\n",
      "Step   9800 (epoch    3.06), loss: 0.031856, time: 2.222 s, error: 0.035651\n",
      "Step   9820 (epoch    3.07), loss: 0.034361, time: 14.243 s\n",
      "Step   9840 (epoch    3.08), loss: 0.031613, time: 2.210 s\n",
      "Step   9860 (epoch    3.08), loss: 0.046586, time: 2.208 s\n",
      "Step   9880 (epoch    3.09), loss: 0.028320, time: 2.207 s\n",
      "Step   9900 (epoch    3.09), loss: 0.025699, time: 2.208 s\n",
      "Step   9920 (epoch    3.10), loss: 0.045037, time: 2.215 s\n",
      "Step   9940 (epoch    3.11), loss: 0.037702, time: 2.212 s\n",
      "Step   9960 (epoch    3.11), loss: 0.028481, time: 2.205 s\n",
      "Step   9980 (epoch    3.12), loss: 0.031763, time: 2.208 s\n",
      "Step  10000 (epoch    3.12), loss: 0.036005, time: 2.208 s, error: 0.034757\n",
      "\n",
      "Time since beginning  : 1712.079 s\n",
      "\n",
      "Step  10020 (epoch    3.13), loss: 0.037013, time: 14.325 s\n",
      "Step  10040 (epoch    3.14), loss: 0.036064, time: 2.216 s\n",
      "Step  10060 (epoch    3.14), loss: 0.029540, time: 2.213 s\n",
      "Step  10080 (epoch    3.15), loss: 0.029035, time: 2.223 s\n",
      "Step  10100 (epoch    3.16), loss: 0.022374, time: 2.213 s\n",
      "Step  10120 (epoch    3.16), loss: 0.032379, time: 2.221 s\n",
      "Step  10140 (epoch    3.17), loss: 0.045565, time: 2.221 s\n",
      "Step  10160 (epoch    3.17), loss: 0.047959, time: 2.214 s\n",
      "Step  10180 (epoch    3.18), loss: 0.033559, time: 2.215 s\n",
      "Step  10200 (epoch    3.19), loss: 0.052792, time: 2.213 s, error: 0.036054\n",
      "Step  10220 (epoch    3.19), loss: 0.027760, time: 14.189 s\n",
      "Step  10240 (epoch    3.20), loss: 0.033670, time: 2.226 s\n",
      "Step  10260 (epoch    3.21), loss: 0.037807, time: 2.225 s\n",
      "Step  10280 (epoch    3.21), loss: 0.037708, time: 2.217 s\n",
      "Step  10300 (epoch    3.22), loss: 0.033951, time: 2.217 s\n",
      "Step  10320 (epoch    3.23), loss: 0.041572, time: 2.222 s\n",
      "Step  10340 (epoch    3.23), loss: 0.029065, time: 2.212 s\n",
      "Step  10360 (epoch    3.24), loss: 0.025562, time: 2.211 s\n",
      "Step  10380 (epoch    3.24), loss: 0.025292, time: 2.216 s\n",
      "Step  10400 (epoch    3.25), loss: 0.069206, time: 2.215 s, error: 0.036229\n",
      "Step  10420 (epoch    3.26), loss: 0.032485, time: 14.082 s\n",
      "Step  10440 (epoch    3.26), loss: 0.024081, time: 2.211 s\n",
      "Step  10460 (epoch    3.27), loss: 0.034825, time: 2.216 s\n",
      "Step  10480 (epoch    3.27), loss: 0.029419, time: 2.215 s\n",
      "Step  10500 (epoch    3.28), loss: 0.050761, time: 2.218 s\n",
      "Step  10520 (epoch    3.29), loss: 0.042673, time: 2.221 s\n",
      "Step  10540 (epoch    3.29), loss: 0.023590, time: 2.218 s\n",
      "Step  10560 (epoch    3.30), loss: 0.021413, time: 2.216 s\n",
      "Step  10580 (epoch    3.31), loss: 0.019001, time: 2.215 s\n",
      "Step  10600 (epoch    3.31), loss: 0.031188, time: 2.220 s, error: 0.034515\n",
      "Step  10620 (epoch    3.32), loss: 0.023847, time: 14.099 s\n",
      "Step  10640 (epoch    3.33), loss: 0.032179, time: 2.200 s\n",
      "Step  10660 (epoch    3.33), loss: 0.050231, time: 2.223 s\n",
      "Step  10680 (epoch    3.34), loss: 0.032655, time: 2.215 s\n",
      "Step  10700 (epoch    3.34), loss: 0.061285, time: 2.213 s\n",
      "Step  10720 (epoch    3.35), loss: 0.034198, time: 2.217 s\n",
      "Step  10740 (epoch    3.36), loss: 0.035979, time: 2.215 s\n",
      "Step  10760 (epoch    3.36), loss: 0.032035, time: 2.219 s\n",
      "Step  10780 (epoch    3.37), loss: 0.030143, time: 2.214 s\n",
      "Step  10800 (epoch    3.38), loss: 0.027607, time: 2.227 s, error: 0.036090\n",
      "Step  10820 (epoch    3.38), loss: 0.026756, time: 14.238 s\n",
      "Step  10840 (epoch    3.39), loss: 0.018142, time: 2.206 s\n",
      "Step  10860 (epoch    3.39), loss: 0.040672, time: 2.222 s\n",
      "Step  10880 (epoch    3.40), loss: 0.037237, time: 2.210 s\n",
      "Step  10900 (epoch    3.41), loss: 0.035266, time: 2.219 s\n",
      "Step  10920 (epoch    3.41), loss: 0.058186, time: 2.216 s\n",
      "Step  10940 (epoch    3.42), loss: 0.033585, time: 2.215 s\n",
      "Step  10960 (epoch    3.42), loss: 0.033184, time: 2.220 s\n",
      "Step  10980 (epoch    3.43), loss: 0.054421, time: 2.211 s\n",
      "Step  11000 (epoch    3.44), loss: 0.036247, time: 2.225 s, error: 0.034126\n",
      "Step  11020 (epoch    3.44), loss: 0.027467, time: 14.314 s\n",
      "Step  11040 (epoch    3.45), loss: 0.039474, time: 2.218 s\n",
      "Step  11060 (epoch    3.46), loss: 0.025445, time: 2.225 s\n",
      "Step  11080 (epoch    3.46), loss: 0.025015, time: 2.215 s\n",
      "Step  11100 (epoch    3.47), loss: 0.041403, time: 2.219 s\n",
      "Step  11120 (epoch    3.48), loss: 0.028899, time: 2.205 s\n",
      "Step  11140 (epoch    3.48), loss: 0.024635, time: 2.212 s\n",
      "Step  11160 (epoch    3.49), loss: 0.027027, time: 2.215 s\n",
      "Step  11180 (epoch    3.49), loss: 0.037102, time: 2.218 s\n",
      "Step  11200 (epoch    3.50), loss: 0.039004, time: 2.216 s, error: 0.044629\n",
      "Step  11220 (epoch    3.51), loss: 0.024896, time: 14.142 s\n",
      "Step  11240 (epoch    3.51), loss: 0.034973, time: 2.219 s\n",
      "Step  11260 (epoch    3.52), loss: 0.027331, time: 2.221 s\n",
      "Step  11280 (epoch    3.52), loss: 0.032265, time: 2.216 s\n",
      "Step  11300 (epoch    3.53), loss: 0.028251, time: 2.210 s\n",
      "Step  11320 (epoch    3.54), loss: 0.030149, time: 2.227 s\n",
      "Step  11340 (epoch    3.54), loss: 0.034598, time: 2.219 s\n",
      "Step  11360 (epoch    3.55), loss: 0.024459, time: 2.222 s\n",
      "Step  11380 (epoch    3.56), loss: 0.041882, time: 2.210 s\n",
      "Step  11400 (epoch    3.56), loss: 0.016722, time: 2.216 s, error: 0.031786\n",
      "Step  11420 (epoch    3.57), loss: 0.041915, time: 14.136 s\n",
      "Step  11440 (epoch    3.58), loss: 0.020249, time: 2.230 s\n",
      "Step  11460 (epoch    3.58), loss: 0.035046, time: 2.220 s\n",
      "Step  11480 (epoch    3.59), loss: 0.027336, time: 2.214 s\n",
      "Step  11500 (epoch    3.59), loss: 0.021900, time: 2.229 s\n",
      "Step  11520 (epoch    3.60), loss: 0.033652, time: 2.227 s\n",
      "Step  11540 (epoch    3.61), loss: 0.025387, time: 2.210 s\n",
      "Step  11560 (epoch    3.61), loss: 0.016688, time: 2.213 s\n",
      "Step  11580 (epoch    3.62), loss: 0.023436, time: 2.220 s\n",
      "Step  11600 (epoch    3.62), loss: 0.016563, time: 2.206 s, error: 0.032858\n",
      "Step  11620 (epoch    3.63), loss: 0.018989, time: 14.145 s\n",
      "Step  11640 (epoch    3.64), loss: 0.032153, time: 2.218 s\n",
      "Step  11660 (epoch    3.64), loss: 0.022460, time: 2.211 s\n",
      "Step  11680 (epoch    3.65), loss: 0.022282, time: 2.220 s\n",
      "Step  11700 (epoch    3.66), loss: 0.037142, time: 2.211 s\n",
      "Step  11720 (epoch    3.66), loss: 0.018905, time: 2.208 s\n",
      "Step  11740 (epoch    3.67), loss: 0.016871, time: 2.222 s\n",
      "Step  11760 (epoch    3.67), loss: 0.032492, time: 2.214 s\n",
      "Step  11780 (epoch    3.68), loss: 0.019126, time: 2.221 s\n",
      "Step  11800 (epoch    3.69), loss: 0.032936, time: 2.229 s, error: 0.033413\n",
      "Step  11820 (epoch    3.69), loss: 0.024320, time: 14.155 s\n",
      "Step  11840 (epoch    3.70), loss: 0.036792, time: 2.226 s\n",
      "Step  11860 (epoch    3.71), loss: 0.023687, time: 2.216 s\n",
      "Step  11880 (epoch    3.71), loss: 0.038269, time: 2.224 s\n",
      "Step  11900 (epoch    3.72), loss: 0.033723, time: 2.210 s\n",
      "Step  11920 (epoch    3.73), loss: 0.036785, time: 2.222 s\n",
      "Step  11940 (epoch    3.73), loss: 0.030529, time: 2.228 s\n",
      "Step  11960 (epoch    3.74), loss: 0.024296, time: 2.218 s\n",
      "Step  11980 (epoch    3.74), loss: 0.044701, time: 2.219 s\n",
      "Step  12000 (epoch    3.75), loss: 0.030015, time: 2.220 s, error: 0.031872\n",
      "\n",
      "Time since beginning  : 2053.482 s\n",
      "\n",
      "Step  12020 (epoch    3.76), loss: 0.033382, time: 14.354 s\n",
      "Step  12040 (epoch    3.76), loss: 0.023312, time: 2.203 s\n",
      "Step  12060 (epoch    3.77), loss: 0.030425, time: 2.221 s\n",
      "Step  12080 (epoch    3.77), loss: 0.031343, time: 2.213 s\n",
      "Step  12100 (epoch    3.78), loss: 0.035773, time: 2.211 s\n",
      "Step  12120 (epoch    3.79), loss: 0.025696, time: 2.205 s\n",
      "Step  12140 (epoch    3.79), loss: 0.021042, time: 2.224 s\n",
      "Step  12160 (epoch    3.80), loss: 0.017609, time: 2.209 s\n",
      "Step  12180 (epoch    3.81), loss: 0.022893, time: 2.213 s\n",
      "Step  12200 (epoch    3.81), loss: 0.026767, time: 2.220 s, error: 0.031213\n",
      "Step  12220 (epoch    3.82), loss: 0.021872, time: 14.145 s\n",
      "Step  12240 (epoch    3.83), loss: 0.023736, time: 2.220 s\n",
      "Step  12260 (epoch    3.83), loss: 0.037301, time: 2.213 s\n",
      "Step  12280 (epoch    3.84), loss: 0.018894, time: 2.212 s\n",
      "Step  12300 (epoch    3.84), loss: 0.021702, time: 2.206 s\n",
      "Step  12320 (epoch    3.85), loss: 0.030525, time: 2.209 s\n",
      "Step  12340 (epoch    3.86), loss: 0.022870, time: 2.203 s\n",
      "Step  12360 (epoch    3.86), loss: 0.028054, time: 2.214 s\n",
      "Step  12380 (epoch    3.87), loss: 0.065786, time: 2.225 s\n",
      "Step  12400 (epoch    3.88), loss: 0.034532, time: 2.224 s, error: 0.037722\n",
      "Step  12420 (epoch    3.88), loss: 0.016358, time: 14.098 s\n",
      "Step  12440 (epoch    3.89), loss: 0.030707, time: 2.215 s\n",
      "Step  12460 (epoch    3.89), loss: 0.028737, time: 2.209 s\n",
      "Step  12480 (epoch    3.90), loss: 0.033722, time: 2.227 s\n",
      "Step  12500 (epoch    3.91), loss: 0.021573, time: 2.224 s\n",
      "Step  12520 (epoch    3.91), loss: 0.028924, time: 2.217 s\n",
      "Step  12540 (epoch    3.92), loss: 0.031566, time: 2.205 s\n",
      "Step  12560 (epoch    3.92), loss: 0.019620, time: 2.207 s\n",
      "Step  12580 (epoch    3.93), loss: 0.030482, time: 2.215 s\n",
      "Step  12600 (epoch    3.94), loss: 0.037507, time: 2.224 s, error: 0.030321\n",
      "Step  12620 (epoch    3.94), loss: 0.017536, time: 14.117 s\n",
      "Step  12640 (epoch    3.95), loss: 0.024944, time: 2.217 s\n",
      "Step  12660 (epoch    3.96), loss: 0.022257, time: 2.205 s\n",
      "Step  12680 (epoch    3.96), loss: 0.025891, time: 2.225 s\n",
      "Step  12700 (epoch    3.97), loss: 0.024819, time: 2.227 s\n",
      "Step  12720 (epoch    3.98), loss: 0.026959, time: 2.216 s\n",
      "Step  12740 (epoch    3.98), loss: 0.021172, time: 2.207 s\n",
      "Step  12760 (epoch    3.99), loss: 0.028491, time: 2.218 s\n",
      "Step  12780 (epoch    3.99), loss: 0.021428, time: 2.221 s\n",
      "Step  12800 (epoch    4.00), loss: 0.022412, time: 2.211 s, error: 0.030458\n",
      "Step  12820 (epoch    4.01), loss: 0.035841, time: 14.139 s\n",
      "Step  12840 (epoch    4.01), loss: 0.026801, time: 2.210 s\n",
      "Step  12860 (epoch    4.02), loss: 0.032664, time: 2.215 s\n",
      "Step  12880 (epoch    4.03), loss: 0.025550, time: 2.203 s\n",
      "Step  12900 (epoch    4.03), loss: 0.023984, time: 2.213 s\n",
      "Step  12920 (epoch    4.04), loss: 0.019016, time: 2.206 s\n",
      "Step  12940 (epoch    4.04), loss: 0.026118, time: 2.210 s\n",
      "Step  12960 (epoch    4.05), loss: 0.024371, time: 2.209 s\n",
      "Step  12980 (epoch    4.06), loss: 0.022806, time: 2.220 s\n",
      "Step  13000 (epoch    4.06), loss: 0.021890, time: 2.213 s, error: 0.030803\n",
      "Step  13020 (epoch    4.07), loss: 0.030010, time: 14.286 s\n",
      "Step  13040 (epoch    4.08), loss: 0.024935, time: 2.225 s\n",
      "Step  13060 (epoch    4.08), loss: 0.026375, time: 2.225 s\n",
      "Step  13080 (epoch    4.09), loss: 0.030869, time: 2.225 s\n",
      "Step  13100 (epoch    4.09), loss: 0.020215, time: 2.217 s\n",
      "Step  13120 (epoch    4.10), loss: 0.033083, time: 2.209 s\n",
      "Step  13140 (epoch    4.11), loss: 0.028073, time: 2.209 s\n",
      "Step  13160 (epoch    4.11), loss: 0.023071, time: 2.220 s\n",
      "Step  13180 (epoch    4.12), loss: 0.043815, time: 2.215 s\n",
      "Step  13200 (epoch    4.12), loss: 0.015578, time: 2.222 s, error: 0.031468\n",
      "Step  13220 (epoch    4.13), loss: 0.026561, time: 14.338 s\n",
      "Step  13240 (epoch    4.14), loss: 0.033048, time: 2.220 s\n",
      "Step  13260 (epoch    4.14), loss: 0.030465, time: 2.214 s\n",
      "Step  13280 (epoch    4.15), loss: 0.016877, time: 2.211 s\n",
      "Step  13300 (epoch    4.16), loss: 0.034054, time: 2.216 s\n",
      "Step  13320 (epoch    4.16), loss: 0.037230, time: 2.210 s\n",
      "Step  13340 (epoch    4.17), loss: 0.024748, time: 2.219 s\n",
      "Step  13360 (epoch    4.17), loss: 0.033258, time: 2.225 s\n",
      "Step  13380 (epoch    4.18), loss: 0.027935, time: 2.206 s\n",
      "Step  13400 (epoch    4.19), loss: 0.026835, time: 2.227 s, error: 0.038033\n",
      "Step  13420 (epoch    4.19), loss: 0.024687, time: 14.125 s\n",
      "Step  13440 (epoch    4.20), loss: 0.037673, time: 2.220 s\n",
      "Step  13460 (epoch    4.21), loss: 0.029693, time: 2.219 s\n",
      "Step  13480 (epoch    4.21), loss: 0.021293, time: 2.218 s\n",
      "Step  13500 (epoch    4.22), loss: 0.033618, time: 2.207 s\n",
      "Step  13520 (epoch    4.22), loss: 0.014724, time: 2.223 s\n",
      "Step  13540 (epoch    4.23), loss: 0.029369, time: 2.206 s\n",
      "Step  13560 (epoch    4.24), loss: 0.026061, time: 2.221 s\n",
      "Step  13580 (epoch    4.24), loss: 0.028425, time: 2.206 s\n",
      "Step  13600 (epoch    4.25), loss: 0.033659, time: 2.214 s, error: 0.029807\n",
      "Step  13620 (epoch    4.26), loss: 0.026939, time: 14.141 s\n",
      "Step  13640 (epoch    4.26), loss: 0.038983, time: 2.218 s\n",
      "Step  13660 (epoch    4.27), loss: 0.043154, time: 2.215 s\n",
      "Step  13680 (epoch    4.28), loss: 0.021293, time: 2.219 s\n",
      "Step  13700 (epoch    4.28), loss: 0.026597, time: 2.218 s\n",
      "Step  13720 (epoch    4.29), loss: 0.030310, time: 2.220 s\n",
      "Step  13740 (epoch    4.29), loss: 0.036314, time: 2.222 s\n",
      "Step  13760 (epoch    4.30), loss: 0.018912, time: 2.225 s\n",
      "Step  13780 (epoch    4.31), loss: 0.026673, time: 2.223 s\n",
      "Step  13800 (epoch    4.31), loss: 0.021919, time: 2.208 s, error: 0.030975\n",
      "Step  13820 (epoch    4.32), loss: 0.022405, time: 14.073 s\n",
      "Step  13840 (epoch    4.33), loss: 0.021197, time: 2.215 s\n",
      "Step  13860 (epoch    4.33), loss: 0.023169, time: 2.221 s\n",
      "Step  13880 (epoch    4.34), loss: 0.018843, time: 2.204 s\n",
      "Step  13900 (epoch    4.34), loss: 0.016311, time: 2.221 s\n",
      "Step  13920 (epoch    4.35), loss: 0.015329, time: 2.202 s\n",
      "Step  13940 (epoch    4.36), loss: 0.031902, time: 2.216 s\n",
      "Step  13960 (epoch    4.36), loss: 0.016415, time: 2.202 s\n",
      "Step  13980 (epoch    4.37), loss: 0.023889, time: 2.215 s\n",
      "Step  14000 (epoch    4.38), loss: 0.050973, time: 2.207 s, error: 0.030283\n",
      "\n",
      "Time since beginning  : 2394.677 s\n",
      "\n",
      "Step  14020 (epoch    4.38), loss: 0.025234, time: 14.374 s\n",
      "Step  14040 (epoch    4.39), loss: 0.023355, time: 2.208 s\n",
      "Step  14060 (epoch    4.39), loss: 0.025172, time: 2.199 s\n",
      "Step  14080 (epoch    4.40), loss: 0.028967, time: 2.208 s\n",
      "Step  14100 (epoch    4.41), loss: 0.016617, time: 2.217 s\n",
      "Step  14120 (epoch    4.41), loss: 0.020874, time: 2.210 s\n",
      "Step  14140 (epoch    4.42), loss: 0.024744, time: 2.214 s\n",
      "Step  14160 (epoch    4.42), loss: 0.032354, time: 2.209 s\n",
      "Step  14180 (epoch    4.43), loss: 0.040890, time: 2.210 s\n",
      "Step  14200 (epoch    4.44), loss: 0.024595, time: 2.205 s, error: 0.028673\n",
      "Step  14220 (epoch    4.44), loss: 0.022547, time: 14.268 s\n",
      "Step  14240 (epoch    4.45), loss: 0.025171, time: 2.220 s\n",
      "Step  14260 (epoch    4.46), loss: 0.035454, time: 2.208 s\n",
      "Step  14280 (epoch    4.46), loss: 0.033517, time: 2.217 s\n",
      "Step  14300 (epoch    4.47), loss: 0.030069, time: 2.213 s\n",
      "Step  14320 (epoch    4.47), loss: 0.036169, time: 2.216 s\n",
      "Step  14340 (epoch    4.48), loss: 0.019558, time: 2.207 s\n",
      "Step  14360 (epoch    4.49), loss: 0.025782, time: 2.217 s\n",
      "Step  14380 (epoch    4.49), loss: 0.029187, time: 2.204 s\n",
      "Step  14400 (epoch    4.50), loss: 0.028344, time: 2.203 s, error: 0.032509\n",
      "Step  14420 (epoch    4.51), loss: 0.031652, time: 14.118 s\n",
      "Step  14440 (epoch    4.51), loss: 0.030505, time: 2.215 s\n",
      "Step  14460 (epoch    4.52), loss: 0.024124, time: 2.228 s\n",
      "Step  14480 (epoch    4.53), loss: 0.028295, time: 2.213 s\n",
      "Step  14500 (epoch    4.53), loss: 0.027029, time: 2.214 s\n",
      "Step  14520 (epoch    4.54), loss: 0.022225, time: 2.215 s\n",
      "Step  14540 (epoch    4.54), loss: 0.037914, time: 2.213 s\n",
      "Step  14560 (epoch    4.55), loss: 0.019188, time: 2.222 s\n",
      "Step  14580 (epoch    4.56), loss: 0.021407, time: 2.212 s\n",
      "Step  14600 (epoch    4.56), loss: 0.018233, time: 2.220 s, error: 0.029418\n",
      "Step  14620 (epoch    4.57), loss: 0.023175, time: 14.159 s\n",
      "Step  14640 (epoch    4.58), loss: 0.027735, time: 2.211 s\n",
      "Step  14660 (epoch    4.58), loss: 0.026970, time: 2.223 s\n",
      "Step  14680 (epoch    4.59), loss: 0.016330, time: 2.218 s\n",
      "Step  14700 (epoch    4.59), loss: 0.031206, time: 2.217 s\n",
      "Step  14720 (epoch    4.60), loss: 0.016413, time: 2.224 s\n",
      "Step  14740 (epoch    4.61), loss: 0.035825, time: 2.217 s\n",
      "Step  14760 (epoch    4.61), loss: 0.026729, time: 2.209 s\n",
      "Step  14780 (epoch    4.62), loss: 0.018227, time: 2.217 s\n",
      "Step  14800 (epoch    4.62), loss: 0.019629, time: 2.214 s, error: 0.030194\n",
      "Step  14820 (epoch    4.63), loss: 0.024585, time: 14.105 s\n",
      "Step  14840 (epoch    4.64), loss: 0.024177, time: 2.225 s\n",
      "Step  14860 (epoch    4.64), loss: 0.020833, time: 2.222 s\n",
      "Step  14880 (epoch    4.65), loss: 0.021719, time: 2.208 s\n",
      "Step  14900 (epoch    4.66), loss: 0.021865, time: 2.211 s\n",
      "Step  14920 (epoch    4.66), loss: 0.018977, time: 2.221 s\n",
      "Step  14940 (epoch    4.67), loss: 0.019419, time: 2.209 s\n",
      "Step  14960 (epoch    4.67), loss: 0.033072, time: 2.212 s\n",
      "Step  14980 (epoch    4.68), loss: 0.015091, time: 2.210 s\n",
      "Step  15000 (epoch    4.69), loss: 0.031238, time: 2.225 s, error: 0.029123\n",
      "Step  15020 (epoch    4.69), loss: 0.015523, time: 14.142 s\n",
      "Step  15040 (epoch    4.70), loss: 0.016470, time: 2.204 s\n",
      "Step  15060 (epoch    4.71), loss: 0.020314, time: 2.205 s\n",
      "Step  15080 (epoch    4.71), loss: 0.019313, time: 2.202 s\n",
      "Step  15100 (epoch    4.72), loss: 0.034414, time: 2.206 s\n",
      "Step  15120 (epoch    4.72), loss: 0.019773, time: 2.220 s\n",
      "Step  15140 (epoch    4.73), loss: 0.024339, time: 2.212 s\n",
      "Step  15160 (epoch    4.74), loss: 0.020354, time: 2.211 s\n",
      "Step  15180 (epoch    4.74), loss: 0.021853, time: 2.211 s\n",
      "Step  15200 (epoch    4.75), loss: 0.018830, time: 2.216 s, error: 0.028790\n",
      "Step  15220 (epoch    4.76), loss: 0.018458, time: 14.261 s\n",
      "Step  15240 (epoch    4.76), loss: 0.040754, time: 2.206 s\n",
      "Step  15260 (epoch    4.77), loss: 0.026706, time: 2.216 s\n",
      "Step  15280 (epoch    4.78), loss: 0.020119, time: 2.224 s\n",
      "Step  15300 (epoch    4.78), loss: 0.022483, time: 2.224 s\n",
      "Step  15320 (epoch    4.79), loss: 0.032574, time: 2.215 s\n",
      "Step  15340 (epoch    4.79), loss: 0.024822, time: 2.214 s\n",
      "Step  15360 (epoch    4.80), loss: 0.035607, time: 2.197 s\n",
      "Step  15380 (epoch    4.81), loss: 0.019962, time: 2.215 s\n",
      "Step  15400 (epoch    4.81), loss: 0.026835, time: 2.210 s, error: 0.027884\n",
      "Step  15420 (epoch    4.82), loss: 0.016837, time: 14.133 s\n",
      "Step  15440 (epoch    4.83), loss: 0.016333, time: 2.227 s\n",
      "Step  15460 (epoch    4.83), loss: 0.021977, time: 2.227 s\n",
      "Step  15480 (epoch    4.84), loss: 0.018951, time: 2.222 s\n",
      "Step  15500 (epoch    4.84), loss: 0.015916, time: 2.223 s\n",
      "Step  15520 (epoch    4.85), loss: 0.018133, time: 2.216 s\n",
      "Step  15540 (epoch    4.86), loss: 0.020214, time: 2.217 s\n",
      "Step  15560 (epoch    4.86), loss: 0.020073, time: 2.211 s\n",
      "Step  15580 (epoch    4.87), loss: 0.029595, time: 2.221 s\n",
      "Step  15600 (epoch    4.88), loss: 0.024890, time: 2.218 s, error: 0.028701\n",
      "Step  15620 (epoch    4.88), loss: 0.023885, time: 14.081 s\n",
      "Step  15640 (epoch    4.89), loss: 0.017865, time: 2.225 s\n",
      "Step  15660 (epoch    4.89), loss: 0.016450, time: 2.214 s\n",
      "Step  15680 (epoch    4.90), loss: 0.026538, time: 2.222 s\n",
      "Step  15700 (epoch    4.91), loss: 0.033219, time: 2.218 s\n",
      "Step  15720 (epoch    4.91), loss: 0.034225, time: 2.230 s\n",
      "Step  15740 (epoch    4.92), loss: 0.017897, time: 2.216 s\n",
      "Step  15760 (epoch    4.92), loss: 0.026603, time: 2.213 s\n",
      "Step  15780 (epoch    4.93), loss: 0.021128, time: 2.217 s\n",
      "Step  15800 (epoch    4.94), loss: 0.013439, time: 2.218 s, error: 0.027185\n",
      "Step  15820 (epoch    4.94), loss: 0.018422, time: 14.167 s\n",
      "Step  15840 (epoch    4.95), loss: 0.050654, time: 2.216 s\n",
      "Step  15860 (epoch    4.96), loss: 0.025432, time: 2.213 s\n",
      "Step  15880 (epoch    4.96), loss: 0.025322, time: 2.204 s\n",
      "Step  15900 (epoch    4.97), loss: 0.016432, time: 2.214 s\n",
      "Step  15920 (epoch    4.97), loss: 0.025908, time: 2.218 s\n",
      "Step  15940 (epoch    4.98), loss: 0.025729, time: 2.213 s\n",
      "Step  15960 (epoch    4.99), loss: 0.024232, time: 2.220 s\n",
      "Step  15980 (epoch    4.99), loss: 0.027114, time: 2.214 s\n",
      "Step  16000 (epoch    5.00), loss: 0.021235, time: 2.224 s, error: 0.029017\n",
      "\n",
      "Time since beginning  : 2735.600 s\n",
      "\n",
      "Step  16020 (epoch    5.01), loss: 0.023063, time: 14.167 s\n",
      "Step  16040 (epoch    5.01), loss: 0.019210, time: 2.222 s\n",
      "Step  16060 (epoch    5.02), loss: 0.017868, time: 2.219 s\n",
      "Step  16080 (epoch    5.03), loss: 0.018886, time: 2.223 s\n",
      "Step  16100 (epoch    5.03), loss: 0.026756, time: 2.216 s\n",
      "Step  16120 (epoch    5.04), loss: 0.017148, time: 2.210 s\n",
      "Step  16140 (epoch    5.04), loss: 0.027764, time: 2.214 s\n",
      "Step  16160 (epoch    5.05), loss: 0.023767, time: 2.211 s\n",
      "Step  16180 (epoch    5.06), loss: 0.020750, time: 2.215 s\n",
      "Step  16200 (epoch    5.06), loss: 0.020991, time: 2.218 s, error: 0.029378\n",
      "Step  16220 (epoch    5.07), loss: 0.017022, time: 14.269 s\n",
      "Step  16240 (epoch    5.08), loss: 0.019795, time: 2.200 s\n",
      "Step  16260 (epoch    5.08), loss: 0.024255, time: 2.218 s\n",
      "Step  16280 (epoch    5.09), loss: 0.021121, time: 2.210 s\n",
      "Step  16300 (epoch    5.09), loss: 0.019817, time: 2.216 s\n",
      "Step  16320 (epoch    5.10), loss: 0.024588, time: 2.213 s\n",
      "Step  16340 (epoch    5.11), loss: 0.016463, time: 2.208 s\n",
      "Step  16360 (epoch    5.11), loss: 0.020312, time: 2.220 s\n",
      "Step  16380 (epoch    5.12), loss: 0.014131, time: 2.212 s\n",
      "Step  16400 (epoch    5.12), loss: 0.016575, time: 2.204 s, error: 0.029313\n",
      "Step  16420 (epoch    5.13), loss: 0.023521, time: 14.111 s\n",
      "Step  16440 (epoch    5.14), loss: 0.027803, time: 2.221 s\n",
      "Step  16460 (epoch    5.14), loss: 0.022227, time: 2.223 s\n",
      "Step  16480 (epoch    5.15), loss: 0.018350, time: 2.217 s\n",
      "Step  16500 (epoch    5.16), loss: 0.023021, time: 2.206 s\n",
      "Step  16520 (epoch    5.16), loss: 0.052224, time: 2.215 s\n",
      "Step  16540 (epoch    5.17), loss: 0.014661, time: 2.209 s\n",
      "Step  16560 (epoch    5.17), loss: 0.025071, time: 2.208 s\n",
      "Step  16580 (epoch    5.18), loss: 0.019706, time: 2.221 s\n",
      "Step  16600 (epoch    5.19), loss: 0.020968, time: 2.212 s, error: 0.030945\n",
      "Step  16620 (epoch    5.19), loss: 0.022880, time: 14.085 s\n",
      "Step  16640 (epoch    5.20), loss: 0.024925, time: 2.206 s\n",
      "Step  16660 (epoch    5.21), loss: 0.024442, time: 2.211 s\n",
      "Step  16680 (epoch    5.21), loss: 0.014082, time: 2.219 s\n",
      "Step  16700 (epoch    5.22), loss: 0.027355, time: 2.218 s\n",
      "Step  16720 (epoch    5.22), loss: 0.028237, time: 2.204 s\n",
      "Step  16740 (epoch    5.23), loss: 0.024676, time: 2.211 s\n",
      "Step  16760 (epoch    5.24), loss: 0.024836, time: 2.217 s\n",
      "Step  16780 (epoch    5.24), loss: 0.020506, time: 2.221 s\n",
      "Step  16800 (epoch    5.25), loss: 0.019074, time: 2.207 s, error: 0.026564\n",
      "Step  16820 (epoch    5.26), loss: 0.018447, time: 14.123 s\n",
      "Step  16840 (epoch    5.26), loss: 0.029488, time: 2.216 s\n",
      "Step  16860 (epoch    5.27), loss: 0.027592, time: 2.211 s\n",
      "Step  16880 (epoch    5.28), loss: 0.019182, time: 2.217 s\n",
      "Step  16900 (epoch    5.28), loss: 0.015740, time: 2.208 s\n",
      "Step  16920 (epoch    5.29), loss: 0.025241, time: 2.210 s\n",
      "Step  16940 (epoch    5.29), loss: 0.014908, time: 2.210 s\n",
      "Step  16960 (epoch    5.30), loss: 0.019311, time: 2.216 s\n",
      "Step  16980 (epoch    5.31), loss: 0.028842, time: 2.223 s\n",
      "Step  17000 (epoch    5.31), loss: 0.020121, time: 2.219 s, error: 0.027693\n",
      "Step  17020 (epoch    5.32), loss: 0.042481, time: 14.134 s\n",
      "Step  17040 (epoch    5.33), loss: 0.016456, time: 2.207 s\n",
      "Step  17060 (epoch    5.33), loss: 0.033926, time: 2.214 s\n",
      "Step  17080 (epoch    5.34), loss: 0.024285, time: 2.220 s\n",
      "Step  17100 (epoch    5.34), loss: 0.019273, time: 2.219 s\n",
      "Step  17120 (epoch    5.35), loss: 0.034002, time: 2.223 s\n",
      "Step  17140 (epoch    5.36), loss: 0.023495, time: 2.223 s\n",
      "Step  17160 (epoch    5.36), loss: 0.019782, time: 2.214 s\n",
      "Step  17180 (epoch    5.37), loss: 0.026246, time: 2.220 s\n",
      "Step  17200 (epoch    5.38), loss: 0.028243, time: 2.221 s, error: 0.027368\n",
      "Step  17220 (epoch    5.38), loss: 0.025978, time: 14.326 s\n",
      "Step  17240 (epoch    5.39), loss: 0.025765, time: 2.215 s\n",
      "Step  17260 (epoch    5.39), loss: 0.018828, time: 2.213 s\n",
      "Step  17280 (epoch    5.40), loss: 0.016310, time: 2.207 s\n",
      "Step  17300 (epoch    5.41), loss: 0.019220, time: 2.218 s\n",
      "Step  17320 (epoch    5.41), loss: 0.025743, time: 2.203 s\n",
      "Step  17340 (epoch    5.42), loss: 0.036808, time: 2.213 s\n",
      "Step  17360 (epoch    5.42), loss: 0.020519, time: 2.215 s\n",
      "Step  17380 (epoch    5.43), loss: 0.019584, time: 2.204 s\n",
      "Step  17400 (epoch    5.44), loss: 0.015802, time: 2.206 s, error: 0.025879\n",
      "Step  17420 (epoch    5.44), loss: 0.025968, time: 14.213 s\n",
      "Step  17440 (epoch    5.45), loss: 0.022670, time: 2.211 s\n",
      "Step  17460 (epoch    5.46), loss: 0.018313, time: 2.221 s\n",
      "Step  17480 (epoch    5.46), loss: 0.027976, time: 2.197 s\n",
      "Step  17500 (epoch    5.47), loss: 0.019401, time: 2.222 s\n",
      "Step  17520 (epoch    5.47), loss: 0.019386, time: 2.214 s\n",
      "Step  17540 (epoch    5.48), loss: 0.017063, time: 2.221 s\n",
      "Step  17560 (epoch    5.49), loss: 0.025158, time: 2.213 s\n",
      "Step  17580 (epoch    5.49), loss: 0.020966, time: 2.221 s\n",
      "Step  17600 (epoch    5.50), loss: 0.018670, time: 2.220 s, error: 0.026509\n",
      "Step  17620 (epoch    5.51), loss: 0.021060, time: 14.139 s\n",
      "Step  17640 (epoch    5.51), loss: 0.023407, time: 2.201 s\n",
      "Step  17660 (epoch    5.52), loss: 0.023886, time: 2.221 s\n",
      "Step  17680 (epoch    5.53), loss: 0.018071, time: 2.228 s\n",
      "Step  17700 (epoch    5.53), loss: 0.022536, time: 2.219 s\n",
      "Step  17720 (epoch    5.54), loss: 0.031486, time: 2.222 s\n",
      "Step  17740 (epoch    5.54), loss: 0.018478, time: 2.222 s\n",
      "Step  17760 (epoch    5.55), loss: 0.021458, time: 2.211 s\n",
      "Step  17780 (epoch    5.56), loss: 0.016399, time: 2.210 s\n",
      "Step  17800 (epoch    5.56), loss: 0.025184, time: 2.219 s, error: 0.027852\n",
      "Step  17820 (epoch    5.57), loss: 0.022332, time: 14.123 s\n",
      "Step  17840 (epoch    5.58), loss: 0.014300, time: 2.217 s\n",
      "Step  17860 (epoch    5.58), loss: 0.019552, time: 2.199 s\n",
      "Step  17880 (epoch    5.59), loss: 0.015097, time: 2.211 s\n",
      "Step  17900 (epoch    5.59), loss: 0.021124, time: 2.200 s\n",
      "Step  17920 (epoch    5.60), loss: 0.015021, time: 2.218 s\n",
      "Step  17940 (epoch    5.61), loss: 0.025128, time: 2.224 s\n",
      "Step  17960 (epoch    5.61), loss: 0.019368, time: 2.224 s\n",
      "Step  17980 (epoch    5.62), loss: 0.014648, time: 2.215 s\n",
      "Step  18000 (epoch    5.62), loss: 0.022609, time: 2.217 s, error: 0.026275\n",
      "\n",
      "Time since beginning  : 3076.588 s\n",
      "\n",
      "Step  18020 (epoch    5.63), loss: 0.030116, time: 14.167 s\n",
      "Step  18040 (epoch    5.64), loss: 0.030294, time: 2.211 s\n",
      "Step  18060 (epoch    5.64), loss: 0.017524, time: 2.203 s\n",
      "Step  18080 (epoch    5.65), loss: 0.019008, time: 2.210 s\n",
      "Step  18100 (epoch    5.66), loss: 0.020780, time: 2.217 s\n",
      "Step  18120 (epoch    5.66), loss: 0.031231, time: 2.213 s\n",
      "Step  18140 (epoch    5.67), loss: 0.017072, time: 2.203 s\n",
      "Step  18160 (epoch    5.67), loss: 0.018537, time: 2.216 s\n",
      "Step  18180 (epoch    5.68), loss: 0.014616, time: 2.199 s\n",
      "Step  18200 (epoch    5.69), loss: 0.024634, time: 2.221 s, error: 0.025880\n",
      "Step  18220 (epoch    5.69), loss: 0.043867, time: 14.256 s\n",
      "Step  18240 (epoch    5.70), loss: 0.017584, time: 2.206 s\n",
      "Step  18260 (epoch    5.71), loss: 0.015787, time: 2.223 s\n",
      "Step  18280 (epoch    5.71), loss: 0.018257, time: 2.215 s\n",
      "Step  18300 (epoch    5.72), loss: 0.025780, time: 2.215 s\n",
      "Step  18320 (epoch    5.72), loss: 0.015476, time: 2.210 s\n",
      "Step  18340 (epoch    5.73), loss: 0.023574, time: 2.221 s\n",
      "Step  18360 (epoch    5.74), loss: 0.035854, time: 2.208 s\n",
      "Step  18380 (epoch    5.74), loss: 0.017048, time: 2.218 s\n",
      "Step  18400 (epoch    5.75), loss: 0.022456, time: 2.213 s, error: 0.025667\n",
      "Step  18420 (epoch    5.76), loss: 0.023731, time: 14.234 s\n",
      "Step  18440 (epoch    5.76), loss: 0.013913, time: 2.218 s\n",
      "Step  18460 (epoch    5.77), loss: 0.014625, time: 2.228 s\n",
      "Step  18480 (epoch    5.78), loss: 0.021005, time: 2.220 s\n",
      "Step  18500 (epoch    5.78), loss: 0.013305, time: 2.216 s\n",
      "Step  18520 (epoch    5.79), loss: 0.023731, time: 2.210 s\n",
      "Step  18540 (epoch    5.79), loss: 0.022699, time: 2.218 s\n",
      "Step  18560 (epoch    5.80), loss: 0.027864, time: 2.218 s\n",
      "Step  18580 (epoch    5.81), loss: 0.019548, time: 2.213 s\n",
      "Step  18600 (epoch    5.81), loss: 0.027580, time: 2.221 s, error: 0.026453\n",
      "Step  18620 (epoch    5.82), loss: 0.020116, time: 14.169 s\n",
      "Step  18640 (epoch    5.83), loss: 0.016634, time: 2.223 s\n",
      "Step  18660 (epoch    5.83), loss: 0.016584, time: 2.225 s\n",
      "Step  18680 (epoch    5.84), loss: 0.022380, time: 2.213 s\n",
      "Step  18700 (epoch    5.84), loss: 0.017808, time: 2.223 s\n",
      "Step  18720 (epoch    5.85), loss: 0.020031, time: 2.216 s\n",
      "Step  18740 (epoch    5.86), loss: 0.020563, time: 2.219 s\n",
      "Step  18760 (epoch    5.86), loss: 0.018585, time: 2.214 s\n",
      "Step  18780 (epoch    5.87), loss: 0.018121, time: 2.219 s\n",
      "Step  18800 (epoch    5.88), loss: 0.019095, time: 2.200 s, error: 0.025700\n",
      "Step  18820 (epoch    5.88), loss: 0.015636, time: 14.148 s\n",
      "Step  18840 (epoch    5.89), loss: 0.014886, time: 2.220 s\n",
      "Step  18860 (epoch    5.89), loss: 0.015514, time: 2.213 s\n",
      "Step  18880 (epoch    5.90), loss: 0.024755, time: 2.217 s\n",
      "Step  18900 (epoch    5.91), loss: 0.022605, time: 2.210 s\n",
      "Step  18920 (epoch    5.91), loss: 0.020271, time: 2.217 s\n",
      "Step  18940 (epoch    5.92), loss: 0.022002, time: 2.216 s\n",
      "Step  18960 (epoch    5.92), loss: 0.029493, time: 2.209 s\n",
      "Step  18980 (epoch    5.93), loss: 0.016986, time: 2.211 s\n",
      "Step  19000 (epoch    5.94), loss: 0.014935, time: 2.203 s, error: 0.026020\n",
      "Step  19020 (epoch    5.94), loss: 0.024465, time: 14.128 s\n",
      "Step  19040 (epoch    5.95), loss: 0.017832, time: 2.212 s\n",
      "Step  19060 (epoch    5.96), loss: 0.014674, time: 2.215 s\n",
      "Step  19080 (epoch    5.96), loss: 0.025633, time: 2.204 s\n",
      "Step  19100 (epoch    5.97), loss: 0.018022, time: 2.218 s\n",
      "Step  19120 (epoch    5.97), loss: 0.024828, time: 2.205 s\n",
      "Step  19140 (epoch    5.98), loss: 0.023626, time: 2.224 s\n",
      "Step  19160 (epoch    5.99), loss: 0.017193, time: 2.213 s\n",
      "Step  19180 (epoch    5.99), loss: 0.023282, time: 2.223 s\n",
      "Step  19200 (epoch    6.00), loss: 0.024926, time: 2.228 s, error: 0.026915\n",
      "Step  19220 (epoch    6.01), loss: 0.022072, time: 14.194 s\n",
      "Step  19240 (epoch    6.01), loss: 0.014172, time: 2.220 s\n",
      "Step  19260 (epoch    6.02), loss: 0.020565, time: 2.219 s\n",
      "Step  19280 (epoch    6.03), loss: 0.020808, time: 2.218 s\n",
      "Step  19300 (epoch    6.03), loss: 0.024076, time: 2.210 s\n",
      "Step  19320 (epoch    6.04), loss: 0.027681, time: 2.217 s\n",
      "Step  19340 (epoch    6.04), loss: 0.017992, time: 2.218 s\n",
      "Step  19360 (epoch    6.05), loss: 0.021904, time: 2.219 s\n",
      "Step  19380 (epoch    6.06), loss: 0.021838, time: 2.206 s\n",
      "Step  19400 (epoch    6.06), loss: 0.030004, time: 2.207 s, error: 0.024774\n",
      "Step  19420 (epoch    6.07), loss: 0.020486, time: 14.240 s\n",
      "Step  19440 (epoch    6.08), loss: 0.015079, time: 2.204 s\n",
      "Step  19460 (epoch    6.08), loss: 0.023180, time: 2.212 s\n",
      "Step  19480 (epoch    6.09), loss: 0.011640, time: 2.224 s\n",
      "Step  19500 (epoch    6.09), loss: 0.022486, time: 2.219 s\n",
      "Step  19520 (epoch    6.10), loss: 0.026701, time: 2.200 s\n",
      "Step  19540 (epoch    6.11), loss: 0.023304, time: 2.219 s\n",
      "Step  19560 (epoch    6.11), loss: 0.022466, time: 2.210 s\n",
      "Step  19580 (epoch    6.12), loss: 0.013650, time: 2.211 s\n",
      "Step  19600 (epoch    6.12), loss: 0.017816, time: 2.222 s, error: 0.033532\n",
      "Step  19620 (epoch    6.13), loss: 0.021264, time: 14.187 s\n",
      "Step  19640 (epoch    6.14), loss: 0.018681, time: 2.228 s\n",
      "Step  19660 (epoch    6.14), loss: 0.017777, time: 2.222 s\n",
      "Step  19680 (epoch    6.15), loss: 0.020550, time: 2.210 s\n",
      "Step  19700 (epoch    6.16), loss: 0.018872, time: 2.228 s\n",
      "Step  19720 (epoch    6.16), loss: 0.017548, time: 2.204 s\n",
      "Step  19740 (epoch    6.17), loss: 0.029434, time: 2.209 s\n",
      "Step  19760 (epoch    6.17), loss: 0.018084, time: 2.214 s\n",
      "Step  19780 (epoch    6.18), loss: 0.026641, time: 2.207 s\n",
      "Step  19800 (epoch    6.19), loss: 0.025876, time: 2.204 s, error: 0.030332\n",
      "Step  19820 (epoch    6.19), loss: 0.018277, time: 14.271 s\n",
      "Step  19840 (epoch    6.20), loss: 0.035839, time: 2.220 s\n",
      "Step  19860 (epoch    6.21), loss: 0.012529, time: 2.218 s\n",
      "Step  19880 (epoch    6.21), loss: 0.015192, time: 2.215 s\n",
      "Step  19900 (epoch    6.22), loss: 0.016538, time: 2.223 s\n",
      "Step  19920 (epoch    6.22), loss: 0.042780, time: 2.223 s\n",
      "Step  19940 (epoch    6.23), loss: 0.015632, time: 2.219 s\n",
      "Step  19960 (epoch    6.24), loss: 0.014966, time: 2.228 s\n",
      "Step  19980 (epoch    6.24), loss: 0.024409, time: 2.214 s\n",
      "Step  20000 (epoch    6.25), loss: 0.020353, time: 2.223 s, error: 0.026375\n",
      "\n",
      "Time since beginning  : 3418.016 s\n",
      "\n",
      "Step  20020 (epoch    6.26), loss: 0.010693, time: 14.231 s\n",
      "Step  20040 (epoch    6.26), loss: 0.030679, time: 2.230 s\n",
      "Step  20060 (epoch    6.27), loss: 0.019392, time: 2.216 s\n",
      "Step  20080 (epoch    6.28), loss: 0.028335, time: 2.214 s\n",
      "Step  20100 (epoch    6.28), loss: 0.024624, time: 2.211 s\n",
      "Step  20120 (epoch    6.29), loss: 0.020730, time: 2.210 s\n",
      "Step  20140 (epoch    6.29), loss: 0.015792, time: 2.220 s\n",
      "Step  20160 (epoch    6.30), loss: 0.020652, time: 2.219 s\n",
      "Step  20180 (epoch    6.31), loss: 0.019369, time: 2.229 s\n",
      "Step  20200 (epoch    6.31), loss: 0.026348, time: 2.220 s, error: 0.024780\n",
      "Step  20220 (epoch    6.32), loss: 0.023953, time: 14.127 s\n",
      "Step  20240 (epoch    6.33), loss: 0.016962, time: 2.224 s\n",
      "Step  20260 (epoch    6.33), loss: 0.016563, time: 2.214 s\n",
      "Step  20280 (epoch    6.34), loss: 0.022221, time: 2.212 s\n",
      "Step  20300 (epoch    6.34), loss: 0.015154, time: 2.218 s\n",
      "Step  20320 (epoch    6.35), loss: 0.024854, time: 2.207 s\n",
      "Step  20340 (epoch    6.36), loss: 0.025813, time: 2.222 s\n",
      "Step  20360 (epoch    6.36), loss: 0.026430, time: 2.212 s\n",
      "Step  20380 (epoch    6.37), loss: 0.026047, time: 2.220 s\n",
      "Step  20400 (epoch    6.38), loss: 0.022133, time: 2.211 s, error: 0.025185\n",
      "Step  20420 (epoch    6.38), loss: 0.018436, time: 14.349 s\n",
      "Step  20440 (epoch    6.39), loss: 0.019089, time: 2.221 s\n",
      "Step  20460 (epoch    6.39), loss: 0.014718, time: 2.211 s\n",
      "Step  20480 (epoch    6.40), loss: 0.021926, time: 2.225 s\n",
      "Step  20500 (epoch    6.41), loss: 0.019882, time: 2.208 s\n",
      "Step  20520 (epoch    6.41), loss: 0.015550, time: 2.212 s\n",
      "Step  20540 (epoch    6.42), loss: 0.015005, time: 2.216 s\n",
      "Step  20560 (epoch    6.42), loss: 0.019091, time: 2.210 s\n",
      "Step  20580 (epoch    6.43), loss: 0.015933, time: 2.212 s\n",
      "Step  20600 (epoch    6.44), loss: 0.026113, time: 2.213 s, error: 0.024174\n",
      "Step  20620 (epoch    6.44), loss: 0.017090, time: 14.247 s\n",
      "Step  20640 (epoch    6.45), loss: 0.015358, time: 2.216 s\n",
      "Step  20660 (epoch    6.46), loss: 0.016349, time: 2.215 s\n",
      "Step  20680 (epoch    6.46), loss: 0.032824, time: 2.206 s\n",
      "Step  20700 (epoch    6.47), loss: 0.038848, time: 2.215 s\n",
      "Step  20720 (epoch    6.47), loss: 0.011520, time: 2.220 s\n",
      "Step  20740 (epoch    6.48), loss: 0.031269, time: 2.205 s\n",
      "Step  20760 (epoch    6.49), loss: 0.033334, time: 2.220 s\n",
      "Step  20780 (epoch    6.49), loss: 0.014501, time: 2.222 s\n",
      "Step  20800 (epoch    6.50), loss: 0.022523, time: 2.212 s, error: 0.026110\n",
      "Step  20820 (epoch    6.51), loss: 0.020547, time: 14.208 s\n",
      "Step  20840 (epoch    6.51), loss: 0.017586, time: 2.223 s\n",
      "Step  20860 (epoch    6.52), loss: 0.019449, time: 2.221 s\n",
      "Step  20880 (epoch    6.53), loss: 0.013522, time: 2.228 s\n",
      "Step  20900 (epoch    6.53), loss: 0.016753, time: 2.223 s\n",
      "Step  20920 (epoch    6.54), loss: 0.017025, time: 2.213 s\n",
      "Step  20940 (epoch    6.54), loss: 0.016670, time: 2.211 s\n",
      "Step  20960 (epoch    6.55), loss: 0.018441, time: 2.220 s\n",
      "Step  20980 (epoch    6.56), loss: 0.017007, time: 2.217 s\n",
      "Step  21000 (epoch    6.56), loss: 0.017760, time: 2.219 s, error: 0.023575\n",
      "Step  21020 (epoch    6.57), loss: 0.014466, time: 14.212 s\n",
      "Step  21040 (epoch    6.58), loss: 0.014265, time: 2.221 s\n",
      "Step  21060 (epoch    6.58), loss: 0.028140, time: 2.219 s\n",
      "Step  21080 (epoch    6.59), loss: 0.039012, time: 2.230 s\n",
      "Step  21100 (epoch    6.59), loss: 0.053597, time: 2.208 s\n",
      "Step  21120 (epoch    6.60), loss: 0.025031, time: 2.218 s\n",
      "Step  21140 (epoch    6.61), loss: 0.021598, time: 2.225 s\n",
      "Step  21160 (epoch    6.61), loss: 0.019515, time: 2.220 s\n",
      "Step  21180 (epoch    6.62), loss: 0.026295, time: 2.202 s\n",
      "Step  21200 (epoch    6.62), loss: 0.020694, time: 2.206 s, error: 0.024811\n",
      "Step  21220 (epoch    6.63), loss: 0.019484, time: 14.094 s\n",
      "Step  21240 (epoch    6.64), loss: 0.015140, time: 2.214 s\n",
      "Step  21260 (epoch    6.64), loss: 0.015437, time: 2.215 s\n",
      "Step  21280 (epoch    6.65), loss: 0.041698, time: 2.223 s\n",
      "Step  21300 (epoch    6.66), loss: 0.023421, time: 2.216 s\n",
      "Step  21320 (epoch    6.66), loss: 0.012843, time: 2.222 s\n",
      "Step  21340 (epoch    6.67), loss: 0.017575, time: 2.211 s\n",
      "Step  21360 (epoch    6.67), loss: 0.027213, time: 2.213 s\n",
      "Step  21380 (epoch    6.68), loss: 0.013286, time: 2.206 s\n",
      "Step  21400 (epoch    6.69), loss: 0.016449, time: 2.219 s, error: 0.023674\n",
      "Step  21420 (epoch    6.69), loss: 0.019086, time: 14.264 s\n",
      "Step  21440 (epoch    6.70), loss: 0.024063, time: 2.207 s\n",
      "Step  21460 (epoch    6.71), loss: 0.016682, time: 2.222 s\n",
      "Step  21480 (epoch    6.71), loss: 0.018082, time: 2.212 s\n",
      "Step  21500 (epoch    6.72), loss: 0.018830, time: 2.214 s\n",
      "Step  21520 (epoch    6.72), loss: 0.016878, time: 2.201 s\n",
      "Step  21540 (epoch    6.73), loss: 0.020204, time: 2.214 s\n",
      "Step  21560 (epoch    6.74), loss: 0.014415, time: 2.224 s\n",
      "Step  21580 (epoch    6.74), loss: 0.026500, time: 2.204 s\n",
      "Step  21600 (epoch    6.75), loss: 0.019032, time: 2.208 s, error: 0.023476\n",
      "Step  21620 (epoch    6.76), loss: 0.019451, time: 14.286 s\n",
      "Step  21640 (epoch    6.76), loss: 0.016635, time: 2.203 s\n",
      "Step  21660 (epoch    6.77), loss: 0.014999, time: 2.216 s\n",
      "Step  21680 (epoch    6.78), loss: 0.018359, time: 2.219 s\n",
      "Step  21700 (epoch    6.78), loss: 0.025840, time: 2.218 s\n",
      "Step  21720 (epoch    6.79), loss: 0.013591, time: 2.213 s\n",
      "Step  21740 (epoch    6.79), loss: 0.015606, time: 2.222 s\n",
      "Step  21760 (epoch    6.80), loss: 0.022453, time: 2.218 s\n",
      "Step  21780 (epoch    6.81), loss: 0.014494, time: 2.216 s\n",
      "Step  21800 (epoch    6.81), loss: 0.022427, time: 2.204 s, error: 0.024508\n",
      "Step  21820 (epoch    6.82), loss: 0.042282, time: 14.165 s\n",
      "Step  21840 (epoch    6.83), loss: 0.015777, time: 2.209 s\n",
      "Step  21860 (epoch    6.83), loss: 0.020926, time: 2.220 s\n",
      "Step  21880 (epoch    6.84), loss: 0.018036, time: 2.224 s\n",
      "Step  21900 (epoch    6.84), loss: 0.025364, time: 2.215 s\n",
      "Step  21920 (epoch    6.85), loss: 0.022389, time: 2.216 s\n",
      "Step  21940 (epoch    6.86), loss: 0.010587, time: 2.225 s\n",
      "Step  21960 (epoch    6.86), loss: 0.020171, time: 2.201 s\n",
      "Step  21980 (epoch    6.87), loss: 0.020363, time: 2.219 s\n",
      "Step  22000 (epoch    6.88), loss: 0.015554, time: 2.218 s, error: 0.023957\n",
      "\n",
      "Time since beginning  : 3759.614 s\n",
      "\n",
      "Step  22020 (epoch    6.88), loss: 0.013649, time: 14.236 s\n",
      "Step  22040 (epoch    6.89), loss: 0.018621, time: 2.220 s\n",
      "Step  22060 (epoch    6.89), loss: 0.016676, time: 2.212 s\n",
      "Step  22080 (epoch    6.90), loss: 0.020425, time: 2.209 s\n",
      "Step  22100 (epoch    6.91), loss: 0.029089, time: 2.202 s\n",
      "Step  22120 (epoch    6.91), loss: 0.028502, time: 2.220 s\n",
      "Step  22140 (epoch    6.92), loss: 0.016265, time: 2.229 s\n",
      "Step  22160 (epoch    6.92), loss: 0.017200, time: 2.227 s\n",
      "Step  22180 (epoch    6.93), loss: 0.022589, time: 2.215 s\n",
      "Step  22200 (epoch    6.94), loss: 0.030360, time: 2.216 s, error: 0.023958\n",
      "Step  22220 (epoch    6.94), loss: 0.016109, time: 14.240 s\n",
      "Step  22240 (epoch    6.95), loss: 0.017143, time: 2.231 s\n",
      "Step  22260 (epoch    6.96), loss: 0.017015, time: 2.228 s\n",
      "Step  22280 (epoch    6.96), loss: 0.029139, time: 2.227 s\n",
      "Step  22300 (epoch    6.97), loss: 0.020098, time: 2.207 s\n",
      "Step  22320 (epoch    6.97), loss: 0.013572, time: 2.230 s\n",
      "Step  22340 (epoch    6.98), loss: 0.028760, time: 2.229 s\n",
      "Step  22360 (epoch    6.99), loss: 0.019883, time: 2.216 s\n",
      "Step  22380 (epoch    6.99), loss: 0.027964, time: 2.218 s\n",
      "Step  22400 (epoch    7.00), loss: 0.013274, time: 2.235 s, error: 0.023178\n",
      "Step  22420 (epoch    7.01), loss: 0.028766, time: 14.435 s\n",
      "Step  22440 (epoch    7.01), loss: 0.018427, time: 2.225 s\n",
      "Step  22460 (epoch    7.02), loss: 0.020741, time: 2.219 s\n",
      "Step  22480 (epoch    7.03), loss: 0.014997, time: 2.218 s\n",
      "Step  22500 (epoch    7.03), loss: 0.014710, time: 2.217 s\n",
      "Step  22520 (epoch    7.04), loss: 0.021973, time: 2.229 s\n",
      "Step  22540 (epoch    7.04), loss: 0.016560, time: 2.219 s\n",
      "Step  22560 (epoch    7.05), loss: 0.014527, time: 2.235 s\n",
      "Step  22580 (epoch    7.06), loss: 0.025758, time: 2.215 s\n",
      "Step  22600 (epoch    7.06), loss: 0.020564, time: 2.235 s, error: 0.023317\n",
      "Step  22620 (epoch    7.07), loss: 0.028319, time: 14.581 s\n",
      "Step  22640 (epoch    7.08), loss: 0.011705, time: 2.225 s\n",
      "Step  22660 (epoch    7.08), loss: 0.020395, time: 2.217 s\n",
      "Step  22680 (epoch    7.09), loss: 0.033971, time: 2.223 s\n",
      "Step  22700 (epoch    7.09), loss: 0.014398, time: 2.221 s\n",
      "Step  22720 (epoch    7.10), loss: 0.015527, time: 2.229 s\n",
      "Step  22740 (epoch    7.11), loss: 0.018549, time: 2.223 s\n",
      "Step  22760 (epoch    7.11), loss: 0.019867, time: 2.220 s\n",
      "Step  22780 (epoch    7.12), loss: 0.024190, time: 2.224 s\n",
      "Step  22800 (epoch    7.12), loss: 0.035532, time: 2.219 s, error: 0.031212\n",
      "Step  22820 (epoch    7.13), loss: 0.015378, time: 14.427 s\n",
      "Step  22840 (epoch    7.14), loss: 0.020721, time: 2.233 s\n",
      "Step  22860 (epoch    7.14), loss: 0.015284, time: 2.207 s\n",
      "Step  22880 (epoch    7.15), loss: 0.012622, time: 2.235 s\n",
      "Step  22900 (epoch    7.16), loss: 0.012411, time: 2.220 s\n",
      "Step  22920 (epoch    7.16), loss: 0.010796, time: 2.228 s\n",
      "Step  22940 (epoch    7.17), loss: 0.014568, time: 2.220 s\n",
      "Step  22960 (epoch    7.17), loss: 0.016062, time: 2.216 s\n",
      "Step  22980 (epoch    7.18), loss: 0.014800, time: 2.221 s\n",
      "Step  23000 (epoch    7.19), loss: 0.017364, time: 2.230 s, error: 0.026570\n",
      "Step  23020 (epoch    7.19), loss: 0.021016, time: 14.368 s\n",
      "Step  23040 (epoch    7.20), loss: 0.025121, time: 2.221 s\n",
      "Step  23060 (epoch    7.21), loss: 0.014292, time: 2.217 s\n",
      "Step  23080 (epoch    7.21), loss: 0.023098, time: 2.223 s\n",
      "Step  23100 (epoch    7.22), loss: 0.018133, time: 2.239 s\n",
      "Step  23120 (epoch    7.22), loss: 0.021873, time: 2.221 s\n",
      "Step  23140 (epoch    7.23), loss: 0.016237, time: 2.225 s\n",
      "Step  23160 (epoch    7.24), loss: 0.020271, time: 2.214 s\n",
      "Step  23180 (epoch    7.24), loss: 0.018282, time: 2.214 s\n",
      "Step  23200 (epoch    7.25), loss: 0.024414, time: 2.222 s, error: 0.024314\n",
      "Step  23220 (epoch    7.26), loss: 0.029083, time: 14.376 s\n",
      "Step  23240 (epoch    7.26), loss: 0.013056, time: 2.228 s\n",
      "Step  23260 (epoch    7.27), loss: 0.016683, time: 2.213 s\n",
      "Step  23280 (epoch    7.28), loss: 0.011994, time: 2.223 s\n",
      "Step  23300 (epoch    7.28), loss: 0.018945, time: 2.222 s\n",
      "Step  23320 (epoch    7.29), loss: 0.022306, time: 2.224 s\n",
      "Step  23340 (epoch    7.29), loss: 0.025307, time: 2.221 s\n",
      "Step  23360 (epoch    7.30), loss: 0.026540, time: 2.232 s\n",
      "Step  23380 (epoch    7.31), loss: 0.019370, time: 2.227 s\n",
      "Step  23400 (epoch    7.31), loss: 0.021175, time: 2.226 s, error: 0.023044\n",
      "Step  23420 (epoch    7.32), loss: 0.017936, time: 14.382 s\n",
      "Step  23440 (epoch    7.33), loss: 0.012428, time: 2.222 s\n",
      "Step  23460 (epoch    7.33), loss: 0.015466, time: 2.217 s\n",
      "Step  23480 (epoch    7.34), loss: 0.016207, time: 2.223 s\n",
      "Step  23500 (epoch    7.34), loss: 0.026882, time: 2.222 s\n",
      "Step  23520 (epoch    7.35), loss: 0.016111, time: 2.218 s\n",
      "Step  23540 (epoch    7.36), loss: 0.012611, time: 2.227 s\n",
      "Step  23560 (epoch    7.36), loss: 0.018737, time: 2.218 s\n",
      "Step  23580 (epoch    7.37), loss: 0.023553, time: 2.218 s\n",
      "Step  23600 (epoch    7.38), loss: 0.011782, time: 2.218 s, error: 0.022897\n",
      "Step  23620 (epoch    7.38), loss: 0.019683, time: 14.573 s\n",
      "Step  23640 (epoch    7.39), loss: 0.011813, time: 2.215 s\n",
      "Step  23660 (epoch    7.39), loss: 0.011053, time: 2.222 s\n",
      "Step  23680 (epoch    7.40), loss: 0.018005, time: 2.216 s\n",
      "Step  23700 (epoch    7.41), loss: 0.014958, time: 2.233 s\n",
      "Step  23720 (epoch    7.41), loss: 0.011562, time: 2.220 s\n",
      "Step  23740 (epoch    7.42), loss: 0.022793, time: 2.222 s\n",
      "Step  23760 (epoch    7.42), loss: 0.018271, time: 2.216 s\n",
      "Step  23780 (epoch    7.43), loss: 0.014993, time: 2.221 s\n",
      "Step  23800 (epoch    7.44), loss: 0.014638, time: 2.219 s, error: 0.023182\n",
      "Step  23820 (epoch    7.44), loss: 0.011260, time: 14.559 s\n",
      "Step  23840 (epoch    7.45), loss: 0.024973, time: 2.220 s\n",
      "Step  23860 (epoch    7.46), loss: 0.019105, time: 2.229 s\n",
      "Step  23880 (epoch    7.46), loss: 0.029291, time: 2.225 s\n",
      "Step  23900 (epoch    7.47), loss: 0.035238, time: 2.224 s\n",
      "Step  23920 (epoch    7.47), loss: 0.015191, time: 2.219 s\n",
      "Step  23940 (epoch    7.48), loss: 0.014358, time: 2.223 s\n",
      "Step  23960 (epoch    7.49), loss: 0.019714, time: 2.231 s\n",
      "Step  23980 (epoch    7.49), loss: 0.012744, time: 2.229 s\n",
      "Step  24000 (epoch    7.50), loss: 0.017993, time: 2.210 s, error: 0.023141\n",
      "\n",
      "Time since beginning  : 4103.975 s\n",
      "\n",
      "Step  24020 (epoch    7.51), loss: 0.011591, time: 14.458 s\n",
      "Step  24040 (epoch    7.51), loss: 0.019462, time: 2.224 s\n",
      "Step  24060 (epoch    7.52), loss: 0.016794, time: 2.235 s\n",
      "Step  24080 (epoch    7.53), loss: 0.023734, time: 2.236 s\n",
      "Step  24100 (epoch    7.53), loss: 0.023100, time: 2.229 s\n",
      "Step  24120 (epoch    7.54), loss: 0.023895, time: 2.229 s\n",
      "Step  24140 (epoch    7.54), loss: 0.026887, time: 2.232 s\n",
      "Step  24160 (epoch    7.55), loss: 0.018924, time: 2.219 s\n",
      "Step  24180 (epoch    7.56), loss: 0.012580, time: 2.230 s\n",
      "Step  24200 (epoch    7.56), loss: 0.014855, time: 2.225 s, error: 0.022746\n",
      "Step  24220 (epoch    7.57), loss: 0.015964, time: 14.390 s\n",
      "Step  24240 (epoch    7.58), loss: 0.011090, time: 2.220 s\n",
      "Step  24260 (epoch    7.58), loss: 0.021021, time: 2.219 s\n",
      "Step  24280 (epoch    7.59), loss: 0.013556, time: 2.219 s\n",
      "Step  24300 (epoch    7.59), loss: 0.022290, time: 2.235 s\n",
      "Step  24320 (epoch    7.60), loss: 0.025588, time: 2.226 s\n",
      "Step  24340 (epoch    7.61), loss: 0.016795, time: 2.242 s\n",
      "Step  24360 (epoch    7.61), loss: 0.013680, time: 2.227 s\n",
      "Step  24380 (epoch    7.62), loss: 0.015935, time: 2.219 s\n",
      "Step  24400 (epoch    7.62), loss: 0.025659, time: 2.234 s, error: 0.025859\n",
      "Step  24420 (epoch    7.63), loss: 0.012869, time: 14.392 s\n",
      "Step  24440 (epoch    7.64), loss: 0.019470, time: 2.225 s\n",
      "Step  24460 (epoch    7.64), loss: 0.028332, time: 2.216 s\n",
      "Step  24480 (epoch    7.65), loss: 0.019473, time: 2.218 s\n",
      "Step  24500 (epoch    7.66), loss: 0.018460, time: 2.224 s\n",
      "Step  24520 (epoch    7.66), loss: 0.031254, time: 2.222 s\n",
      "Step  24540 (epoch    7.67), loss: 0.013589, time: 2.228 s\n",
      "Step  24560 (epoch    7.67), loss: 0.015303, time: 2.205 s\n",
      "Step  24580 (epoch    7.68), loss: 0.011451, time: 2.226 s\n",
      "Step  24600 (epoch    7.69), loss: 0.048503, time: 2.230 s, error: 0.024775\n",
      "Step  24620 (epoch    7.69), loss: 0.018870, time: 14.416 s\n",
      "Step  24640 (epoch    7.70), loss: 0.014118, time: 2.231 s\n",
      "Step  24660 (epoch    7.71), loss: 0.019138, time: 2.202 s\n",
      "Step  24680 (epoch    7.71), loss: 0.025702, time: 2.211 s\n",
      "Step  24700 (epoch    7.72), loss: 0.027044, time: 2.214 s\n",
      "Step  24720 (epoch    7.72), loss: 0.013746, time: 2.228 s\n",
      "Step  24740 (epoch    7.73), loss: 0.013624, time: 2.215 s\n",
      "Step  24760 (epoch    7.74), loss: 0.023282, time: 2.217 s\n",
      "Step  24780 (epoch    7.74), loss: 0.009077, time: 2.229 s\n",
      "Step  24800 (epoch    7.75), loss: 0.015293, time: 2.226 s, error: 0.021735\n",
      "Step  24820 (epoch    7.76), loss: 0.009043, time: 14.583 s\n",
      "Step  24840 (epoch    7.76), loss: 0.013449, time: 2.209 s\n",
      "Step  24860 (epoch    7.77), loss: 0.014351, time: 2.226 s\n",
      "Step  24880 (epoch    7.78), loss: 0.016447, time: 2.227 s\n",
      "Step  24900 (epoch    7.78), loss: 0.016987, time: 2.226 s\n",
      "Step  24920 (epoch    7.79), loss: 0.017877, time: 2.216 s\n",
      "Step  24940 (epoch    7.79), loss: 0.014999, time: 2.228 s\n",
      "Step  24960 (epoch    7.80), loss: 0.017112, time: 2.230 s\n",
      "Step  24980 (epoch    7.81), loss: 0.017845, time: 2.229 s\n",
      "Step  25000 (epoch    7.81), loss: 0.024781, time: 2.220 s, error: 0.023829\n",
      "Step  25020 (epoch    7.82), loss: 0.010018, time: 14.382 s\n",
      "Step  25040 (epoch    7.83), loss: 0.012289, time: 2.239 s\n",
      "Step  25060 (epoch    7.83), loss: 0.010342, time: 2.230 s\n",
      "Step  25080 (epoch    7.84), loss: 0.014959, time: 2.223 s\n",
      "Step  25100 (epoch    7.84), loss: 0.014981, time: 2.230 s\n",
      "Step  25120 (epoch    7.85), loss: 0.012756, time: 2.223 s\n",
      "Step  25140 (epoch    7.86), loss: 0.013657, time: 2.222 s\n",
      "Step  25160 (epoch    7.86), loss: 0.027115, time: 2.226 s\n",
      "Step  25180 (epoch    7.87), loss: 0.020128, time: 2.227 s\n",
      "Step  25200 (epoch    7.88), loss: 0.020260, time: 2.212 s, error: 0.022272\n",
      "Step  25220 (epoch    7.88), loss: 0.013670, time: 14.398 s\n",
      "Step  25240 (epoch    7.89), loss: 0.015395, time: 2.227 s\n",
      "Step  25260 (epoch    7.89), loss: 0.011928, time: 2.223 s\n",
      "Step  25280 (epoch    7.90), loss: 0.015023, time: 2.230 s\n",
      "Step  25300 (epoch    7.91), loss: 0.015491, time: 2.238 s\n",
      "Step  25320 (epoch    7.91), loss: 0.018265, time: 2.231 s\n",
      "Step  25340 (epoch    7.92), loss: 0.015727, time: 2.220 s\n",
      "Step  25360 (epoch    7.92), loss: 0.016075, time: 2.222 s\n",
      "Step  25380 (epoch    7.93), loss: 0.019259, time: 2.208 s\n",
      "Step  25400 (epoch    7.94), loss: 0.014080, time: 2.224 s, error: 0.022069\n",
      "Step  25420 (epoch    7.94), loss: 0.012341, time: 14.375 s\n",
      "Step  25440 (epoch    7.95), loss: 0.020093, time: 2.209 s\n",
      "Step  25460 (epoch    7.96), loss: 0.009826, time: 2.218 s\n",
      "Step  25480 (epoch    7.96), loss: 0.018904, time: 2.223 s\n",
      "Step  25500 (epoch    7.97), loss: 0.028822, time: 2.234 s\n",
      "Step  25520 (epoch    7.97), loss: 0.010449, time: 2.232 s\n",
      "Step  25540 (epoch    7.98), loss: 0.009715, time: 2.221 s\n",
      "Step  25560 (epoch    7.99), loss: 0.016814, time: 2.236 s\n",
      "Step  25580 (epoch    7.99), loss: 0.018174, time: 2.226 s\n",
      "Step  25600 (epoch    8.00), loss: 0.018014, time: 2.223 s, error: 0.021701\n",
      "Step  25620 (epoch    8.01), loss: 0.010501, time: 14.427 s\n",
      "Step  25640 (epoch    8.01), loss: 0.013533, time: 2.235 s\n",
      "Step  25660 (epoch    8.02), loss: 0.015002, time: 2.227 s\n",
      "Step  25680 (epoch    8.03), loss: 0.020833, time: 2.227 s\n",
      "Step  25700 (epoch    8.03), loss: 0.014184, time: 2.231 s\n",
      "Step  25720 (epoch    8.04), loss: 0.014884, time: 2.224 s\n",
      "Step  25740 (epoch    8.04), loss: 0.014545, time: 2.224 s\n",
      "Step  25760 (epoch    8.05), loss: 0.023387, time: 2.225 s\n",
      "Step  25780 (epoch    8.06), loss: 0.012435, time: 2.231 s\n",
      "Step  25800 (epoch    8.06), loss: 0.018750, time: 2.216 s, error: 0.021863\n",
      "Step  25820 (epoch    8.07), loss: 0.016333, time: 14.546 s\n",
      "Step  25840 (epoch    8.07), loss: 0.016195, time: 2.214 s\n",
      "Step  25860 (epoch    8.08), loss: 0.017554, time: 2.219 s\n",
      "Step  25880 (epoch    8.09), loss: 0.020282, time: 2.212 s\n",
      "Step  25900 (epoch    8.09), loss: 0.014227, time: 2.222 s\n",
      "Step  25920 (epoch    8.10), loss: 0.021228, time: 2.223 s\n",
      "Step  25940 (epoch    8.11), loss: 0.021863, time: 2.222 s\n",
      "Step  25960 (epoch    8.11), loss: 0.020660, time: 2.225 s\n",
      "Step  25980 (epoch    8.12), loss: 0.014987, time: 2.224 s\n",
      "Step  26000 (epoch    8.12), loss: 0.028126, time: 2.231 s, error: 0.026632\n",
      "\n",
      "Time since beginning  : 4448.713 s\n",
      "\n",
      "Step  26020 (epoch    8.13), loss: 0.016905, time: 14.630 s\n",
      "Step  26040 (epoch    8.14), loss: 0.014628, time: 2.235 s\n",
      "Step  26060 (epoch    8.14), loss: 0.013645, time: 2.236 s\n",
      "Step  26080 (epoch    8.15), loss: 0.017378, time: 2.205 s\n",
      "Step  26100 (epoch    8.16), loss: 0.011491, time: 2.231 s\n",
      "Step  26120 (epoch    8.16), loss: 0.021844, time: 2.238 s\n",
      "Step  26140 (epoch    8.17), loss: 0.020587, time: 2.226 s\n",
      "Step  26160 (epoch    8.18), loss: 0.021690, time: 2.224 s\n",
      "Step  26180 (epoch    8.18), loss: 0.023194, time: 2.222 s\n",
      "Step  26200 (epoch    8.19), loss: 0.014239, time: 2.233 s, error: 0.022178\n",
      "Step  26220 (epoch    8.19), loss: 0.012073, time: 14.469 s\n",
      "Step  26240 (epoch    8.20), loss: 0.014682, time: 2.212 s\n",
      "Step  26260 (epoch    8.21), loss: 0.021592, time: 2.238 s\n",
      "Step  26280 (epoch    8.21), loss: 0.011027, time: 2.231 s\n",
      "Step  26300 (epoch    8.22), loss: 0.012601, time: 2.229 s\n",
      "Step  26320 (epoch    8.22), loss: 0.019709, time: 2.218 s\n",
      "Step  26340 (epoch    8.23), loss: 0.016896, time: 2.227 s\n",
      "Step  26360 (epoch    8.24), loss: 0.009094, time: 2.225 s\n",
      "Step  26380 (epoch    8.24), loss: 0.017997, time: 2.224 s\n",
      "Step  26400 (epoch    8.25), loss: 0.014533, time: 2.214 s, error: 0.021392\n",
      "Step  26420 (epoch    8.26), loss: 0.013746, time: 14.414 s\n",
      "Step  26440 (epoch    8.26), loss: 0.014210, time: 2.228 s\n",
      "Step  26460 (epoch    8.27), loss: 0.020420, time: 2.237 s\n",
      "Step  26480 (epoch    8.28), loss: 0.016081, time: 2.223 s\n",
      "Step  26500 (epoch    8.28), loss: 0.015149, time: 2.226 s\n",
      "Step  26520 (epoch    8.29), loss: 0.016767, time: 2.228 s\n",
      "Step  26540 (epoch    8.29), loss: 0.014846, time: 2.232 s\n",
      "Step  26560 (epoch    8.30), loss: 0.019251, time: 2.223 s\n",
      "Step  26580 (epoch    8.31), loss: 0.021896, time: 2.223 s\n",
      "Step  26600 (epoch    8.31), loss: 0.018715, time: 2.226 s, error: 0.021857\n",
      "Step  26620 (epoch    8.32), loss: 0.014506, time: 14.421 s\n",
      "Step  26640 (epoch    8.32), loss: 0.013246, time: 2.221 s\n",
      "Step  26660 (epoch    8.33), loss: 0.015313, time: 2.219 s\n",
      "Step  26680 (epoch    8.34), loss: 0.011225, time: 2.227 s\n",
      "Step  26700 (epoch    8.34), loss: 0.014279, time: 2.224 s\n",
      "Step  26720 (epoch    8.35), loss: 0.022970, time: 2.233 s\n",
      "Step  26740 (epoch    8.36), loss: 0.017586, time: 2.213 s\n",
      "Step  26760 (epoch    8.36), loss: 0.011677, time: 2.230 s\n",
      "Step  26780 (epoch    8.37), loss: 0.012355, time: 2.225 s\n",
      "Step  26800 (epoch    8.38), loss: 0.011509, time: 2.235 s, error: 0.021538\n",
      "Step  26820 (epoch    8.38), loss: 0.013172, time: 14.434 s\n",
      "Step  26840 (epoch    8.39), loss: 0.014080, time: 2.232 s\n",
      "Step  26860 (epoch    8.39), loss: 0.014656, time: 2.215 s\n",
      "Step  26880 (epoch    8.40), loss: 0.014823, time: 2.234 s\n",
      "Step  26900 (epoch    8.41), loss: 0.014607, time: 2.221 s\n",
      "Step  26920 (epoch    8.41), loss: 0.018220, time: 2.225 s\n",
      "Step  26940 (epoch    8.42), loss: 0.024954, time: 2.223 s\n",
      "Step  26960 (epoch    8.43), loss: 0.024348, time: 2.220 s\n",
      "Step  26980 (epoch    8.43), loss: 0.017038, time: 2.219 s\n",
      "Step  27000 (epoch    8.44), loss: 0.019005, time: 2.226 s, error: 0.022310\n",
      "Step  27020 (epoch    8.44), loss: 0.012844, time: 14.589 s\n",
      "Step  27040 (epoch    8.45), loss: 0.012894, time: 2.226 s\n",
      "Step  27060 (epoch    8.46), loss: 0.013089, time: 2.228 s\n",
      "Step  27080 (epoch    8.46), loss: 0.014232, time: 2.222 s\n",
      "Step  27100 (epoch    8.47), loss: 0.011243, time: 2.226 s\n",
      "Step  27120 (epoch    8.47), loss: 0.013655, time: 2.220 s\n",
      "Step  27140 (epoch    8.48), loss: 0.013672, time: 2.222 s\n",
      "Step  27160 (epoch    8.49), loss: 0.015985, time: 2.226 s\n",
      "Step  27180 (epoch    8.49), loss: 0.026032, time: 2.231 s\n",
      "Step  27200 (epoch    8.50), loss: 0.018247, time: 2.214 s, error: 0.021776\n",
      "Step  27220 (epoch    8.51), loss: 0.018172, time: 14.504 s\n",
      "Step  27240 (epoch    8.51), loss: 0.014194, time: 2.235 s\n",
      "Step  27260 (epoch    8.52), loss: 0.013457, time: 2.228 s\n",
      "Step  27280 (epoch    8.53), loss: 0.014907, time: 2.234 s\n",
      "Step  27300 (epoch    8.53), loss: 0.012095, time: 2.221 s\n",
      "Step  27320 (epoch    8.54), loss: 0.031082, time: 2.229 s\n",
      "Step  27340 (epoch    8.54), loss: 0.023909, time: 2.227 s\n",
      "Step  27360 (epoch    8.55), loss: 0.012859, time: 2.230 s\n",
      "Step  27380 (epoch    8.56), loss: 0.014316, time: 2.232 s\n",
      "Step  27400 (epoch    8.56), loss: 0.017187, time: 2.238 s, error: 0.023811\n",
      "Step  27420 (epoch    8.57), loss: 0.015860, time: 14.430 s\n",
      "Step  27440 (epoch    8.57), loss: 0.017534, time: 2.229 s\n",
      "Step  27460 (epoch    8.58), loss: 0.022691, time: 2.233 s\n",
      "Step  27480 (epoch    8.59), loss: 0.015351, time: 2.220 s\n",
      "Step  27500 (epoch    8.59), loss: 0.015238, time: 2.239 s\n",
      "Step  27520 (epoch    8.60), loss: 0.016903, time: 2.236 s\n",
      "Step  27540 (epoch    8.61), loss: 0.013469, time: 2.219 s\n",
      "Step  27560 (epoch    8.61), loss: 0.012316, time: 2.226 s\n",
      "Step  27580 (epoch    8.62), loss: 0.036252, time: 2.218 s\n",
      "Step  27600 (epoch    8.62), loss: 0.027357, time: 2.230 s, error: 0.023437\n",
      "Step  27620 (epoch    8.63), loss: 0.027927, time: 14.465 s\n",
      "Step  27640 (epoch    8.64), loss: 0.015847, time: 2.236 s\n",
      "Step  27660 (epoch    8.64), loss: 0.013888, time: 2.232 s\n",
      "Step  27680 (epoch    8.65), loss: 0.030237, time: 2.231 s\n",
      "Step  27700 (epoch    8.66), loss: 0.014482, time: 2.235 s\n",
      "Step  27720 (epoch    8.66), loss: 0.020553, time: 2.224 s\n",
      "Step  27740 (epoch    8.67), loss: 0.011775, time: 2.233 s\n",
      "Step  27760 (epoch    8.68), loss: 0.029955, time: 2.230 s\n",
      "Step  27780 (epoch    8.68), loss: 0.012727, time: 2.235 s\n",
      "Step  27800 (epoch    8.69), loss: 0.018574, time: 2.223 s, error: 0.023600\n",
      "Step  27820 (epoch    8.69), loss: 0.014775, time: 14.454 s\n",
      "Step  27840 (epoch    8.70), loss: 0.017696, time: 2.224 s\n",
      "Step  27860 (epoch    8.71), loss: 0.015019, time: 2.229 s\n",
      "Step  27880 (epoch    8.71), loss: 0.012218, time: 2.218 s\n",
      "Step  27900 (epoch    8.72), loss: 0.017961, time: 2.234 s\n",
      "Step  27920 (epoch    8.72), loss: 0.013971, time: 2.234 s\n",
      "Step  27940 (epoch    8.73), loss: 0.016430, time: 2.222 s\n",
      "Step  27960 (epoch    8.74), loss: 0.021068, time: 2.223 s\n",
      "Step  27980 (epoch    8.74), loss: 0.022623, time: 2.234 s\n",
      "Step  28000 (epoch    8.75), loss: 0.018198, time: 2.235 s, error: 0.020511\n",
      "\n",
      "Time since beginning  : 4794.001 s\n",
      "\n",
      "Step  28020 (epoch    8.76), loss: 0.013874, time: 14.675 s\n",
      "Step  28040 (epoch    8.76), loss: 0.013941, time: 2.228 s\n",
      "Step  28060 (epoch    8.77), loss: 0.021922, time: 2.222 s\n",
      "Step  28080 (epoch    8.78), loss: 0.010298, time: 2.220 s\n",
      "Step  28100 (epoch    8.78), loss: 0.011170, time: 2.218 s\n",
      "Step  28120 (epoch    8.79), loss: 0.013724, time: 2.216 s\n",
      "Step  28140 (epoch    8.79), loss: 0.011312, time: 2.225 s\n",
      "Step  28160 (epoch    8.80), loss: 0.015785, time: 2.214 s\n",
      "Step  28180 (epoch    8.81), loss: 0.015063, time: 2.226 s\n",
      "Step  28200 (epoch    8.81), loss: 0.017772, time: 2.224 s, error: 0.022672\n",
      "Step  28220 (epoch    8.82), loss: 0.014740, time: 14.638 s\n",
      "Step  28240 (epoch    8.82), loss: 0.016400, time: 2.218 s\n",
      "Step  28260 (epoch    8.83), loss: 0.013898, time: 2.227 s\n",
      "Step  28280 (epoch    8.84), loss: 0.013048, time: 2.224 s\n",
      "Step  28300 (epoch    8.84), loss: 0.019520, time: 2.228 s\n",
      "Step  28320 (epoch    8.85), loss: 0.009352, time: 2.218 s\n",
      "Step  28340 (epoch    8.86), loss: 0.008322, time: 2.224 s\n",
      "Step  28360 (epoch    8.86), loss: 0.015863, time: 2.224 s\n",
      "Step  28380 (epoch    8.87), loss: 0.012645, time: 2.224 s\n",
      "Step  28400 (epoch    8.88), loss: 0.012882, time: 2.221 s, error: 0.021299\n",
      "Step  28420 (epoch    8.88), loss: 0.012046, time: 14.481 s\n",
      "Step  28440 (epoch    8.89), loss: 0.011213, time: 2.230 s\n",
      "Step  28460 (epoch    8.89), loss: 0.014821, time: 2.237 s\n",
      "Step  28480 (epoch    8.90), loss: 0.007980, time: 2.232 s\n",
      "Step  28500 (epoch    8.91), loss: 0.016017, time: 2.225 s\n",
      "Step  28520 (epoch    8.91), loss: 0.026483, time: 2.234 s\n",
      "Step  28540 (epoch    8.92), loss: 0.016259, time: 2.224 s\n",
      "Step  28560 (epoch    8.93), loss: 0.019793, time: 2.223 s\n",
      "Step  28580 (epoch    8.93), loss: 0.011779, time: 2.223 s\n",
      "Step  28600 (epoch    8.94), loss: 0.016820, time: 2.223 s, error: 0.021248\n",
      "Step  28620 (epoch    8.94), loss: 0.014181, time: 14.396 s\n",
      "Step  28640 (epoch    8.95), loss: 0.009899, time: 2.228 s\n",
      "Step  28660 (epoch    8.96), loss: 0.009109, time: 2.222 s\n",
      "Step  28680 (epoch    8.96), loss: 0.013627, time: 2.227 s\n",
      "Step  28700 (epoch    8.97), loss: 0.017968, time: 2.226 s\n",
      "Step  28720 (epoch    8.97), loss: 0.014236, time: 2.232 s\n",
      "Step  28740 (epoch    8.98), loss: 0.026241, time: 2.226 s\n",
      "Step  28760 (epoch    8.99), loss: 0.019350, time: 2.225 s\n",
      "Step  28780 (epoch    8.99), loss: 0.020556, time: 2.222 s\n",
      "Step  28800 (epoch    9.00), loss: 0.014181, time: 2.216 s, error: 0.020576\n",
      "Step  28820 (epoch    9.01), loss: 0.017384, time: 14.434 s\n",
      "Step  28840 (epoch    9.01), loss: 0.010485, time: 2.224 s\n",
      "Step  28860 (epoch    9.02), loss: 0.014160, time: 2.228 s\n",
      "Step  28880 (epoch    9.03), loss: 0.014692, time: 2.227 s\n",
      "Step  28900 (epoch    9.03), loss: 0.018089, time: 2.226 s\n",
      "Step  28920 (epoch    9.04), loss: 0.017081, time: 2.223 s\n",
      "Step  28940 (epoch    9.04), loss: 0.017881, time: 2.215 s\n",
      "Step  28960 (epoch    9.05), loss: 0.014865, time: 2.227 s\n",
      "Step  28980 (epoch    9.06), loss: 0.021750, time: 2.217 s\n",
      "Step  29000 (epoch    9.06), loss: 0.023317, time: 2.240 s, error: 0.021131\n",
      "Step  29020 (epoch    9.07), loss: 0.012781, time: 14.451 s\n",
      "Step  29040 (epoch    9.07), loss: 0.012318, time: 2.224 s\n",
      "Step  29060 (epoch    9.08), loss: 0.016802, time: 2.226 s\n",
      "Step  29080 (epoch    9.09), loss: 0.026478, time: 2.233 s\n",
      "Step  29100 (epoch    9.09), loss: 0.020571, time: 2.229 s\n",
      "Step  29120 (epoch    9.10), loss: 0.012904, time: 2.229 s\n",
      "Step  29140 (epoch    9.11), loss: 0.013487, time: 2.227 s\n",
      "Step  29160 (epoch    9.11), loss: 0.015308, time: 2.223 s\n",
      "Step  29180 (epoch    9.12), loss: 0.018100, time: 2.228 s\n",
      "Step  29200 (epoch    9.12), loss: 0.012564, time: 2.231 s, error: 0.022033\n",
      "Step  29220 (epoch    9.13), loss: 0.014265, time: 14.635 s\n",
      "Step  29240 (epoch    9.14), loss: 0.021748, time: 2.230 s\n",
      "Step  29260 (epoch    9.14), loss: 0.010815, time: 2.224 s\n",
      "Step  29280 (epoch    9.15), loss: 0.013580, time: 2.230 s\n",
      "Step  29300 (epoch    9.16), loss: 0.012561, time: 2.229 s\n",
      "Step  29320 (epoch    9.16), loss: 0.014871, time: 2.221 s\n",
      "Step  29340 (epoch    9.17), loss: 0.017094, time: 2.225 s\n",
      "Step  29360 (epoch    9.18), loss: 0.014585, time: 2.231 s\n",
      "Step  29380 (epoch    9.18), loss: 0.022580, time: 2.236 s\n",
      "Step  29400 (epoch    9.19), loss: 0.016773, time: 2.220 s, error: 0.022065\n",
      "Step  29420 (epoch    9.19), loss: 0.013830, time: 14.506 s\n",
      "Step  29440 (epoch    9.20), loss: 0.014989, time: 2.229 s\n",
      "Step  29460 (epoch    9.21), loss: 0.022698, time: 2.229 s\n",
      "Step  29480 (epoch    9.21), loss: 0.010404, time: 2.224 s\n",
      "Step  29500 (epoch    9.22), loss: 0.013820, time: 2.221 s\n",
      "Step  29520 (epoch    9.22), loss: 0.018698, time: 2.233 s\n",
      "Step  29540 (epoch    9.23), loss: 0.015355, time: 2.222 s\n",
      "Step  29560 (epoch    9.24), loss: 0.015276, time: 2.223 s\n",
      "Step  29580 (epoch    9.24), loss: 0.011679, time: 2.224 s\n",
      "Step  29600 (epoch    9.25), loss: 0.008851, time: 2.232 s, error: 0.020031\n",
      "Step  29620 (epoch    9.26), loss: 0.012846, time: 14.442 s\n",
      "Step  29640 (epoch    9.26), loss: 0.011186, time: 2.222 s\n",
      "Step  29660 (epoch    9.27), loss: 0.014897, time: 2.226 s\n",
      "Step  29680 (epoch    9.28), loss: 0.011059, time: 2.228 s\n",
      "Step  29700 (epoch    9.28), loss: 0.009029, time: 2.241 s\n",
      "Step  29720 (epoch    9.29), loss: 0.022444, time: 2.215 s\n",
      "Step  29740 (epoch    9.29), loss: 0.012974, time: 2.229 s\n",
      "Step  29760 (epoch    9.30), loss: 0.015250, time: 2.212 s\n",
      "Step  29780 (epoch    9.31), loss: 0.027616, time: 2.228 s\n",
      "Step  29800 (epoch    9.31), loss: 0.014645, time: 2.220 s, error: 0.020240\n",
      "Step  29820 (epoch    9.32), loss: 0.012113, time: 14.473 s\n",
      "Step  29840 (epoch    9.32), loss: 0.020187, time: 2.236 s\n",
      "Step  29860 (epoch    9.33), loss: 0.016240, time: 2.227 s\n",
      "Step  29880 (epoch    9.34), loss: 0.009155, time: 2.228 s\n",
      "Step  29900 (epoch    9.34), loss: 0.009986, time: 2.219 s\n",
      "Step  29920 (epoch    9.35), loss: 0.016118, time: 2.223 s\n",
      "Step  29940 (epoch    9.36), loss: 0.018073, time: 2.226 s\n",
      "Step  29960 (epoch    9.36), loss: 0.014886, time: 2.228 s\n",
      "Step  29980 (epoch    9.37), loss: 0.013441, time: 2.231 s\n",
      "Step  30000 (epoch    9.38), loss: 0.013740, time: 2.223 s, error: 0.020873\n",
      "\n",
      "Time since beginning  : 5139.277 s\n",
      "\n",
      "Step  30020 (epoch    9.38), loss: 0.014370, time: 14.527 s\n",
      "Step  30040 (epoch    9.39), loss: 0.013322, time: 2.226 s\n",
      "Step  30060 (epoch    9.39), loss: 0.016824, time: 2.225 s\n",
      "Step  30080 (epoch    9.40), loss: 0.011286, time: 2.227 s\n",
      "Step  30100 (epoch    9.41), loss: 0.025067, time: 2.228 s\n",
      "Step  30120 (epoch    9.41), loss: 0.024642, time: 2.222 s\n",
      "Step  30140 (epoch    9.42), loss: 0.016296, time: 2.230 s\n",
      "Step  30160 (epoch    9.43), loss: 0.017253, time: 2.218 s\n",
      "Step  30180 (epoch    9.43), loss: 0.013625, time: 2.226 s\n",
      "Step  30200 (epoch    9.44), loss: 0.016212, time: 2.235 s, error: 0.020889\n",
      "Step  30220 (epoch    9.44), loss: 0.012839, time: 14.640 s\n",
      "Step  30240 (epoch    9.45), loss: 0.015236, time: 2.217 s\n",
      "Step  30260 (epoch    9.46), loss: 0.016370, time: 2.228 s\n",
      "Step  30280 (epoch    9.46), loss: 0.016690, time: 2.217 s\n",
      "Step  30300 (epoch    9.47), loss: 0.015739, time: 2.235 s\n",
      "Step  30320 (epoch    9.47), loss: 0.012440, time: 2.224 s\n",
      "Step  30340 (epoch    9.48), loss: 0.017360, time: 2.234 s\n",
      "Step  30360 (epoch    9.49), loss: 0.015759, time: 2.221 s\n",
      "Step  30380 (epoch    9.49), loss: 0.011104, time: 2.233 s\n",
      "Step  30400 (epoch    9.50), loss: 0.020014, time: 2.214 s, error: 0.021680\n",
      "Step  30420 (epoch    9.51), loss: 0.008876, time: 14.583 s\n",
      "Step  30440 (epoch    9.51), loss: 0.013427, time: 2.228 s\n",
      "Step  30460 (epoch    9.52), loss: 0.012812, time: 2.227 s\n",
      "Step  30480 (epoch    9.53), loss: 0.014311, time: 2.231 s\n",
      "Step  30500 (epoch    9.53), loss: 0.012476, time: 2.217 s\n",
      "Step  30520 (epoch    9.54), loss: 0.016665, time: 2.229 s\n",
      "Step  30540 (epoch    9.54), loss: 0.016845, time: 2.229 s\n",
      "Step  30560 (epoch    9.55), loss: 0.011688, time: 2.229 s\n",
      "Step  30580 (epoch    9.56), loss: 0.014277, time: 2.227 s\n",
      "Step  30600 (epoch    9.56), loss: 0.014426, time: 2.222 s, error: 0.021778\n",
      "Step  30620 (epoch    9.57), loss: 0.014069, time: 14.442 s\n",
      "Step  30640 (epoch    9.57), loss: 0.017191, time: 2.233 s\n",
      "Step  30660 (epoch    9.58), loss: 0.014072, time: 2.227 s\n",
      "Step  30680 (epoch    9.59), loss: 0.013124, time: 2.222 s\n",
      "Step  30700 (epoch    9.59), loss: 0.023152, time: 2.229 s\n",
      "Step  30720 (epoch    9.60), loss: 0.015315, time: 2.230 s\n",
      "Step  30740 (epoch    9.61), loss: 0.011665, time: 2.220 s\n",
      "Step  30760 (epoch    9.61), loss: 0.014672, time: 2.228 s\n",
      "Step  30780 (epoch    9.62), loss: 0.013183, time: 2.217 s\n",
      "Step  30800 (epoch    9.62), loss: 0.014040, time: 2.234 s, error: 0.022315\n",
      "Step  30820 (epoch    9.63), loss: 0.007532, time: 14.496 s\n",
      "Step  30840 (epoch    9.64), loss: 0.017335, time: 2.232 s\n",
      "Step  30860 (epoch    9.64), loss: 0.014474, time: 2.235 s\n",
      "Step  30880 (epoch    9.65), loss: 0.020954, time: 2.221 s\n",
      "Step  30900 (epoch    9.66), loss: 0.017186, time: 2.228 s\n",
      "Step  30920 (epoch    9.66), loss: 0.015334, time: 2.234 s\n",
      "Step  30940 (epoch    9.67), loss: 0.011821, time: 2.225 s\n",
      "Step  30960 (epoch    9.68), loss: 0.012577, time: 2.222 s\n",
      "Step  30980 (epoch    9.68), loss: 0.015992, time: 2.228 s\n",
      "Step  31000 (epoch    9.69), loss: 0.012547, time: 2.234 s, error: 0.019793\n",
      "Step  31020 (epoch    9.69), loss: 0.026290, time: 14.422 s\n",
      "Step  31040 (epoch    9.70), loss: 0.012151, time: 2.227 s\n",
      "Step  31060 (epoch    9.71), loss: 0.011864, time: 2.230 s\n",
      "Step  31080 (epoch    9.71), loss: 0.012771, time: 2.222 s\n",
      "Step  31100 (epoch    9.72), loss: 0.014674, time: 2.215 s\n",
      "Step  31120 (epoch    9.72), loss: 0.015573, time: 2.230 s\n",
      "Step  31140 (epoch    9.73), loss: 0.014794, time: 2.233 s\n",
      "Step  31160 (epoch    9.74), loss: 0.016764, time: 2.224 s\n",
      "Step  31180 (epoch    9.74), loss: 0.018154, time: 2.229 s\n",
      "Step  31200 (epoch    9.75), loss: 0.017407, time: 2.227 s, error: 0.019695\n",
      "Step  31220 (epoch    9.76), loss: 0.012750, time: 14.493 s\n",
      "Step  31240 (epoch    9.76), loss: 0.012151, time: 2.231 s\n",
      "Step  31260 (epoch    9.77), loss: 0.015248, time: 2.216 s\n",
      "Step  31280 (epoch    9.78), loss: 0.015090, time: 2.221 s\n",
      "Step  31300 (epoch    9.78), loss: 0.012931, time: 2.235 s\n",
      "Step  31320 (epoch    9.79), loss: 0.012095, time: 2.235 s\n",
      "Step  31340 (epoch    9.79), loss: 0.034707, time: 2.236 s\n",
      "Step  31360 (epoch    9.80), loss: 0.009627, time: 2.230 s\n",
      "Step  31380 (epoch    9.81), loss: 0.014130, time: 2.233 s\n",
      "Step  31400 (epoch    9.81), loss: 0.014086, time: 2.222 s, error: 0.020487\n",
      "Step  31420 (epoch    9.82), loss: 0.021619, time: 14.553 s\n",
      "Step  31440 (epoch    9.82), loss: 0.012617, time: 2.215 s\n",
      "Step  31460 (epoch    9.83), loss: 0.011607, time: 2.208 s\n",
      "Step  31480 (epoch    9.84), loss: 0.014730, time: 2.231 s\n",
      "Step  31500 (epoch    9.84), loss: 0.013818, time: 2.215 s\n",
      "Step  31520 (epoch    9.85), loss: 0.014797, time: 2.225 s\n",
      "Step  31540 (epoch    9.86), loss: 0.010168, time: 2.235 s\n",
      "Step  31560 (epoch    9.86), loss: 0.012891, time: 2.232 s\n",
      "Step  31580 (epoch    9.87), loss: 0.011698, time: 2.228 s\n",
      "Step  31600 (epoch    9.88), loss: 0.025079, time: 2.222 s, error: 0.019866\n",
      "Step  31620 (epoch    9.88), loss: 0.015292, time: 14.488 s\n",
      "Step  31640 (epoch    9.89), loss: 0.010031, time: 2.237 s\n",
      "Step  31660 (epoch    9.89), loss: 0.007990, time: 2.235 s\n",
      "Step  31680 (epoch    9.90), loss: 0.011538, time: 2.228 s\n",
      "Step  31700 (epoch    9.91), loss: 0.014185, time: 2.233 s\n",
      "Step  31720 (epoch    9.91), loss: 0.017557, time: 2.216 s\n",
      "Step  31740 (epoch    9.92), loss: 0.014911, time: 2.224 s\n",
      "Step  31760 (epoch    9.93), loss: 0.017954, time: 2.227 s\n",
      "Step  31780 (epoch    9.93), loss: 0.016959, time: 2.232 s\n",
      "Step  31800 (epoch    9.94), loss: 0.013703, time: 2.228 s, error: 0.021106\n",
      "Step  31820 (epoch    9.94), loss: 0.012249, time: 14.446 s\n",
      "Step  31840 (epoch    9.95), loss: 0.010833, time: 2.236 s\n",
      "Step  31860 (epoch    9.96), loss: 0.019369, time: 2.229 s\n",
      "Step  31880 (epoch    9.96), loss: 0.013357, time: 2.237 s\n",
      "Step  31900 (epoch    9.97), loss: 0.013617, time: 2.239 s\n",
      "Step  31920 (epoch    9.97), loss: 0.021977, time: 2.233 s\n",
      "Step  31940 (epoch    9.98), loss: 0.015350, time: 2.230 s\n",
      "Step  31960 (epoch    9.99), loss: 0.019760, time: 2.233 s\n",
      "Step  31980 (epoch    9.99), loss: 0.015838, time: 2.212 s\n",
      "Step  32000 (epoch   10.00), loss: 0.028311, time: 2.221 s, error: 0.020726\n",
      "\n",
      "Time since beginning  : 5484.858 s\n",
      "\n",
      "Step  32020 (epoch   10.01), loss: 0.021662, time: 14.579 s\n",
      "Step  32040 (epoch   10.01), loss: 0.009487, time: 2.220 s\n",
      "Step  32060 (epoch   10.02), loss: 0.024628, time: 2.208 s\n",
      "Step  32080 (epoch   10.03), loss: 0.009860, time: 2.220 s\n",
      "Step  32100 (epoch   10.03), loss: 0.012633, time: 2.232 s\n",
      "Step  32120 (epoch   10.04), loss: 0.014324, time: 2.231 s\n",
      "Step  32140 (epoch   10.04), loss: 0.014232, time: 2.226 s\n",
      "Step  32160 (epoch   10.05), loss: 0.010057, time: 2.236 s\n",
      "Step  32180 (epoch   10.06), loss: 0.013110, time: 2.228 s\n",
      "Step  32200 (epoch   10.06), loss: 0.015865, time: 2.220 s, error: 0.020534\n",
      "Step  32220 (epoch   10.07), loss: 0.010692, time: 14.608 s\n",
      "Step  32240 (epoch   10.07), loss: 0.011818, time: 2.236 s\n",
      "Step  32260 (epoch   10.08), loss: 0.016568, time: 2.216 s\n",
      "Step  32280 (epoch   10.09), loss: 0.010888, time: 2.229 s\n",
      "Step  32300 (epoch   10.09), loss: 0.018315, time: 2.214 s\n",
      "Step  32320 (epoch   10.10), loss: 0.010210, time: 2.231 s\n",
      "Step  32340 (epoch   10.11), loss: 0.008720, time: 2.226 s\n",
      "Step  32360 (epoch   10.11), loss: 0.014359, time: 2.226 s\n",
      "Step  32380 (epoch   10.12), loss: 0.016491, time: 2.219 s\n",
      "Step  32400 (epoch   10.12), loss: 0.013746, time: 2.218 s, error: 0.019778\n",
      "Step  32420 (epoch   10.13), loss: 0.015215, time: 14.605 s\n",
      "Step  32440 (epoch   10.14), loss: 0.021686, time: 2.229 s\n",
      "Step  32460 (epoch   10.14), loss: 0.010294, time: 2.237 s\n",
      "Step  32480 (epoch   10.15), loss: 0.011174, time: 2.230 s\n",
      "Step  32500 (epoch   10.16), loss: 0.008882, time: 2.234 s\n",
      "Step  32520 (epoch   10.16), loss: 0.014012, time: 2.217 s\n",
      "Step  32540 (epoch   10.17), loss: 0.015250, time: 2.230 s\n",
      "Step  32560 (epoch   10.18), loss: 0.016820, time: 2.232 s\n",
      "Step  32580 (epoch   10.18), loss: 0.013764, time: 2.228 s\n",
      "Step  32600 (epoch   10.19), loss: 0.017205, time: 2.228 s, error: 0.021529\n",
      "Step  32620 (epoch   10.19), loss: 0.009432, time: 14.622 s\n",
      "Step  32640 (epoch   10.20), loss: 0.016847, time: 2.228 s\n",
      "Step  32660 (epoch   10.21), loss: 0.012704, time: 2.234 s\n",
      "Step  32680 (epoch   10.21), loss: 0.013265, time: 2.229 s\n",
      "Step  32700 (epoch   10.22), loss: 0.013068, time: 2.237 s\n",
      "Step  32720 (epoch   10.22), loss: 0.008975, time: 2.234 s\n",
      "Step  32740 (epoch   10.23), loss: 0.012702, time: 2.221 s\n",
      "Step  32760 (epoch   10.24), loss: 0.014885, time: 2.222 s\n",
      "Step  32780 (epoch   10.24), loss: 0.016410, time: 2.217 s\n",
      "Step  32800 (epoch   10.25), loss: 0.013493, time: 2.229 s, error: 0.019510\n",
      "Step  32820 (epoch   10.26), loss: 0.013663, time: 14.443 s\n",
      "Step  32840 (epoch   10.26), loss: 0.012306, time: 2.231 s\n",
      "Step  32860 (epoch   10.27), loss: 0.010328, time: 2.237 s\n",
      "Step  32880 (epoch   10.28), loss: 0.013507, time: 2.223 s\n",
      "Step  32900 (epoch   10.28), loss: 0.010846, time: 2.220 s\n",
      "Step  32920 (epoch   10.29), loss: 0.011685, time: 2.219 s\n",
      "Step  32940 (epoch   10.29), loss: 0.010910, time: 2.225 s\n",
      "Step  32960 (epoch   10.30), loss: 0.015614, time: 2.221 s\n",
      "Step  32980 (epoch   10.31), loss: 0.013156, time: 2.229 s\n",
      "Step  33000 (epoch   10.31), loss: 0.015407, time: 2.234 s, error: 0.019593\n",
      "Step  33020 (epoch   10.32), loss: 0.018079, time: 14.538 s\n",
      "Step  33040 (epoch   10.32), loss: 0.017828, time: 2.225 s\n",
      "Step  33060 (epoch   10.33), loss: 0.017666, time: 2.229 s\n",
      "Step  33080 (epoch   10.34), loss: 0.010017, time: 2.226 s\n",
      "Step  33100 (epoch   10.34), loss: 0.011556, time: 2.223 s\n",
      "Step  33120 (epoch   10.35), loss: 0.012481, time: 2.241 s\n",
      "Step  33140 (epoch   10.36), loss: 0.013776, time: 2.235 s\n",
      "Step  33160 (epoch   10.36), loss: 0.020344, time: 2.236 s\n",
      "Step  33180 (epoch   10.37), loss: 0.018347, time: 2.224 s\n",
      "Step  33200 (epoch   10.38), loss: 0.011707, time: 2.234 s, error: 0.019887\n",
      "Step  33220 (epoch   10.38), loss: 0.036874, time: 14.401 s\n",
      "Step  33240 (epoch   10.39), loss: 0.021737, time: 2.217 s\n",
      "Step  33260 (epoch   10.39), loss: 0.013184, time: 2.229 s\n",
      "Step  33280 (epoch   10.40), loss: 0.016073, time: 2.213 s\n",
      "Step  33300 (epoch   10.41), loss: 0.015235, time: 2.218 s\n",
      "Step  33320 (epoch   10.41), loss: 0.010939, time: 2.225 s\n",
      "Step  33340 (epoch   10.42), loss: 0.013451, time: 2.223 s\n",
      "Step  33360 (epoch   10.43), loss: 0.012309, time: 2.229 s\n",
      "Step  33380 (epoch   10.43), loss: 0.017842, time: 2.227 s\n",
      "Step  33400 (epoch   10.44), loss: 0.013581, time: 2.243 s, error: 0.019502\n",
      "Step  33420 (epoch   10.44), loss: 0.021105, time: 14.412 s\n",
      "Step  33440 (epoch   10.45), loss: 0.011436, time: 2.229 s\n",
      "Step  33460 (epoch   10.46), loss: 0.008151, time: 2.226 s\n",
      "Step  33480 (epoch   10.46), loss: 0.016209, time: 2.226 s\n",
      "Step  33500 (epoch   10.47), loss: 0.011175, time: 2.220 s\n",
      "Step  33520 (epoch   10.47), loss: 0.010765, time: 2.228 s\n",
      "Step  33540 (epoch   10.48), loss: 0.013048, time: 2.234 s\n",
      "Step  33560 (epoch   10.49), loss: 0.014945, time: 2.232 s\n",
      "Step  33580 (epoch   10.49), loss: 0.012162, time: 2.221 s\n",
      "Step  33600 (epoch   10.50), loss: 0.015650, time: 2.220 s, error: 0.020038\n",
      "Step  33620 (epoch   10.51), loss: 0.014057, time: 14.615 s\n",
      "Step  33640 (epoch   10.51), loss: 0.014501, time: 2.227 s\n",
      "Step  33660 (epoch   10.52), loss: 0.009413, time: 2.234 s\n",
      "Step  33680 (epoch   10.53), loss: 0.011233, time: 2.233 s\n",
      "Step  33700 (epoch   10.53), loss: 0.013166, time: 2.226 s\n",
      "Step  33720 (epoch   10.54), loss: 0.014500, time: 2.223 s\n",
      "Step  33740 (epoch   10.54), loss: 0.012316, time: 2.224 s\n",
      "Step  33760 (epoch   10.55), loss: 0.017821, time: 2.215 s\n",
      "Step  33780 (epoch   10.56), loss: 0.014047, time: 2.215 s\n",
      "Step  33800 (epoch   10.56), loss: 0.018777, time: 2.223 s, error: 0.020189\n",
      "Step  33820 (epoch   10.57), loss: 0.016579, time: 14.381 s\n",
      "Step  33840 (epoch   10.57), loss: 0.008279, time: 2.239 s\n",
      "Step  33860 (epoch   10.58), loss: 0.013897, time: 2.234 s\n",
      "Step  33880 (epoch   10.59), loss: 0.013460, time: 2.222 s\n",
      "Step  33900 (epoch   10.59), loss: 0.015075, time: 2.222 s\n",
      "Step  33920 (epoch   10.60), loss: 0.014753, time: 2.222 s\n",
      "Step  33940 (epoch   10.61), loss: 0.015056, time: 2.221 s\n",
      "Step  33960 (epoch   10.61), loss: 0.010836, time: 2.231 s\n",
      "Step  33980 (epoch   10.62), loss: 0.015946, time: 2.223 s\n",
      "Step  34000 (epoch   10.62), loss: 0.014945, time: 2.214 s, error: 0.020855\n",
      "\n",
      "Time since beginning  : 5830.412 s\n",
      "\n",
      "Step  34020 (epoch   10.63), loss: 0.012412, time: 14.566 s\n",
      "Step  34040 (epoch   10.64), loss: 0.022111, time: 2.230 s\n",
      "Step  34060 (epoch   10.64), loss: 0.012344, time: 2.216 s\n",
      "Step  34080 (epoch   10.65), loss: 0.012281, time: 2.227 s\n",
      "Step  34100 (epoch   10.66), loss: 0.020730, time: 2.237 s\n",
      "Step  34120 (epoch   10.66), loss: 0.011981, time: 2.227 s\n",
      "Step  34140 (epoch   10.67), loss: 0.010872, time: 2.206 s\n",
      "Step  34160 (epoch   10.68), loss: 0.017145, time: 2.226 s\n",
      "Step  34180 (epoch   10.68), loss: 0.014899, time: 2.229 s\n",
      "Step  34200 (epoch   10.69), loss: 0.013182, time: 2.218 s, error: 0.019416\n",
      "Step  34220 (epoch   10.69), loss: 0.010926, time: 14.385 s\n",
      "Step  34240 (epoch   10.70), loss: 0.016133, time: 2.217 s\n",
      "Step  34260 (epoch   10.71), loss: 0.011552, time: 2.226 s\n",
      "Step  34280 (epoch   10.71), loss: 0.015160, time: 2.217 s\n",
      "Step  34300 (epoch   10.72), loss: 0.013763, time: 2.226 s\n",
      "Step  34320 (epoch   10.72), loss: 0.013724, time: 2.224 s\n",
      "Step  34340 (epoch   10.73), loss: 0.018259, time: 2.215 s\n",
      "Step  34360 (epoch   10.74), loss: 0.010947, time: 2.236 s\n",
      "Step  34380 (epoch   10.74), loss: 0.011808, time: 2.220 s\n",
      "Step  34400 (epoch   10.75), loss: 0.019307, time: 2.228 s, error: 0.018869\n",
      "Step  34420 (epoch   10.76), loss: 0.011341, time: 14.387 s\n",
      "Step  34440 (epoch   10.76), loss: 0.007549, time: 2.227 s\n",
      "Step  34460 (epoch   10.77), loss: 0.013679, time: 2.231 s\n",
      "Step  34480 (epoch   10.78), loss: 0.014253, time: 2.232 s\n",
      "Step  34500 (epoch   10.78), loss: 0.015949, time: 2.224 s\n",
      "Step  34520 (epoch   10.79), loss: 0.017900, time: 2.231 s\n",
      "Step  34540 (epoch   10.79), loss: 0.011870, time: 2.234 s\n",
      "Step  34560 (epoch   10.80), loss: 0.016128, time: 2.234 s\n",
      "Step  34580 (epoch   10.81), loss: 0.018688, time: 2.233 s\n",
      "Step  34600 (epoch   10.81), loss: 0.013240, time: 2.233 s, error: 0.018615\n",
      "Step  34620 (epoch   10.82), loss: 0.017292, time: 14.584 s\n",
      "Step  34640 (epoch   10.82), loss: 0.011287, time: 2.235 s\n",
      "Step  34660 (epoch   10.83), loss: 0.011828, time: 2.231 s\n",
      "Step  34680 (epoch   10.84), loss: 0.045895, time: 2.233 s\n",
      "Step  34700 (epoch   10.84), loss: 0.010342, time: 2.223 s\n",
      "Step  34720 (epoch   10.85), loss: 0.010448, time: 2.217 s\n",
      "Step  34740 (epoch   10.86), loss: 0.008875, time: 2.233 s\n",
      "Step  34760 (epoch   10.86), loss: 0.012020, time: 2.226 s\n",
      "Step  34780 (epoch   10.87), loss: 0.012292, time: 2.220 s\n",
      "Step  34800 (epoch   10.88), loss: 0.016537, time: 2.223 s, error: 0.018668\n",
      "Step  34820 (epoch   10.88), loss: 0.010704, time: 14.554 s\n",
      "Step  34840 (epoch   10.89), loss: 0.013142, time: 2.226 s\n",
      "Step  34860 (epoch   10.89), loss: 0.010527, time: 2.209 s\n",
      "Step  34880 (epoch   10.90), loss: 0.010342, time: 2.228 s\n",
      "Step  34900 (epoch   10.91), loss: 0.029673, time: 2.219 s\n",
      "Step  34920 (epoch   10.91), loss: 0.014447, time: 2.221 s\n",
      "Step  34940 (epoch   10.92), loss: 0.021976, time: 2.202 s\n",
      "Step  34960 (epoch   10.93), loss: 0.016602, time: 2.222 s\n",
      "Step  34980 (epoch   10.93), loss: 0.012238, time: 2.206 s\n",
      "Step  35000 (epoch   10.94), loss: 0.012338, time: 2.217 s, error: 0.020644\n",
      "Step  35020 (epoch   10.94), loss: 0.009859, time: 14.426 s\n",
      "Step  35040 (epoch   10.95), loss: 0.012231, time: 2.214 s\n",
      "Step  35060 (epoch   10.96), loss: 0.008069, time: 2.235 s\n",
      "Step  35080 (epoch   10.96), loss: 0.015297, time: 2.227 s\n",
      "Step  35100 (epoch   10.97), loss: 0.023915, time: 2.221 s\n",
      "Step  35120 (epoch   10.97), loss: 0.029991, time: 2.224 s\n",
      "Step  35140 (epoch   10.98), loss: 0.009254, time: 2.217 s\n",
      "Step  35160 (epoch   10.99), loss: 0.012055, time: 2.222 s\n",
      "Step  35180 (epoch   10.99), loss: 0.011786, time: 2.219 s\n",
      "Step  35200 (epoch   11.00), loss: 0.015407, time: 2.233 s, error: 0.021988\n",
      "Step  35220 (epoch   11.01), loss: 0.009336, time: 14.390 s\n",
      "Step  35240 (epoch   11.01), loss: 0.015428, time: 2.221 s\n",
      "Step  35260 (epoch   11.02), loss: 0.009684, time: 2.232 s\n",
      "Step  35280 (epoch   11.03), loss: 0.012127, time: 2.215 s\n",
      "Step  35300 (epoch   11.03), loss: 0.011387, time: 2.213 s\n",
      "Step  35320 (epoch   11.04), loss: 0.013686, time: 2.217 s\n",
      "Step  35340 (epoch   11.04), loss: 0.013485, time: 2.231 s\n",
      "Step  35360 (epoch   11.05), loss: 0.012089, time: 2.224 s\n",
      "Step  35380 (epoch   11.06), loss: 0.013911, time: 2.230 s\n",
      "Step  35400 (epoch   11.06), loss: 0.014252, time: 2.224 s, error: 0.020028\n",
      "Step  35420 (epoch   11.07), loss: 0.009589, time: 14.416 s\n",
      "Step  35440 (epoch   11.07), loss: 0.012531, time: 2.232 s\n",
      "Step  35460 (epoch   11.08), loss: 0.012219, time: 2.223 s\n",
      "Step  35480 (epoch   11.09), loss: 0.009555, time: 2.227 s\n",
      "Step  35500 (epoch   11.09), loss: 0.012776, time: 2.235 s\n",
      "Step  35520 (epoch   11.10), loss: 0.015667, time: 2.208 s\n",
      "Step  35540 (epoch   11.11), loss: 0.011528, time: 2.224 s\n",
      "Step  35560 (epoch   11.11), loss: 0.013239, time: 2.223 s\n",
      "Step  35580 (epoch   11.12), loss: 0.014876, time: 2.231 s\n",
      "Step  35600 (epoch   11.12), loss: 0.011877, time: 2.231 s, error: 0.019696\n",
      "Step  35620 (epoch   11.13), loss: 0.010608, time: 14.416 s\n",
      "Step  35640 (epoch   11.14), loss: 0.012456, time: 2.211 s\n",
      "Step  35660 (epoch   11.14), loss: 0.016839, time: 2.228 s\n",
      "Step  35680 (epoch   11.15), loss: 0.021148, time: 2.230 s\n",
      "Step  35700 (epoch   11.16), loss: 0.010892, time: 2.219 s\n",
      "Step  35720 (epoch   11.16), loss: 0.009804, time: 2.226 s\n",
      "Step  35740 (epoch   11.17), loss: 0.020313, time: 2.219 s\n",
      "Step  35760 (epoch   11.18), loss: 0.030837, time: 2.224 s\n",
      "Step  35780 (epoch   11.18), loss: 0.014150, time: 2.231 s\n",
      "Step  35800 (epoch   11.19), loss: 0.011274, time: 2.222 s, error: 0.020703\n",
      "Step  35820 (epoch   11.19), loss: 0.021834, time: 14.580 s\n",
      "Step  35840 (epoch   11.20), loss: 0.015369, time: 2.233 s\n",
      "Step  35860 (epoch   11.21), loss: 0.016014, time: 2.229 s\n",
      "Step  35880 (epoch   11.21), loss: 0.009129, time: 2.234 s\n",
      "Step  35900 (epoch   11.22), loss: 0.010694, time: 2.229 s\n",
      "Step  35920 (epoch   11.22), loss: 0.020392, time: 2.220 s\n",
      "Step  35940 (epoch   11.23), loss: 0.012850, time: 2.227 s\n",
      "Step  35960 (epoch   11.24), loss: 0.015608, time: 2.231 s\n",
      "Step  35980 (epoch   11.24), loss: 0.017866, time: 2.221 s\n",
      "Step  36000 (epoch   11.25), loss: 0.011155, time: 2.222 s, error: 0.019895\n",
      "\n",
      "Time since beginning  : 6175.235 s\n",
      "\n",
      "Step  36020 (epoch   11.26), loss: 0.011092, time: 14.485 s\n",
      "Step  36040 (epoch   11.26), loss: 0.010307, time: 2.236 s\n",
      "Step  36060 (epoch   11.27), loss: 0.015256, time: 2.218 s\n",
      "Step  36080 (epoch   11.28), loss: 0.009503, time: 2.223 s\n",
      "Step  36100 (epoch   11.28), loss: 0.015162, time: 2.225 s\n",
      "Step  36120 (epoch   11.29), loss: 0.016652, time: 2.222 s\n",
      "Step  36140 (epoch   11.29), loss: 0.011461, time: 2.220 s\n",
      "Step  36160 (epoch   11.30), loss: 0.013684, time: 2.221 s\n",
      "Step  36180 (epoch   11.31), loss: 0.009230, time: 2.228 s\n",
      "Step  36200 (epoch   11.31), loss: 0.011574, time: 2.232 s, error: 0.019226\n",
      "Step  36220 (epoch   11.32), loss: 0.008678, time: 14.444 s\n",
      "Step  36240 (epoch   11.32), loss: 0.011431, time: 2.226 s\n",
      "Step  36260 (epoch   11.33), loss: 0.020279, time: 2.232 s\n",
      "Step  36280 (epoch   11.34), loss: 0.017719, time: 2.232 s\n",
      "Step  36300 (epoch   11.34), loss: 0.013262, time: 2.239 s\n",
      "Step  36320 (epoch   11.35), loss: 0.013157, time: 2.228 s\n",
      "Step  36340 (epoch   11.36), loss: 0.017364, time: 2.222 s\n",
      "Step  36360 (epoch   11.36), loss: 0.014705, time: 2.217 s\n",
      "Step  36380 (epoch   11.37), loss: 0.011381, time: 2.218 s\n",
      "Step  36400 (epoch   11.38), loss: 0.010628, time: 2.218 s, error: 0.019080\n",
      "Step  36420 (epoch   11.38), loss: 0.018325, time: 14.403 s\n",
      "Step  36440 (epoch   11.39), loss: 0.024358, time: 2.214 s\n",
      "Step  36460 (epoch   11.39), loss: 0.012869, time: 2.218 s\n",
      "Step  36480 (epoch   11.40), loss: 0.012083, time: 2.226 s\n",
      "Step  36500 (epoch   11.41), loss: 0.015764, time: 2.227 s\n",
      "Step  36520 (epoch   11.41), loss: 0.010500, time: 2.226 s\n",
      "Step  36540 (epoch   11.42), loss: 0.014549, time: 2.230 s\n",
      "Step  36560 (epoch   11.43), loss: 0.010487, time: 2.228 s\n",
      "Step  36580 (epoch   11.43), loss: 0.013152, time: 2.240 s\n",
      "Step  36600 (epoch   11.44), loss: 0.010594, time: 2.217 s, error: 0.018266\n",
      "Step  36620 (epoch   11.44), loss: 0.012199, time: 14.414 s\n",
      "Step  36640 (epoch   11.45), loss: 0.013402, time: 2.225 s\n",
      "Step  36660 (epoch   11.46), loss: 0.017886, time: 2.222 s\n",
      "Step  36680 (epoch   11.46), loss: 0.016073, time: 2.230 s\n",
      "Step  36700 (epoch   11.47), loss: 0.010997, time: 2.228 s\n",
      "Step  36720 (epoch   11.47), loss: 0.013103, time: 2.225 s\n",
      "Step  36740 (epoch   11.48), loss: 0.010731, time: 2.227 s\n",
      "Step  36760 (epoch   11.49), loss: 0.012035, time: 2.220 s\n",
      "Step  36780 (epoch   11.49), loss: 0.013134, time: 2.231 s\n",
      "Step  36800 (epoch   11.50), loss: 0.012073, time: 2.223 s, error: 0.018896\n",
      "Step  36820 (epoch   11.51), loss: 0.016871, time: 14.674 s\n",
      "Step  36840 (epoch   11.51), loss: 0.013484, time: 2.231 s\n",
      "Step  36860 (epoch   11.52), loss: 0.015290, time: 2.228 s\n",
      "Step  36880 (epoch   11.53), loss: 0.013217, time: 2.226 s\n",
      "Step  36900 (epoch   11.53), loss: 0.007872, time: 2.227 s\n",
      "Step  36920 (epoch   11.54), loss: 0.013185, time: 2.223 s\n",
      "Step  36940 (epoch   11.54), loss: 0.014068, time: 2.235 s\n",
      "Step  36960 (epoch   11.55), loss: 0.009943, time: 2.222 s\n",
      "Step  36980 (epoch   11.56), loss: 0.011206, time: 2.229 s\n",
      "Step  37000 (epoch   11.56), loss: 0.010458, time: 2.222 s, error: 0.019325\n",
      "Step  37020 (epoch   11.57), loss: 0.014140, time: 14.601 s\n",
      "Step  37040 (epoch   11.57), loss: 0.016110, time: 2.220 s\n",
      "Step  37060 (epoch   11.58), loss: 0.013283, time: 2.219 s\n",
      "Step  37080 (epoch   11.59), loss: 0.012241, time: 2.225 s\n",
      "Step  37100 (epoch   11.59), loss: 0.023178, time: 2.218 s\n",
      "Step  37120 (epoch   11.60), loss: 0.014173, time: 2.233 s\n",
      "Step  37140 (epoch   11.61), loss: 0.013957, time: 2.220 s\n",
      "Step  37160 (epoch   11.61), loss: 0.012863, time: 2.230 s\n",
      "Step  37180 (epoch   11.62), loss: 0.017190, time: 2.234 s\n",
      "Step  37200 (epoch   11.62), loss: 0.012960, time: 2.224 s, error: 0.018977\n",
      "Step  37220 (epoch   11.63), loss: 0.014697, time: 14.401 s\n",
      "Step  37240 (epoch   11.64), loss: 0.015388, time: 2.224 s\n",
      "Step  37260 (epoch   11.64), loss: 0.011138, time: 2.235 s\n",
      "Step  37280 (epoch   11.65), loss: 0.020627, time: 2.233 s\n",
      "Step  37300 (epoch   11.66), loss: 0.012896, time: 2.227 s\n",
      "Step  37320 (epoch   11.66), loss: 0.010214, time: 2.216 s\n",
      "Step  37340 (epoch   11.67), loss: 0.013949, time: 2.229 s\n",
      "Step  37360 (epoch   11.68), loss: 0.013045, time: 2.218 s\n",
      "Step  37380 (epoch   11.68), loss: 0.012412, time: 2.236 s\n",
      "Step  37400 (epoch   11.69), loss: 0.017787, time: 2.227 s, error: 0.020312\n",
      "Step  37420 (epoch   11.69), loss: 0.013258, time: 14.419 s\n",
      "Step  37440 (epoch   11.70), loss: 0.012951, time: 2.227 s\n",
      "Step  37460 (epoch   11.71), loss: 0.011269, time: 2.222 s\n",
      "Step  37480 (epoch   11.71), loss: 0.011525, time: 2.227 s\n",
      "Step  37500 (epoch   11.72), loss: 0.013112, time: 2.222 s\n",
      "Step  37520 (epoch   11.72), loss: 0.013337, time: 2.212 s\n",
      "Step  37540 (epoch   11.73), loss: 0.012928, time: 2.232 s\n",
      "Step  37560 (epoch   11.74), loss: 0.013498, time: 2.228 s\n",
      "Step  37580 (epoch   11.74), loss: 0.014086, time: 2.231 s\n",
      "Step  37600 (epoch   11.75), loss: 0.013529, time: 2.217 s, error: 0.018162\n",
      "Step  37620 (epoch   11.76), loss: 0.013500, time: 14.458 s\n",
      "Step  37640 (epoch   11.76), loss: 0.013060, time: 2.224 s\n",
      "Step  37660 (epoch   11.77), loss: 0.014341, time: 2.215 s\n",
      "Step  37680 (epoch   11.78), loss: 0.008662, time: 2.212 s\n",
      "Step  37700 (epoch   11.78), loss: 0.013865, time: 2.218 s\n",
      "Step  37720 (epoch   11.79), loss: 0.014087, time: 2.224 s\n",
      "Step  37740 (epoch   11.79), loss: 0.011612, time: 2.233 s\n",
      "Step  37760 (epoch   11.80), loss: 0.014170, time: 2.227 s\n",
      "Step  37780 (epoch   11.81), loss: 0.008823, time: 2.226 s\n",
      "Step  37800 (epoch   11.81), loss: 0.013065, time: 2.231 s, error: 0.017989\n",
      "Step  37820 (epoch   11.82), loss: 0.016213, time: 14.578 s\n",
      "Step  37840 (epoch   11.82), loss: 0.009886, time: 2.220 s\n",
      "Step  37860 (epoch   11.83), loss: 0.010571, time: 2.215 s\n",
      "Step  37880 (epoch   11.84), loss: 0.014014, time: 2.231 s\n",
      "Step  37900 (epoch   11.84), loss: 0.010391, time: 2.219 s\n",
      "Step  37920 (epoch   11.85), loss: 0.010816, time: 2.228 s\n",
      "Step  37940 (epoch   11.86), loss: 0.007284, time: 2.233 s\n",
      "Step  37960 (epoch   11.86), loss: 0.010440, time: 2.226 s\n",
      "Step  37980 (epoch   11.87), loss: 0.012401, time: 2.215 s\n",
      "Step  38000 (epoch   11.88), loss: 0.017560, time: 2.225 s, error: 0.017927\n",
      "\n",
      "Time since beginning  : 6520.502 s\n",
      "\n",
      "Step  38020 (epoch   11.88), loss: 0.008746, time: 14.620 s\n",
      "Step  38040 (epoch   11.89), loss: 0.012530, time: 2.229 s\n",
      "Step  38060 (epoch   11.89), loss: 0.012701, time: 2.225 s\n",
      "Step  38080 (epoch   11.90), loss: 0.009111, time: 2.220 s\n",
      "Step  38100 (epoch   11.91), loss: 0.015304, time: 2.220 s\n",
      "Step  38120 (epoch   11.91), loss: 0.010889, time: 2.229 s\n",
      "Step  38140 (epoch   11.92), loss: 0.020752, time: 2.227 s\n",
      "Step  38160 (epoch   11.93), loss: 0.010752, time: 2.217 s\n",
      "Step  38180 (epoch   11.93), loss: 0.012951, time: 2.214 s\n",
      "Step  38200 (epoch   11.94), loss: 0.010633, time: 2.231 s, error: 0.020349\n",
      "Step  38220 (epoch   11.94), loss: 0.015859, time: 14.429 s\n",
      "Step  38240 (epoch   11.95), loss: 0.009694, time: 2.238 s\n",
      "Step  38260 (epoch   11.96), loss: 0.010971, time: 2.231 s\n",
      "Step  38280 (epoch   11.96), loss: 0.010045, time: 2.221 s\n",
      "Step  38300 (epoch   11.97), loss: 0.010837, time: 2.224 s\n",
      "Step  38320 (epoch   11.97), loss: 0.012293, time: 2.215 s\n",
      "Step  38340 (epoch   11.98), loss: 0.018409, time: 2.215 s\n",
      "Step  38360 (epoch   11.99), loss: 0.009921, time: 2.206 s\n",
      "Step  38380 (epoch   11.99), loss: 0.011438, time: 2.224 s\n",
      "Step  38400 (epoch   12.00), loss: 0.015295, time: 2.227 s, error: 0.021445\n",
      "Step  38420 (epoch   12.01), loss: 0.009455, time: 14.371 s\n",
      "Step  38440 (epoch   12.01), loss: 0.013815, time: 2.221 s\n",
      "Step  38460 (epoch   12.02), loss: 0.014648, time: 2.231 s\n",
      "Step  38480 (epoch   12.03), loss: 0.012564, time: 2.209 s\n",
      "Step  38500 (epoch   12.03), loss: 0.012110, time: 2.241 s\n",
      "Step  38520 (epoch   12.04), loss: 0.010000, time: 2.223 s\n",
      "Step  38540 (epoch   12.04), loss: 0.009910, time: 2.225 s\n",
      "Step  38560 (epoch   12.05), loss: 0.008657, time: 2.204 s\n",
      "Step  38580 (epoch   12.06), loss: 0.018275, time: 2.227 s\n",
      "Step  38600 (epoch   12.06), loss: 0.014048, time: 2.205 s, error: 0.018951\n",
      "Step  38620 (epoch   12.07), loss: 0.013796, time: 14.376 s\n",
      "Step  38640 (epoch   12.07), loss: 0.015085, time: 2.217 s\n",
      "Step  38660 (epoch   12.08), loss: 0.011116, time: 2.218 s\n",
      "Step  38680 (epoch   12.09), loss: 0.009003, time: 2.224 s\n",
      "Step  38700 (epoch   12.09), loss: 0.012219, time: 2.220 s\n",
      "Step  38720 (epoch   12.10), loss: 0.010148, time: 2.226 s\n",
      "Step  38740 (epoch   12.11), loss: 0.011683, time: 2.222 s\n",
      "Step  38760 (epoch   12.11), loss: 0.010166, time: 2.226 s\n",
      "Step  38780 (epoch   12.12), loss: 0.030681, time: 2.242 s\n",
      "Step  38800 (epoch   12.12), loss: 0.014370, time: 2.229 s, error: 0.018759\n",
      "Step  38820 (epoch   12.13), loss: 0.010155, time: 14.409 s\n",
      "Step  38840 (epoch   12.14), loss: 0.014086, time: 2.229 s\n",
      "Step  38860 (epoch   12.14), loss: 0.010450, time: 2.236 s\n",
      "Step  38880 (epoch   12.15), loss: 0.013244, time: 2.230 s\n",
      "Step  38900 (epoch   12.16), loss: 0.012158, time: 2.227 s\n",
      "Step  38920 (epoch   12.16), loss: 0.013405, time: 2.212 s\n",
      "Step  38940 (epoch   12.17), loss: 0.012527, time: 2.228 s\n",
      "Step  38960 (epoch   12.18), loss: 0.010058, time: 2.228 s\n",
      "Step  38980 (epoch   12.18), loss: 0.010606, time: 2.221 s\n",
      "Step  39000 (epoch   12.19), loss: 0.012502, time: 2.228 s, error: 0.019139\n",
      "Step  39020 (epoch   12.19), loss: 0.010324, time: 14.641 s\n",
      "Step  39040 (epoch   12.20), loss: 0.010449, time: 2.214 s\n",
      "Step  39060 (epoch   12.21), loss: 0.012173, time: 2.223 s\n",
      "Step  39080 (epoch   12.21), loss: 0.010198, time: 2.213 s\n",
      "Step  39100 (epoch   12.22), loss: 0.017800, time: 2.221 s\n",
      "Step  39120 (epoch   12.22), loss: 0.008992, time: 2.223 s\n",
      "Step  39140 (epoch   12.23), loss: 0.010477, time: 2.222 s\n",
      "Step  39160 (epoch   12.24), loss: 0.016442, time: 2.218 s\n",
      "Step  39180 (epoch   12.24), loss: 0.013461, time: 2.217 s\n",
      "Step  39200 (epoch   12.25), loss: 0.009247, time: 2.210 s, error: 0.020353\n",
      "Step  39220 (epoch   12.26), loss: 0.010711, time: 14.613 s\n",
      "Step  39240 (epoch   12.26), loss: 0.015232, time: 2.215 s\n",
      "Step  39260 (epoch   12.27), loss: 0.009766, time: 2.221 s\n",
      "Step  39280 (epoch   12.28), loss: 0.014117, time: 2.234 s\n",
      "Step  39300 (epoch   12.28), loss: 0.012794, time: 2.214 s\n",
      "Step  39320 (epoch   12.29), loss: 0.011768, time: 2.211 s\n",
      "Step  39340 (epoch   12.29), loss: 0.015890, time: 2.219 s\n",
      "Step  39360 (epoch   12.30), loss: 0.010777, time: 2.233 s\n",
      "Step  39380 (epoch   12.31), loss: 0.013571, time: 2.234 s\n",
      "Step  39400 (epoch   12.31), loss: 0.015892, time: 2.228 s, error: 0.018108\n",
      "Step  39420 (epoch   12.32), loss: 0.011434, time: 14.426 s\n",
      "Step  39440 (epoch   12.32), loss: 0.010354, time: 2.225 s\n",
      "Step  39460 (epoch   12.33), loss: 0.014555, time: 2.236 s\n",
      "Step  39480 (epoch   12.34), loss: 0.011625, time: 2.222 s\n",
      "Step  39500 (epoch   12.34), loss: 0.015503, time: 2.228 s\n",
      "Step  39520 (epoch   12.35), loss: 0.011061, time: 2.220 s\n",
      "Step  39540 (epoch   12.36), loss: 0.012336, time: 2.219 s\n",
      "Step  39560 (epoch   12.36), loss: 0.010053, time: 2.215 s\n",
      "Step  39580 (epoch   12.37), loss: 0.009094, time: 2.227 s\n",
      "Step  39600 (epoch   12.38), loss: 0.009783, time: 2.215 s, error: 0.018133\n",
      "Step  39620 (epoch   12.38), loss: 0.015190, time: 14.413 s\n",
      "Step  39640 (epoch   12.39), loss: 0.019057, time: 2.238 s\n",
      "Step  39660 (epoch   12.39), loss: 0.013477, time: 2.237 s\n",
      "Step  39680 (epoch   12.40), loss: 0.013487, time: 2.230 s\n",
      "Step  39700 (epoch   12.41), loss: 0.010130, time: 2.229 s\n",
      "Step  39720 (epoch   12.41), loss: 0.014517, time: 2.220 s\n",
      "Step  39740 (epoch   12.42), loss: 0.010705, time: 2.227 s\n",
      "Step  39760 (epoch   12.43), loss: 0.008527, time: 2.230 s\n",
      "Step  39780 (epoch   12.43), loss: 0.008559, time: 2.215 s\n",
      "Step  39800 (epoch   12.44), loss: 0.015120, time: 2.228 s, error: 0.017564\n",
      "Step  39820 (epoch   12.44), loss: 0.013391, time: 14.382 s\n",
      "Step  39840 (epoch   12.45), loss: 0.018987, time: 2.227 s\n",
      "Step  39860 (epoch   12.46), loss: 0.013761, time: 2.230 s\n",
      "Step  39880 (epoch   12.46), loss: 0.012439, time: 2.228 s\n",
      "Step  39900 (epoch   12.47), loss: 0.006897, time: 2.219 s\n",
      "Step  39920 (epoch   12.47), loss: 0.011750, time: 2.218 s\n",
      "Step  39940 (epoch   12.48), loss: 0.034189, time: 2.230 s\n",
      "Step  39960 (epoch   12.49), loss: 0.008321, time: 2.222 s\n",
      "Step  39980 (epoch   12.49), loss: 0.011954, time: 2.225 s\n",
      "Step  40000 (epoch   12.50), loss: 0.014461, time: 2.235 s, error: 0.018933\n",
      "\n",
      "Time since beginning  : 6865.232 s\n",
      "\n",
      "Step  40020 (epoch   12.51), loss: 0.012128, time: 14.528 s\n",
      "Step  40040 (epoch   12.51), loss: 0.013602, time: 2.223 s\n",
      "Step  40060 (epoch   12.52), loss: 0.014594, time: 2.239 s\n",
      "Step  40080 (epoch   12.53), loss: 0.011255, time: 2.235 s\n",
      "Step  40100 (epoch   12.53), loss: 0.010771, time: 2.221 s\n",
      "Step  40120 (epoch   12.54), loss: 0.016445, time: 2.223 s\n",
      "Step  40140 (epoch   12.54), loss: 0.009803, time: 2.226 s\n",
      "Step  40160 (epoch   12.55), loss: 0.013802, time: 2.227 s\n",
      "Step  40180 (epoch   12.56), loss: 0.010694, time: 2.220 s\n",
      "Step  40200 (epoch   12.56), loss: 0.006538, time: 2.226 s, error: 0.018148\n",
      "Step  40220 (epoch   12.57), loss: 0.010820, time: 14.574 s\n",
      "Step  40240 (epoch   12.57), loss: 0.010610, time: 2.226 s\n",
      "Step  40260 (epoch   12.58), loss: 0.011266, time: 2.223 s\n",
      "Step  40280 (epoch   12.59), loss: 0.010172, time: 2.224 s\n",
      "Step  40300 (epoch   12.59), loss: 0.018306, time: 2.230 s\n",
      "Step  40320 (epoch   12.60), loss: 0.024127, time: 2.222 s\n",
      "Step  40340 (epoch   12.61), loss: 0.022046, time: 2.223 s\n",
      "Step  40360 (epoch   12.61), loss: 0.013651, time: 2.225 s\n",
      "Step  40380 (epoch   12.62), loss: 0.015416, time: 2.220 s\n",
      "Step  40400 (epoch   12.62), loss: 0.020545, time: 2.232 s, error: 0.018034\n",
      "Step  40420 (epoch   12.63), loss: 0.014804, time: 14.481 s\n",
      "Step  40440 (epoch   12.64), loss: 0.014203, time: 2.230 s\n",
      "Step  40460 (epoch   12.64), loss: 0.015882, time: 2.224 s\n",
      "Step  40480 (epoch   12.65), loss: 0.010028, time: 2.227 s\n",
      "Step  40500 (epoch   12.66), loss: 0.009535, time: 2.228 s\n",
      "Step  40520 (epoch   12.66), loss: 0.011398, time: 2.228 s\n",
      "Step  40540 (epoch   12.67), loss: 0.010327, time: 2.231 s\n",
      "Step  40560 (epoch   12.68), loss: 0.016985, time: 2.221 s\n",
      "Step  40580 (epoch   12.68), loss: 0.013109, time: 2.217 s\n",
      "Step  40600 (epoch   12.69), loss: 0.010768, time: 2.227 s, error: 0.019895\n",
      "Step  40620 (epoch   12.69), loss: 0.013990, time: 14.393 s\n",
      "Step  40640 (epoch   12.70), loss: 0.011131, time: 2.223 s\n",
      "Step  40660 (epoch   12.71), loss: 0.007938, time: 2.213 s\n",
      "Step  40680 (epoch   12.71), loss: 0.008595, time: 2.234 s\n",
      "Step  40700 (epoch   12.72), loss: 0.016599, time: 2.237 s\n",
      "Step  40720 (epoch   12.72), loss: 0.021124, time: 2.235 s\n",
      "Step  40740 (epoch   12.73), loss: 0.010969, time: 2.206 s\n",
      "Step  40760 (epoch   12.74), loss: 0.019422, time: 2.224 s\n",
      "Step  40780 (epoch   12.74), loss: 0.010161, time: 2.222 s\n",
      "Step  40800 (epoch   12.75), loss: 0.013648, time: 2.223 s, error: 0.017510\n",
      "Step  40820 (epoch   12.76), loss: 0.014901, time: 14.396 s\n",
      "Step  40840 (epoch   12.76), loss: 0.012258, time: 2.219 s\n",
      "Step  40860 (epoch   12.77), loss: 0.009585, time: 2.223 s\n",
      "Step  40880 (epoch   12.78), loss: 0.009492, time: 2.222 s\n",
      "Step  40900 (epoch   12.78), loss: 0.009909, time: 2.220 s\n",
      "Step  40920 (epoch   12.79), loss: 0.010464, time: 2.237 s\n",
      "Step  40940 (epoch   12.79), loss: 0.009296, time: 2.227 s\n",
      "Step  40960 (epoch   12.80), loss: 0.013237, time: 2.233 s\n",
      "Step  40980 (epoch   12.81), loss: 0.007999, time: 2.241 s\n",
      "Step  41000 (epoch   12.81), loss: 0.010377, time: 2.219 s, error: 0.018970\n",
      "Step  41020 (epoch   12.82), loss: 0.010205, time: 14.438 s\n",
      "Step  41040 (epoch   12.82), loss: 0.011471, time: 2.233 s\n",
      "Step  41060 (epoch   12.83), loss: 0.009922, time: 2.228 s\n",
      "Step  41080 (epoch   12.84), loss: 0.013206, time: 2.223 s\n",
      "Step  41100 (epoch   12.84), loss: 0.009144, time: 2.214 s\n",
      "Step  41120 (epoch   12.85), loss: 0.012056, time: 2.223 s\n",
      "Step  41140 (epoch   12.86), loss: 0.010252, time: 2.226 s\n",
      "Step  41160 (epoch   12.86), loss: 0.015339, time: 2.224 s\n",
      "Step  41180 (epoch   12.87), loss: 0.015575, time: 2.235 s\n",
      "Step  41200 (epoch   12.88), loss: 0.014667, time: 2.225 s, error: 0.017218\n",
      "Step  41220 (epoch   12.88), loss: 0.013200, time: 14.597 s\n",
      "Step  41240 (epoch   12.89), loss: 0.013602, time: 2.230 s\n",
      "Step  41260 (epoch   12.89), loss: 0.008354, time: 2.219 s\n",
      "Step  41280 (epoch   12.90), loss: 0.019939, time: 2.227 s\n",
      "Step  41300 (epoch   12.91), loss: 0.018408, time: 2.213 s\n",
      "Step  41320 (epoch   12.91), loss: 0.008921, time: 2.229 s\n",
      "Step  41340 (epoch   12.92), loss: 0.011395, time: 2.231 s\n",
      "Step  41360 (epoch   12.93), loss: 0.007563, time: 2.216 s\n",
      "Step  41380 (epoch   12.93), loss: 0.011104, time: 2.224 s\n",
      "Step  41400 (epoch   12.94), loss: 0.009260, time: 2.213 s, error: 0.019432\n",
      "Step  41420 (epoch   12.94), loss: 0.011743, time: 14.569 s\n",
      "Step  41440 (epoch   12.95), loss: 0.009412, time: 2.222 s\n",
      "Step  41460 (epoch   12.96), loss: 0.013067, time: 2.227 s\n",
      "Step  41480 (epoch   12.96), loss: 0.015985, time: 2.222 s\n",
      "Step  41500 (epoch   12.97), loss: 0.011639, time: 2.226 s\n",
      "Step  41520 (epoch   12.97), loss: 0.012096, time: 2.217 s\n",
      "Step  41540 (epoch   12.98), loss: 0.009317, time: 2.214 s\n",
      "Step  41560 (epoch   12.99), loss: 0.009374, time: 2.227 s\n",
      "Step  41580 (epoch   12.99), loss: 0.011549, time: 2.208 s\n",
      "Step  41600 (epoch   13.00), loss: 0.011589, time: 2.213 s, error: 0.020611\n",
      "Step  41620 (epoch   13.01), loss: 0.013069, time: 14.441 s\n",
      "Step  41640 (epoch   13.01), loss: 0.007267, time: 2.217 s\n",
      "Step  41660 (epoch   13.02), loss: 0.010387, time: 2.233 s\n",
      "Step  41680 (epoch   13.03), loss: 0.009877, time: 2.228 s\n",
      "Step  41700 (epoch   13.03), loss: 0.011296, time: 2.223 s\n",
      "Step  41720 (epoch   13.04), loss: 0.017672, time: 2.230 s\n",
      "Step  41740 (epoch   13.04), loss: 0.007326, time: 2.224 s\n",
      "Step  41760 (epoch   13.05), loss: 0.022141, time: 2.226 s\n",
      "Step  41780 (epoch   13.06), loss: 0.011958, time: 2.230 s\n",
      "Step  41800 (epoch   13.06), loss: 0.013324, time: 2.212 s, error: 0.017984\n",
      "Step  41820 (epoch   13.07), loss: 0.011425, time: 14.470 s\n",
      "Step  41840 (epoch   13.07), loss: 0.007207, time: 2.227 s\n",
      "Step  41860 (epoch   13.08), loss: 0.010090, time: 2.220 s\n",
      "Step  41880 (epoch   13.09), loss: 0.009932, time: 2.219 s\n",
      "Step  41900 (epoch   13.09), loss: 0.010223, time: 2.222 s\n",
      "Step  41920 (epoch   13.10), loss: 0.013967, time: 2.227 s\n",
      "Step  41940 (epoch   13.11), loss: 0.014161, time: 2.236 s\n",
      "Step  41960 (epoch   13.11), loss: 0.009112, time: 2.230 s\n",
      "Step  41980 (epoch   13.12), loss: 0.012711, time: 2.235 s\n",
      "Step  42000 (epoch   13.12), loss: 0.014227, time: 2.226 s, error: 0.017770\n",
      "\n",
      "Time since beginning  : 7210.286 s\n",
      "\n",
      "Step  42020 (epoch   13.13), loss: 0.010956, time: 14.454 s\n",
      "Step  42040 (epoch   13.14), loss: 0.016694, time: 2.241 s\n",
      "Step  42060 (epoch   13.14), loss: 0.010613, time: 2.222 s\n",
      "Step  42080 (epoch   13.15), loss: 0.011380, time: 2.209 s\n",
      "Step  42100 (epoch   13.16), loss: 0.020279, time: 2.231 s\n",
      "Step  42120 (epoch   13.16), loss: 0.009804, time: 2.222 s\n",
      "Step  42140 (epoch   13.17), loss: 0.018157, time: 2.229 s\n",
      "Step  42160 (epoch   13.18), loss: 0.012585, time: 2.210 s\n",
      "Step  42180 (epoch   13.18), loss: 0.011157, time: 2.213 s\n",
      "Step  42200 (epoch   13.19), loss: 0.016202, time: 2.228 s, error: 0.017766\n",
      "Step  42220 (epoch   13.19), loss: 0.009835, time: 14.481 s\n",
      "Step  42240 (epoch   13.20), loss: 0.013244, time: 2.226 s\n",
      "Step  42260 (epoch   13.21), loss: 0.018884, time: 2.219 s\n",
      "Step  42280 (epoch   13.21), loss: 0.011639, time: 2.231 s\n",
      "Step  42300 (epoch   13.22), loss: 0.012521, time: 2.230 s\n",
      "Step  42320 (epoch   13.22), loss: 0.009065, time: 2.229 s\n",
      "Step  42340 (epoch   13.23), loss: 0.028284, time: 2.222 s\n",
      "Step  42360 (epoch   13.24), loss: 0.011181, time: 2.233 s\n",
      "Step  42380 (epoch   13.24), loss: 0.011502, time: 2.217 s\n",
      "Step  42400 (epoch   13.25), loss: 0.017430, time: 2.224 s, error: 0.018966\n",
      "Step  42420 (epoch   13.26), loss: 0.020250, time: 14.565 s\n",
      "Step  42440 (epoch   13.26), loss: 0.011611, time: 2.216 s\n",
      "Step  42460 (epoch   13.27), loss: 0.010580, time: 2.219 s\n",
      "Step  42480 (epoch   13.28), loss: 0.014135, time: 2.220 s\n",
      "Step  42500 (epoch   13.28), loss: 0.040007, time: 2.220 s\n",
      "Step  42520 (epoch   13.29), loss: 0.012384, time: 2.217 s\n",
      "Step  42540 (epoch   13.29), loss: 0.016477, time: 2.219 s\n",
      "Step  42560 (epoch   13.30), loss: 0.008331, time: 2.215 s\n",
      "Step  42580 (epoch   13.31), loss: 0.016519, time: 2.221 s\n",
      "Step  42600 (epoch   13.31), loss: 0.011399, time: 2.225 s, error: 0.017942\n",
      "Step  42620 (epoch   13.32), loss: 0.017664, time: 14.502 s\n",
      "Step  42640 (epoch   13.32), loss: 0.011930, time: 2.235 s\n",
      "Step  42660 (epoch   13.33), loss: 0.017623, time: 2.234 s\n",
      "Step  42680 (epoch   13.34), loss: 0.010304, time: 2.223 s\n",
      "Step  42700 (epoch   13.34), loss: 0.011434, time: 2.219 s\n",
      "Step  42720 (epoch   13.35), loss: 0.009179, time: 2.216 s\n",
      "Step  42740 (epoch   13.36), loss: 0.010199, time: 2.217 s\n",
      "Step  42760 (epoch   13.36), loss: 0.007454, time: 2.221 s\n",
      "Step  42780 (epoch   13.37), loss: 0.013291, time: 2.217 s\n",
      "Step  42800 (epoch   13.38), loss: 0.008453, time: 2.222 s, error: 0.017411\n",
      "Step  42820 (epoch   13.38), loss: 0.009855, time: 14.391 s\n",
      "Step  42840 (epoch   13.39), loss: 0.018009, time: 2.224 s\n",
      "Step  42860 (epoch   13.39), loss: 0.011080, time: 2.220 s\n",
      "Step  42880 (epoch   13.40), loss: 0.012395, time: 2.230 s\n",
      "Step  42900 (epoch   13.41), loss: 0.011872, time: 2.236 s\n",
      "Step  42920 (epoch   13.41), loss: 0.011896, time: 2.227 s\n",
      "Step  42940 (epoch   13.42), loss: 0.011089, time: 2.222 s\n",
      "Step  42960 (epoch   13.43), loss: 0.010282, time: 2.222 s\n",
      "Step  42980 (epoch   13.43), loss: 0.012506, time: 2.221 s\n",
      "Step  43000 (epoch   13.44), loss: 0.014871, time: 2.219 s, error: 0.017094\n",
      "Step  43020 (epoch   13.44), loss: 0.013722, time: 14.429 s\n",
      "Step  43040 (epoch   13.45), loss: 0.014323, time: 2.213 s\n",
      "Step  43060 (epoch   13.46), loss: 0.015468, time: 2.222 s\n",
      "Step  43080 (epoch   13.46), loss: 0.021187, time: 2.227 s\n",
      "Step  43100 (epoch   13.47), loss: 0.011496, time: 2.227 s\n",
      "Step  43120 (epoch   13.47), loss: 0.014284, time: 2.222 s\n",
      "Step  43140 (epoch   13.48), loss: 0.020854, time: 2.215 s\n",
      "Step  43160 (epoch   13.49), loss: 0.008511, time: 2.235 s\n",
      "Step  43180 (epoch   13.49), loss: 0.015120, time: 2.231 s\n",
      "Step  43200 (epoch   13.50), loss: 0.011858, time: 2.218 s, error: 0.019302\n",
      "Step  43220 (epoch   13.51), loss: 0.012515, time: 14.355 s\n",
      "Step  43240 (epoch   13.51), loss: 0.013836, time: 2.218 s\n",
      "Step  43260 (epoch   13.52), loss: 0.012622, time: 2.231 s\n",
      "Step  43280 (epoch   13.53), loss: 0.010481, time: 2.225 s\n",
      "Step  43300 (epoch   13.53), loss: 0.013014, time: 2.222 s\n",
      "Step  43320 (epoch   13.54), loss: 0.010610, time: 2.226 s\n",
      "Step  43340 (epoch   13.54), loss: 0.010835, time: 2.223 s\n",
      "Step  43360 (epoch   13.55), loss: 0.008852, time: 2.227 s\n",
      "Step  43380 (epoch   13.56), loss: 0.010365, time: 2.228 s\n",
      "Step  43400 (epoch   13.56), loss: 0.013030, time: 2.220 s, error: 0.018178\n",
      "Step  43420 (epoch   13.57), loss: 0.015580, time: 14.540 s\n",
      "Step  43440 (epoch   13.57), loss: 0.020854, time: 2.231 s\n",
      "Step  43460 (epoch   13.58), loss: 0.009465, time: 2.228 s\n",
      "Step  43480 (epoch   13.59), loss: 0.008946, time: 2.234 s\n",
      "Step  43500 (epoch   13.59), loss: 0.015110, time: 2.215 s\n",
      "Step  43520 (epoch   13.60), loss: 0.010820, time: 2.211 s\n",
      "Step  43540 (epoch   13.61), loss: 0.011379, time: 2.216 s\n",
      "Step  43560 (epoch   13.61), loss: 0.008830, time: 2.205 s\n",
      "Step  43580 (epoch   13.62), loss: 0.013074, time: 2.222 s\n",
      "Step  43600 (epoch   13.62), loss: 0.009239, time: 2.229 s, error: 0.017569\n",
      "Step  43620 (epoch   13.63), loss: 0.012442, time: 14.585 s\n",
      "Step  43640 (epoch   13.64), loss: 0.015395, time: 2.221 s\n",
      "Step  43660 (epoch   13.64), loss: 0.010823, time: 2.214 s\n",
      "Step  43680 (epoch   13.65), loss: 0.009568, time: 2.224 s\n",
      "Step  43700 (epoch   13.66), loss: 0.013928, time: 2.232 s\n",
      "Step  43720 (epoch   13.66), loss: 0.013912, time: 2.230 s\n",
      "Step  43740 (epoch   13.67), loss: 0.020671, time: 2.221 s\n",
      "Step  43760 (epoch   13.68), loss: 0.014814, time: 2.235 s\n",
      "Step  43780 (epoch   13.68), loss: 0.010464, time: 2.212 s\n",
      "Step  43800 (epoch   13.69), loss: 0.012209, time: 2.215 s, error: 0.019369\n",
      "Step  43820 (epoch   13.69), loss: 0.009988, time: 14.387 s\n",
      "Step  43840 (epoch   13.70), loss: 0.006873, time: 2.218 s\n",
      "Step  43860 (epoch   13.71), loss: 0.012753, time: 2.239 s\n",
      "Step  43880 (epoch   13.71), loss: 0.014573, time: 2.229 s\n",
      "Step  43900 (epoch   13.72), loss: 0.011126, time: 2.229 s\n",
      "Step  43920 (epoch   13.72), loss: 0.014212, time: 2.211 s\n",
      "Step  43940 (epoch   13.73), loss: 0.024444, time: 2.217 s\n",
      "Step  43960 (epoch   13.74), loss: 0.016270, time: 2.207 s\n",
      "Step  43980 (epoch   13.74), loss: 0.010524, time: 2.215 s\n",
      "Step  44000 (epoch   13.75), loss: 0.008641, time: 2.217 s, error: 0.016876\n",
      "\n",
      "Time since beginning  : 7554.993 s\n",
      "\n",
      "Step  44020 (epoch   13.76), loss: 0.011615, time: 14.443 s\n",
      "Step  44040 (epoch   13.76), loss: 0.012155, time: 2.231 s\n",
      "Step  44060 (epoch   13.77), loss: 0.009554, time: 2.232 s\n",
      "Step  44080 (epoch   13.78), loss: 0.015905, time: 2.232 s\n",
      "Step  44100 (epoch   13.78), loss: 0.009358, time: 2.223 s\n",
      "Step  44120 (epoch   13.79), loss: 0.008799, time: 2.232 s\n",
      "Step  44140 (epoch   13.79), loss: 0.013086, time: 2.233 s\n",
      "Step  44160 (epoch   13.80), loss: 0.013764, time: 2.218 s\n",
      "Step  44180 (epoch   13.81), loss: 0.008925, time: 2.220 s\n",
      "Step  44200 (epoch   13.81), loss: 0.014073, time: 2.234 s, error: 0.019684\n",
      "Step  44220 (epoch   13.82), loss: 0.007710, time: 14.381 s\n",
      "Step  44240 (epoch   13.82), loss: 0.010647, time: 2.222 s\n",
      "Step  44260 (epoch   13.83), loss: 0.018095, time: 2.215 s\n",
      "Step  44280 (epoch   13.84), loss: 0.011614, time: 2.228 s\n",
      "Step  44300 (epoch   13.84), loss: 0.016392, time: 2.227 s\n",
      "Step  44320 (epoch   13.85), loss: 0.010262, time: 2.213 s\n",
      "Step  44340 (epoch   13.86), loss: 0.009394, time: 2.209 s\n",
      "Step  44360 (epoch   13.86), loss: 0.011302, time: 2.213 s\n",
      "Step  44380 (epoch   13.87), loss: 0.013081, time: 2.218 s\n",
      "Step  44400 (epoch   13.88), loss: 0.009711, time: 2.229 s, error: 0.016984\n",
      "Step  44420 (epoch   13.88), loss: 0.013199, time: 14.474 s\n",
      "Step  44440 (epoch   13.89), loss: 0.009366, time: 2.213 s\n",
      "Step  44460 (epoch   13.89), loss: 0.010494, time: 2.220 s\n",
      "Step  44480 (epoch   13.90), loss: 0.009878, time: 2.225 s\n",
      "Step  44500 (epoch   13.91), loss: 0.015196, time: 2.219 s\n",
      "Step  44520 (epoch   13.91), loss: 0.018610, time: 2.213 s\n",
      "Step  44540 (epoch   13.92), loss: 0.011270, time: 2.228 s\n",
      "Step  44560 (epoch   13.93), loss: 0.007274, time: 2.220 s\n",
      "Step  44580 (epoch   13.93), loss: 0.010447, time: 2.227 s\n",
      "Step  44600 (epoch   13.94), loss: 0.012602, time: 2.213 s, error: 0.018118\n",
      "Step  44620 (epoch   13.94), loss: 0.009674, time: 14.548 s\n",
      "Step  44640 (epoch   13.95), loss: 0.011000, time: 2.217 s\n",
      "Step  44660 (epoch   13.96), loss: 0.012365, time: 2.214 s\n",
      "Step  44680 (epoch   13.96), loss: 0.024423, time: 2.224 s\n",
      "Step  44700 (epoch   13.97), loss: 0.016687, time: 2.214 s\n",
      "Step  44720 (epoch   13.97), loss: 0.011132, time: 2.224 s\n",
      "Step  44740 (epoch   13.98), loss: 0.018676, time: 2.222 s\n",
      "Step  44760 (epoch   13.99), loss: 0.012790, time: 2.227 s\n",
      "Step  44780 (epoch   13.99), loss: 0.011825, time: 2.232 s\n",
      "Step  44800 (epoch   14.00), loss: 0.013719, time: 2.226 s, error: 0.018722\n",
      "Step  44820 (epoch   14.01), loss: 0.009412, time: 14.433 s\n",
      "Step  44840 (epoch   14.01), loss: 0.007475, time: 2.238 s\n",
      "Step  44860 (epoch   14.02), loss: 0.009438, time: 2.236 s\n",
      "Step  44880 (epoch   14.03), loss: 0.014434, time: 2.220 s\n",
      "Step  44900 (epoch   14.03), loss: 0.012361, time: 2.224 s\n",
      "Step  44920 (epoch   14.04), loss: 0.014110, time: 2.214 s\n",
      "Step  44940 (epoch   14.04), loss: 0.010164, time: 2.223 s\n",
      "Step  44960 (epoch   14.05), loss: 0.012000, time: 2.226 s\n",
      "Step  44980 (epoch   14.06), loss: 0.007886, time: 2.233 s\n",
      "Step  45000 (epoch   14.06), loss: 0.008847, time: 2.219 s, error: 0.017657\n",
      "Step  45020 (epoch   14.07), loss: 0.008056, time: 14.401 s\n",
      "Step  45040 (epoch   14.07), loss: 0.009850, time: 2.216 s\n",
      "Step  45060 (epoch   14.08), loss: 0.012978, time: 2.216 s\n",
      "Step  45080 (epoch   14.09), loss: 0.010378, time: 2.234 s\n",
      "Step  45100 (epoch   14.09), loss: 0.007382, time: 2.239 s\n",
      "Step  45120 (epoch   14.10), loss: 0.010351, time: 2.224 s\n",
      "Step  45140 (epoch   14.11), loss: 0.012651, time: 2.218 s\n",
      "Step  45160 (epoch   14.11), loss: 0.010265, time: 2.230 s\n",
      "Step  45180 (epoch   14.12), loss: 0.011998, time: 2.216 s\n",
      "Step  45200 (epoch   14.12), loss: 0.022018, time: 2.222 s, error: 0.017359\n",
      "Step  45220 (epoch   14.13), loss: 0.010169, time: 14.409 s\n",
      "Step  45240 (epoch   14.14), loss: 0.024328, time: 2.222 s\n",
      "Step  45260 (epoch   14.14), loss: 0.011593, time: 2.220 s\n",
      "Step  45280 (epoch   14.15), loss: 0.019552, time: 2.222 s\n",
      "Step  45300 (epoch   14.16), loss: 0.009992, time: 2.212 s\n",
      "Step  45320 (epoch   14.16), loss: 0.011687, time: 2.217 s\n",
      "Step  45340 (epoch   14.17), loss: 0.010758, time: 2.213 s\n",
      "Step  45360 (epoch   14.18), loss: 0.009731, time: 2.236 s\n",
      "Step  45380 (epoch   14.18), loss: 0.007391, time: 2.225 s\n",
      "Step  45400 (epoch   14.19), loss: 0.011866, time: 2.223 s, error: 0.016925\n",
      "Step  45420 (epoch   14.19), loss: 0.009323, time: 14.365 s\n",
      "Step  45440 (epoch   14.20), loss: 0.012434, time: 2.221 s\n",
      "Step  45460 (epoch   14.21), loss: 0.007592, time: 2.215 s\n",
      "Step  45480 (epoch   14.21), loss: 0.009543, time: 2.214 s\n",
      "Step  45500 (epoch   14.22), loss: 0.008359, time: 2.211 s\n",
      "Step  45520 (epoch   14.22), loss: 0.010925, time: 2.225 s\n",
      "Step  45540 (epoch   14.23), loss: 0.012737, time: 2.216 s\n",
      "Step  45560 (epoch   14.24), loss: 0.010449, time: 2.230 s\n",
      "Step  45580 (epoch   14.24), loss: 0.012247, time: 2.226 s\n",
      "Step  45600 (epoch   14.25), loss: 0.007743, time: 2.235 s, error: 0.016755\n",
      "Step  45620 (epoch   14.26), loss: 0.007692, time: 14.543 s\n",
      "Step  45640 (epoch   14.26), loss: 0.009215, time: 2.207 s\n",
      "Step  45660 (epoch   14.27), loss: 0.007587, time: 2.226 s\n",
      "Step  45680 (epoch   14.28), loss: 0.008127, time: 2.225 s\n",
      "Step  45700 (epoch   14.28), loss: 0.008517, time: 2.226 s\n",
      "Step  45720 (epoch   14.29), loss: 0.014263, time: 2.209 s\n",
      "Step  45740 (epoch   14.29), loss: 0.009054, time: 2.211 s\n",
      "Step  45760 (epoch   14.30), loss: 0.008850, time: 2.215 s\n",
      "Step  45780 (epoch   14.31), loss: 0.010783, time: 2.220 s\n",
      "Step  45800 (epoch   14.31), loss: 0.014565, time: 2.225 s, error: 0.017779\n",
      "Step  45820 (epoch   14.32), loss: 0.006354, time: 14.593 s\n",
      "Step  45840 (epoch   14.32), loss: 0.010102, time: 2.215 s\n",
      "Step  45860 (epoch   14.33), loss: 0.007195, time: 2.226 s\n",
      "Step  45880 (epoch   14.34), loss: 0.008453, time: 2.210 s\n",
      "Step  45900 (epoch   14.34), loss: 0.013144, time: 2.224 s\n",
      "Step  45920 (epoch   14.35), loss: 0.010770, time: 2.221 s\n",
      "Step  45940 (epoch   14.36), loss: 0.015249, time: 2.222 s\n",
      "Step  45960 (epoch   14.36), loss: 0.017765, time: 2.218 s\n",
      "Step  45980 (epoch   14.37), loss: 0.008551, time: 2.222 s\n",
      "Step  46000 (epoch   14.38), loss: 0.007893, time: 2.205 s, error: 0.017256\n",
      "\n",
      "Time since beginning  : 7899.557 s\n",
      "\n",
      "Step  46020 (epoch   14.38), loss: 0.011767, time: 14.442 s\n",
      "Step  46040 (epoch   14.39), loss: 0.013757, time: 2.218 s\n",
      "Step  46060 (epoch   14.39), loss: 0.014627, time: 2.237 s\n",
      "Step  46080 (epoch   14.40), loss: 0.007218, time: 2.222 s\n",
      "Step  46100 (epoch   14.41), loss: 0.010397, time: 2.222 s\n",
      "Step  46120 (epoch   14.41), loss: 0.013128, time: 2.212 s\n",
      "Step  46140 (epoch   14.42), loss: 0.013927, time: 2.223 s\n",
      "Step  46160 (epoch   14.43), loss: 0.009520, time: 2.224 s\n",
      "Step  46180 (epoch   14.43), loss: 0.008598, time: 2.233 s\n",
      "Step  46200 (epoch   14.44), loss: 0.009980, time: 2.210 s, error: 0.016645\n",
      "Step  46220 (epoch   14.44), loss: 0.011010, time: 14.359 s\n",
      "Step  46240 (epoch   14.45), loss: 0.008439, time: 2.221 s\n",
      "Step  46260 (epoch   14.46), loss: 0.009961, time: 2.213 s\n",
      "Step  46280 (epoch   14.46), loss: 0.009793, time: 2.232 s\n",
      "Step  46300 (epoch   14.47), loss: 0.010284, time: 2.220 s\n",
      "Step  46320 (epoch   14.47), loss: 0.009979, time: 2.222 s\n",
      "Step  46340 (epoch   14.48), loss: 0.008890, time: 2.222 s\n",
      "Step  46360 (epoch   14.49), loss: 0.009189, time: 2.220 s\n",
      "Step  46380 (epoch   14.49), loss: 0.010459, time: 2.218 s\n",
      "Step  46400 (epoch   14.50), loss: 0.010923, time: 2.220 s, error: 0.018554\n",
      "Step  46420 (epoch   14.51), loss: 0.012306, time: 14.349 s\n",
      "Step  46440 (epoch   14.51), loss: 0.012453, time: 2.216 s\n",
      "Step  46460 (epoch   14.52), loss: 0.012749, time: 2.224 s\n",
      "Step  46480 (epoch   14.53), loss: 0.018597, time: 2.234 s\n",
      "Step  46500 (epoch   14.53), loss: 0.014248, time: 2.226 s\n",
      "Step  46520 (epoch   14.54), loss: 0.008749, time: 2.222 s\n",
      "Step  46540 (epoch   14.54), loss: 0.010625, time: 2.225 s\n",
      "Step  46560 (epoch   14.55), loss: 0.009849, time: 2.228 s\n",
      "Step  46580 (epoch   14.56), loss: 0.011694, time: 2.208 s\n",
      "Step  46600 (epoch   14.56), loss: 0.013265, time: 2.226 s, error: 0.018065\n",
      "Step  46620 (epoch   14.57), loss: 0.014474, time: 14.367 s\n",
      "Step  46640 (epoch   14.57), loss: 0.013937, time: 2.206 s\n",
      "Step  46660 (epoch   14.58), loss: 0.023853, time: 2.223 s\n",
      "Step  46680 (epoch   14.59), loss: 0.010969, time: 2.212 s\n",
      "Step  46700 (epoch   14.59), loss: 0.011571, time: 2.229 s\n",
      "Step  46720 (epoch   14.60), loss: 0.010656, time: 2.225 s\n",
      "Step  46740 (epoch   14.61), loss: 0.015154, time: 2.211 s\n",
      "Step  46760 (epoch   14.61), loss: 0.006532, time: 2.230 s\n",
      "Step  46780 (epoch   14.62), loss: 0.011483, time: 2.211 s\n",
      "Step  46800 (epoch   14.62), loss: 0.013547, time: 2.221 s, error: 0.016960\n",
      "Step  46820 (epoch   14.63), loss: 0.011024, time: 14.567 s\n",
      "Step  46840 (epoch   14.64), loss: 0.012192, time: 2.224 s\n",
      "Step  46860 (epoch   14.64), loss: 0.009803, time: 2.214 s\n",
      "Step  46880 (epoch   14.65), loss: 0.009298, time: 2.217 s\n",
      "Step  46900 (epoch   14.66), loss: 0.010031, time: 2.224 s\n",
      "Step  46920 (epoch   14.66), loss: 0.009799, time: 2.220 s\n",
      "Step  46940 (epoch   14.67), loss: 0.009293, time: 2.236 s\n",
      "Step  46960 (epoch   14.68), loss: 0.014786, time: 2.217 s\n",
      "Step  46980 (epoch   14.68), loss: 0.014640, time: 2.221 s\n",
      "Step  47000 (epoch   14.69), loss: 0.007082, time: 2.220 s, error: 0.018356\n",
      "Step  47020 (epoch   14.69), loss: 0.010660, time: 14.431 s\n",
      "Step  47040 (epoch   14.70), loss: 0.008245, time: 2.226 s\n",
      "Step  47060 (epoch   14.71), loss: 0.009302, time: 2.232 s\n",
      "Step  47080 (epoch   14.71), loss: 0.006954, time: 2.217 s\n",
      "Step  47100 (epoch   14.72), loss: 0.011162, time: 2.229 s\n",
      "Step  47120 (epoch   14.72), loss: 0.008509, time: 2.221 s\n",
      "Step  47140 (epoch   14.73), loss: 0.007704, time: 2.219 s\n",
      "Step  47160 (epoch   14.74), loss: 0.010099, time: 2.213 s\n",
      "Step  47180 (epoch   14.74), loss: 0.007682, time: 2.215 s\n",
      "Step  47200 (epoch   14.75), loss: 0.011762, time: 2.216 s, error: 0.016782\n",
      "Step  47220 (epoch   14.76), loss: 0.012441, time: 14.410 s\n",
      "Step  47240 (epoch   14.76), loss: 0.015135, time: 2.228 s\n",
      "Step  47260 (epoch   14.77), loss: 0.008212, time: 2.229 s\n",
      "Step  47280 (epoch   14.78), loss: 0.008277, time: 2.215 s\n",
      "Step  47300 (epoch   14.78), loss: 0.014135, time: 2.228 s\n",
      "Step  47320 (epoch   14.79), loss: 0.008777, time: 2.219 s\n",
      "Step  47340 (epoch   14.79), loss: 0.013503, time: 2.222 s\n",
      "Step  47360 (epoch   14.80), loss: 0.009985, time: 2.223 s\n",
      "Step  47380 (epoch   14.81), loss: 0.011437, time: 2.208 s\n",
      "Step  47400 (epoch   14.81), loss: 0.011650, time: 2.230 s, error: 0.018521\n",
      "Step  47420 (epoch   14.82), loss: 0.010788, time: 14.408 s\n",
      "Step  47440 (epoch   14.82), loss: 0.013468, time: 2.225 s\n",
      "Step  47460 (epoch   14.83), loss: 0.011338, time: 2.231 s\n",
      "Step  47480 (epoch   14.84), loss: 0.008165, time: 2.221 s\n",
      "Step  47500 (epoch   14.84), loss: 0.008403, time: 2.213 s\n",
      "Step  47520 (epoch   14.85), loss: 0.011128, time: 2.212 s\n",
      "Step  47540 (epoch   14.86), loss: 0.009818, time: 2.230 s\n",
      "Step  47560 (epoch   14.86), loss: 0.009317, time: 2.238 s\n",
      "Step  47580 (epoch   14.87), loss: 0.012154, time: 2.227 s\n",
      "Step  47600 (epoch   14.88), loss: 0.008162, time: 2.224 s, error: 0.016831\n",
      "Step  47620 (epoch   14.88), loss: 0.012467, time: 14.380 s\n",
      "Step  47640 (epoch   14.89), loss: 0.010132, time: 2.221 s\n",
      "Step  47660 (epoch   14.89), loss: 0.012365, time: 2.226 s\n",
      "Step  47680 (epoch   14.90), loss: 0.016195, time: 2.224 s\n",
      "Step  47700 (epoch   14.91), loss: 0.021405, time: 2.227 s\n",
      "Step  47720 (epoch   14.91), loss: 0.007320, time: 2.209 s\n",
      "Step  47740 (epoch   14.92), loss: 0.011682, time: 2.216 s\n",
      "Step  47760 (epoch   14.93), loss: 0.008509, time: 2.215 s\n",
      "Step  47780 (epoch   14.93), loss: 0.013906, time: 2.217 s\n",
      "Step  47800 (epoch   14.94), loss: 0.008364, time: 2.231 s, error: 0.016996\n",
      "Step  47820 (epoch   14.94), loss: 0.014244, time: 14.533 s\n",
      "Step  47840 (epoch   14.95), loss: 0.007954, time: 2.216 s\n",
      "Step  47860 (epoch   14.96), loss: 0.011949, time: 2.218 s\n",
      "Step  47880 (epoch   14.96), loss: 0.009192, time: 2.227 s\n",
      "Step  47900 (epoch   14.97), loss: 0.012920, time: 2.218 s\n",
      "Step  47920 (epoch   14.97), loss: 0.015016, time: 2.221 s\n",
      "Step  47940 (epoch   14.98), loss: 0.010117, time: 2.213 s\n",
      "Step  47960 (epoch   14.99), loss: 0.007928, time: 2.223 s\n",
      "Step  47980 (epoch   14.99), loss: 0.019772, time: 2.207 s\n",
      "Step  48000 (epoch   15.00), loss: 0.014159, time: 2.219 s, error: 0.017709\n",
      "\n",
      "Time since beginning  : 8243.880 s\n",
      "\n",
      "Step  48020 (epoch   15.01), loss: 0.012440, time: 14.592 s\n",
      "Step  48040 (epoch   15.01), loss: 0.008452, time: 2.214 s\n",
      "Step  48060 (epoch   15.02), loss: 0.012635, time: 2.228 s\n",
      "Step  48080 (epoch   15.03), loss: 0.010486, time: 2.211 s\n",
      "Step  48100 (epoch   15.03), loss: 0.008127, time: 2.222 s\n",
      "Step  48120 (epoch   15.04), loss: 0.012501, time: 2.230 s\n",
      "Step  48140 (epoch   15.04), loss: 0.012682, time: 2.232 s\n",
      "Step  48160 (epoch   15.05), loss: 0.009210, time: 2.215 s\n",
      "Step  48180 (epoch   15.06), loss: 0.011314, time: 2.217 s\n",
      "Step  48200 (epoch   15.06), loss: 0.012829, time: 2.228 s, error: 0.017259\n",
      "Step  48220 (epoch   15.07), loss: 0.012162, time: 14.381 s\n",
      "Step  48240 (epoch   15.07), loss: 0.013946, time: 2.213 s\n",
      "Step  48260 (epoch   15.08), loss: 0.011197, time: 2.230 s\n",
      "Step  48280 (epoch   15.09), loss: 0.012857, time: 2.224 s\n",
      "Step  48300 (epoch   15.09), loss: 0.007350, time: 2.229 s\n",
      "Step  48320 (epoch   15.10), loss: 0.012341, time: 2.201 s\n",
      "Step  48340 (epoch   15.11), loss: 0.011120, time: 2.221 s\n",
      "Step  48360 (epoch   15.11), loss: 0.009099, time: 2.222 s\n",
      "Step  48380 (epoch   15.12), loss: 0.008892, time: 2.229 s\n",
      "Step  48400 (epoch   15.12), loss: 0.011539, time: 2.208 s, error: 0.016317\n",
      "Step  48420 (epoch   15.13), loss: 0.010716, time: 14.428 s\n",
      "Step  48440 (epoch   15.14), loss: 0.011994, time: 2.220 s\n",
      "Step  48460 (epoch   15.14), loss: 0.010572, time: 2.214 s\n",
      "Step  48480 (epoch   15.15), loss: 0.012115, time: 2.218 s\n",
      "Step  48500 (epoch   15.16), loss: 0.010600, time: 2.219 s\n",
      "Step  48520 (epoch   15.16), loss: 0.009800, time: 2.236 s\n",
      "Step  48540 (epoch   15.17), loss: 0.026975, time: 2.231 s\n",
      "Step  48560 (epoch   15.18), loss: 0.009137, time: 2.214 s\n",
      "Step  48580 (epoch   15.18), loss: 0.013520, time: 2.217 s\n",
      "Step  48600 (epoch   15.19), loss: 0.009962, time: 2.228 s, error: 0.016999\n",
      "Step  48620 (epoch   15.19), loss: 0.019491, time: 14.382 s\n",
      "Step  48640 (epoch   15.20), loss: 0.008791, time: 2.216 s\n",
      "Step  48660 (epoch   15.21), loss: 0.007190, time: 2.221 s\n",
      "Step  48680 (epoch   15.21), loss: 0.012260, time: 2.214 s\n",
      "Step  48700 (epoch   15.22), loss: 0.017346, time: 2.215 s\n",
      "Step  48720 (epoch   15.22), loss: 0.013366, time: 2.221 s\n",
      "Step  48740 (epoch   15.23), loss: 0.007360, time: 2.224 s\n",
      "Step  48760 (epoch   15.24), loss: 0.024491, time: 2.225 s\n",
      "Step  48780 (epoch   15.24), loss: 0.007703, time: 2.234 s\n",
      "Step  48800 (epoch   15.25), loss: 0.008949, time: 2.241 s, error: 0.016408\n",
      "Step  48820 (epoch   15.26), loss: 0.010116, time: 14.404 s\n",
      "Step  48840 (epoch   15.26), loss: 0.010384, time: 2.215 s\n",
      "Step  48860 (epoch   15.27), loss: 0.009254, time: 2.206 s\n",
      "Step  48880 (epoch   15.28), loss: 0.010219, time: 2.225 s\n",
      "Step  48900 (epoch   15.28), loss: 0.011757, time: 2.218 s\n",
      "Step  48920 (epoch   15.29), loss: 0.009575, time: 2.223 s\n",
      "Step  48940 (epoch   15.29), loss: 0.013249, time: 2.218 s\n",
      "Step  48960 (epoch   15.30), loss: 0.008316, time: 2.228 s\n",
      "Step  48980 (epoch   15.31), loss: 0.006734, time: 2.219 s\n",
      "Step  49000 (epoch   15.31), loss: 0.021329, time: 2.226 s, error: 0.017894\n",
      "Step  49020 (epoch   15.32), loss: 0.015906, time: 14.549 s\n",
      "Step  49040 (epoch   15.32), loss: 0.010218, time: 2.235 s\n",
      "Step  49060 (epoch   15.33), loss: 0.015711, time: 2.226 s\n",
      "Step  49080 (epoch   15.34), loss: 0.008946, time: 2.222 s\n",
      "Step  49100 (epoch   15.34), loss: 0.008609, time: 2.214 s\n",
      "Step  49120 (epoch   15.35), loss: 0.012719, time: 2.212 s\n",
      "Step  49140 (epoch   15.36), loss: 0.009984, time: 2.214 s\n",
      "Step  49160 (epoch   15.36), loss: 0.011175, time: 2.221 s\n",
      "Step  49180 (epoch   15.37), loss: 0.010420, time: 2.223 s\n",
      "Step  49200 (epoch   15.38), loss: 0.009274, time: 2.222 s, error: 0.017151\n",
      "Step  49220 (epoch   15.38), loss: 0.020633, time: 14.399 s\n",
      "Step  49240 (epoch   15.39), loss: 0.012726, time: 2.223 s\n",
      "Step  49260 (epoch   15.39), loss: 0.006958, time: 2.222 s\n",
      "Step  49280 (epoch   15.40), loss: 0.016418, time: 2.219 s\n",
      "Step  49300 (epoch   15.41), loss: 0.011564, time: 2.228 s\n",
      "Step  49320 (epoch   15.41), loss: 0.007573, time: 2.226 s\n",
      "Step  49340 (epoch   15.42), loss: 0.010045, time: 2.230 s\n",
      "Step  49360 (epoch   15.43), loss: 0.010465, time: 2.213 s\n",
      "Step  49380 (epoch   15.43), loss: 0.013536, time: 2.217 s\n",
      "Step  49400 (epoch   15.44), loss: 0.009450, time: 2.225 s, error: 0.016255\n",
      "Step  49420 (epoch   15.44), loss: 0.013479, time: 14.376 s\n",
      "Step  49440 (epoch   15.45), loss: 0.008373, time: 2.224 s\n",
      "Step  49460 (epoch   15.46), loss: 0.010795, time: 2.232 s\n",
      "Step  49480 (epoch   15.46), loss: 0.008670, time: 2.231 s\n",
      "Step  49500 (epoch   15.47), loss: 0.009465, time: 2.218 s\n",
      "Step  49520 (epoch   15.47), loss: 0.010227, time: 2.217 s\n",
      "Step  49540 (epoch   15.48), loss: 0.007842, time: 2.222 s\n",
      "Step  49560 (epoch   15.49), loss: 0.008434, time: 2.224 s\n",
      "Step  49580 (epoch   15.49), loss: 0.014840, time: 2.223 s\n",
      "Step  49600 (epoch   15.50), loss: 0.009227, time: 2.224 s, error: 0.017193\n",
      "Step  49620 (epoch   15.51), loss: 0.012402, time: 14.376 s\n",
      "Step  49640 (epoch   15.51), loss: 0.013126, time: 2.217 s\n",
      "Step  49660 (epoch   15.52), loss: 0.014027, time: 2.224 s\n",
      "Step  49680 (epoch   15.53), loss: 0.013510, time: 2.229 s\n",
      "Step  49700 (epoch   15.53), loss: 0.009777, time: 2.219 s\n",
      "Step  49720 (epoch   15.54), loss: 0.014713, time: 2.224 s\n",
      "Step  49740 (epoch   15.54), loss: 0.008111, time: 2.213 s\n",
      "Step  49760 (epoch   15.55), loss: 0.015602, time: 2.238 s\n",
      "Step  49780 (epoch   15.56), loss: 0.006646, time: 2.226 s\n",
      "Step  49800 (epoch   15.56), loss: 0.011660, time: 2.224 s, error: 0.017274\n",
      "Step  49820 (epoch   15.57), loss: 0.009920, time: 14.376 s\n",
      "Step  49840 (epoch   15.57), loss: 0.020435, time: 2.221 s\n",
      "Step  49860 (epoch   15.58), loss: 0.015253, time: 2.221 s\n",
      "Step  49880 (epoch   15.59), loss: 0.021206, time: 2.212 s\n",
      "Step  49900 (epoch   15.59), loss: 0.009296, time: 2.228 s\n",
      "Step  49920 (epoch   15.60), loss: 0.010099, time: 2.232 s\n",
      "Step  49940 (epoch   15.61), loss: 0.008905, time: 2.229 s\n",
      "Step  49960 (epoch   15.61), loss: 0.008986, time: 2.222 s\n",
      "Step  49980 (epoch   15.62), loss: 0.007859, time: 2.216 s\n",
      "Step  50000 (epoch   15.62), loss: 0.011173, time: 2.224 s, error: 0.016285\n",
      "\n",
      "Time since beginning  : 8588.156 s\n",
      "\n",
      "Step  50020 (epoch   15.63), loss: 0.007235, time: 14.654 s\n",
      "Step  50040 (epoch   15.64), loss: 0.013960, time: 2.233 s\n",
      "Step  50060 (epoch   15.64), loss: 0.014326, time: 2.222 s\n",
      "Step  50080 (epoch   15.65), loss: 0.009398, time: 2.232 s\n",
      "Step  50100 (epoch   15.66), loss: 0.012582, time: 2.223 s\n",
      "Step  50120 (epoch   15.66), loss: 0.010079, time: 2.235 s\n",
      "Step  50140 (epoch   15.67), loss: 0.008335, time: 2.222 s\n",
      "Step  50160 (epoch   15.68), loss: 0.010429, time: 2.233 s\n",
      "Step  50180 (epoch   15.68), loss: 0.008050, time: 2.233 s\n",
      "Step  50200 (epoch   15.69), loss: 0.006265, time: 2.238 s, error: 0.017203\n",
      "Step  50220 (epoch   15.69), loss: 0.016038, time: 14.823 s\n",
      "Step  50240 (epoch   15.70), loss: 0.013862, time: 2.228 s\n",
      "Step  50260 (epoch   15.71), loss: 0.011727, time: 2.234 s\n",
      "Step  50280 (epoch   15.71), loss: 0.007729, time: 2.235 s\n",
      "Step  50300 (epoch   15.72), loss: 0.009908, time: 2.230 s\n",
      "Step  50320 (epoch   15.72), loss: 0.008780, time: 2.230 s\n",
      "Step  50340 (epoch   15.73), loss: 0.011579, time: 2.231 s\n",
      "Step  50360 (epoch   15.74), loss: 0.010051, time: 2.237 s\n",
      "Step  50380 (epoch   15.74), loss: 0.006628, time: 2.229 s\n",
      "Step  50400 (epoch   15.75), loss: 0.011511, time: 2.229 s, error: 0.016311\n",
      "Step  50420 (epoch   15.76), loss: 0.011338, time: 14.578 s\n",
      "Step  50440 (epoch   15.76), loss: 0.022256, time: 2.234 s\n",
      "Step  50460 (epoch   15.77), loss: 0.009325, time: 2.236 s\n",
      "Step  50480 (epoch   15.78), loss: 0.011529, time: 2.219 s\n",
      "Step  50500 (epoch   15.78), loss: 0.014477, time: 2.227 s\n",
      "Step  50520 (epoch   15.79), loss: 0.014191, time: 2.233 s\n",
      "Step  50540 (epoch   15.79), loss: 0.011577, time: 2.228 s\n",
      "Step  50560 (epoch   15.80), loss: 0.020868, time: 2.225 s\n",
      "Step  50580 (epoch   15.81), loss: 0.012953, time: 2.241 s\n",
      "Step  50600 (epoch   15.81), loss: 0.009780, time: 2.233 s, error: 0.016654\n",
      "Step  50620 (epoch   15.82), loss: 0.007043, time: 14.665 s\n",
      "Step  50640 (epoch   15.82), loss: 0.009974, time: 2.231 s\n",
      "Step  50660 (epoch   15.83), loss: 0.008206, time: 2.232 s\n",
      "Step  50680 (epoch   15.84), loss: 0.010441, time: 2.223 s\n",
      "Step  50700 (epoch   15.84), loss: 0.010042, time: 2.237 s\n",
      "Step  50720 (epoch   15.85), loss: 0.012012, time: 2.243 s\n",
      "Step  50740 (epoch   15.86), loss: 0.009984, time: 2.231 s\n",
      "Step  50760 (epoch   15.86), loss: 0.009959, time: 2.223 s\n",
      "Step  50780 (epoch   15.87), loss: 0.011515, time: 2.232 s\n",
      "Step  50800 (epoch   15.88), loss: 0.008335, time: 2.219 s, error: 0.016683\n",
      "Step  50820 (epoch   15.88), loss: 0.010468, time: 14.572 s\n",
      "Step  50840 (epoch   15.89), loss: 0.006417, time: 2.231 s\n",
      "Step  50860 (epoch   15.89), loss: 0.010063, time: 2.232 s\n",
      "Step  50880 (epoch   15.90), loss: 0.013798, time: 2.240 s\n",
      "Step  50900 (epoch   15.91), loss: 0.010842, time: 2.227 s\n",
      "Step  50920 (epoch   15.91), loss: 0.020797, time: 2.228 s\n",
      "Step  50940 (epoch   15.92), loss: 0.009822, time: 2.225 s\n",
      "Step  50960 (epoch   15.93), loss: 0.007413, time: 2.220 s\n",
      "Step  50980 (epoch   15.93), loss: 0.011081, time: 2.242 s\n",
      "Step  51000 (epoch   15.94), loss: 0.008339, time: 2.235 s, error: 0.016134\n",
      "Step  51020 (epoch   15.94), loss: 0.015545, time: 14.588 s\n",
      "Step  51040 (epoch   15.95), loss: 0.008242, time: 2.226 s\n",
      "Step  51060 (epoch   15.96), loss: 0.009833, time: 2.223 s\n",
      "Step  51080 (epoch   15.96), loss: 0.014711, time: 2.230 s\n",
      "Step  51100 (epoch   15.97), loss: 0.016109, time: 2.223 s\n",
      "Step  51120 (epoch   15.97), loss: 0.009936, time: 2.233 s\n",
      "Step  51140 (epoch   15.98), loss: 0.014566, time: 2.234 s\n",
      "Step  51160 (epoch   15.99), loss: 0.011962, time: 2.238 s\n",
      "Step  51180 (epoch   15.99), loss: 0.006909, time: 2.237 s\n",
      "Step  51200 (epoch   16.00), loss: 0.009120, time: 2.234 s, error: 0.017631\n",
      "Step  51220 (epoch   16.01), loss: 0.013079, time: 14.783 s\n",
      "Step  51240 (epoch   16.01), loss: 0.007891, time: 2.219 s\n",
      "Step  51260 (epoch   16.02), loss: 0.010089, time: 2.213 s\n",
      "Step  51280 (epoch   16.02), loss: 0.012256, time: 2.229 s\n",
      "Step  51300 (epoch   16.03), loss: 0.011517, time: 2.222 s\n",
      "Step  51320 (epoch   16.04), loss: 0.013034, time: 2.228 s\n",
      "Step  51340 (epoch   16.04), loss: 0.011319, time: 2.234 s\n",
      "Step  51360 (epoch   16.05), loss: 0.009356, time: 2.225 s\n",
      "Step  51380 (epoch   16.06), loss: 0.013507, time: 2.240 s\n",
      "Step  51400 (epoch   16.06), loss: 0.008175, time: 2.215 s, error: 0.016847\n",
      "Step  51420 (epoch   16.07), loss: 0.011025, time: 14.744 s\n",
      "Step  51440 (epoch   16.07), loss: 0.010165, time: 2.237 s\n",
      "Step  51460 (epoch   16.08), loss: 0.030218, time: 2.215 s\n",
      "Step  51480 (epoch   16.09), loss: 0.007671, time: 2.226 s\n",
      "Step  51500 (epoch   16.09), loss: 0.007344, time: 2.238 s\n",
      "Step  51520 (epoch   16.10), loss: 0.013013, time: 2.228 s\n",
      "Step  51540 (epoch   16.11), loss: 0.009060, time: 2.231 s\n",
      "Step  51560 (epoch   16.11), loss: 0.010104, time: 2.233 s\n",
      "Step  51580 (epoch   16.12), loss: 0.009437, time: 2.219 s\n",
      "Step  51600 (epoch   16.12), loss: 0.015695, time: 2.236 s, error: 0.015679\n",
      "Step  51620 (epoch   16.13), loss: 0.010850, time: 14.662 s\n",
      "Step  51640 (epoch   16.14), loss: 0.010145, time: 2.235 s\n",
      "Step  51660 (epoch   16.14), loss: 0.009819, time: 2.234 s\n",
      "Step  51680 (epoch   16.15), loss: 0.011077, time: 2.243 s\n",
      "Step  51700 (epoch   16.16), loss: 0.012501, time: 2.232 s\n",
      "Step  51720 (epoch   16.16), loss: 0.008611, time: 2.227 s\n",
      "Step  51740 (epoch   16.17), loss: 0.014284, time: 2.209 s\n",
      "Step  51760 (epoch   16.18), loss: 0.014247, time: 2.230 s\n",
      "Step  51780 (epoch   16.18), loss: 0.007625, time: 2.223 s\n",
      "Step  51800 (epoch   16.19), loss: 0.011970, time: 2.221 s, error: 0.017047\n",
      "Step  51820 (epoch   16.19), loss: 0.010796, time: 14.690 s\n",
      "Step  51840 (epoch   16.20), loss: 0.010709, time: 2.226 s\n",
      "Step  51860 (epoch   16.21), loss: 0.010585, time: 2.217 s\n",
      "Step  51880 (epoch   16.21), loss: 0.011844, time: 2.226 s\n",
      "Step  51900 (epoch   16.22), loss: 0.014612, time: 2.235 s\n",
      "Step  51920 (epoch   16.23), loss: 0.007992, time: 2.229 s\n",
      "Step  51940 (epoch   16.23), loss: 0.011812, time: 2.245 s\n",
      "Step  51960 (epoch   16.24), loss: 0.007701, time: 2.222 s\n",
      "Step  51980 (epoch   16.24), loss: 0.009449, time: 2.230 s\n",
      "Step  52000 (epoch   16.25), loss: 0.008798, time: 2.236 s, error: 0.017308\n",
      "\n",
      "Time since beginning  : 8935.624 s\n",
      "\n",
      "Step  52020 (epoch   16.26), loss: 0.008029, time: 14.672 s\n",
      "Step  52040 (epoch   16.26), loss: 0.016695, time: 2.225 s\n",
      "Step  52060 (epoch   16.27), loss: 0.013224, time: 2.215 s\n",
      "Step  52080 (epoch   16.27), loss: 0.013898, time: 2.231 s\n",
      "Step  52100 (epoch   16.28), loss: 0.016151, time: 2.226 s\n",
      "Step  52120 (epoch   16.29), loss: 0.010996, time: 2.227 s\n",
      "Step  52140 (epoch   16.29), loss: 0.012969, time: 2.234 s\n",
      "Step  52160 (epoch   16.30), loss: 0.007465, time: 2.224 s\n",
      "Step  52180 (epoch   16.31), loss: 0.009426, time: 2.228 s\n",
      "Step  52200 (epoch   16.31), loss: 0.017322, time: 2.242 s, error: 0.016617\n",
      "Step  52220 (epoch   16.32), loss: 0.012764, time: 14.649 s\n",
      "Step  52240 (epoch   16.32), loss: 0.010850, time: 2.224 s\n",
      "Step  52260 (epoch   16.33), loss: 0.007390, time: 2.209 s\n",
      "Step  52280 (epoch   16.34), loss: 0.010401, time: 2.235 s\n",
      "Step  52300 (epoch   16.34), loss: 0.011730, time: 2.226 s\n",
      "Step  52320 (epoch   16.35), loss: 0.017726, time: 2.225 s\n",
      "Step  52340 (epoch   16.36), loss: 0.012195, time: 2.217 s\n",
      "Step  52360 (epoch   16.36), loss: 0.015731, time: 2.220 s\n",
      "Step  52380 (epoch   16.37), loss: 0.011402, time: 2.230 s\n",
      "Step  52400 (epoch   16.38), loss: 0.016097, time: 2.228 s, error: 0.016468\n",
      "Step  52420 (epoch   16.38), loss: 0.010338, time: 14.763 s\n",
      "Step  52440 (epoch   16.39), loss: 0.009779, time: 2.225 s\n",
      "Step  52460 (epoch   16.39), loss: 0.016294, time: 2.224 s\n",
      "Step  52480 (epoch   16.40), loss: 0.014027, time: 2.221 s\n",
      "Step  52500 (epoch   16.41), loss: 0.015481, time: 2.223 s\n",
      "Step  52520 (epoch   16.41), loss: 0.008092, time: 2.215 s\n",
      "Step  52540 (epoch   16.42), loss: 0.008930, time: 2.231 s\n",
      "Step  52560 (epoch   16.43), loss: 0.013821, time: 2.215 s\n",
      "Step  52580 (epoch   16.43), loss: 0.011603, time: 2.226 s\n",
      "Step  52600 (epoch   16.44), loss: 0.007707, time: 2.218 s, error: 0.016007\n",
      "Step  52620 (epoch   16.44), loss: 0.008036, time: 14.575 s\n",
      "Step  52640 (epoch   16.45), loss: 0.008434, time: 2.241 s\n",
      "Step  52660 (epoch   16.46), loss: 0.010586, time: 2.216 s\n",
      "Step  52680 (epoch   16.46), loss: 0.014929, time: 2.228 s\n",
      "Step  52700 (epoch   16.47), loss: 0.009893, time: 2.233 s\n",
      "Step  52720 (epoch   16.48), loss: 0.007442, time: 2.228 s\n",
      "Step  52740 (epoch   16.48), loss: 0.008464, time: 2.228 s\n",
      "Step  52760 (epoch   16.49), loss: 0.009205, time: 2.234 s\n",
      "Step  52780 (epoch   16.49), loss: 0.010480, time: 2.223 s\n",
      "Step  52800 (epoch   16.50), loss: 0.011308, time: 2.216 s, error: 0.016501\n",
      "Step  52820 (epoch   16.51), loss: 0.009370, time: 14.638 s\n",
      "Step  52840 (epoch   16.51), loss: 0.009556, time: 2.237 s\n",
      "Step  52860 (epoch   16.52), loss: 0.008729, time: 2.229 s\n",
      "Step  52880 (epoch   16.52), loss: 0.013867, time: 2.236 s\n",
      "Step  52900 (epoch   16.53), loss: 0.008771, time: 2.231 s\n",
      "Step  52920 (epoch   16.54), loss: 0.014720, time: 2.238 s\n",
      "Step  52940 (epoch   16.54), loss: 0.008348, time: 2.236 s\n",
      "Step  52960 (epoch   16.55), loss: 0.010617, time: 2.233 s\n",
      "Step  52980 (epoch   16.56), loss: 0.007474, time: 2.227 s\n",
      "Step  53000 (epoch   16.56), loss: 0.007014, time: 2.227 s, error: 0.015915\n",
      "Step  53020 (epoch   16.57), loss: 0.007017, time: 14.638 s\n",
      "Step  53040 (epoch   16.57), loss: 0.012940, time: 2.224 s\n",
      "Step  53060 (epoch   16.58), loss: 0.012969, time: 2.224 s\n",
      "Step  53080 (epoch   16.59), loss: 0.009213, time: 2.242 s\n",
      "Step  53100 (epoch   16.59), loss: 0.011974, time: 2.236 s\n",
      "Step  53120 (epoch   16.60), loss: 0.017342, time: 2.232 s\n",
      "Step  53140 (epoch   16.61), loss: 0.013120, time: 2.242 s\n",
      "Step  53160 (epoch   16.61), loss: 0.010703, time: 2.242 s\n",
      "Step  53180 (epoch   16.62), loss: 0.009895, time: 2.225 s\n",
      "Step  53200 (epoch   16.62), loss: 0.006675, time: 2.220 s, error: 0.015750\n",
      "Step  53220 (epoch   16.63), loss: 0.009530, time: 14.628 s\n",
      "Step  53240 (epoch   16.64), loss: 0.009356, time: 2.229 s\n",
      "Step  53260 (epoch   16.64), loss: 0.013787, time: 2.216 s\n",
      "Step  53280 (epoch   16.65), loss: 0.011646, time: 2.234 s\n",
      "Step  53300 (epoch   16.66), loss: 0.012391, time: 2.236 s\n",
      "Step  53320 (epoch   16.66), loss: 0.007347, time: 2.231 s\n",
      "Step  53340 (epoch   16.67), loss: 0.011032, time: 2.234 s\n",
      "Step  53360 (epoch   16.68), loss: 0.011244, time: 2.235 s\n",
      "Step  53380 (epoch   16.68), loss: 0.010615, time: 2.233 s\n",
      "Step  53400 (epoch   16.69), loss: 0.011432, time: 2.229 s, error: 0.016176\n",
      "Step  53420 (epoch   16.69), loss: 0.011533, time: 14.907 s\n",
      "Step  53440 (epoch   16.70), loss: 0.013025, time: 2.215 s\n",
      "Step  53460 (epoch   16.71), loss: 0.011643, time: 2.238 s\n",
      "Step  53480 (epoch   16.71), loss: 0.010881, time: 2.213 s\n",
      "Step  53500 (epoch   16.72), loss: 0.009893, time: 2.221 s\n",
      "Step  53520 (epoch   16.73), loss: 0.014512, time: 2.221 s\n",
      "Step  53540 (epoch   16.73), loss: 0.010959, time: 2.216 s\n",
      "Step  53560 (epoch   16.74), loss: 0.012493, time: 2.225 s\n",
      "Step  53580 (epoch   16.74), loss: 0.016906, time: 2.231 s\n",
      "Step  53600 (epoch   16.75), loss: 0.007123, time: 2.224 s, error: 0.015855\n",
      "Step  53620 (epoch   16.76), loss: 0.012854, time: 14.893 s\n",
      "Step  53640 (epoch   16.76), loss: 0.009425, time: 2.237 s\n",
      "Step  53660 (epoch   16.77), loss: 0.008247, time: 2.232 s\n",
      "Step  53680 (epoch   16.77), loss: 0.007670, time: 2.225 s\n",
      "Step  53700 (epoch   16.78), loss: 0.007816, time: 2.224 s\n",
      "Step  53720 (epoch   16.79), loss: 0.015883, time: 2.231 s\n",
      "Step  53740 (epoch   16.79), loss: 0.010436, time: 2.223 s\n",
      "Step  53760 (epoch   16.80), loss: 0.007062, time: 2.243 s\n",
      "Step  53780 (epoch   16.81), loss: 0.014000, time: 2.239 s\n",
      "Step  53800 (epoch   16.81), loss: 0.008912, time: 2.229 s, error: 0.015608\n",
      "Step  53820 (epoch   16.82), loss: 0.005542, time: 14.668 s\n",
      "Step  53840 (epoch   16.82), loss: 0.007341, time: 2.237 s\n",
      "Step  53860 (epoch   16.83), loss: 0.008115, time: 2.224 s\n",
      "Step  53880 (epoch   16.84), loss: 0.008138, time: 2.233 s\n",
      "Step  53900 (epoch   16.84), loss: 0.006612, time: 2.219 s\n",
      "Step  53920 (epoch   16.85), loss: 0.009042, time: 2.235 s\n",
      "Step  53940 (epoch   16.86), loss: 0.018463, time: 2.229 s\n",
      "Step  53960 (epoch   16.86), loss: 0.015091, time: 2.228 s\n",
      "Step  53980 (epoch   16.87), loss: 0.006876, time: 2.236 s\n",
      "Step  54000 (epoch   16.88), loss: 0.008762, time: 2.216 s, error: 0.016048\n",
      "\n",
      "Time since beginning  : 9283.160 s\n",
      "\n",
      "Step  54020 (epoch   16.88), loss: 0.007344, time: 14.662 s\n",
      "Step  54040 (epoch   16.89), loss: 0.007261, time: 2.229 s\n",
      "Step  54060 (epoch   16.89), loss: 0.007096, time: 2.216 s\n",
      "Step  54080 (epoch   16.90), loss: 0.014521, time: 2.239 s\n",
      "Step  54100 (epoch   16.91), loss: 0.012113, time: 2.232 s\n",
      "Step  54120 (epoch   16.91), loss: 0.010115, time: 2.237 s\n",
      "Step  54140 (epoch   16.92), loss: 0.012635, time: 2.221 s\n",
      "Step  54160 (epoch   16.93), loss: 0.010161, time: 2.212 s\n",
      "Step  54180 (epoch   16.93), loss: 0.010180, time: 2.218 s\n",
      "Step  54200 (epoch   16.94), loss: 0.010203, time: 2.205 s, error: 0.015896\n",
      "Step  54220 (epoch   16.94), loss: 0.011059, time: 14.617 s\n",
      "Step  54240 (epoch   16.95), loss: 0.008917, time: 2.225 s\n",
      "Step  54260 (epoch   16.96), loss: 0.007833, time: 2.221 s\n",
      "Step  54280 (epoch   16.96), loss: 0.009184, time: 2.221 s\n",
      "Step  54300 (epoch   16.97), loss: 0.010303, time: 2.217 s\n",
      "Step  54320 (epoch   16.98), loss: 0.007924, time: 2.219 s\n",
      "Step  54340 (epoch   16.98), loss: 0.015987, time: 2.226 s\n",
      "Step  54360 (epoch   16.99), loss: 0.008238, time: 2.235 s\n",
      "Step  54380 (epoch   16.99), loss: 0.008509, time: 2.234 s\n",
      "Step  54400 (epoch   17.00), loss: 0.009044, time: 2.222 s, error: 0.017206\n",
      "Step  54420 (epoch   17.01), loss: 0.008607, time: 14.575 s\n",
      "Step  54440 (epoch   17.01), loss: 0.009179, time: 2.232 s\n",
      "Step  54460 (epoch   17.02), loss: 0.006465, time: 2.234 s\n",
      "Step  54480 (epoch   17.02), loss: 0.008732, time: 2.214 s\n",
      "Step  54500 (epoch   17.03), loss: 0.010522, time: 2.217 s\n",
      "Step  54520 (epoch   17.04), loss: 0.010650, time: 2.235 s\n",
      "Step  54540 (epoch   17.04), loss: 0.011455, time: 2.225 s\n",
      "Step  54560 (epoch   17.05), loss: 0.008401, time: 2.235 s\n",
      "Step  54580 (epoch   17.06), loss: 0.010560, time: 2.239 s\n",
      "Step  54600 (epoch   17.06), loss: 0.009681, time: 2.226 s, error: 0.016303\n",
      "Step  54620 (epoch   17.07), loss: 0.012927, time: 14.816 s\n",
      "Step  54640 (epoch   17.07), loss: 0.011336, time: 2.237 s\n",
      "Step  54660 (epoch   17.08), loss: 0.009234, time: 2.221 s\n",
      "Step  54680 (epoch   17.09), loss: 0.008664, time: 2.232 s\n",
      "Step  54700 (epoch   17.09), loss: 0.014149, time: 2.210 s\n",
      "Step  54720 (epoch   17.10), loss: 0.007818, time: 2.233 s\n",
      "Step  54740 (epoch   17.11), loss: 0.009469, time: 2.228 s\n",
      "Step  54760 (epoch   17.11), loss: 0.011681, time: 2.224 s\n",
      "Step  54780 (epoch   17.12), loss: 0.011084, time: 2.219 s\n",
      "Step  54800 (epoch   17.12), loss: 0.011518, time: 2.238 s, error: 0.015337\n",
      "Step  54820 (epoch   17.13), loss: 0.010091, time: 14.846 s\n",
      "Step  54840 (epoch   17.14), loss: 0.017130, time: 2.221 s\n",
      "Step  54860 (epoch   17.14), loss: 0.012447, time: 2.226 s\n",
      "Step  54880 (epoch   17.15), loss: 0.009709, time: 2.238 s\n",
      "Step  54900 (epoch   17.16), loss: 0.014067, time: 2.232 s\n",
      "Step  54920 (epoch   17.16), loss: 0.008940, time: 2.241 s\n",
      "Step  54940 (epoch   17.17), loss: 0.017297, time: 2.230 s\n",
      "Step  54960 (epoch   17.18), loss: 0.009672, time: 2.239 s\n",
      "Step  54980 (epoch   17.18), loss: 0.013023, time: 2.227 s\n",
      "Step  55000 (epoch   17.19), loss: 0.007927, time: 2.240 s, error: 0.016220\n",
      "Step  55020 (epoch   17.19), loss: 0.009338, time: 14.654 s\n",
      "Step  55040 (epoch   17.20), loss: 0.008379, time: 2.243 s\n",
      "Step  55060 (epoch   17.21), loss: 0.007949, time: 2.236 s\n",
      "Step  55080 (epoch   17.21), loss: 0.006111, time: 2.220 s\n",
      "Step  55100 (epoch   17.22), loss: 0.013365, time: 2.224 s\n",
      "Step  55120 (epoch   17.23), loss: 0.009358, time: 2.224 s\n",
      "Step  55140 (epoch   17.23), loss: 0.008992, time: 2.232 s\n",
      "Step  55160 (epoch   17.24), loss: 0.008673, time: 2.242 s\n",
      "Step  55180 (epoch   17.24), loss: 0.014386, time: 2.242 s\n",
      "Step  55200 (epoch   17.25), loss: 0.009423, time: 2.236 s, error: 0.017180\n",
      "Step  55220 (epoch   17.26), loss: 0.009285, time: 14.667 s\n",
      "Step  55240 (epoch   17.26), loss: 0.015946, time: 2.222 s\n",
      "Step  55260 (epoch   17.27), loss: 0.009204, time: 2.237 s\n",
      "Step  55280 (epoch   17.27), loss: 0.007937, time: 2.227 s\n",
      "Step  55300 (epoch   17.28), loss: 0.008034, time: 2.229 s\n",
      "Step  55320 (epoch   17.29), loss: 0.012107, time: 2.244 s\n",
      "Step  55340 (epoch   17.29), loss: 0.008922, time: 2.232 s\n",
      "Step  55360 (epoch   17.30), loss: 0.008812, time: 2.226 s\n",
      "Step  55380 (epoch   17.31), loss: 0.009225, time: 2.222 s\n",
      "Step  55400 (epoch   17.31), loss: 0.007384, time: 2.233 s, error: 0.015604\n",
      "Step  55420 (epoch   17.32), loss: 0.007784, time: 14.656 s\n",
      "Step  55440 (epoch   17.32), loss: 0.014377, time: 2.225 s\n",
      "Step  55460 (epoch   17.33), loss: 0.013949, time: 2.234 s\n",
      "Step  55480 (epoch   17.34), loss: 0.010367, time: 2.227 s\n",
      "Step  55500 (epoch   17.34), loss: 0.013311, time: 2.239 s\n",
      "Step  55520 (epoch   17.35), loss: 0.018357, time: 2.236 s\n",
      "Step  55540 (epoch   17.36), loss: 0.010639, time: 2.226 s\n",
      "Step  55560 (epoch   17.36), loss: 0.006614, time: 2.225 s\n",
      "Step  55580 (epoch   17.37), loss: 0.008216, time: 2.240 s\n",
      "Step  55600 (epoch   17.38), loss: 0.007463, time: 2.235 s, error: 0.015977\n",
      "Step  55620 (epoch   17.38), loss: 0.010918, time: 14.616 s\n",
      "Step  55640 (epoch   17.39), loss: 0.009213, time: 2.224 s\n",
      "Step  55660 (epoch   17.39), loss: 0.010953, time: 2.221 s\n",
      "Step  55680 (epoch   17.40), loss: 0.012803, time: 2.217 s\n",
      "Step  55700 (epoch   17.41), loss: 0.017341, time: 2.238 s\n",
      "Step  55720 (epoch   17.41), loss: 0.009082, time: 2.239 s\n",
      "Step  55740 (epoch   17.42), loss: 0.018167, time: 2.239 s\n",
      "Step  55760 (epoch   17.43), loss: 0.008010, time: 2.239 s\n",
      "Step  55780 (epoch   17.43), loss: 0.006654, time: 2.228 s\n",
      "Step  55800 (epoch   17.44), loss: 0.010675, time: 2.233 s, error: 0.016014\n",
      "Step  55820 (epoch   17.44), loss: 0.017312, time: 14.866 s\n",
      "Step  55840 (epoch   17.45), loss: 0.006794, time: 2.232 s\n",
      "Step  55860 (epoch   17.46), loss: 0.007461, time: 2.233 s\n",
      "Step  55880 (epoch   17.46), loss: 0.008535, time: 2.228 s\n",
      "Step  55900 (epoch   17.47), loss: 0.008667, time: 2.228 s\n",
      "Step  55920 (epoch   17.48), loss: 0.008883, time: 2.231 s\n",
      "Step  55940 (epoch   17.48), loss: 0.009829, time: 2.221 s\n",
      "Step  55960 (epoch   17.49), loss: 0.010216, time: 2.226 s\n",
      "Step  55980 (epoch   17.49), loss: 0.009238, time: 2.234 s\n",
      "Step  56000 (epoch   17.50), loss: 0.009997, time: 2.210 s, error: 0.017585\n",
      "\n",
      "Time since beginning  : 9630.933 s\n",
      "\n",
      "Step  56020 (epoch   17.51), loss: 0.007461, time: 14.870 s\n",
      "Step  56040 (epoch   17.51), loss: 0.015146, time: 2.234 s\n",
      "Step  56060 (epoch   17.52), loss: 0.010352, time: 2.218 s\n",
      "Step  56080 (epoch   17.52), loss: 0.008837, time: 2.213 s\n",
      "Step  56100 (epoch   17.53), loss: 0.008128, time: 2.229 s\n",
      "Step  56120 (epoch   17.54), loss: 0.010210, time: 2.220 s\n",
      "Step  56140 (epoch   17.54), loss: 0.012701, time: 2.231 s\n",
      "Step  56160 (epoch   17.55), loss: 0.011406, time: 2.213 s\n",
      "Step  56180 (epoch   17.56), loss: 0.008152, time: 2.231 s\n",
      "Step  56200 (epoch   17.56), loss: 0.007133, time: 2.215 s, error: 0.015647\n",
      "Step  56220 (epoch   17.57), loss: 0.011576, time: 14.616 s\n",
      "Step  56240 (epoch   17.57), loss: 0.013966, time: 2.226 s\n",
      "Step  56260 (epoch   17.58), loss: 0.011545, time: 2.227 s\n",
      "Step  56280 (epoch   17.59), loss: 0.010782, time: 2.243 s\n",
      "Step  56300 (epoch   17.59), loss: 0.009544, time: 2.227 s\n",
      "Step  56320 (epoch   17.60), loss: 0.005685, time: 2.239 s\n",
      "Step  56340 (epoch   17.61), loss: 0.009240, time: 2.231 s\n",
      "Step  56360 (epoch   17.61), loss: 0.014326, time: 2.226 s\n",
      "Step  56380 (epoch   17.62), loss: 0.013205, time: 2.223 s\n",
      "Step  56400 (epoch   17.62), loss: 0.007355, time: 2.230 s, error: 0.015493\n",
      "Step  56420 (epoch   17.63), loss: 0.020229, time: 14.628 s\n",
      "Step  56440 (epoch   17.64), loss: 0.008665, time: 2.224 s\n",
      "Step  56460 (epoch   17.64), loss: 0.011797, time: 2.230 s\n",
      "Step  56480 (epoch   17.65), loss: 0.012668, time: 2.230 s\n",
      "Step  56500 (epoch   17.66), loss: 0.011500, time: 2.232 s\n",
      "Step  56520 (epoch   17.66), loss: 0.006582, time: 2.235 s\n",
      "Step  56540 (epoch   17.67), loss: 0.008166, time: 2.247 s\n",
      "Step  56560 (epoch   17.68), loss: 0.010468, time: 2.229 s\n",
      "Step  56580 (epoch   17.68), loss: 0.008144, time: 2.217 s\n",
      "Step  56600 (epoch   17.69), loss: 0.010072, time: 2.227 s, error: 0.015900\n",
      "Step  56620 (epoch   17.69), loss: 0.014320, time: 14.596 s\n",
      "Step  56640 (epoch   17.70), loss: 0.006858, time: 2.220 s\n",
      "Step  56660 (epoch   17.71), loss: 0.013141, time: 2.228 s\n",
      "Step  56680 (epoch   17.71), loss: 0.011309, time: 2.214 s\n",
      "Step  56700 (epoch   17.72), loss: 0.012070, time: 2.224 s\n",
      "Step  56720 (epoch   17.73), loss: 0.011140, time: 2.223 s\n",
      "Step  56740 (epoch   17.73), loss: 0.013644, time: 2.218 s\n",
      "Step  56760 (epoch   17.74), loss: 0.008274, time: 2.219 s\n",
      "Step  56780 (epoch   17.74), loss: 0.010950, time: 2.224 s\n",
      "Step  56800 (epoch   17.75), loss: 0.007700, time: 2.239 s, error: 0.015448\n",
      "Step  56820 (epoch   17.76), loss: 0.010230, time: 14.678 s\n",
      "Step  56840 (epoch   17.76), loss: 0.011009, time: 2.228 s\n",
      "Step  56860 (epoch   17.77), loss: 0.007494, time: 2.242 s\n",
      "Step  56880 (epoch   17.77), loss: 0.011006, time: 2.231 s\n",
      "Step  56900 (epoch   17.78), loss: 0.014712, time: 2.230 s\n",
      "Step  56920 (epoch   17.79), loss: 0.008704, time: 2.221 s\n",
      "Step  56940 (epoch   17.79), loss: 0.012728, time: 2.240 s\n",
      "Step  56960 (epoch   17.80), loss: 0.008067, time: 2.226 s\n",
      "Step  56980 (epoch   17.81), loss: 0.006907, time: 2.230 s\n",
      "Step  57000 (epoch   17.81), loss: 0.011832, time: 2.221 s, error: 0.016351\n",
      "Step  57020 (epoch   17.82), loss: 0.010484, time: 14.844 s\n",
      "Step  57040 (epoch   17.82), loss: 0.012089, time: 2.237 s\n",
      "Step  57060 (epoch   17.83), loss: 0.007449, time: 2.227 s\n",
      "Step  57080 (epoch   17.84), loss: 0.012587, time: 2.221 s\n",
      "Step  57100 (epoch   17.84), loss: 0.009069, time: 2.214 s\n",
      "Step  57120 (epoch   17.85), loss: 0.008216, time: 2.226 s\n",
      "Step  57140 (epoch   17.86), loss: 0.015354, time: 2.235 s\n",
      "Step  57160 (epoch   17.86), loss: 0.008484, time: 2.226 s\n",
      "Step  57180 (epoch   17.87), loss: 0.009870, time: 2.230 s\n",
      "Step  57200 (epoch   17.88), loss: 0.017852, time: 2.228 s, error: 0.016099\n",
      "Step  57220 (epoch   17.88), loss: 0.007198, time: 14.632 s\n",
      "Step  57240 (epoch   17.89), loss: 0.010371, time: 2.239 s\n",
      "Step  57260 (epoch   17.89), loss: 0.011288, time: 2.216 s\n",
      "Step  57280 (epoch   17.90), loss: 0.011012, time: 2.225 s\n",
      "Step  57300 (epoch   17.91), loss: 0.009747, time: 2.228 s\n",
      "Step  57320 (epoch   17.91), loss: 0.007172, time: 2.220 s\n",
      "Step  57340 (epoch   17.92), loss: 0.009653, time: 2.232 s\n",
      "Step  57360 (epoch   17.93), loss: 0.012507, time: 2.240 s\n",
      "Step  57380 (epoch   17.93), loss: 0.014293, time: 2.239 s\n",
      "Step  57400 (epoch   17.94), loss: 0.011236, time: 2.232 s, error: 0.016090\n",
      "Step  57420 (epoch   17.94), loss: 0.010199, time: 14.637 s\n",
      "Step  57440 (epoch   17.95), loss: 0.006428, time: 2.221 s\n",
      "Step  57460 (epoch   17.96), loss: 0.009457, time: 2.238 s\n",
      "Step  57480 (epoch   17.96), loss: 0.006789, time: 2.236 s\n",
      "Step  57500 (epoch   17.97), loss: 0.011978, time: 2.246 s\n",
      "Step  57520 (epoch   17.98), loss: 0.008184, time: 2.225 s\n",
      "Step  57540 (epoch   17.98), loss: 0.011488, time: 2.226 s\n",
      "Step  57560 (epoch   17.99), loss: 0.015774, time: 2.222 s\n",
      "Step  57580 (epoch   17.99), loss: 0.011569, time: 2.236 s\n",
      "Step  57600 (epoch   18.00), loss: 0.009309, time: 2.229 s, error: 0.016585\n",
      "Step  57620 (epoch   18.01), loss: 0.012665, time: 14.631 s\n",
      "Step  57640 (epoch   18.01), loss: 0.008442, time: 2.224 s\n",
      "Step  57660 (epoch   18.02), loss: 0.010481, time: 2.235 s\n",
      "Step  57680 (epoch   18.02), loss: 0.009532, time: 2.227 s\n",
      "Step  57700 (epoch   18.03), loss: 0.009578, time: 2.237 s\n",
      "Step  57720 (epoch   18.04), loss: 0.012312, time: 2.224 s\n",
      "Step  57740 (epoch   18.04), loss: 0.014733, time: 2.228 s\n",
      "Step  57760 (epoch   18.05), loss: 0.012521, time: 2.243 s\n",
      "Step  57780 (epoch   18.06), loss: 0.013757, time: 2.238 s\n",
      "Step  57800 (epoch   18.06), loss: 0.007816, time: 2.229 s, error: 0.015364\n",
      "Step  57820 (epoch   18.07), loss: 0.008180, time: 14.666 s\n",
      "Step  57840 (epoch   18.07), loss: 0.007940, time: 2.230 s\n",
      "Step  57860 (epoch   18.08), loss: 0.018887, time: 2.228 s\n",
      "Step  57880 (epoch   18.09), loss: 0.006106, time: 2.224 s\n",
      "Step  57900 (epoch   18.09), loss: 0.021051, time: 2.236 s\n",
      "Step  57920 (epoch   18.10), loss: 0.009749, time: 2.222 s\n",
      "Step  57940 (epoch   18.11), loss: 0.007365, time: 2.238 s\n",
      "Step  57960 (epoch   18.11), loss: 0.008357, time: 2.236 s\n",
      "Step  57980 (epoch   18.12), loss: 0.014982, time: 2.239 s\n",
      "Step  58000 (epoch   18.12), loss: 0.011854, time: 2.229 s, error: 0.015116\n",
      "\n",
      "Time since beginning  : 9978.349 s\n",
      "\n",
      "Step  58020 (epoch   18.13), loss: 0.007492, time: 14.936 s\n",
      "Step  58040 (epoch   18.14), loss: 0.010883, time: 2.226 s\n",
      "Step  58060 (epoch   18.14), loss: 0.006968, time: 2.239 s\n",
      "Step  58080 (epoch   18.15), loss: 0.005738, time: 2.240 s\n",
      "Step  58100 (epoch   18.16), loss: 0.008152, time: 2.225 s\n",
      "Step  58120 (epoch   18.16), loss: 0.006399, time: 2.243 s\n",
      "Step  58140 (epoch   18.17), loss: 0.010982, time: 2.236 s\n",
      "Step  58160 (epoch   18.18), loss: 0.007164, time: 2.244 s\n",
      "Step  58180 (epoch   18.18), loss: 0.007655, time: 2.227 s\n",
      "Step  58200 (epoch   18.19), loss: 0.008253, time: 2.236 s, error: 0.015497\n",
      "Step  58220 (epoch   18.19), loss: 0.007879, time: 15.164 s\n",
      "Step  58240 (epoch   18.20), loss: 0.008211, time: 2.248 s\n",
      "Step  58260 (epoch   18.21), loss: 0.009692, time: 2.248 s\n",
      "Step  58280 (epoch   18.21), loss: 0.008697, time: 2.238 s\n",
      "Step  58300 (epoch   18.22), loss: 0.007252, time: 2.243 s\n",
      "Step  58320 (epoch   18.23), loss: 0.011165, time: 2.230 s\n",
      "Step  58340 (epoch   18.23), loss: 0.006607, time: 2.243 s\n",
      "Step  58360 (epoch   18.24), loss: 0.010567, time: 2.250 s\n",
      "Step  58380 (epoch   18.24), loss: 0.008394, time: 2.236 s\n",
      "Step  58400 (epoch   18.25), loss: 0.010676, time: 2.247 s, error: 0.015442\n",
      "Step  58420 (epoch   18.26), loss: 0.012359, time: 14.945 s\n",
      "Step  58440 (epoch   18.26), loss: 0.007911, time: 2.243 s\n",
      "Step  58460 (epoch   18.27), loss: 0.007705, time: 2.242 s\n",
      "Step  58480 (epoch   18.27), loss: 0.010106, time: 2.226 s\n",
      "Step  58500 (epoch   18.28), loss: 0.008256, time: 2.242 s\n",
      "Step  58520 (epoch   18.29), loss: 0.011915, time: 2.243 s\n",
      "Step  58540 (epoch   18.29), loss: 0.008168, time: 2.244 s\n",
      "Step  58560 (epoch   18.30), loss: 0.012070, time: 2.228 s\n",
      "Step  58580 (epoch   18.31), loss: 0.009366, time: 2.238 s\n",
      "Step  58600 (epoch   18.31), loss: 0.009326, time: 2.238 s, error: 0.016674\n",
      "Step  58620 (epoch   18.32), loss: 0.006954, time: 14.969 s\n",
      "Step  58640 (epoch   18.32), loss: 0.008495, time: 2.237 s\n",
      "Step  58660 (epoch   18.33), loss: 0.009489, time: 2.238 s\n",
      "Step  58680 (epoch   18.34), loss: 0.012144, time: 2.243 s\n",
      "Step  58700 (epoch   18.34), loss: 0.010029, time: 2.251 s\n",
      "Step  58720 (epoch   18.35), loss: 0.010132, time: 2.246 s\n",
      "Step  58740 (epoch   18.36), loss: 0.008904, time: 2.235 s\n",
      "Step  58760 (epoch   18.36), loss: 0.006810, time: 2.239 s\n",
      "Step  58780 (epoch   18.37), loss: 0.008819, time: 2.229 s\n",
      "Step  58800 (epoch   18.38), loss: 0.007047, time: 2.242 s, error: 0.015322\n",
      "Step  58820 (epoch   18.38), loss: 0.016707, time: 15.060 s\n",
      "Step  58840 (epoch   18.39), loss: 0.017173, time: 2.237 s\n",
      "Step  58860 (epoch   18.39), loss: 0.010370, time: 2.249 s\n",
      "Step  58880 (epoch   18.40), loss: 0.019852, time: 2.233 s\n",
      "Step  58900 (epoch   18.41), loss: 0.010764, time: 2.236 s\n",
      "Step  58920 (epoch   18.41), loss: 0.009997, time: 2.240 s\n",
      "Step  58940 (epoch   18.42), loss: 0.010653, time: 2.240 s\n",
      "Step  58960 (epoch   18.43), loss: 0.008087, time: 2.241 s\n",
      "Step  58980 (epoch   18.43), loss: 0.013753, time: 2.250 s\n",
      "Step  59000 (epoch   18.44), loss: 0.008540, time: 2.240 s, error: 0.015595\n",
      "Step  59020 (epoch   18.44), loss: 0.010606, time: 14.931 s\n",
      "Step  59040 (epoch   18.45), loss: 0.013107, time: 2.247 s\n",
      "Step  59060 (epoch   18.46), loss: 0.008018, time: 2.253 s\n",
      "Step  59080 (epoch   18.46), loss: 0.009161, time: 2.250 s\n",
      "Step  59100 (epoch   18.47), loss: 0.008155, time: 2.236 s\n",
      "Step  59120 (epoch   18.48), loss: 0.008303, time: 2.235 s\n",
      "Step  59140 (epoch   18.48), loss: 0.012291, time: 2.238 s\n",
      "Step  59160 (epoch   18.49), loss: 0.014374, time: 2.244 s\n",
      "Step  59180 (epoch   18.49), loss: 0.030288, time: 2.244 s\n",
      "Step  59200 (epoch   18.50), loss: 0.010141, time: 2.240 s, error: 0.017359\n",
      "Step  59220 (epoch   18.51), loss: 0.007853, time: 15.113 s\n",
      "Step  59240 (epoch   18.51), loss: 0.011433, time: 2.223 s\n",
      "Step  59260 (epoch   18.52), loss: 0.022599, time: 2.248 s\n",
      "Step  59280 (epoch   18.52), loss: 0.010040, time: 2.251 s\n",
      "Step  59300 (epoch   18.53), loss: 0.009356, time: 2.240 s\n",
      "Step  59320 (epoch   18.54), loss: 0.009887, time: 2.236 s\n",
      "Step  59340 (epoch   18.54), loss: 0.014405, time: 2.237 s\n",
      "Step  59360 (epoch   18.55), loss: 0.019396, time: 2.239 s\n",
      "Step  59380 (epoch   18.56), loss: 0.014889, time: 2.226 s\n",
      "Step  59400 (epoch   18.56), loss: 0.009080, time: 2.241 s, error: 0.016259\n",
      "Step  59420 (epoch   18.57), loss: 0.012662, time: 15.154 s\n",
      "Step  59440 (epoch   18.57), loss: 0.008584, time: 2.251 s\n",
      "Step  59460 (epoch   18.58), loss: 0.007346, time: 2.240 s\n",
      "Step  59480 (epoch   18.59), loss: 0.008190, time: 2.240 s\n",
      "Step  59500 (epoch   18.59), loss: 0.007401, time: 2.236 s\n",
      "Step  59520 (epoch   18.60), loss: 0.014037, time: 2.238 s\n",
      "Step  59540 (epoch   18.61), loss: 0.005818, time: 2.251 s\n",
      "Step  59560 (epoch   18.61), loss: 0.007334, time: 2.241 s\n",
      "Step  59580 (epoch   18.62), loss: 0.011497, time: 2.233 s\n",
      "Step  59600 (epoch   18.62), loss: 0.009439, time: 2.250 s, error: 0.015062\n",
      "Step  59620 (epoch   18.63), loss: 0.007161, time: 14.958 s\n",
      "Step  59640 (epoch   18.64), loss: 0.006996, time: 2.255 s\n",
      "Step  59660 (epoch   18.64), loss: 0.007187, time: 2.251 s\n",
      "Step  59680 (epoch   18.65), loss: 0.009145, time: 2.237 s\n",
      "Step  59700 (epoch   18.66), loss: 0.008836, time: 2.240 s\n",
      "Step  59720 (epoch   18.66), loss: 0.007852, time: 2.231 s\n",
      "Step  59740 (epoch   18.67), loss: 0.006904, time: 2.236 s\n",
      "Step  59760 (epoch   18.68), loss: 0.011179, time: 2.244 s\n",
      "Step  59780 (epoch   18.68), loss: 0.008620, time: 2.237 s\n",
      "Step  59800 (epoch   18.69), loss: 0.009750, time: 2.241 s, error: 0.015606\n",
      "Step  59820 (epoch   18.69), loss: 0.008000, time: 14.978 s\n",
      "Step  59840 (epoch   18.70), loss: 0.008686, time: 2.236 s\n",
      "Step  59860 (epoch   18.71), loss: 0.007958, time: 2.237 s\n",
      "Step  59880 (epoch   18.71), loss: 0.011278, time: 2.237 s\n",
      "Step  59900 (epoch   18.72), loss: 0.006853, time: 2.247 s\n",
      "Step  59920 (epoch   18.73), loss: 0.009714, time: 2.241 s\n",
      "Step  59940 (epoch   18.73), loss: 0.007881, time: 2.225 s\n",
      "Step  59960 (epoch   18.74), loss: 0.009036, time: 2.225 s\n",
      "Step  59980 (epoch   18.74), loss: 0.011760, time: 2.238 s\n",
      "Step  60000 (epoch   18.75), loss: 0.009784, time: 2.246 s, error: 0.015060\n",
      "\n",
      "Time since beginning  : 10330.280 s\n",
      "\n",
      "Step  60020 (epoch   18.76), loss: 0.008311, time: 15.056 s\n",
      "Step  60040 (epoch   18.76), loss: 0.010113, time: 2.242 s\n",
      "Step  60060 (epoch   18.77), loss: 0.006770, time: 2.241 s\n",
      "Step  60080 (epoch   18.77), loss: 0.011729, time: 2.239 s\n",
      "Step  60100 (epoch   18.78), loss: 0.013476, time: 2.244 s\n",
      "Step  60120 (epoch   18.79), loss: 0.007156, time: 2.230 s\n",
      "Step  60140 (epoch   18.79), loss: 0.008279, time: 2.240 s\n",
      "Step  60160 (epoch   18.80), loss: 0.008888, time: 2.250 s\n",
      "Step  60180 (epoch   18.81), loss: 0.010698, time: 2.237 s\n",
      "Step  60200 (epoch   18.81), loss: 0.013868, time: 2.237 s, error: 0.019043\n",
      "Step  60220 (epoch   18.82), loss: 0.018844, time: 14.980 s\n",
      "Step  60240 (epoch   18.82), loss: 0.013004, time: 2.230 s\n",
      "Step  60260 (epoch   18.83), loss: 0.009810, time: 2.224 s\n",
      "Step  60280 (epoch   18.84), loss: 0.008918, time: 2.228 s\n",
      "Step  60300 (epoch   18.84), loss: 0.007433, time: 2.228 s\n",
      "Step  60320 (epoch   18.85), loss: 0.008040, time: 2.245 s\n",
      "Step  60340 (epoch   18.86), loss: 0.011511, time: 2.236 s\n",
      "Step  60360 (epoch   18.86), loss: 0.010872, time: 2.242 s\n",
      "Step  60380 (epoch   18.87), loss: 0.007121, time: 2.234 s\n",
      "Step  60400 (epoch   18.88), loss: 0.010353, time: 2.245 s, error: 0.015920\n",
      "Step  60420 (epoch   18.88), loss: 0.010073, time: 15.160 s\n",
      "Step  60440 (epoch   18.89), loss: 0.010739, time: 2.246 s\n",
      "Step  60460 (epoch   18.89), loss: 0.007602, time: 2.241 s\n",
      "Step  60480 (epoch   18.90), loss: 0.010065, time: 2.246 s\n",
      "Step  60500 (epoch   18.91), loss: 0.011464, time: 2.248 s\n",
      "Step  60520 (epoch   18.91), loss: 0.008503, time: 2.241 s\n",
      "Step  60540 (epoch   18.92), loss: 0.019530, time: 2.229 s\n",
      "Step  60560 (epoch   18.93), loss: 0.009909, time: 2.241 s\n",
      "Step  60580 (epoch   18.93), loss: 0.014114, time: 2.236 s\n",
      "Step  60600 (epoch   18.94), loss: 0.008761, time: 2.249 s, error: 0.016027\n",
      "Step  60620 (epoch   18.94), loss: 0.012784, time: 15.185 s\n",
      "Step  60640 (epoch   18.95), loss: 0.008680, time: 2.247 s\n",
      "Step  60660 (epoch   18.96), loss: 0.007992, time: 2.241 s\n",
      "Step  60680 (epoch   18.96), loss: 0.008261, time: 2.240 s\n",
      "Step  60700 (epoch   18.97), loss: 0.014973, time: 2.241 s\n",
      "Step  60720 (epoch   18.98), loss: 0.013016, time: 2.239 s\n",
      "Step  60740 (epoch   18.98), loss: 0.008976, time: 2.237 s\n",
      "Step  60760 (epoch   18.99), loss: 0.011510, time: 2.238 s\n",
      "Step  60780 (epoch   18.99), loss: 0.009654, time: 2.246 s\n",
      "Step  60800 (epoch   19.00), loss: 0.007825, time: 2.244 s, error: 0.015769\n",
      "Step  60820 (epoch   19.01), loss: 0.007431, time: 15.055 s\n",
      "Step  60840 (epoch   19.01), loss: 0.007417, time: 2.254 s\n",
      "Step  60860 (epoch   19.02), loss: 0.006619, time: 2.240 s\n",
      "Step  60880 (epoch   19.02), loss: 0.011579, time: 2.229 s\n",
      "Step  60900 (epoch   19.03), loss: 0.009828, time: 2.239 s\n",
      "Step  60920 (epoch   19.04), loss: 0.011790, time: 2.221 s\n",
      "Step  60940 (epoch   19.04), loss: 0.011468, time: 2.220 s\n",
      "Step  60960 (epoch   19.05), loss: 0.013892, time: 2.234 s\n",
      "Step  60980 (epoch   19.06), loss: 0.011217, time: 2.225 s\n",
      "Step  61000 (epoch   19.06), loss: 0.007115, time: 2.243 s, error: 0.015452\n",
      "Step  61020 (epoch   19.07), loss: 0.008583, time: 14.941 s\n",
      "Step  61040 (epoch   19.07), loss: 0.007947, time: 2.239 s\n",
      "Step  61060 (epoch   19.08), loss: 0.005580, time: 2.235 s\n",
      "Step  61080 (epoch   19.09), loss: 0.015982, time: 2.230 s\n",
      "Step  61100 (epoch   19.09), loss: 0.009223, time: 2.242 s\n",
      "Step  61120 (epoch   19.10), loss: 0.006278, time: 2.240 s\n",
      "Step  61140 (epoch   19.11), loss: 0.009344, time: 2.239 s\n",
      "Step  61160 (epoch   19.11), loss: 0.009164, time: 2.233 s\n",
      "Step  61180 (epoch   19.12), loss: 0.012390, time: 2.241 s\n",
      "Step  61200 (epoch   19.12), loss: 0.004983, time: 2.239 s, error: 0.014829\n",
      "Step  61220 (epoch   19.13), loss: 0.005973, time: 14.988 s\n",
      "Step  61240 (epoch   19.14), loss: 0.009104, time: 2.243 s\n",
      "Step  61260 (epoch   19.14), loss: 0.007262, time: 2.238 s\n",
      "Step  61280 (epoch   19.15), loss: 0.013580, time: 2.241 s\n",
      "Step  61300 (epoch   19.16), loss: 0.009319, time: 2.238 s\n",
      "Step  61320 (epoch   19.16), loss: 0.008228, time: 2.235 s\n",
      "Step  61340 (epoch   19.17), loss: 0.007726, time: 2.246 s\n",
      "Step  61360 (epoch   19.18), loss: 0.016828, time: 2.240 s\n",
      "Step  61380 (epoch   19.18), loss: 0.009368, time: 2.236 s\n",
      "Step  61400 (epoch   19.19), loss: 0.008309, time: 2.239 s, error: 0.015125\n",
      "Step  61420 (epoch   19.19), loss: 0.008603, time: 14.946 s\n",
      "Step  61440 (epoch   19.20), loss: 0.008046, time: 2.240 s\n",
      "Step  61460 (epoch   19.21), loss: 0.007104, time: 2.229 s\n",
      "Step  61480 (epoch   19.21), loss: 0.008661, time: 2.236 s\n",
      "Step  61500 (epoch   19.22), loss: 0.010149, time: 2.248 s\n",
      "Step  61520 (epoch   19.23), loss: 0.008487, time: 2.240 s\n",
      "Step  61540 (epoch   19.23), loss: 0.007346, time: 2.233 s\n",
      "Step  61560 (epoch   19.24), loss: 0.008112, time: 2.239 s\n",
      "Step  61580 (epoch   19.24), loss: 0.012070, time: 2.243 s\n",
      "Step  61600 (epoch   19.25), loss: 0.008611, time: 2.238 s, error: 0.015122\n",
      "Step  61620 (epoch   19.26), loss: 0.012790, time: 15.281 s\n",
      "Step  61640 (epoch   19.26), loss: 0.009716, time: 2.244 s\n",
      "Step  61660 (epoch   19.27), loss: 0.008375, time: 2.235 s\n",
      "Step  61680 (epoch   19.27), loss: 0.009743, time: 2.242 s\n",
      "Step  61700 (epoch   19.28), loss: 0.007722, time: 2.235 s\n",
      "Step  61720 (epoch   19.29), loss: 0.012683, time: 2.230 s\n",
      "Step  61740 (epoch   19.29), loss: 0.010959, time: 2.243 s\n",
      "Step  61760 (epoch   19.30), loss: 0.009019, time: 2.224 s\n",
      "Step  61780 (epoch   19.31), loss: 0.008992, time: 2.228 s\n",
      "Step  61800 (epoch   19.31), loss: 0.011957, time: 2.245 s, error: 0.018224\n",
      "Step  61820 (epoch   19.32), loss: 0.013033, time: 15.113 s\n",
      "Step  61840 (epoch   19.32), loss: 0.007870, time: 2.234 s\n",
      "Step  61860 (epoch   19.33), loss: 0.006869, time: 2.246 s\n",
      "Step  61880 (epoch   19.34), loss: 0.008551, time: 2.249 s\n",
      "Step  61900 (epoch   19.34), loss: 0.007272, time: 2.238 s\n",
      "Step  61920 (epoch   19.35), loss: 0.016745, time: 2.229 s\n",
      "Step  61940 (epoch   19.36), loss: 0.007888, time: 2.231 s\n",
      "Step  61960 (epoch   19.36), loss: 0.010910, time: 2.228 s\n",
      "Step  61980 (epoch   19.37), loss: 0.008525, time: 2.238 s\n",
      "Step  62000 (epoch   19.38), loss: 0.010619, time: 2.234 s, error: 0.014807\n",
      "\n",
      "Time since beginning  : 10682.475 s\n",
      "\n",
      "Step  62020 (epoch   19.38), loss: 0.008858, time: 15.139 s\n",
      "Step  62040 (epoch   19.39), loss: 0.006357, time: 2.245 s\n",
      "Step  62060 (epoch   19.39), loss: 0.007911, time: 2.236 s\n",
      "Step  62080 (epoch   19.40), loss: 0.009637, time: 2.234 s\n",
      "Step  62100 (epoch   19.41), loss: 0.010717, time: 2.235 s\n",
      "Step  62120 (epoch   19.41), loss: 0.012015, time: 2.242 s\n",
      "Step  62140 (epoch   19.42), loss: 0.009601, time: 2.225 s\n",
      "Step  62160 (epoch   19.43), loss: 0.013290, time: 2.222 s\n",
      "Step  62180 (epoch   19.43), loss: 0.016799, time: 2.239 s\n",
      "Step  62200 (epoch   19.44), loss: 0.008352, time: 2.241 s, error: 0.015365\n",
      "Step  62220 (epoch   19.44), loss: 0.007972, time: 14.946 s\n",
      "Step  62240 (epoch   19.45), loss: 0.007072, time: 2.236 s\n",
      "Step  62260 (epoch   19.46), loss: 0.006517, time: 2.241 s\n",
      "Step  62280 (epoch   19.46), loss: 0.032519, time: 2.236 s\n",
      "Step  62300 (epoch   19.47), loss: 0.008644, time: 2.253 s\n",
      "Step  62320 (epoch   19.48), loss: 0.005961, time: 2.250 s\n",
      "Step  62340 (epoch   19.48), loss: 0.008712, time: 2.236 s\n",
      "Step  62360 (epoch   19.49), loss: 0.007753, time: 2.246 s\n",
      "Step  62380 (epoch   19.49), loss: 0.006655, time: 2.237 s\n",
      "Step  62400 (epoch   19.50), loss: 0.015137, time: 2.235 s, error: 0.016889\n",
      "Step  62420 (epoch   19.51), loss: 0.010606, time: 15.069 s\n",
      "Step  62440 (epoch   19.51), loss: 0.010435, time: 2.236 s\n",
      "Step  62460 (epoch   19.52), loss: 0.009031, time: 2.236 s\n",
      "Step  62480 (epoch   19.52), loss: 0.007877, time: 2.242 s\n",
      "Step  62500 (epoch   19.53), loss: 0.009706, time: 2.233 s\n",
      "Step  62520 (epoch   19.54), loss: 0.005317, time: 2.237 s\n",
      "Step  62540 (epoch   19.54), loss: 0.006784, time: 2.230 s\n",
      "Step  62560 (epoch   19.55), loss: 0.008055, time: 2.250 s\n",
      "Step  62580 (epoch   19.56), loss: 0.018800, time: 2.241 s\n",
      "Step  62600 (epoch   19.56), loss: 0.009091, time: 2.241 s, error: 0.016067\n",
      "Step  62620 (epoch   19.57), loss: 0.008558, time: 14.977 s\n",
      "Step  62640 (epoch   19.57), loss: 0.008488, time: 2.246 s\n",
      "Step  62660 (epoch   19.58), loss: 0.006870, time: 2.243 s\n",
      "Step  62680 (epoch   19.59), loss: 0.007926, time: 2.243 s\n",
      "Step  62700 (epoch   19.59), loss: 0.006689, time: 2.238 s\n",
      "Step  62720 (epoch   19.60), loss: 0.007291, time: 2.241 s\n",
      "Step  62740 (epoch   19.61), loss: 0.011946, time: 2.233 s\n",
      "Step  62760 (epoch   19.61), loss: 0.009760, time: 2.246 s\n",
      "Step  62780 (epoch   19.62), loss: 0.010725, time: 2.237 s\n",
      "Step  62800 (epoch   19.62), loss: 0.013329, time: 2.232 s, error: 0.014821\n",
      "Step  62820 (epoch   19.63), loss: 0.007416, time: 15.199 s\n",
      "Step  62840 (epoch   19.64), loss: 0.011435, time: 2.250 s\n",
      "Step  62860 (epoch   19.64), loss: 0.008023, time: 2.232 s\n",
      "Step  62880 (epoch   19.65), loss: 0.007521, time: 2.233 s\n",
      "Step  62900 (epoch   19.66), loss: 0.010413, time: 2.236 s\n",
      "Step  62920 (epoch   19.66), loss: 0.014033, time: 2.241 s\n",
      "Step  62940 (epoch   19.67), loss: 0.006188, time: 2.238 s\n",
      "Step  62960 (epoch   19.68), loss: 0.011137, time: 2.240 s\n",
      "Step  62980 (epoch   19.68), loss: 0.008214, time: 2.227 s\n",
      "Step  63000 (epoch   19.69), loss: 0.009178, time: 2.237 s, error: 0.015400\n",
      "Step  63020 (epoch   19.69), loss: 0.009834, time: 15.159 s\n",
      "Step  63040 (epoch   19.70), loss: 0.006233, time: 2.227 s\n",
      "Step  63060 (epoch   19.71), loss: 0.011197, time: 2.243 s\n",
      "Step  63080 (epoch   19.71), loss: 0.007795, time: 2.233 s\n",
      "Step  63100 (epoch   19.72), loss: 0.011725, time: 2.245 s\n",
      "Step  63120 (epoch   19.73), loss: 0.007786, time: 2.245 s\n",
      "Step  63140 (epoch   19.73), loss: 0.006988, time: 2.231 s\n",
      "Step  63160 (epoch   19.74), loss: 0.006422, time: 2.222 s\n",
      "Step  63180 (epoch   19.74), loss: 0.009196, time: 2.246 s\n",
      "Step  63200 (epoch   19.75), loss: 0.008444, time: 2.246 s, error: 0.014696\n",
      "Step  63220 (epoch   19.76), loss: 0.007801, time: 15.041 s\n",
      "Step  63240 (epoch   19.76), loss: 0.014409, time: 2.249 s\n",
      "Step  63260 (epoch   19.77), loss: 0.008994, time: 2.234 s\n",
      "Step  63280 (epoch   19.77), loss: 0.008292, time: 2.234 s\n",
      "Step  63300 (epoch   19.78), loss: 0.007488, time: 2.234 s\n",
      "Step  63320 (epoch   19.79), loss: 0.009781, time: 2.229 s\n",
      "Step  63340 (epoch   19.79), loss: 0.009537, time: 2.238 s\n",
      "Step  63360 (epoch   19.80), loss: 0.011891, time: 2.230 s\n",
      "Step  63380 (epoch   19.81), loss: 0.011088, time: 2.230 s\n",
      "Step  63400 (epoch   19.81), loss: 0.013387, time: 2.235 s, error: 0.018137\n",
      "Step  63420 (epoch   19.82), loss: 0.006914, time: 14.949 s\n",
      "Step  63440 (epoch   19.82), loss: 0.007312, time: 2.235 s\n",
      "Step  63460 (epoch   19.83), loss: 0.005667, time: 2.240 s\n",
      "Step  63480 (epoch   19.84), loss: 0.007650, time: 2.246 s\n",
      "Step  63500 (epoch   19.84), loss: 0.004880, time: 2.251 s\n",
      "Step  63520 (epoch   19.85), loss: 0.008554, time: 2.238 s\n",
      "Step  63540 (epoch   19.86), loss: 0.013609, time: 2.251 s\n",
      "Step  63560 (epoch   19.86), loss: 0.008114, time: 2.248 s\n",
      "Step  63580 (epoch   19.87), loss: 0.006590, time: 2.241 s\n",
      "Step  63600 (epoch   19.88), loss: 0.009555, time: 2.242 s, error: 0.015873\n",
      "Step  63620 (epoch   19.88), loss: 0.008085, time: 14.997 s\n",
      "Step  63640 (epoch   19.89), loss: 0.010726, time: 2.237 s\n",
      "Step  63660 (epoch   19.89), loss: 0.005618, time: 2.243 s\n",
      "Step  63680 (epoch   19.90), loss: 0.007384, time: 2.233 s\n",
      "Step  63700 (epoch   19.91), loss: 0.024946, time: 2.252 s\n",
      "Step  63720 (epoch   19.91), loss: 0.008273, time: 2.236 s\n",
      "Step  63740 (epoch   19.92), loss: 0.009726, time: 2.248 s\n",
      "Step  63760 (epoch   19.93), loss: 0.011674, time: 2.251 s\n",
      "Step  63780 (epoch   19.93), loss: 0.008294, time: 2.247 s\n",
      "Step  63800 (epoch   19.94), loss: 0.008133, time: 2.239 s, error: 0.015232\n",
      "Step  63820 (epoch   19.94), loss: 0.007472, time: 14.974 s\n",
      "Step  63840 (epoch   19.95), loss: 0.009089, time: 2.239 s\n",
      "Step  63860 (epoch   19.96), loss: 0.008018, time: 2.232 s\n",
      "Step  63880 (epoch   19.96), loss: 0.014484, time: 2.250 s\n",
      "Step  63900 (epoch   19.97), loss: 0.010664, time: 2.228 s\n",
      "Step  63920 (epoch   19.98), loss: 0.007693, time: 2.246 s\n",
      "Step  63940 (epoch   19.98), loss: 0.007358, time: 2.241 s\n",
      "Step  63960 (epoch   19.99), loss: 0.015039, time: 2.241 s\n",
      "Step  63980 (epoch   19.99), loss: 0.018097, time: 2.238 s\n",
      "Step  64000 (epoch   20.00), loss: 0.011453, time: 2.247 s, error: 0.015275\n",
      "\n",
      "Time since beginning  : 11034.554 s\n",
      "\n",
      "Step  64020 (epoch   20.01), loss: 0.006790, time: 15.245 s\n",
      "Step  64040 (epoch   20.01), loss: 0.006422, time: 2.235 s\n",
      "Step  64060 (epoch   20.02), loss: 0.008531, time: 2.239 s\n",
      "Step  64080 (epoch   20.02), loss: 0.007547, time: 2.248 s\n",
      "Step  64100 (epoch   20.03), loss: 0.009877, time: 2.230 s\n",
      "Step  64120 (epoch   20.04), loss: 0.019152, time: 2.225 s\n",
      "Step  64140 (epoch   20.04), loss: 0.007778, time: 2.233 s\n",
      "Step  64160 (epoch   20.05), loss: 0.010559, time: 2.234 s\n",
      "Step  64180 (epoch   20.06), loss: 0.008673, time: 2.239 s\n",
      "Step  64200 (epoch   20.06), loss: 0.008777, time: 2.246 s, error: 0.016166\n",
      "Step  64220 (epoch   20.07), loss: 0.008543, time: 15.122 s\n",
      "Step  64240 (epoch   20.07), loss: 0.010092, time: 2.245 s\n",
      "Step  64260 (epoch   20.08), loss: 0.007512, time: 2.241 s\n",
      "Step  64280 (epoch   20.09), loss: 0.008728, time: 2.240 s\n",
      "Step  64300 (epoch   20.09), loss: 0.009715, time: 2.232 s\n",
      "Step  64320 (epoch   20.10), loss: 0.007230, time: 2.233 s\n",
      "Step  64340 (epoch   20.11), loss: 0.008248, time: 2.237 s\n",
      "Step  64360 (epoch   20.11), loss: 0.006037, time: 2.233 s\n",
      "Step  64380 (epoch   20.12), loss: 0.008824, time: 2.244 s\n",
      "Step  64400 (epoch   20.12), loss: 0.009345, time: 2.232 s, error: 0.014657\n",
      "Step  64420 (epoch   20.13), loss: 0.007540, time: 14.977 s\n",
      "Step  64440 (epoch   20.14), loss: 0.008078, time: 2.233 s\n",
      "Step  64460 (epoch   20.14), loss: 0.012300, time: 2.243 s\n",
      "Step  64480 (epoch   20.15), loss: 0.009757, time: 2.224 s\n",
      "Step  64500 (epoch   20.16), loss: 0.009808, time: 2.227 s\n",
      "Step  64520 (epoch   20.16), loss: 0.010207, time: 2.240 s\n",
      "Step  64540 (epoch   20.17), loss: 0.007007, time: 2.238 s\n",
      "Step  64560 (epoch   20.18), loss: 0.007903, time: 2.236 s\n",
      "Step  64580 (epoch   20.18), loss: 0.007378, time: 2.241 s\n",
      "Step  64600 (epoch   20.19), loss: 0.007543, time: 2.238 s, error: 0.015111\n",
      "Step  64620 (epoch   20.19), loss: 0.009142, time: 14.984 s\n",
      "Step  64640 (epoch   20.20), loss: 0.011035, time: 2.237 s\n",
      "Step  64660 (epoch   20.21), loss: 0.006908, time: 2.230 s\n",
      "Step  64680 (epoch   20.21), loss: 0.005730, time: 2.236 s\n",
      "Step  64700 (epoch   20.22), loss: 0.009521, time: 2.246 s\n",
      "Step  64720 (epoch   20.23), loss: 0.006437, time: 2.248 s\n",
      "Step  64740 (epoch   20.23), loss: 0.011664, time: 2.234 s\n",
      "Step  64760 (epoch   20.24), loss: 0.008159, time: 2.239 s\n",
      "Step  64780 (epoch   20.24), loss: 0.009980, time: 2.233 s\n",
      "Step  64800 (epoch   20.25), loss: 0.010418, time: 2.247 s, error: 0.016176\n",
      "Step  64820 (epoch   20.26), loss: 0.008241, time: 14.997 s\n",
      "Step  64840 (epoch   20.26), loss: 0.009477, time: 2.225 s\n",
      "Step  64860 (epoch   20.27), loss: 0.006159, time: 2.233 s\n",
      "Step  64880 (epoch   20.27), loss: 0.007811, time: 2.239 s\n",
      "Step  64900 (epoch   20.28), loss: 0.007048, time: 2.243 s\n",
      "Step  64920 (epoch   20.29), loss: 0.007859, time: 2.234 s\n",
      "Step  64940 (epoch   20.29), loss: 0.008020, time: 2.242 s\n",
      "Step  64960 (epoch   20.30), loss: 0.009250, time: 2.249 s\n",
      "Step  64980 (epoch   20.31), loss: 0.011571, time: 2.235 s\n",
      "Step  65000 (epoch   20.31), loss: 0.010913, time: 2.218 s, error: 0.017325\n",
      "Step  65020 (epoch   20.32), loss: 0.011664, time: 14.956 s\n",
      "Step  65040 (epoch   20.32), loss: 0.013720, time: 2.245 s\n",
      "Step  65060 (epoch   20.33), loss: 0.006960, time: 2.245 s\n",
      "Step  65080 (epoch   20.34), loss: 0.011288, time: 2.242 s\n",
      "Step  65100 (epoch   20.34), loss: 0.013214, time: 2.246 s\n",
      "Step  65120 (epoch   20.35), loss: 0.011712, time: 2.246 s\n",
      "Step  65140 (epoch   20.36), loss: 0.011462, time: 2.238 s\n",
      "Step  65160 (epoch   20.36), loss: 0.011200, time: 2.241 s\n",
      "Step  65180 (epoch   20.37), loss: 0.007457, time: 2.240 s\n",
      "Step  65200 (epoch   20.38), loss: 0.009200, time: 2.236 s, error: 0.015012\n",
      "Step  65220 (epoch   20.38), loss: 0.015542, time: 15.106 s\n",
      "Step  65240 (epoch   20.39), loss: 0.012063, time: 2.235 s\n",
      "Step  65260 (epoch   20.39), loss: 0.010456, time: 2.243 s\n",
      "Step  65280 (epoch   20.40), loss: 0.012803, time: 2.232 s\n",
      "Step  65300 (epoch   20.41), loss: 0.004916, time: 2.238 s\n",
      "Step  65320 (epoch   20.41), loss: 0.009923, time: 2.244 s\n",
      "Step  65340 (epoch   20.42), loss: 0.008299, time: 2.249 s\n",
      "Step  65360 (epoch   20.43), loss: 0.008054, time: 2.250 s\n",
      "Step  65380 (epoch   20.43), loss: 0.007747, time: 2.246 s\n",
      "Step  65400 (epoch   20.44), loss: 0.007935, time: 2.241 s, error: 0.015278\n",
      "Step  65420 (epoch   20.44), loss: 0.008041, time: 15.219 s\n",
      "Step  65440 (epoch   20.45), loss: 0.008945, time: 2.243 s\n",
      "Step  65460 (epoch   20.46), loss: 0.008566, time: 2.248 s\n",
      "Step  65480 (epoch   20.46), loss: 0.005713, time: 2.238 s\n",
      "Step  65500 (epoch   20.47), loss: 0.014779, time: 2.226 s\n",
      "Step  65520 (epoch   20.48), loss: 0.009282, time: 2.240 s\n",
      "Step  65540 (epoch   20.48), loss: 0.005666, time: 2.240 s\n",
      "Step  65560 (epoch   20.49), loss: 0.007672, time: 2.243 s\n",
      "Step  65580 (epoch   20.49), loss: 0.008923, time: 2.226 s\n",
      "Step  65600 (epoch   20.50), loss: 0.008078, time: 2.229 s, error: 0.015483\n",
      "Step  65620 (epoch   20.51), loss: 0.009685, time: 14.970 s\n",
      "Step  65640 (epoch   20.51), loss: 0.009968, time: 2.241 s\n",
      "Step  65660 (epoch   20.52), loss: 0.014999, time: 2.244 s\n",
      "Step  65680 (epoch   20.52), loss: 0.006954, time: 2.245 s\n",
      "Step  65700 (epoch   20.53), loss: 0.010705, time: 2.235 s\n",
      "Step  65720 (epoch   20.54), loss: 0.010403, time: 2.239 s\n",
      "Step  65740 (epoch   20.54), loss: 0.010167, time: 2.236 s\n",
      "Step  65760 (epoch   20.55), loss: 0.011099, time: 2.246 s\n",
      "Step  65780 (epoch   20.56), loss: 0.009016, time: 2.242 s\n",
      "Step  65800 (epoch   20.56), loss: 0.007341, time: 2.229 s, error: 0.015236\n",
      "Step  65820 (epoch   20.57), loss: 0.008664, time: 15.036 s\n",
      "Step  65840 (epoch   20.57), loss: 0.006229, time: 2.244 s\n",
      "Step  65860 (epoch   20.58), loss: 0.006103, time: 2.247 s\n",
      "Step  65880 (epoch   20.59), loss: 0.006774, time: 2.250 s\n",
      "Step  65900 (epoch   20.59), loss: 0.011741, time: 2.253 s\n",
      "Step  65920 (epoch   20.60), loss: 0.009721, time: 2.234 s\n",
      "Step  65940 (epoch   20.61), loss: 0.006347, time: 2.232 s\n",
      "Step  65960 (epoch   20.61), loss: 0.007873, time: 2.240 s\n",
      "Step  65980 (epoch   20.62), loss: 0.007232, time: 2.239 s\n",
      "Step  66000 (epoch   20.62), loss: 0.010041, time: 2.240 s, error: 0.014567\n",
      "\n",
      "Time since beginning  : 11386.582 s\n",
      "\n",
      "Step  66020 (epoch   20.63), loss: 0.008668, time: 15.184 s\n",
      "Step  66040 (epoch   20.64), loss: 0.010111, time: 2.236 s\n",
      "Step  66060 (epoch   20.64), loss: 0.006505, time: 2.229 s\n",
      "Step  66080 (epoch   20.65), loss: 0.008105, time: 2.242 s\n",
      "Step  66100 (epoch   20.66), loss: 0.007803, time: 2.235 s\n",
      "Step  66120 (epoch   20.66), loss: 0.009482, time: 2.234 s\n",
      "Step  66140 (epoch   20.67), loss: 0.012480, time: 2.253 s\n",
      "Step  66160 (epoch   20.68), loss: 0.010112, time: 2.252 s\n",
      "Step  66180 (epoch   20.68), loss: 0.009681, time: 2.240 s\n",
      "Step  66200 (epoch   20.69), loss: 0.009022, time: 2.237 s, error: 0.014863\n",
      "Step  66220 (epoch   20.69), loss: 0.009855, time: 14.998 s\n",
      "Step  66240 (epoch   20.70), loss: 0.009738, time: 2.230 s\n",
      "Step  66260 (epoch   20.71), loss: 0.005558, time: 2.230 s\n",
      "Step  66280 (epoch   20.71), loss: 0.007174, time: 2.231 s\n",
      "Step  66300 (epoch   20.72), loss: 0.006537, time: 2.218 s\n",
      "Step  66320 (epoch   20.73), loss: 0.029755, time: 2.246 s\n",
      "Step  66340 (epoch   20.73), loss: 0.011279, time: 2.240 s\n",
      "Step  66360 (epoch   20.74), loss: 0.006429, time: 2.235 s\n",
      "Step  66380 (epoch   20.74), loss: 0.006497, time: 2.233 s\n",
      "Step  66400 (epoch   20.75), loss: 0.006025, time: 2.238 s, error: 0.014652\n",
      "Step  66420 (epoch   20.76), loss: 0.008970, time: 15.065 s\n",
      "Step  66440 (epoch   20.76), loss: 0.011626, time: 2.237 s\n",
      "Step  66460 (epoch   20.77), loss: 0.007520, time: 2.242 s\n",
      "Step  66480 (epoch   20.77), loss: 0.014053, time: 2.232 s\n",
      "Step  66500 (epoch   20.78), loss: 0.007470, time: 2.240 s\n",
      "Step  66520 (epoch   20.79), loss: 0.009617, time: 2.240 s\n",
      "Step  66540 (epoch   20.79), loss: 0.012937, time: 2.239 s\n",
      "Step  66560 (epoch   20.80), loss: 0.012558, time: 2.237 s\n",
      "Step  66580 (epoch   20.81), loss: 0.010100, time: 2.228 s\n",
      "Step  66600 (epoch   20.81), loss: 0.009773, time: 2.243 s, error: 0.015623\n",
      "Step  66620 (epoch   20.82), loss: 0.007610, time: 15.137 s\n",
      "Step  66640 (epoch   20.82), loss: 0.008808, time: 2.239 s\n",
      "Step  66660 (epoch   20.83), loss: 0.007991, time: 2.234 s\n",
      "Step  66680 (epoch   20.84), loss: 0.010377, time: 2.238 s\n",
      "Step  66700 (epoch   20.84), loss: 0.010938, time: 2.242 s\n",
      "Step  66720 (epoch   20.85), loss: 0.006652, time: 2.237 s\n",
      "Step  66740 (epoch   20.86), loss: 0.007741, time: 2.226 s\n",
      "Step  66760 (epoch   20.86), loss: 0.007307, time: 2.241 s\n",
      "Step  66780 (epoch   20.87), loss: 0.009750, time: 2.243 s\n",
      "Step  66800 (epoch   20.88), loss: 0.008052, time: 2.233 s, error: 0.015737\n",
      "Step  66820 (epoch   20.88), loss: 0.009481, time: 15.024 s\n",
      "Step  66840 (epoch   20.89), loss: 0.007926, time: 2.246 s\n",
      "Step  66860 (epoch   20.89), loss: 0.013482, time: 2.223 s\n",
      "Step  66880 (epoch   20.90), loss: 0.008237, time: 2.230 s\n",
      "Step  66900 (epoch   20.91), loss: 0.007364, time: 2.230 s\n",
      "Step  66920 (epoch   20.91), loss: 0.015253, time: 2.232 s\n",
      "Step  66940 (epoch   20.92), loss: 0.014369, time: 2.225 s\n",
      "Step  66960 (epoch   20.93), loss: 0.010871, time: 2.239 s\n",
      "Step  66980 (epoch   20.93), loss: 0.010210, time: 2.224 s\n",
      "Step  67000 (epoch   20.94), loss: 0.006156, time: 2.242 s, error: 0.014804\n",
      "Step  67020 (epoch   20.94), loss: 0.007346, time: 14.996 s\n",
      "Step  67040 (epoch   20.95), loss: 0.008869, time: 2.228 s\n",
      "Step  67060 (epoch   20.96), loss: 0.009554, time: 2.249 s\n",
      "Step  67080 (epoch   20.96), loss: 0.008839, time: 2.250 s\n",
      "Step  67100 (epoch   20.97), loss: 0.007982, time: 2.245 s\n",
      "Step  67120 (epoch   20.98), loss: 0.009236, time: 2.235 s\n",
      "Step  67140 (epoch   20.98), loss: 0.014166, time: 2.248 s\n",
      "Step  67160 (epoch   20.99), loss: 0.015109, time: 2.249 s\n",
      "Step  67180 (epoch   20.99), loss: 0.009146, time: 2.246 s\n",
      "Step  67200 (epoch   21.00), loss: 0.007747, time: 2.237 s, error: 0.015021\n",
      "Step  67220 (epoch   21.01), loss: 0.006503, time: 14.948 s\n",
      "Step  67240 (epoch   21.01), loss: 0.007886, time: 2.240 s\n",
      "Step  67260 (epoch   21.02), loss: 0.005662, time: 2.240 s\n",
      "Step  67280 (epoch   21.02), loss: 0.010817, time: 2.229 s\n",
      "Step  67300 (epoch   21.03), loss: 0.007797, time: 2.238 s\n",
      "Step  67320 (epoch   21.04), loss: 0.008090, time: 2.239 s\n",
      "Step  67340 (epoch   21.04), loss: 0.010089, time: 2.248 s\n",
      "Step  67360 (epoch   21.05), loss: 0.010292, time: 2.250 s\n",
      "Step  67380 (epoch   21.06), loss: 0.009925, time: 2.231 s\n",
      "Step  67400 (epoch   21.06), loss: 0.012435, time: 2.239 s, error: 0.016217\n",
      "Step  67420 (epoch   21.07), loss: 0.011295, time: 14.971 s\n",
      "Step  67440 (epoch   21.07), loss: 0.005346, time: 2.246 s\n",
      "Step  67460 (epoch   21.08), loss: 0.012306, time: 2.241 s\n",
      "Step  67480 (epoch   21.09), loss: 0.009058, time: 2.232 s\n",
      "Step  67500 (epoch   21.09), loss: 0.007639, time: 2.221 s\n",
      "Step  67520 (epoch   21.10), loss: 0.009155, time: 2.239 s\n",
      "Step  67540 (epoch   21.11), loss: 0.006877, time: 2.240 s\n",
      "Step  67560 (epoch   21.11), loss: 0.007661, time: 2.243 s\n",
      "Step  67580 (epoch   21.12), loss: 0.006491, time: 2.238 s\n",
      "Step  67600 (epoch   21.12), loss: 0.008446, time: 2.237 s, error: 0.014475\n",
      "Step  67620 (epoch   21.13), loss: 0.024256, time: 15.067 s\n",
      "Step  67640 (epoch   21.14), loss: 0.010034, time: 2.234 s\n",
      "Step  67660 (epoch   21.14), loss: 0.006260, time: 2.251 s\n",
      "Step  67680 (epoch   21.15), loss: 0.011245, time: 2.248 s\n",
      "Step  67700 (epoch   21.16), loss: 0.007492, time: 2.236 s\n",
      "Step  67720 (epoch   21.16), loss: 0.008454, time: 2.243 s\n",
      "Step  67740 (epoch   21.17), loss: 0.011074, time: 2.248 s\n",
      "Step  67760 (epoch   21.18), loss: 0.008156, time: 2.249 s\n",
      "Step  67780 (epoch   21.18), loss: 0.009052, time: 2.245 s\n",
      "Step  67800 (epoch   21.19), loss: 0.007057, time: 2.222 s, error: 0.014648\n",
      "Step  67820 (epoch   21.19), loss: 0.007260, time: 15.182 s\n",
      "Step  67840 (epoch   21.20), loss: 0.006483, time: 2.247 s\n",
      "Step  67860 (epoch   21.21), loss: 0.007103, time: 2.250 s\n",
      "Step  67880 (epoch   21.21), loss: 0.010725, time: 2.241 s\n",
      "Step  67900 (epoch   21.22), loss: 0.005934, time: 2.225 s\n",
      "Step  67920 (epoch   21.23), loss: 0.022669, time: 2.248 s\n",
      "Step  67940 (epoch   21.23), loss: 0.008208, time: 2.239 s\n",
      "Step  67960 (epoch   21.24), loss: 0.009323, time: 2.239 s\n",
      "Step  67980 (epoch   21.24), loss: 0.006001, time: 2.237 s\n",
      "Step  68000 (epoch   21.25), loss: 0.010896, time: 2.230 s, error: 0.016194\n",
      "\n",
      "Time since beginning  : 11738.528 s\n",
      "\n",
      "Step  68020 (epoch   21.26), loss: 0.012142, time: 15.139 s\n",
      "Step  68040 (epoch   21.26), loss: 0.009621, time: 2.242 s\n",
      "Step  68060 (epoch   21.27), loss: 0.007119, time: 2.247 s\n",
      "Step  68080 (epoch   21.27), loss: 0.004785, time: 2.243 s\n",
      "Step  68100 (epoch   21.28), loss: 0.020996, time: 2.239 s\n",
      "Step  68120 (epoch   21.29), loss: 0.008186, time: 2.242 s\n",
      "Step  68140 (epoch   21.29), loss: 0.006708, time: 2.238 s\n",
      "Step  68160 (epoch   21.30), loss: 0.006157, time: 2.246 s\n",
      "Step  68180 (epoch   21.31), loss: 0.012197, time: 2.232 s\n",
      "Step  68200 (epoch   21.31), loss: 0.013366, time: 2.245 s, error: 0.015349\n",
      "Step  68220 (epoch   21.32), loss: 0.008903, time: 14.985 s\n",
      "Step  68240 (epoch   21.32), loss: 0.019173, time: 2.246 s\n",
      "Step  68260 (epoch   21.33), loss: 0.014537, time: 2.249 s\n",
      "Step  68280 (epoch   21.34), loss: 0.010364, time: 2.253 s\n",
      "Step  68300 (epoch   21.34), loss: 0.010657, time: 2.240 s\n",
      "Step  68320 (epoch   21.35), loss: 0.013018, time: 2.244 s\n",
      "Step  68340 (epoch   21.36), loss: 0.009079, time: 2.242 s\n",
      "Step  68360 (epoch   21.36), loss: 0.008331, time: 2.240 s\n",
      "Step  68380 (epoch   21.37), loss: 0.007199, time: 2.244 s\n",
      "Step  68400 (epoch   21.38), loss: 0.007015, time: 2.234 s, error: 0.014988\n",
      "Step  68420 (epoch   21.38), loss: 0.005885, time: 14.999 s\n",
      "Step  68440 (epoch   21.39), loss: 0.008881, time: 2.241 s\n",
      "Step  68460 (epoch   21.39), loss: 0.008178, time: 2.233 s\n",
      "Step  68480 (epoch   21.40), loss: 0.007656, time: 2.247 s\n",
      "Step  68500 (epoch   21.41), loss: 0.010691, time: 2.241 s\n",
      "Step  68520 (epoch   21.41), loss: 0.007275, time: 2.235 s\n",
      "Step  68540 (epoch   21.42), loss: 0.007771, time: 2.254 s\n",
      "Step  68560 (epoch   21.43), loss: 0.009279, time: 2.244 s\n",
      "Step  68580 (epoch   21.43), loss: 0.008683, time: 2.238 s\n",
      "Step  68600 (epoch   21.44), loss: 0.011149, time: 2.233 s, error: 0.015007\n",
      "Step  68620 (epoch   21.44), loss: 0.010202, time: 14.970 s\n",
      "Step  68640 (epoch   21.45), loss: 0.011233, time: 2.240 s\n",
      "Step  68660 (epoch   21.46), loss: 0.007397, time: 2.244 s\n",
      "Step  68680 (epoch   21.46), loss: 0.008287, time: 2.231 s\n",
      "Step  68700 (epoch   21.47), loss: 0.007004, time: 2.242 s\n",
      "Step  68720 (epoch   21.48), loss: 0.010572, time: 2.235 s\n",
      "Step  68740 (epoch   21.48), loss: 0.025376, time: 2.234 s\n",
      "Step  68760 (epoch   21.49), loss: 0.005835, time: 2.243 s\n",
      "Step  68780 (epoch   21.49), loss: 0.006923, time: 2.238 s\n",
      "Step  68800 (epoch   21.50), loss: 0.009159, time: 2.253 s, error: 0.014549\n",
      "Step  68820 (epoch   21.51), loss: 0.007972, time: 14.992 s\n",
      "Step  68840 (epoch   21.51), loss: 0.007669, time: 2.247 s\n",
      "Step  68860 (epoch   21.52), loss: 0.009685, time: 2.238 s\n",
      "Step  68880 (epoch   21.52), loss: 0.005834, time: 2.239 s\n",
      "Step  68900 (epoch   21.53), loss: 0.017406, time: 2.249 s\n",
      "Step  68920 (epoch   21.54), loss: 0.010545, time: 2.235 s\n",
      "Step  68940 (epoch   21.54), loss: 0.006245, time: 2.230 s\n",
      "Step  68960 (epoch   21.55), loss: 0.010794, time: 2.237 s\n",
      "Step  68980 (epoch   21.56), loss: 0.009628, time: 2.245 s\n",
      "Step  69000 (epoch   21.56), loss: 0.007957, time: 2.228 s, error: 0.014426\n",
      "Step  69020 (epoch   21.57), loss: 0.009783, time: 15.151 s\n",
      "Step  69040 (epoch   21.57), loss: 0.006273, time: 2.245 s\n",
      "Step  69060 (epoch   21.58), loss: 0.007857, time: 2.236 s\n",
      "Step  69080 (epoch   21.59), loss: 0.008595, time: 2.239 s\n",
      "Step  69100 (epoch   21.59), loss: 0.009720, time: 2.238 s\n",
      "Step  69120 (epoch   21.60), loss: 0.007664, time: 2.241 s\n",
      "Step  69140 (epoch   21.61), loss: 0.010138, time: 2.250 s\n",
      "Step  69160 (epoch   21.61), loss: 0.014600, time: 2.241 s\n",
      "Step  69180 (epoch   21.62), loss: 0.007031, time: 2.226 s\n",
      "Step  69200 (epoch   21.62), loss: 0.006521, time: 2.227 s, error: 0.014277\n",
      "Step  69220 (epoch   21.63), loss: 0.009170, time: 15.084 s\n",
      "Step  69240 (epoch   21.64), loss: 0.008331, time: 2.239 s\n",
      "Step  69260 (epoch   21.64), loss: 0.010241, time: 2.237 s\n",
      "Step  69280 (epoch   21.65), loss: 0.009058, time: 2.233 s\n",
      "Step  69300 (epoch   21.66), loss: 0.012487, time: 2.239 s\n",
      "Step  69320 (epoch   21.66), loss: 0.015741, time: 2.237 s\n",
      "Step  69340 (epoch   21.67), loss: 0.005141, time: 2.226 s\n",
      "Step  69360 (epoch   21.68), loss: 0.006768, time: 2.233 s\n",
      "Step  69380 (epoch   21.68), loss: 0.008865, time: 2.229 s\n",
      "Step  69400 (epoch   21.69), loss: 0.009159, time: 2.236 s, error: 0.014663\n",
      "Step  69420 (epoch   21.69), loss: 0.008978, time: 14.997 s\n",
      "Step  69440 (epoch   21.70), loss: 0.005394, time: 2.242 s\n",
      "Step  69460 (epoch   21.71), loss: 0.007746, time: 2.248 s\n",
      "Step  69480 (epoch   21.71), loss: 0.006839, time: 2.254 s\n",
      "Step  69500 (epoch   21.72), loss: 0.011785, time: 2.247 s\n",
      "Step  69520 (epoch   21.73), loss: 0.010502, time: 2.247 s\n",
      "Step  69540 (epoch   21.73), loss: 0.012692, time: 2.231 s\n",
      "Step  69560 (epoch   21.74), loss: 0.005704, time: 2.236 s\n",
      "Step  69580 (epoch   21.74), loss: 0.006808, time: 2.233 s\n",
      "Step  69600 (epoch   21.75), loss: 0.007224, time: 2.241 s, error: 0.014520\n",
      "Step  69620 (epoch   21.76), loss: 0.008018, time: 14.975 s\n",
      "Step  69640 (epoch   21.76), loss: 0.008156, time: 2.224 s\n",
      "Step  69660 (epoch   21.77), loss: 0.007475, time: 2.236 s\n",
      "Step  69680 (epoch   21.77), loss: 0.008147, time: 2.233 s\n",
      "Step  69700 (epoch   21.78), loss: 0.005807, time: 2.243 s\n",
      "Step  69720 (epoch   21.79), loss: 0.010691, time: 2.244 s\n",
      "Step  69740 (epoch   21.79), loss: 0.009426, time: 2.242 s\n",
      "Step  69760 (epoch   21.80), loss: 0.006283, time: 2.235 s\n",
      "Step  69780 (epoch   21.81), loss: 0.021141, time: 2.245 s\n",
      "Step  69800 (epoch   21.81), loss: 0.010889, time: 2.225 s, error: 0.014429\n",
      "Step  69820 (epoch   21.82), loss: 0.007017, time: 14.985 s\n",
      "Step  69840 (epoch   21.82), loss: 0.005534, time: 2.251 s\n",
      "Step  69860 (epoch   21.83), loss: 0.007874, time: 2.238 s\n",
      "Step  69880 (epoch   21.84), loss: 0.009936, time: 2.244 s\n",
      "Step  69900 (epoch   21.84), loss: 0.006881, time: 2.251 s\n",
      "Step  69920 (epoch   21.85), loss: 0.010028, time: 2.241 s\n",
      "Step  69940 (epoch   21.86), loss: 0.009311, time: 2.241 s\n",
      "Step  69960 (epoch   21.86), loss: 0.009683, time: 2.244 s\n",
      "Step  69980 (epoch   21.87), loss: 0.011924, time: 2.244 s\n",
      "Step  70000 (epoch   21.88), loss: 0.007571, time: 2.254 s, error: 0.015483\n",
      "\n",
      "Time since beginning  : 12090.378 s\n",
      "\n",
      "Step  70020 (epoch   21.88), loss: 0.007827, time: 15.102 s\n",
      "Step  70040 (epoch   21.89), loss: 0.009565, time: 2.243 s\n",
      "Step  70060 (epoch   21.89), loss: 0.010356, time: 2.237 s\n",
      "Step  70080 (epoch   21.90), loss: 0.006084, time: 2.239 s\n",
      "Step  70100 (epoch   21.91), loss: 0.006393, time: 2.243 s\n",
      "Step  70120 (epoch   21.91), loss: 0.009306, time: 2.245 s\n",
      "Step  70140 (epoch   21.92), loss: 0.011805, time: 2.232 s\n",
      "Step  70160 (epoch   21.93), loss: 0.007678, time: 2.228 s\n",
      "Step  70180 (epoch   21.93), loss: 0.009896, time: 2.249 s\n",
      "Step  70200 (epoch   21.94), loss: 0.006996, time: 2.239 s, error: 0.015168\n",
      "Step  70220 (epoch   21.94), loss: 0.006765, time: 15.180 s\n",
      "Step  70240 (epoch   21.95), loss: 0.009794, time: 2.237 s\n",
      "Step  70260 (epoch   21.96), loss: 0.006698, time: 2.236 s\n",
      "Step  70280 (epoch   21.96), loss: 0.010756, time: 2.223 s\n",
      "Step  70300 (epoch   21.97), loss: 0.005624, time: 2.227 s\n",
      "Step  70320 (epoch   21.98), loss: 0.007915, time: 2.232 s\n",
      "Step  70340 (epoch   21.98), loss: 0.008235, time: 2.238 s\n",
      "Step  70360 (epoch   21.99), loss: 0.009274, time: 2.235 s\n",
      "Step  70380 (epoch   21.99), loss: 0.006840, time: 2.227 s\n",
      "Step  70400 (epoch   22.00), loss: 0.013663, time: 2.242 s, error: 0.014646\n",
      "Step  70420 (epoch   22.01), loss: 0.013441, time: 15.097 s\n",
      "Step  70440 (epoch   22.01), loss: 0.014641, time: 2.241 s\n",
      "Step  70460 (epoch   22.02), loss: 0.007829, time: 2.239 s\n",
      "Step  70480 (epoch   22.02), loss: 0.009015, time: 2.230 s\n",
      "Step  70500 (epoch   22.03), loss: 0.006531, time: 2.234 s\n",
      "Step  70520 (epoch   22.04), loss: 0.008173, time: 2.235 s\n",
      "Step  70540 (epoch   22.04), loss: 0.014508, time: 2.248 s\n",
      "Step  70560 (epoch   22.05), loss: 0.006802, time: 2.247 s\n",
      "Step  70580 (epoch   22.06), loss: 0.006831, time: 2.251 s\n",
      "Step  70600 (epoch   22.06), loss: 0.008901, time: 2.236 s, error: 0.014840\n",
      "Step  70620 (epoch   22.07), loss: 0.009250, time: 14.993 s\n",
      "Step  70640 (epoch   22.07), loss: 0.007327, time: 2.238 s\n",
      "Step  70660 (epoch   22.08), loss: 0.009893, time: 2.252 s\n",
      "Step  70680 (epoch   22.09), loss: 0.011222, time: 2.255 s\n",
      "Step  70700 (epoch   22.09), loss: 0.006319, time: 2.249 s\n",
      "Step  70720 (epoch   22.10), loss: 0.007071, time: 2.250 s\n",
      "Step  70740 (epoch   22.11), loss: 0.007233, time: 2.236 s\n",
      "Step  70760 (epoch   22.11), loss: 0.013483, time: 2.244 s\n",
      "Step  70780 (epoch   22.12), loss: 0.008099, time: 2.242 s\n",
      "Step  70800 (epoch   22.12), loss: 0.007069, time: 2.229 s, error: 0.014391\n",
      "Step  70820 (epoch   22.13), loss: 0.009612, time: 15.012 s\n",
      "Step  70840 (epoch   22.14), loss: 0.007777, time: 2.242 s\n",
      "Step  70860 (epoch   22.14), loss: 0.007560, time: 2.239 s\n",
      "Step  70880 (epoch   22.15), loss: 0.011669, time: 2.236 s\n",
      "Step  70900 (epoch   22.16), loss: 0.010462, time: 2.237 s\n",
      "Step  70920 (epoch   22.16), loss: 0.007686, time: 2.245 s\n",
      "Step  70940 (epoch   22.17), loss: 0.015375, time: 2.249 s\n",
      "Step  70960 (epoch   22.18), loss: 0.008062, time: 2.225 s\n",
      "Step  70980 (epoch   22.18), loss: 0.008981, time: 2.242 s\n",
      "Step  71000 (epoch   22.19), loss: 0.005097, time: 2.229 s, error: 0.014249\n",
      "Step  71020 (epoch   22.19), loss: 0.007400, time: 15.033 s\n",
      "Step  71040 (epoch   22.20), loss: 0.009487, time: 2.240 s\n",
      "Step  71060 (epoch   22.21), loss: 0.006911, time: 2.230 s\n",
      "Step  71080 (epoch   22.21), loss: 0.009468, time: 2.234 s\n",
      "Step  71100 (epoch   22.22), loss: 0.005810, time: 2.234 s\n",
      "Step  71120 (epoch   22.23), loss: 0.020345, time: 2.241 s\n",
      "Step  71140 (epoch   22.23), loss: 0.008489, time: 2.230 s\n",
      "Step  71160 (epoch   22.24), loss: 0.011126, time: 2.228 s\n",
      "Step  71180 (epoch   22.24), loss: 0.005291, time: 2.250 s\n",
      "Step  71200 (epoch   22.25), loss: 0.007926, time: 2.245 s, error: 0.015944\n",
      "Step  71220 (epoch   22.26), loss: 0.006408, time: 15.022 s\n",
      "Step  71240 (epoch   22.26), loss: 0.006995, time: 2.248 s\n",
      "Step  71260 (epoch   22.27), loss: 0.008362, time: 2.246 s\n",
      "Step  71280 (epoch   22.27), loss: 0.007298, time: 2.247 s\n",
      "Step  71300 (epoch   22.28), loss: 0.009027, time: 2.236 s\n",
      "Step  71320 (epoch   22.29), loss: 0.006559, time: 2.242 s\n",
      "Step  71340 (epoch   22.29), loss: 0.009431, time: 2.237 s\n",
      "Step  71360 (epoch   22.30), loss: 0.006031, time: 2.236 s\n",
      "Step  71380 (epoch   22.31), loss: 0.009890, time: 2.220 s\n",
      "Step  71400 (epoch   22.31), loss: 0.013716, time: 2.238 s, error: 0.015201\n",
      "Step  71420 (epoch   22.32), loss: 0.009402, time: 15.148 s\n",
      "Step  71440 (epoch   22.32), loss: 0.007778, time: 2.229 s\n",
      "Step  71460 (epoch   22.33), loss: 0.007178, time: 2.237 s\n",
      "Step  71480 (epoch   22.34), loss: 0.007934, time: 2.234 s\n",
      "Step  71500 (epoch   22.34), loss: 0.007220, time: 2.241 s\n",
      "Step  71520 (epoch   22.35), loss: 0.013850, time: 2.242 s\n",
      "Step  71540 (epoch   22.36), loss: 0.008275, time: 2.244 s\n",
      "Step  71560 (epoch   22.36), loss: 0.012828, time: 2.239 s\n",
      "Step  71580 (epoch   22.37), loss: 0.010002, time: 2.245 s\n",
      "Step  71600 (epoch   22.38), loss: 0.006025, time: 2.233 s, error: 0.014347\n",
      "Step  71620 (epoch   22.38), loss: 0.011077, time: 15.125 s\n",
      "Step  71640 (epoch   22.39), loss: 0.008445, time: 2.247 s\n",
      "Step  71660 (epoch   22.39), loss: 0.009547, time: 2.236 s\n",
      "Step  71680 (epoch   22.40), loss: 0.010886, time: 2.249 s\n",
      "Step  71700 (epoch   22.41), loss: 0.007324, time: 2.250 s\n",
      "Step  71720 (epoch   22.41), loss: 0.009661, time: 2.247 s\n",
      "Step  71740 (epoch   22.42), loss: 0.007375, time: 2.248 s\n",
      "Step  71760 (epoch   22.43), loss: 0.005427, time: 2.232 s\n",
      "Step  71780 (epoch   22.43), loss: 0.007596, time: 2.243 s\n",
      "Step  71800 (epoch   22.44), loss: 0.007487, time: 2.247 s, error: 0.014684\n",
      "Step  71820 (epoch   22.44), loss: 0.011419, time: 14.975 s\n",
      "Step  71840 (epoch   22.45), loss: 0.009618, time: 2.239 s\n",
      "Step  71860 (epoch   22.46), loss: 0.008330, time: 2.254 s\n",
      "Step  71880 (epoch   22.46), loss: 0.007239, time: 2.246 s\n",
      "Step  71900 (epoch   22.47), loss: 0.005868, time: 2.245 s\n",
      "Step  71920 (epoch   22.48), loss: 0.008755, time: 2.240 s\n",
      "Step  71940 (epoch   22.48), loss: 0.008711, time: 2.246 s\n",
      "Step  71960 (epoch   22.49), loss: 0.010790, time: 2.240 s\n",
      "Step  71980 (epoch   22.49), loss: 0.008829, time: 2.230 s\n",
      "Step  72000 (epoch   22.50), loss: 0.012365, time: 2.223 s, error: 0.014362\n",
      "\n",
      "Time since beginning  : 12442.639 s\n",
      "\n",
      "Step  72020 (epoch   22.51), loss: 0.006474, time: 15.127 s\n",
      "Step  72040 (epoch   22.51), loss: 0.014995, time: 2.245 s\n",
      "Step  72060 (epoch   22.52), loss: 0.008573, time: 2.242 s\n",
      "Step  72080 (epoch   22.52), loss: 0.008346, time: 2.244 s\n",
      "Step  72100 (epoch   22.53), loss: 0.007670, time: 2.238 s\n",
      "Step  72120 (epoch   22.54), loss: 0.013010, time: 2.246 s\n",
      "Step  72140 (epoch   22.54), loss: 0.005102, time: 2.239 s\n",
      "Step  72160 (epoch   22.55), loss: 0.011479, time: 2.243 s\n",
      "Step  72180 (epoch   22.56), loss: 0.009863, time: 2.247 s\n",
      "Step  72200 (epoch   22.56), loss: 0.007114, time: 2.236 s, error: 0.014151\n",
      "Step  72220 (epoch   22.57), loss: 0.006317, time: 15.059 s\n",
      "Step  72240 (epoch   22.57), loss: 0.008391, time: 2.249 s\n",
      "Step  72260 (epoch   22.58), loss: 0.007322, time: 2.242 s\n",
      "Step  72280 (epoch   22.59), loss: 0.007210, time: 2.237 s\n",
      "Step  72300 (epoch   22.59), loss: 0.009395, time: 2.226 s\n",
      "Step  72320 (epoch   22.60), loss: 0.012315, time: 2.235 s\n",
      "Step  72340 (epoch   22.61), loss: 0.005789, time: 2.234 s\n",
      "Step  72360 (epoch   22.61), loss: 0.010515, time: 2.230 s\n",
      "Step  72380 (epoch   22.62), loss: 0.005957, time: 2.249 s\n",
      "Step  72400 (epoch   22.62), loss: 0.005336, time: 2.245 s, error: 0.014057\n",
      "Step  72420 (epoch   22.63), loss: 0.011858, time: 14.985 s\n",
      "Step  72440 (epoch   22.64), loss: 0.006830, time: 2.229 s\n",
      "Step  72460 (epoch   22.64), loss: 0.007035, time: 2.237 s\n",
      "Step  72480 (epoch   22.65), loss: 0.005824, time: 2.232 s\n",
      "Step  72500 (epoch   22.66), loss: 0.006859, time: 2.236 s\n",
      "Step  72520 (epoch   22.66), loss: 0.006382, time: 2.231 s\n",
      "Step  72540 (epoch   22.67), loss: 0.012505, time: 2.239 s\n",
      "Step  72560 (epoch   22.68), loss: 0.010525, time: 2.236 s\n",
      "Step  72580 (epoch   22.68), loss: 0.012476, time: 2.225 s\n",
      "Step  72600 (epoch   22.69), loss: 0.020182, time: 2.241 s, error: 0.014558\n",
      "Step  72620 (epoch   22.69), loss: 0.010720, time: 15.144 s\n",
      "Step  72640 (epoch   22.70), loss: 0.007442, time: 2.235 s\n",
      "Step  72660 (epoch   22.71), loss: 0.007412, time: 2.249 s\n",
      "Step  72680 (epoch   22.71), loss: 0.034555, time: 2.247 s\n",
      "Step  72700 (epoch   22.72), loss: 0.007214, time: 2.250 s\n",
      "Step  72720 (epoch   22.73), loss: 0.011082, time: 2.244 s\n",
      "Step  72740 (epoch   22.73), loss: 0.008330, time: 2.250 s\n",
      "Step  72760 (epoch   22.74), loss: 0.010566, time: 2.247 s\n",
      "Step  72780 (epoch   22.74), loss: 0.009096, time: 2.238 s\n",
      "Step  72800 (epoch   22.75), loss: 0.007821, time: 2.237 s, error: 0.014279\n",
      "Step  72820 (epoch   22.76), loss: 0.007446, time: 15.201 s\n",
      "Step  72840 (epoch   22.76), loss: 0.009725, time: 2.240 s\n",
      "Step  72860 (epoch   22.77), loss: 0.010632, time: 2.239 s\n",
      "Step  72880 (epoch   22.77), loss: 0.005575, time: 2.243 s\n",
      "Step  72900 (epoch   22.78), loss: 0.008083, time: 2.236 s\n",
      "Step  72920 (epoch   22.79), loss: 0.017319, time: 2.243 s\n",
      "Step  72940 (epoch   22.79), loss: 0.009802, time: 2.241 s\n",
      "Step  72960 (epoch   22.80), loss: 0.007932, time: 2.232 s\n",
      "Step  72980 (epoch   22.81), loss: 0.011955, time: 2.249 s\n",
      "Step  73000 (epoch   22.81), loss: 0.011648, time: 2.245 s, error: 0.014320\n",
      "Step  73020 (epoch   22.82), loss: 0.006933, time: 15.056 s\n",
      "Step  73040 (epoch   22.82), loss: 0.011468, time: 2.245 s\n",
      "Step  73060 (epoch   22.83), loss: 0.007653, time: 2.241 s\n",
      "Step  73080 (epoch   22.84), loss: 0.010995, time: 2.242 s\n",
      "Step  73100 (epoch   22.84), loss: 0.009857, time: 2.233 s\n",
      "Step  73120 (epoch   22.85), loss: 0.011813, time: 2.242 s\n",
      "Step  73140 (epoch   22.86), loss: 0.007332, time: 2.235 s\n",
      "Step  73160 (epoch   22.86), loss: 0.004705, time: 2.236 s\n",
      "Step  73180 (epoch   22.87), loss: 0.012648, time: 2.246 s\n",
      "Step  73200 (epoch   22.88), loss: 0.010017, time: 2.238 s, error: 0.014777\n",
      "Step  73220 (epoch   22.88), loss: 0.008298, time: 15.072 s\n",
      "Step  73240 (epoch   22.89), loss: 0.010000, time: 2.229 s\n",
      "Step  73260 (epoch   22.89), loss: 0.009404, time: 2.245 s\n",
      "Step  73280 (epoch   22.90), loss: 0.010698, time: 2.226 s\n",
      "Step  73300 (epoch   22.91), loss: 0.012996, time: 2.247 s\n",
      "Step  73320 (epoch   22.91), loss: 0.006143, time: 2.254 s\n",
      "Step  73340 (epoch   22.92), loss: 0.007813, time: 2.240 s\n",
      "Step  73360 (epoch   22.93), loss: 0.009733, time: 2.231 s\n",
      "Step  73380 (epoch   22.93), loss: 0.009508, time: 2.237 s\n",
      "Step  73400 (epoch   22.94), loss: 0.010649, time: 2.239 s, error: 0.015479\n",
      "Step  73420 (epoch   22.94), loss: 0.007739, time: 14.965 s\n",
      "Step  73440 (epoch   22.95), loss: 0.008742, time: 2.235 s\n",
      "Step  73460 (epoch   22.96), loss: 0.006877, time: 2.232 s\n",
      "Step  73480 (epoch   22.96), loss: 0.011651, time: 2.239 s\n",
      "Step  73500 (epoch   22.97), loss: 0.006604, time: 2.229 s\n",
      "Step  73520 (epoch   22.98), loss: 0.011295, time: 2.229 s\n",
      "Step  73540 (epoch   22.98), loss: 0.006080, time: 2.238 s\n",
      "Step  73560 (epoch   22.99), loss: 0.008064, time: 2.226 s\n",
      "Step  73580 (epoch   22.99), loss: 0.008956, time: 2.251 s\n",
      "Step  73600 (epoch   23.00), loss: 0.008664, time: 2.225 s, error: 0.014536\n",
      "Step  73620 (epoch   23.01), loss: 0.006195, time: 14.983 s\n",
      "Step  73640 (epoch   23.01), loss: 0.005937, time: 2.238 s\n",
      "Step  73660 (epoch   23.02), loss: 0.008367, time: 2.238 s\n",
      "Step  73680 (epoch   23.02), loss: 0.005437, time: 2.243 s\n",
      "Step  73700 (epoch   23.03), loss: 0.007589, time: 2.236 s\n",
      "Step  73720 (epoch   23.04), loss: 0.008719, time: 2.241 s\n",
      "Step  73740 (epoch   23.04), loss: 0.008199, time: 2.229 s\n",
      "Step  73760 (epoch   23.05), loss: 0.007938, time: 2.235 s\n",
      "Step  73780 (epoch   23.06), loss: 0.008518, time: 2.238 s\n",
      "Step  73800 (epoch   23.06), loss: 0.006607, time: 2.241 s, error: 0.014268\n",
      "Step  73820 (epoch   23.07), loss: 0.009547, time: 15.134 s\n",
      "Step  73840 (epoch   23.07), loss: 0.020627, time: 2.242 s\n",
      "Step  73860 (epoch   23.08), loss: 0.011457, time: 2.247 s\n",
      "Step  73880 (epoch   23.09), loss: 0.004864, time: 2.241 s\n",
      "Step  73900 (epoch   23.09), loss: 0.008047, time: 2.236 s\n",
      "Step  73920 (epoch   23.10), loss: 0.005455, time: 2.229 s\n",
      "Step  73940 (epoch   23.11), loss: 0.010032, time: 2.246 s\n",
      "Step  73960 (epoch   23.11), loss: 0.009732, time: 2.243 s\n",
      "Step  73980 (epoch   23.12), loss: 0.005038, time: 2.246 s\n",
      "Step  74000 (epoch   23.12), loss: 0.010271, time: 2.231 s, error: 0.014367\n",
      "\n",
      "Time since beginning  : 12794.982 s\n",
      "\n",
      "Step  74020 (epoch   23.13), loss: 0.007518, time: 15.239 s\n",
      "Step  74040 (epoch   23.14), loss: 0.008474, time: 2.251 s\n",
      "Step  74060 (epoch   23.14), loss: 0.006047, time: 2.237 s\n",
      "Step  74080 (epoch   23.15), loss: 0.009061, time: 2.244 s\n",
      "Step  74100 (epoch   23.16), loss: 0.010120, time: 2.240 s\n",
      "Step  74120 (epoch   23.16), loss: 0.007146, time: 2.248 s\n",
      "Step  74140 (epoch   23.17), loss: 0.013409, time: 2.239 s\n",
      "Step  74160 (epoch   23.18), loss: 0.005230, time: 2.243 s\n",
      "Step  74180 (epoch   23.18), loss: 0.019782, time: 2.233 s\n",
      "Step  74200 (epoch   23.19), loss: 0.004655, time: 2.245 s, error: 0.013912\n",
      "Step  74220 (epoch   23.19), loss: 0.008301, time: 15.055 s\n",
      "Step  74240 (epoch   23.20), loss: 0.008131, time: 2.252 s\n",
      "Step  74260 (epoch   23.21), loss: 0.008750, time: 2.253 s\n",
      "Step  74280 (epoch   23.21), loss: 0.009328, time: 2.237 s\n",
      "Step  74300 (epoch   23.22), loss: 0.012184, time: 2.236 s\n",
      "Step  74320 (epoch   23.23), loss: 0.005627, time: 2.240 s\n",
      "Step  74340 (epoch   23.23), loss: 0.010555, time: 2.240 s\n",
      "Step  74360 (epoch   23.24), loss: 0.008399, time: 2.230 s\n",
      "Step  74380 (epoch   23.24), loss: 0.024323, time: 2.234 s\n",
      "Step  74400 (epoch   23.25), loss: 0.010119, time: 2.237 s, error: 0.015346\n",
      "Step  74420 (epoch   23.26), loss: 0.007452, time: 14.957 s\n",
      "Step  74440 (epoch   23.26), loss: 0.009195, time: 2.249 s\n",
      "Step  74460 (epoch   23.27), loss: 0.010598, time: 2.233 s\n",
      "Step  74480 (epoch   23.27), loss: 0.026746, time: 2.244 s\n",
      "Step  74500 (epoch   23.28), loss: 0.014560, time: 2.247 s\n",
      "Step  74520 (epoch   23.29), loss: 0.007191, time: 2.255 s\n",
      "Step  74540 (epoch   23.29), loss: 0.009667, time: 2.243 s\n",
      "Step  74560 (epoch   23.30), loss: 0.006820, time: 2.248 s\n",
      "Step  74580 (epoch   23.31), loss: 0.008936, time: 2.239 s\n",
      "Step  74600 (epoch   23.31), loss: 0.006553, time: 2.234 s, error: 0.014479\n",
      "Step  74620 (epoch   23.32), loss: 0.013370, time: 15.053 s\n",
      "Step  74640 (epoch   23.32), loss: 0.018797, time: 2.237 s\n",
      "Step  74660 (epoch   23.33), loss: 0.008067, time: 2.246 s\n",
      "Step  74680 (epoch   23.34), loss: 0.009088, time: 2.234 s\n",
      "Step  74700 (epoch   23.34), loss: 0.012195, time: 2.227 s\n",
      "Step  74720 (epoch   23.35), loss: 0.009929, time: 2.230 s\n",
      "Step  74740 (epoch   23.36), loss: 0.009333, time: 2.237 s\n",
      "Step  74760 (epoch   23.36), loss: 0.007459, time: 2.241 s\n",
      "Step  74780 (epoch   23.37), loss: 0.006903, time: 2.250 s\n",
      "Step  74800 (epoch   23.38), loss: 0.009432, time: 2.242 s, error: 0.014247\n",
      "Step  74820 (epoch   23.38), loss: 0.007701, time: 15.066 s\n",
      "Step  74840 (epoch   23.39), loss: 0.008148, time: 2.252 s\n",
      "Step  74860 (epoch   23.39), loss: 0.005834, time: 2.244 s\n",
      "Step  74880 (epoch   23.40), loss: 0.013371, time: 2.228 s\n",
      "Step  74900 (epoch   23.41), loss: 0.008085, time: 2.234 s\n",
      "Step  74920 (epoch   23.41), loss: 0.008962, time: 2.242 s\n",
      "Step  74940 (epoch   23.42), loss: 0.006323, time: 2.243 s\n",
      "Step  74960 (epoch   23.43), loss: 0.008006, time: 2.240 s\n",
      "Step  74980 (epoch   23.43), loss: 0.005728, time: 2.247 s\n",
      "Step  75000 (epoch   23.44), loss: 0.008497, time: 2.236 s, error: 0.014387\n",
      "Step  75020 (epoch   23.44), loss: 0.015434, time: 15.185 s\n",
      "Step  75040 (epoch   23.45), loss: 0.008728, time: 2.241 s\n",
      "Step  75060 (epoch   23.46), loss: 0.009474, time: 2.247 s\n",
      "Step  75080 (epoch   23.46), loss: 0.008211, time: 2.231 s\n",
      "Step  75100 (epoch   23.47), loss: 0.008101, time: 2.242 s\n",
      "Step  75120 (epoch   23.48), loss: 0.007569, time: 2.245 s\n",
      "Step  75140 (epoch   23.48), loss: 0.007593, time: 2.237 s\n",
      "Step  75160 (epoch   23.49), loss: 0.007397, time: 2.234 s\n",
      "Step  75180 (epoch   23.49), loss: 0.011739, time: 2.252 s\n",
      "Step  75200 (epoch   23.50), loss: 0.008561, time: 2.249 s, error: 0.014568\n",
      "Step  75220 (epoch   23.51), loss: 0.010279, time: 15.126 s\n",
      "Step  75240 (epoch   23.51), loss: 0.010705, time: 2.238 s\n",
      "Step  75260 (epoch   23.52), loss: 0.007804, time: 2.245 s\n",
      "Step  75280 (epoch   23.52), loss: 0.007665, time: 2.233 s\n",
      "Step  75300 (epoch   23.53), loss: 0.007870, time: 2.238 s\n",
      "Step  75320 (epoch   23.54), loss: 0.008721, time: 2.239 s\n",
      "Step  75340 (epoch   23.54), loss: 0.005643, time: 2.249 s\n",
      "Step  75360 (epoch   23.55), loss: 0.007103, time: 2.245 s\n",
      "Step  75380 (epoch   23.56), loss: 0.006514, time: 2.240 s\n",
      "Step  75400 (epoch   23.56), loss: 0.015001, time: 2.239 s, error: 0.014057\n",
      "Step  75420 (epoch   23.57), loss: 0.006550, time: 15.067 s\n",
      "Step  75440 (epoch   23.57), loss: 0.008288, time: 2.248 s\n",
      "Step  75460 (epoch   23.58), loss: 0.007329, time: 2.238 s\n",
      "Step  75480 (epoch   23.59), loss: 0.007897, time: 2.236 s\n",
      "Step  75500 (epoch   23.59), loss: 0.004731, time: 2.237 s\n",
      "Step  75520 (epoch   23.60), loss: 0.008772, time: 2.237 s\n",
      "Step  75540 (epoch   23.61), loss: 0.006646, time: 2.230 s\n",
      "Step  75560 (epoch   23.61), loss: 0.008532, time: 2.242 s\n",
      "Step  75580 (epoch   23.62), loss: 0.005419, time: 2.237 s\n",
      "Step  75600 (epoch   23.62), loss: 0.008046, time: 2.240 s, error: 0.013931\n",
      "Step  75620 (epoch   23.63), loss: 0.009066, time: 14.976 s\n",
      "Step  75640 (epoch   23.64), loss: 0.010938, time: 2.248 s\n",
      "Step  75660 (epoch   23.64), loss: 0.007597, time: 2.237 s\n",
      "Step  75680 (epoch   23.65), loss: 0.012660, time: 2.227 s\n",
      "Step  75700 (epoch   23.66), loss: 0.005963, time: 2.247 s\n",
      "Step  75720 (epoch   23.66), loss: 0.005804, time: 2.255 s\n",
      "Step  75740 (epoch   23.67), loss: 0.009342, time: 2.247 s\n",
      "Step  75760 (epoch   23.68), loss: 0.005836, time: 2.238 s\n",
      "Step  75780 (epoch   23.68), loss: 0.010091, time: 2.242 s\n",
      "Step  75800 (epoch   23.69), loss: 0.010133, time: 2.232 s, error: 0.014420\n",
      "Step  75820 (epoch   23.69), loss: 0.011004, time: 14.978 s\n",
      "Step  75840 (epoch   23.70), loss: 0.007951, time: 2.235 s\n",
      "Step  75860 (epoch   23.71), loss: 0.010890, time: 2.248 s\n",
      "Step  75880 (epoch   23.71), loss: 0.012585, time: 2.230 s\n",
      "Step  75900 (epoch   23.72), loss: 0.006346, time: 2.247 s\n",
      "Step  75920 (epoch   23.73), loss: 0.010898, time: 2.227 s\n",
      "Step  75940 (epoch   23.73), loss: 0.011153, time: 2.239 s\n",
      "Step  75960 (epoch   23.74), loss: 0.007907, time: 2.241 s\n",
      "Step  75980 (epoch   23.74), loss: 0.007114, time: 2.252 s\n",
      "Step  76000 (epoch   23.75), loss: 0.008718, time: 2.229 s, error: 0.014062\n",
      "\n",
      "Time since beginning  : 13147.196 s\n",
      "\n",
      "Step  76020 (epoch   23.76), loss: 0.004290, time: 15.082 s\n",
      "Step  76040 (epoch   23.76), loss: 0.009203, time: 2.237 s\n",
      "Step  76060 (epoch   23.77), loss: 0.010765, time: 2.230 s\n",
      "Step  76080 (epoch   23.77), loss: 0.008026, time: 2.240 s\n",
      "Step  76100 (epoch   23.78), loss: 0.007623, time: 2.233 s\n",
      "Step  76120 (epoch   23.79), loss: 0.007037, time: 2.243 s\n",
      "Step  76140 (epoch   23.79), loss: 0.007976, time: 2.231 s\n",
      "Step  76160 (epoch   23.80), loss: 0.007954, time: 2.243 s\n",
      "Step  76180 (epoch   23.81), loss: 0.009094, time: 2.231 s\n",
      "Step  76200 (epoch   23.81), loss: 0.008669, time: 2.243 s, error: 0.014364\n",
      "Step  76220 (epoch   23.82), loss: 0.004901, time: 15.227 s\n",
      "Step  76240 (epoch   23.82), loss: 0.009241, time: 2.240 s\n",
      "Step  76260 (epoch   23.83), loss: 0.007374, time: 2.241 s\n",
      "Step  76280 (epoch   23.84), loss: 0.007855, time: 2.246 s\n",
      "Step  76300 (epoch   23.84), loss: 0.007483, time: 2.247 s\n",
      "Step  76320 (epoch   23.85), loss: 0.007196, time: 2.225 s\n",
      "Step  76340 (epoch   23.86), loss: 0.009034, time: 2.235 s\n",
      "Step  76360 (epoch   23.86), loss: 0.010512, time: 2.233 s\n",
      "Step  76380 (epoch   23.87), loss: 0.010586, time: 2.241 s\n",
      "Step  76400 (epoch   23.88), loss: 0.008314, time: 2.241 s, error: 0.014338\n",
      "Step  76420 (epoch   23.88), loss: 0.010100, time: 15.222 s\n",
      "Step  76440 (epoch   23.89), loss: 0.007631, time: 2.245 s\n",
      "Step  76460 (epoch   23.89), loss: 0.008056, time: 2.244 s\n",
      "Step  76480 (epoch   23.90), loss: 0.008952, time: 2.242 s\n",
      "Step  76500 (epoch   23.91), loss: 0.012346, time: 2.242 s\n",
      "Step  76520 (epoch   23.91), loss: 0.007467, time: 2.243 s\n",
      "Step  76540 (epoch   23.92), loss: 0.007493, time: 2.240 s\n",
      "Step  76560 (epoch   23.93), loss: 0.007373, time: 2.236 s\n",
      "Step  76580 (epoch   23.93), loss: 0.006983, time: 2.232 s\n",
      "Step  76600 (epoch   23.94), loss: 0.009983, time: 2.238 s, error: 0.015147\n",
      "Step  76620 (epoch   23.94), loss: 0.008029, time: 15.002 s\n",
      "Step  76640 (epoch   23.95), loss: 0.007211, time: 2.253 s\n",
      "Step  76660 (epoch   23.96), loss: 0.009934, time: 2.254 s\n",
      "Step  76680 (epoch   23.96), loss: 0.007543, time: 2.232 s\n",
      "Step  76700 (epoch   23.97), loss: 0.008227, time: 2.243 s\n",
      "Step  76720 (epoch   23.98), loss: 0.008407, time: 2.234 s\n",
      "Step  76740 (epoch   23.98), loss: 0.006977, time: 2.244 s\n",
      "Step  76760 (epoch   23.99), loss: 0.006332, time: 2.249 s\n",
      "Step  76780 (epoch   23.99), loss: 0.012765, time: 2.248 s\n",
      "Step  76800 (epoch   24.00), loss: 0.009562, time: 2.238 s, error: 0.013978\n",
      "Step  76820 (epoch   24.01), loss: 0.009226, time: 15.032 s\n",
      "Step  76840 (epoch   24.01), loss: 0.009317, time: 2.234 s\n",
      "Step  76860 (epoch   24.02), loss: 0.006442, time: 2.231 s\n",
      "Step  76880 (epoch   24.02), loss: 0.009240, time: 2.246 s\n",
      "Step  76900 (epoch   24.03), loss: 0.005196, time: 2.252 s\n",
      "Step  76920 (epoch   24.04), loss: 0.006732, time: 2.245 s\n",
      "Step  76940 (epoch   24.04), loss: 0.009524, time: 2.231 s\n",
      "Step  76960 (epoch   24.05), loss: 0.006371, time: 2.243 s\n",
      "Step  76980 (epoch   24.06), loss: 0.007512, time: 2.235 s\n",
      "Step  77000 (epoch   24.06), loss: 0.006747, time: 2.230 s, error: 0.014092\n",
      "Step  77020 (epoch   24.07), loss: 0.009792, time: 15.000 s\n",
      "Step  77040 (epoch   24.07), loss: 0.009287, time: 2.239 s\n",
      "Step  77060 (epoch   24.08), loss: 0.008488, time: 2.234 s\n",
      "Step  77080 (epoch   24.09), loss: 0.010095, time: 2.229 s\n",
      "Step  77100 (epoch   24.09), loss: 0.006231, time: 2.247 s\n",
      "Step  77120 (epoch   24.10), loss: 0.007925, time: 2.239 s\n",
      "Step  77140 (epoch   24.11), loss: 0.007510, time: 2.241 s\n",
      "Step  77160 (epoch   24.11), loss: 0.016362, time: 2.238 s\n",
      "Step  77180 (epoch   24.12), loss: 0.010278, time: 2.252 s\n",
      "Step  77200 (epoch   24.12), loss: 0.006963, time: 2.231 s, error: 0.014191\n",
      "Step  77220 (epoch   24.13), loss: 0.010738, time: 15.032 s\n",
      "Step  77240 (epoch   24.14), loss: 0.008994, time: 2.244 s\n",
      "Step  77260 (epoch   24.14), loss: 0.007088, time: 2.241 s\n",
      "Step  77280 (epoch   24.15), loss: 0.008765, time: 2.240 s\n",
      "Step  77300 (epoch   24.16), loss: 0.008289, time: 2.238 s\n",
      "Step  77320 (epoch   24.16), loss: 0.005416, time: 2.240 s\n",
      "Step  78600 (epoch   24.56), loss: 0.006711, time: 2.230 s, error: 0.014465\n",
      "Step  78620 (epoch   24.57), loss: 0.006845, time: 15.214 s\n",
      "Step  88720 (epoch   27.73), loss: 0.007105, time: 2.247 s\n",
      "Step  88740 (epoch   27.73), loss: 0.007094, time: 2.242 s\n",
      "Step  88760 (epoch   27.74), loss: 0.005159, time: 2.246 s\n",
      "Step  88780 (epoch   27.74), loss: 0.009593, time: 2.246 s\n",
      "Step  88800 (epoch   27.75), loss: 0.007685, time: 2.250 s, error: 0.013489\n",
      "Step  88820 (epoch   27.76), loss: 0.010168, time: 15.026 s\n",
      "Step  88840 (epoch   27.76), loss: 0.006353, time: 2.256 s\n",
      "Step  88860 (epoch   27.77), loss: 0.007853, time: 2.242 s\n",
      "Step  88880 (epoch   27.77), loss: 0.005368, time: 2.235 s\n",
      "Step  88900 (epoch   27.78), loss: 0.007726, time: 2.250 s\n",
      "Step  88920 (epoch   27.79), loss: 0.006941, time: 2.239 s\n",
      "Step  88940 (epoch   27.79), loss: 0.006417, time: 2.244 s\n",
      "Step  88960 (epoch   27.80), loss: 0.007142, time: 2.246 s\n",
      "Step  88980 (epoch   27.81), loss: 0.011763, time: 2.251 s\n",
      "Step  89000 (epoch   27.81), loss: 0.006237, time: 2.248 s, error: 0.013645\n",
      "Step  89020 (epoch   27.82), loss: 0.006536, time: 15.041 s\n",
      "Step  89040 (epoch   27.82), loss: 0.007205, time: 2.245 s\n",
      "Step  89060 (epoch   27.83), loss: 0.011008, time: 2.237 s\n",
      "Step  89080 (epoch   27.84), loss: 0.009258, time: 2.251 s\n",
      "Step  89100 (epoch   27.84), loss: 0.006132, time: 2.254 s\n",
      "Step  89120 (epoch   27.85), loss: 0.004727, time: 2.243 s\n",
      "Step  89140 (epoch   27.86), loss: 0.007529, time: 2.235 s\n",
      "Step  89160 (epoch   27.86), loss: 0.008096, time: 2.229 s\n",
      "Step  89180 (epoch   27.87), loss: 0.006660, time: 2.247 s\n",
      "Step  89200 (epoch   27.88), loss: 0.005956, time: 2.243 s, error: 0.013478\n",
      "Step  89220 (epoch   27.88), loss: 0.008637, time: 15.060 s\n",
      "Step  89240 (epoch   27.89), loss: 0.007454, time: 2.239 s\n",
      "Step  89260 (epoch   27.89), loss: 0.006104, time: 2.235 s\n",
      "Step  89280 (epoch   27.90), loss: 0.007079, time: 2.230 s\n",
      "Step  89300 (epoch   27.91), loss: 0.015776, time: 2.239 s\n",
      "Step  89320 (epoch   27.91), loss: 0.007747, time: 2.237 s\n",
      "Step  89340 (epoch   27.92), loss: 0.008313, time: 2.228 s\n",
      "Step  89360 (epoch   27.93), loss: 0.005978, time: 2.247 s\n",
      "Step  89380 (epoch   27.93), loss: 0.005900, time: 2.252 s\n",
      "Step  89400 (epoch   27.94), loss: 0.005906, time: 2.239 s, error: 0.013344\n",
      "Step  89420 (epoch   27.94), loss: 0.008704, time: 15.001 s\n",
      "Step  89440 (epoch   27.95), loss: 0.006854, time: 2.240 s\n",
      "Step  89460 (epoch   27.96), loss: 0.008002, time: 2.248 s\n",
      "Step  89480 (epoch   27.96), loss: 0.010209, time: 2.249 s\n",
      "Step  89500 (epoch   27.97), loss: 0.005152, time: 2.243 s\n",
      "Step  89520 (epoch   27.98), loss: 0.005712, time: 2.248 s\n",
      "Step  89540 (epoch   27.98), loss: 0.010473, time: 2.248 s\n",
      "Step  89560 (epoch   27.99), loss: 0.008683, time: 2.247 s\n",
      "Step  89580 (epoch   27.99), loss: 0.010311, time: 2.243 s\n",
      "Step  89600 (epoch   28.00), loss: 0.005885, time: 2.239 s, error: 0.013395\n",
      "Step  89620 (epoch   28.01), loss: 0.007724, time: 15.282 s\n",
      "Step  89640 (epoch   28.01), loss: 0.011749, time: 2.243 s\n",
      "Step  89660 (epoch   28.02), loss: 0.008340, time: 2.245 s\n",
      "Step  89680 (epoch   28.02), loss: 0.006038, time: 2.234 s\n",
      "Step  89700 (epoch   28.03), loss: 0.006568, time: 2.236 s\n",
      "Step  89720 (epoch   28.04), loss: 0.006567, time: 2.244 s\n",
      "Step  89740 (epoch   28.04), loss: 0.008507, time: 2.251 s\n",
      "Step  89760 (epoch   28.05), loss: 0.007034, time: 2.243 s\n",
      "Step  89780 (epoch   28.06), loss: 0.009458, time: 2.236 s\n",
      "Step  89800 (epoch   28.06), loss: 0.007351, time: 2.234 s, error: 0.013921\n",
      "Step  89820 (epoch   28.07), loss: 0.005947, time: 15.197 s\n",
      "Step  89840 (epoch   28.07), loss: 0.010034, time: 2.252 s\n",
      "Step  89860 (epoch   28.08), loss: 0.009587, time: 2.248 s\n",
      "Step  89880 (epoch   28.09), loss: 0.007694, time: 2.252 s\n",
      "Step  89900 (epoch   28.09), loss: 0.005604, time: 2.252 s\n",
      "Step  89920 (epoch   28.10), loss: 0.009704, time: 2.243 s\n",
      "Step  89940 (epoch   28.11), loss: 0.006025, time: 2.245 s\n",
      "Step  89960 (epoch   28.11), loss: 0.005519, time: 2.241 s\n",
      "Step  89980 (epoch   28.12), loss: 0.013119, time: 2.248 s\n",
      "Step  90000 (epoch   28.12), loss: 0.004534, time: 2.237 s, error: 0.013318\n",
      "\n",
      "Time since beginning  : 15616.950 s\n",
      "\n",
      "Step  90020 (epoch   28.13), loss: 0.004882, time: 15.134 s\n",
      "Step  90040 (epoch   28.14), loss: 0.006768, time: 2.250 s\n",
      "Step  90060 (epoch   28.14), loss: 0.007573, time: 2.246 s\n",
      "Step  90080 (epoch   28.15), loss: 0.009062, time: 2.244 s\n",
      "Step  90100 (epoch   28.16), loss: 0.013087, time: 2.236 s\n",
      "Step  90120 (epoch   28.16), loss: 0.007957, time: 2.247 s\n",
      "Step  90140 (epoch   28.17), loss: 0.008580, time: 2.244 s\n",
      "Step  90160 (epoch   28.18), loss: 0.008916, time: 2.247 s\n",
      "Step  90180 (epoch   28.18), loss: 0.005450, time: 2.247 s\n",
      "Step  90200 (epoch   28.19), loss: 0.005593, time: 2.252 s, error: 0.013200\n",
      "Step  90220 (epoch   28.19), loss: 0.012599, time: 15.133 s\n",
      "Step  90240 (epoch   28.20), loss: 0.008217, time: 2.248 s\n",
      "Step  90260 (epoch   28.21), loss: 0.004598, time: 2.246 s\n",
      "Step  90280 (epoch   28.21), loss: 0.007218, time: 2.245 s\n",
      "Step  90300 (epoch   28.22), loss: 0.011973, time: 2.258 s\n",
      "Step  90320 (epoch   28.23), loss: 0.011776, time: 2.233 s\n",
      "Step  90340 (epoch   28.23), loss: 0.006644, time: 2.251 s\n",
      "Step  90360 (epoch   28.24), loss: 0.008803, time: 2.245 s\n",
      "Step  90380 (epoch   28.24), loss: 0.005599, time: 2.236 s\n",
      "Step  90400 (epoch   28.25), loss: 0.007443, time: 2.240 s, error: 0.013590\n",
      "Step  90420 (epoch   28.26), loss: 0.005923, time: 15.066 s\n",
      "Step  90440 (epoch   28.26), loss: 0.006632, time: 2.237 s\n",
      "Step  90460 (epoch   28.27), loss: 0.006733, time: 2.230 s\n",
      "Step  90480 (epoch   28.27), loss: 0.007156, time: 2.249 s\n",
      "Step  90500 (epoch   28.28), loss: 0.006891, time: 2.245 s\n",
      "Step  90520 (epoch   28.29), loss: 0.007741, time: 2.245 s\n",
      "Step  90540 (epoch   28.29), loss: 0.006149, time: 2.244 s\n",
      "Step  90560 (epoch   28.30), loss: 0.007541, time: 2.253 s\n",
      "Step  90580 (epoch   28.31), loss: 0.007042, time: 2.250 s\n",
      "Step  90600 (epoch   28.31), loss: 0.007373, time: 2.241 s, error: 0.013624\n",
      "Step  90620 (epoch   28.32), loss: 0.007869, time: 15.108 s\n",
      "Step  90640 (epoch   28.32), loss: 0.007203, time: 2.242 s\n",
      "Step  90660 (epoch   28.33), loss: 0.006917, time: 2.244 s\n",
      "Step  90680 (epoch   28.34), loss: 0.010366, time: 2.231 s\n",
      "Step  90700 (epoch   28.34), loss: 0.012368, time: 2.231 s\n",
      "Step  90720 (epoch   28.35), loss: 0.005624, time: 2.236 s\n",
      "Step  90740 (epoch   28.36), loss: 0.006302, time: 2.249 s\n",
      "Step  90760 (epoch   28.36), loss: 0.006913, time: 2.236 s\n",
      "Step  90780 (epoch   28.37), loss: 0.005338, time: 2.242 s\n",
      "Step  90800 (epoch   28.38), loss: 0.008375, time: 2.238 s, error: 0.014218\n",
      "Step  90820 (epoch   28.38), loss: 0.007863, time: 15.251 s\n",
      "Step  90840 (epoch   28.39), loss: 0.006131, time: 2.246 s\n",
      "Step  90860 (epoch   28.39), loss: 0.004959, time: 2.239 s\n",
      "Step  90880 (epoch   28.40), loss: 0.006325, time: 2.239 s\n",
      "Step  90900 (epoch   28.41), loss: 0.010318, time: 2.225 s\n",
      "Step  90920 (epoch   28.41), loss: 0.009919, time: 2.248 s\n",
      "Step  90940 (epoch   28.42), loss: 0.010149, time: 2.251 s\n",
      "Step  90960 (epoch   28.43), loss: 0.008184, time: 2.255 s\n",
      "Step  90980 (epoch   28.43), loss: 0.010950, time: 2.244 s\n",
      "Step  91000 (epoch   28.44), loss: 0.005838, time: 2.235 s, error: 0.013411\n",
      "Step  91020 (epoch   28.44), loss: 0.006688, time: 15.214 s\n",
      "Step  91040 (epoch   28.45), loss: 0.005269, time: 2.242 s\n",
      "Step  91060 (epoch   28.46), loss: 0.007119, time: 2.250 s\n",
      "Step  91080 (epoch   28.46), loss: 0.006620, time: 2.250 s\n",
      "Step  91100 (epoch   28.47), loss: 0.004959, time: 2.249 s\n",
      "Step  91120 (epoch   28.48), loss: 0.006091, time: 2.243 s\n",
      "Step  91140 (epoch   28.48), loss: 0.006507, time: 2.251 s\n",
      "Step  91160 (epoch   28.49), loss: 0.011262, time: 2.253 s\n",
      "Step  91180 (epoch   28.49), loss: 0.007694, time: 2.240 s\n",
      "Step  91200 (epoch   28.50), loss: 0.005407, time: 2.252 s, error: 0.013663\n",
      "Step  91220 (epoch   28.51), loss: 0.005338, time: 15.099 s\n",
      "Step  91240 (epoch   28.51), loss: 0.005059, time: 2.241 s\n",
      "Step  91260 (epoch   28.52), loss: 0.009485, time: 2.236 s\n",
      "Step  91280 (epoch   28.52), loss: 0.006786, time: 2.238 s\n",
      "Step  91300 (epoch   28.53), loss: 0.018353, time: 2.242 s\n",
      "Step  91320 (epoch   28.54), loss: 0.006752, time: 2.256 s\n",
      "Step  91340 (epoch   28.54), loss: 0.006568, time: 2.251 s\n",
      "Step  91360 (epoch   28.55), loss: 0.010234, time: 2.247 s\n",
      "Step  91380 (epoch   28.56), loss: 0.007146, time: 2.251 s\n",
      "Step  91400 (epoch   28.56), loss: 0.008301, time: 2.242 s, error: 0.014243\n",
      "Step  91420 (epoch   28.57), loss: 0.006701, time: 15.148 s\n",
      "Step  91440 (epoch   28.57), loss: 0.008663, time: 2.248 s\n",
      "Step  91460 (epoch   28.58), loss: 0.006424, time: 2.242 s\n",
      "Step  91480 (epoch   28.59), loss: 0.008920, time: 2.240 s\n",
      "Step  91500 (epoch   28.59), loss: 0.005252, time: 2.253 s\n",
      "Step  91520 (epoch   28.60), loss: 0.006244, time: 2.227 s\n",
      "Step  91540 (epoch   28.61), loss: 0.005745, time: 2.246 s\n",
      "Step  91560 (epoch   28.61), loss: 0.008130, time: 2.239 s\n",
      "Step  91580 (epoch   28.62), loss: 0.009883, time: 2.246 s\n",
      "Step  91600 (epoch   28.62), loss: 0.011229, time: 2.234 s, error: 0.013484\n",
      "Step  91620 (epoch   28.63), loss: 0.006229, time: 15.093 s\n",
      "Step  91640 (epoch   28.64), loss: 0.006705, time: 2.251 s\n",
      "Step  91660 (epoch   28.64), loss: 0.014324, time: 2.243 s\n",
      "Step  91680 (epoch   28.65), loss: 0.006559, time: 2.237 s\n",
      "Step  91700 (epoch   28.66), loss: 0.005750, time: 2.242 s\n",
      "Step  91720 (epoch   28.66), loss: 0.006455, time: 2.233 s\n",
      "Step  91740 (epoch   28.67), loss: 0.010786, time: 2.247 s\n",
      "Step  91760 (epoch   28.68), loss: 0.006774, time: 2.249 s\n",
      "Step  91780 (epoch   28.68), loss: 0.008348, time: 2.252 s\n",
      "Step  91800 (epoch   28.69), loss: 0.005920, time: 2.245 s, error: 0.014258\n",
      "Step  91820 (epoch   28.69), loss: 0.009196, time: 15.042 s\n",
      "Step  91840 (epoch   28.70), loss: 0.007550, time: 2.237 s\n",
      "Step  91860 (epoch   28.71), loss: 0.008438, time: 2.247 s\n",
      "Step  91880 (epoch   28.71), loss: 0.012159, time: 2.244 s\n",
      "Step  91900 (epoch   28.72), loss: 0.005163, time: 2.235 s\n",
      "Step  91920 (epoch   28.73), loss: 0.010302, time: 2.240 s\n",
      "Step  91940 (epoch   28.73), loss: 0.013289, time: 2.245 s\n",
      "Step  91960 (epoch   28.74), loss: 0.008595, time: 2.241 s\n",
      "Step  91980 (epoch   28.74), loss: 0.009018, time: 2.250 s\n",
      "Step  92000 (epoch   28.75), loss: 0.006526, time: 2.254 s, error: 0.013193\n",
      "\n",
      "Time since beginning  : 15970.364 s\n",
      "\n",
      "Step  92020 (epoch   28.76), loss: 0.006521, time: 15.321 s\n",
      "Step  92040 (epoch   28.76), loss: 0.007116, time: 2.241 s\n",
      "Step  92060 (epoch   28.77), loss: 0.006407, time: 2.246 s\n",
      "Step  92080 (epoch   28.77), loss: 0.006375, time: 2.234 s\n",
      "Step  92100 (epoch   28.78), loss: 0.006968, time: 2.235 s\n",
      "Step  92120 (epoch   28.79), loss: 0.007685, time: 2.245 s\n",
      "Step  92140 (epoch   28.79), loss: 0.008253, time: 2.240 s\n",
      "Step  92160 (epoch   28.80), loss: 0.010389, time: 2.244 s\n",
      "Step  92180 (epoch   28.81), loss: 0.011026, time: 2.232 s\n",
      "Step  92200 (epoch   28.81), loss: 0.004848, time: 2.235 s, error: 0.013418\n",
      "Step  92220 (epoch   28.82), loss: 0.008528, time: 15.211 s\n",
      "Step  92240 (epoch   28.82), loss: 0.006330, time: 2.234 s\n",
      "Step  92260 (epoch   28.83), loss: 0.005790, time: 2.247 s\n",
      "Step  92280 (epoch   28.84), loss: 0.011715, time: 2.235 s\n",
      "Step  92300 (epoch   28.84), loss: 0.003632, time: 2.249 s\n",
      "Step  92320 (epoch   28.85), loss: 0.006835, time: 2.236 s\n",
      "Step  92340 (epoch   28.86), loss: 0.007068, time: 2.249 s\n",
      "Step  92360 (epoch   28.86), loss: 0.006372, time: 2.237 s\n",
      "Step  92380 (epoch   28.87), loss: 0.007304, time: 2.246 s\n",
      "Step  92400 (epoch   28.88), loss: 0.005862, time: 2.239 s, error: 0.013590\n",
      "Step  92420 (epoch   28.88), loss: 0.007504, time: 15.031 s\n",
      "Step  92440 (epoch   28.89), loss: 0.007351, time: 2.243 s\n",
      "Step  92460 (epoch   28.89), loss: 0.006296, time: 2.240 s\n",
      "Step  92480 (epoch   28.90), loss: 0.009748, time: 2.238 s\n",
      "Step  92500 (epoch   28.91), loss: 0.011863, time: 2.219 s\n",
      "Step  92520 (epoch   28.91), loss: 0.006712, time: 2.238 s\n",
      "Step  92540 (epoch   28.92), loss: 0.010507, time: 2.240 s\n",
      "Step  92560 (epoch   28.93), loss: 0.004809, time: 2.241 s\n",
      "Step  92580 (epoch   28.93), loss: 0.009486, time: 2.241 s\n",
      "Step  92600 (epoch   28.94), loss: 0.006412, time: 2.241 s, error: 0.013236\n",
      "Step  92620 (epoch   28.94), loss: 0.004786, time: 15.044 s\n",
      "Step  92640 (epoch   28.95), loss: 0.006308, time: 2.235 s\n",
      "Step  92660 (epoch   28.96), loss: 0.007563, time: 2.247 s\n",
      "Step  92680 (epoch   28.96), loss: 0.011315, time: 2.252 s\n",
      "Step  92700 (epoch   28.97), loss: 0.006446, time: 2.251 s\n",
      "Step  92720 (epoch   28.98), loss: 0.010340, time: 2.231 s\n",
      "Step  92740 (epoch   28.98), loss: 0.007096, time: 2.246 s\n",
      "Step  92760 (epoch   28.99), loss: 0.010845, time: 2.245 s\n",
      "Step  92780 (epoch   28.99), loss: 0.007859, time: 2.254 s\n",
      "Step  92800 (epoch   29.00), loss: 0.008019, time: 2.246 s, error: 0.013145\n",
      "Step  92820 (epoch   29.01), loss: 0.004205, time: 15.104 s\n",
      "Step  92840 (epoch   29.01), loss: 0.007946, time: 2.242 s\n",
      "Step  92860 (epoch   29.02), loss: 0.005617, time: 2.231 s\n",
      "Step  92880 (epoch   29.02), loss: 0.006189, time: 2.236 s\n",
      "Step  92900 (epoch   29.03), loss: 0.008140, time: 2.238 s\n",
      "Step  92920 (epoch   29.04), loss: 0.007650, time: 2.241 s\n",
      "Step  92940 (epoch   29.04), loss: 0.008224, time: 2.248 s\n",
      "Step  92960 (epoch   29.05), loss: 0.007643, time: 2.252 s\n",
      "Step  92980 (epoch   29.06), loss: 0.011196, time: 2.232 s\n",
      "Step  93000 (epoch   29.06), loss: 0.005604, time: 2.240 s, error: 0.013700\n",
      "Step  93020 (epoch   29.07), loss: 0.008860, time: 15.050 s\n",
      "Step  93040 (epoch   29.07), loss: 0.007282, time: 2.238 s\n",
      "Step  93060 (epoch   29.08), loss: 0.008877, time: 2.236 s\n",
      "Step  93080 (epoch   29.09), loss: 0.007346, time: 2.233 s\n",
      "Step  93100 (epoch   29.09), loss: 0.005909, time: 2.253 s\n",
      "Step  93120 (epoch   29.10), loss: 0.007000, time: 2.242 s\n",
      "Step  93140 (epoch   29.11), loss: 0.007381, time: 2.239 s\n",
      "Step  93160 (epoch   29.11), loss: 0.006190, time: 2.240 s\n",
      "Step  93180 (epoch   29.12), loss: 0.006030, time: 2.237 s\n",
      "Step  93200 (epoch   29.12), loss: 0.005829, time: 2.250 s, error: 0.013308\n",
      "Step  93220 (epoch   29.13), loss: 0.011812, time: 15.089 s\n",
      "Step  93240 (epoch   29.14), loss: 0.006118, time: 2.250 s\n",
      "Step  93260 (epoch   29.14), loss: 0.008548, time: 2.244 s\n",
      "Step  93280 (epoch   29.15), loss: 0.006597, time: 2.240 s\n",
      "Step  93300 (epoch   29.16), loss: 0.007265, time: 2.236 s\n",
      "Step  93320 (epoch   29.16), loss: 0.008075, time: 2.239 s\n",
      "Step  93340 (epoch   29.17), loss: 0.004648, time: 2.236 s\n",
      "Step  93360 (epoch   29.18), loss: 0.012966, time: 2.236 s\n",
      "Step  93380 (epoch   29.18), loss: 0.008545, time: 2.240 s\n",
      "Step  93400 (epoch   29.19), loss: 0.007693, time: 2.232 s, error: 0.013176\n",
      "Step  93420 (epoch   29.19), loss: 0.009930, time: 15.277 s\n",
      "Step  93440 (epoch   29.20), loss: 0.012593, time: 2.245 s\n",
      "Step  93460 (epoch   29.21), loss: 0.006684, time: 2.251 s\n",
      "Step  93480 (epoch   29.21), loss: 0.004654, time: 2.241 s\n",
      "Step  93500 (epoch   29.22), loss: 0.011635, time: 2.246 s\n",
      "Step  93520 (epoch   29.23), loss: 0.011458, time: 2.227 s\n",
      "Step  93540 (epoch   29.23), loss: 0.008346, time: 2.237 s\n",
      "Step  93560 (epoch   29.24), loss: 0.006641, time: 2.240 s\n",
      "Step  93580 (epoch   29.24), loss: 0.005033, time: 2.242 s\n",
      "Step  93600 (epoch   29.25), loss: 0.007292, time: 2.243 s, error: 0.013499\n",
      "Step  93620 (epoch   29.26), loss: 0.006418, time: 15.077 s\n",
      "Step  93640 (epoch   29.26), loss: 0.005785, time: 2.250 s\n",
      "Step  93660 (epoch   29.27), loss: 0.007093, time: 2.239 s\n",
      "Step  93680 (epoch   29.27), loss: 0.005187, time: 2.250 s\n",
      "Step  93700 (epoch   29.28), loss: 0.012342, time: 2.250 s\n",
      "Step  93720 (epoch   29.29), loss: 0.007354, time: 2.237 s\n",
      "Step  93740 (epoch   29.29), loss: 0.004627, time: 2.237 s\n",
      "Step  93760 (epoch   29.30), loss: 0.005987, time: 2.240 s\n",
      "Step  93780 (epoch   29.31), loss: 0.008337, time: 2.248 s\n",
      "Step  93800 (epoch   29.31), loss: 0.005851, time: 2.239 s, error: 0.013707\n",
      "Step  93820 (epoch   29.32), loss: 0.008029, time: 15.043 s\n",
      "Step  93840 (epoch   29.32), loss: 0.006532, time: 2.229 s\n",
      "Step  93860 (epoch   29.33), loss: 0.005435, time: 2.243 s\n",
      "Step  93880 (epoch   29.34), loss: 0.007233, time: 2.257 s\n",
      "Step  93900 (epoch   29.34), loss: 0.005533, time: 2.255 s\n",
      "Step  93920 (epoch   29.35), loss: 0.007039, time: 2.233 s\n",
      "Step  93940 (epoch   29.36), loss: 0.007941, time: 2.247 s\n",
      "Step  93960 (epoch   29.36), loss: 0.005903, time: 2.243 s\n",
      "Step  93980 (epoch   29.37), loss: 0.007083, time: 2.249 s\n",
      "Step  94000 (epoch   29.38), loss: 0.010042, time: 2.233 s, error: 0.013884\n",
      "\n",
      "Time since beginning  : 16323.184 s\n",
      "\n",
      "Step  94020 (epoch   29.38), loss: 0.005993, time: 15.176 s\n",
      "Step  94040 (epoch   29.39), loss: 0.005924, time: 2.247 s\n",
      "Step  94060 (epoch   29.39), loss: 0.005278, time: 2.232 s\n",
      "Step  94080 (epoch   29.40), loss: 0.009067, time: 2.230 s\n",
      "Step  94100 (epoch   29.41), loss: 0.015279, time: 2.236 s\n",
      "Step  94120 (epoch   29.41), loss: 0.007687, time: 2.249 s\n",
      "Step  94140 (epoch   29.42), loss: 0.009148, time: 2.255 s\n",
      "Step  94160 (epoch   29.43), loss: 0.008265, time: 2.254 s\n",
      "Step  94180 (epoch   29.43), loss: 0.007162, time: 2.245 s\n",
      "Step  94200 (epoch   29.44), loss: 0.011379, time: 2.249 s, error: 0.013319\n",
      "Step  94220 (epoch   29.44), loss: 0.010432, time: 15.002 s\n",
      "Step  94240 (epoch   29.45), loss: 0.004487, time: 2.230 s\n",
      "Step  94260 (epoch   29.46), loss: 0.005365, time: 2.238 s\n",
      "Step  94280 (epoch   29.46), loss: 0.009722, time: 2.241 s\n",
      "Step  94300 (epoch   29.47), loss: 0.005052, time: 2.237 s\n",
      "Step  94320 (epoch   29.48), loss: 0.006904, time: 2.233 s\n",
      "Step  94340 (epoch   29.48), loss: 0.012815, time: 2.235 s\n",
      "Step  94360 (epoch   29.49), loss: 0.007272, time: 2.238 s\n",
      "Step  94380 (epoch   29.49), loss: 0.005904, time: 2.245 s\n",
      "Step  94400 (epoch   29.50), loss: 0.008004, time: 2.252 s, error: 0.013503\n",
      "Step  94420 (epoch   29.51), loss: 0.006631, time: 15.049 s\n",
      "Step  94440 (epoch   29.51), loss: 0.007596, time: 2.252 s\n",
      "Step  94460 (epoch   29.52), loss: 0.007800, time: 2.249 s\n",
      "Step  94480 (epoch   29.52), loss: 0.006569, time: 2.247 s\n",
      "Step  94500 (epoch   29.53), loss: 0.009973, time: 2.246 s\n",
      "Step  94520 (epoch   29.54), loss: 0.005824, time: 2.245 s\n",
      "Step  94540 (epoch   29.54), loss: 0.006789, time: 2.247 s\n",
      "Step  94560 (epoch   29.55), loss: 0.006066, time: 2.249 s\n",
      "Step  94580 (epoch   29.56), loss: 0.007781, time: 2.237 s\n",
      "Step  94600 (epoch   29.56), loss: 0.006778, time: 2.245 s, error: 0.013519\n",
      "Step  94620 (epoch   29.57), loss: 0.007644, time: 15.181 s\n",
      "Step  94640 (epoch   29.57), loss: 0.006654, time: 2.242 s\n",
      "Step  94660 (epoch   29.58), loss: 0.010392, time: 2.249 s\n",
      "Step  94680 (epoch   29.59), loss: 0.008561, time: 2.249 s\n",
      "Step  94700 (epoch   29.59), loss: 0.009439, time: 2.249 s\n",
      "Step  94720 (epoch   29.60), loss: 0.008969, time: 2.243 s\n",
      "Step  94740 (epoch   29.61), loss: 0.008361, time: 2.245 s\n",
      "Step  94760 (epoch   29.61), loss: 0.005624, time: 2.248 s\n",
      "Step  94780 (epoch   29.62), loss: 0.005804, time: 2.250 s\n",
      "Step  94800 (epoch   29.62), loss: 0.005600, time: 2.241 s, error: 0.013386\n",
      "Step  94820 (epoch   29.63), loss: 0.006880, time: 15.079 s\n",
      "Step  94840 (epoch   29.64), loss: 0.010400, time: 2.242 s\n",
      "Step  94860 (epoch   29.64), loss: 0.009800, time: 2.235 s\n",
      "Step  94880 (epoch   29.65), loss: 0.006167, time: 2.247 s\n",
      "Step  94900 (epoch   29.66), loss: 0.005727, time: 2.235 s\n",
      "Step  94920 (epoch   29.66), loss: 0.006725, time: 2.249 s\n",
      "Step  94940 (epoch   29.67), loss: 0.007282, time: 2.242 s\n",
      "Step  94960 (epoch   29.68), loss: 0.010081, time: 2.245 s\n",
      "Step  94980 (epoch   29.68), loss: 0.008352, time: 2.244 s\n",
      "Step  95000 (epoch   29.69), loss: 0.010161, time: 2.244 s, error: 0.014031\n",
      "Step  95020 (epoch   29.69), loss: 0.009305, time: 15.039 s\n",
      "Step  95040 (epoch   29.70), loss: 0.006328, time: 2.250 s\n",
      "Step  95060 (epoch   29.71), loss: 0.007113, time: 2.240 s\n",
      "Step  95080 (epoch   29.71), loss: 0.008189, time: 2.245 s\n",
      "Step  95100 (epoch   29.72), loss: 0.009272, time: 2.244 s\n",
      "Step  95120 (epoch   29.73), loss: 0.009187, time: 2.234 s\n",
      "Step  95140 (epoch   29.73), loss: 0.006678, time: 2.238 s\n",
      "Step  95160 (epoch   29.74), loss: 0.011060, time: 2.240 s\n",
      "Step  95180 (epoch   29.74), loss: 0.010069, time: 2.239 s\n",
      "Step  95200 (epoch   29.75), loss: 0.006830, time: 2.235 s, error: 0.013112\n",
      "Step  95220 (epoch   29.76), loss: 0.005590, time: 15.026 s\n",
      "Step  95240 (epoch   29.76), loss: 0.006899, time: 2.245 s\n",
      "Step  95260 (epoch   29.77), loss: 0.005466, time: 2.240 s\n",
      "Step  95280 (epoch   29.77), loss: 0.006290, time: 2.232 s\n",
      "Step  95300 (epoch   29.78), loss: 0.004739, time: 2.250 s\n",
      "Step  95320 (epoch   29.79), loss: 0.005810, time: 2.245 s\n",
      "Step  95340 (epoch   29.79), loss: 0.004951, time: 2.255 s\n",
      "Step  95360 (epoch   29.80), loss: 0.006950, time: 2.247 s\n",
      "Step  95380 (epoch   29.81), loss: 0.008602, time: 2.244 s\n",
      "Step  95400 (epoch   29.81), loss: 0.008757, time: 2.243 s, error: 0.013326\n",
      "Step  95420 (epoch   29.82), loss: 0.006381, time: 14.976 s\n",
      "Step  95440 (epoch   29.82), loss: 0.007121, time: 2.241 s\n",
      "Step  95460 (epoch   29.83), loss: 0.007364, time: 2.242 s\n",
      "Step  95480 (epoch   29.84), loss: 0.005885, time: 2.241 s\n",
      "Step  95500 (epoch   29.84), loss: 0.007667, time: 2.247 s\n",
      "Step  95520 (epoch   29.85), loss: 0.006289, time: 2.231 s\n",
      "Step  95540 (epoch   29.86), loss: 0.004961, time: 2.236 s\n",
      "Step  95560 (epoch   29.86), loss: 0.006900, time: 2.234 s\n",
      "Step  95580 (epoch   29.87), loss: 0.010905, time: 2.240 s\n",
      "Step  95600 (epoch   29.88), loss: 0.006712, time: 2.254 s, error: 0.013368\n",
      "Step  95620 (epoch   29.88), loss: 0.006955, time: 15.073 s\n",
      "Step  95640 (epoch   29.89), loss: 0.003631, time: 2.241 s\n",
      "Step  95660 (epoch   29.89), loss: 0.005751, time: 2.248 s\n",
      "Step  95680 (epoch   29.90), loss: 0.009550, time: 2.236 s\n",
      "Step  95700 (epoch   29.91), loss: 0.008101, time: 2.235 s\n",
      "Step  95720 (epoch   29.91), loss: 0.008155, time: 2.246 s\n",
      "Step  95740 (epoch   29.92), loss: 0.013753, time: 2.246 s\n",
      "Step  95760 (epoch   29.93), loss: 0.009517, time: 2.241 s\n",
      "Step  95780 (epoch   29.93), loss: 0.007762, time: 2.233 s\n",
      "Step  95800 (epoch   29.94), loss: 0.009843, time: 2.240 s, error: 0.013034\n",
      "Step  95820 (epoch   29.94), loss: 0.010362, time: 15.170 s\n",
      "Step  95840 (epoch   29.95), loss: 0.008225, time: 2.239 s\n",
      "Step  95860 (epoch   29.96), loss: 0.007125, time: 2.234 s\n",
      "Step  95880 (epoch   29.96), loss: 0.005710, time: 2.229 s\n",
      "Step  95900 (epoch   29.97), loss: 0.008243, time: 2.236 s\n",
      "Step  95920 (epoch   29.98), loss: 0.007045, time: 2.238 s\n",
      "Step  95940 (epoch   29.98), loss: 0.012490, time: 2.240 s\n",
      "Step  95960 (epoch   29.99), loss: 0.004689, time: 2.243 s\n",
      "Step  95980 (epoch   29.99), loss: 0.019205, time: 2.235 s\n",
      "Step  96000 (epoch   30.00), loss: 0.009175, time: 2.243 s, error: 0.013060\n",
      "\n",
      "Time since beginning  : 16675.722 s\n",
      "\n",
      "Step  96020 (epoch   30.01), loss: 0.007162, time: 15.156 s\n",
      "Step  96040 (epoch   30.01), loss: 0.011905, time: 2.245 s\n",
      "Step  96060 (epoch   30.02), loss: 0.005932, time: 2.246 s\n",
      "Step  96080 (epoch   30.02), loss: 0.006558, time: 2.243 s\n",
      "Step  96100 (epoch   30.03), loss: 0.007701, time: 2.250 s\n",
      "Step  96120 (epoch   30.04), loss: 0.007498, time: 2.240 s\n",
      "Step  96140 (epoch   30.04), loss: 0.009805, time: 2.245 s\n",
      "Step  96160 (epoch   30.05), loss: 0.006523, time: 2.237 s\n",
      "Step  96180 (epoch   30.06), loss: 0.008517, time: 2.244 s\n",
      "Step  96200 (epoch   30.06), loss: 0.008318, time: 2.247 s, error: 0.013227\n",
      "Step  96220 (epoch   30.07), loss: 0.006822, time: 14.981 s\n",
      "Step  96240 (epoch   30.07), loss: 0.010643, time: 2.248 s\n",
      "Step  96260 (epoch   30.08), loss: 0.007202, time: 2.250 s\n",
      "Step  96280 (epoch   30.09), loss: 0.012843, time: 2.253 s\n",
      "Step  96300 (epoch   30.09), loss: 0.007697, time: 2.233 s\n",
      "Step  96320 (epoch   30.10), loss: 0.006092, time: 2.244 s\n",
      "Step  96340 (epoch   30.11), loss: 0.006957, time: 2.236 s\n",
      "Step  96360 (epoch   30.11), loss: 0.009981, time: 2.235 s\n",
      "Step  96380 (epoch   30.12), loss: 0.007635, time: 2.238 s\n",
      "Step  96400 (epoch   30.12), loss: 0.010178, time: 2.243 s, error: 0.013142\n",
      "Step  96420 (epoch   30.13), loss: 0.011789, time: 14.986 s\n",
      "Step  96440 (epoch   30.14), loss: 0.006199, time: 2.230 s\n",
      "Step  96460 (epoch   30.14), loss: 0.007746, time: 2.230 s\n",
      "Step  96480 (epoch   30.15), loss: 0.008392, time: 2.233 s\n",
      "Step  96500 (epoch   30.16), loss: 0.005740, time: 2.234 s\n",
      "Step  96520 (epoch   30.16), loss: 0.010751, time: 2.246 s\n",
      "Step  96540 (epoch   30.17), loss: 0.007080, time: 2.242 s\n",
      "Step  96560 (epoch   30.18), loss: 0.009123, time: 2.237 s\n",
      "Step  96580 (epoch   30.18), loss: 0.013797, time: 2.237 s\n",
      "Step  96600 (epoch   30.19), loss: 0.005614, time: 2.239 s, error: 0.013206\n",
      "Step  96620 (epoch   30.19), loss: 0.006465, time: 14.948 s\n",
      "Step  96640 (epoch   30.20), loss: 0.006070, time: 2.241 s\n",
      "Step  96660 (epoch   30.21), loss: 0.006700, time: 2.241 s\n",
      "Step  96680 (epoch   30.21), loss: 0.005406, time: 2.235 s\n",
      "Step  96700 (epoch   30.22), loss: 0.004682, time: 2.238 s\n",
      "Step  96720 (epoch   30.23), loss: 0.010171, time: 2.246 s\n",
      "Step  96740 (epoch   30.23), loss: 0.005620, time: 2.246 s\n",
      "Step  96760 (epoch   30.24), loss: 0.007564, time: 2.243 s\n",
      "Step  96780 (epoch   30.24), loss: 0.005792, time: 2.227 s\n",
      "Step  96800 (epoch   30.25), loss: 0.006648, time: 2.255 s, error: 0.013284\n",
      "Step  96820 (epoch   30.26), loss: 0.006943, time: 14.958 s\n",
      "Step  96840 (epoch   30.26), loss: 0.005247, time: 2.237 s\n",
      "Step  96860 (epoch   30.27), loss: 0.005863, time: 2.237 s\n",
      "Step  96880 (epoch   30.27), loss: 0.005142, time: 2.242 s\n",
      "Step  96900 (epoch   30.28), loss: 0.004748, time: 2.243 s\n",
      "Step  96920 (epoch   30.29), loss: 0.005587, time: 2.234 s\n",
      "Step  96940 (epoch   30.29), loss: 0.005572, time: 2.249 s\n",
      "Step  96960 (epoch   30.30), loss: 0.005240, time: 2.243 s\n",
      "Step  96980 (epoch   30.31), loss: 0.007974, time: 2.246 s\n",
      "Step  97000 (epoch   30.31), loss: 0.010035, time: 2.242 s, error: 0.013634\n",
      "Step  97020 (epoch   30.32), loss: 0.008335, time: 15.150 s\n",
      "Step  97040 (epoch   30.32), loss: 0.009436, time: 2.247 s\n",
      "Step  97060 (epoch   30.33), loss: 0.007721, time: 2.242 s\n",
      "Step  97080 (epoch   30.34), loss: 0.008855, time: 2.241 s\n",
      "Step  97100 (epoch   30.34), loss: 0.008481, time: 2.244 s\n",
      "Step  97120 (epoch   30.35), loss: 0.007779, time: 2.236 s\n",
      "Step  97140 (epoch   30.36), loss: 0.007565, time: 2.236 s\n",
      "Step  97160 (epoch   30.36), loss: 0.005267, time: 2.232 s\n",
      "Step  97180 (epoch   30.37), loss: 0.006945, time: 2.242 s\n",
      "Step  97200 (epoch   30.38), loss: 0.007478, time: 2.242 s, error: 0.013539\n",
      "Step  97220 (epoch   30.38), loss: 0.011590, time: 15.056 s\n",
      "Step  97240 (epoch   30.39), loss: 0.005249, time: 2.232 s\n",
      "Step  97260 (epoch   30.39), loss: 0.011382, time: 2.246 s\n",
      "Step  97280 (epoch   30.40), loss: 0.006743, time: 2.236 s\n",
      "Step  97300 (epoch   30.41), loss: 0.006542, time: 2.245 s\n",
      "Step  97320 (epoch   30.41), loss: 0.005243, time: 2.240 s\n",
      "Step  97340 (epoch   30.42), loss: 0.007550, time: 2.243 s\n",
      "Step  97360 (epoch   30.43), loss: 0.010281, time: 2.248 s\n",
      "Step  97380 (epoch   30.43), loss: 0.005503, time: 2.246 s\n",
      "Step  97400 (epoch   30.44), loss: 0.005005, time: 2.240 s, error: 0.013290\n",
      "Step  97420 (epoch   30.44), loss: 0.005614, time: 14.953 s\n",
      "Step  97440 (epoch   30.45), loss: 0.003609, time: 2.234 s\n",
      "Step  97460 (epoch   30.46), loss: 0.009884, time: 2.248 s\n",
      "Step  97480 (epoch   30.46), loss: 0.006009, time: 2.241 s\n",
      "Step  97500 (epoch   30.47), loss: 0.004537, time: 2.243 s\n",
      "Step  97520 (epoch   30.48), loss: 0.010376, time: 2.235 s\n",
      "Step  97540 (epoch   30.48), loss: 0.007881, time: 2.232 s\n",
      "Step  97560 (epoch   30.49), loss: 0.006642, time: 2.236 s\n",
      "Step  97580 (epoch   30.49), loss: 0.008473, time: 2.234 s\n",
      "Step  97600 (epoch   30.50), loss: 0.005915, time: 2.241 s, error: 0.012909\n",
      "Step  97620 (epoch   30.51), loss: 0.006587, time: 14.995 s\n",
      "Step  97640 (epoch   30.51), loss: 0.004600, time: 2.233 s\n",
      "Step  97660 (epoch   30.52), loss: 0.009965, time: 2.235 s\n",
      "Step  97680 (epoch   30.52), loss: 0.005057, time: 2.248 s\n",
      "Step  97700 (epoch   30.53), loss: 0.006626, time: 2.234 s\n",
      "Step  97720 (epoch   30.54), loss: 0.005581, time: 2.237 s\n",
      "Step  97740 (epoch   30.54), loss: 0.012446, time: 2.256 s\n",
      "Step  97760 (epoch   30.55), loss: 0.004893, time: 2.230 s\n",
      "Step  97780 (epoch   30.56), loss: 0.004576, time: 2.245 s\n",
      "Step  97800 (epoch   30.56), loss: 0.007244, time: 2.231 s, error: 0.012944\n",
      "Step  97820 (epoch   30.57), loss: 0.004843, time: 14.975 s\n",
      "Step  97840 (epoch   30.57), loss: 0.006609, time: 2.241 s\n",
      "Step  97860 (epoch   30.58), loss: 0.006691, time: 2.233 s\n",
      "Step  97880 (epoch   30.59), loss: 0.007292, time: 2.241 s\n",
      "Step  97900 (epoch   30.59), loss: 0.007074, time: 2.237 s\n",
      "Step  97920 (epoch   30.60), loss: 0.007185, time: 2.240 s\n",
      "Step  97940 (epoch   30.61), loss: 0.010236, time: 2.237 s\n",
      "Step  97960 (epoch   30.61), loss: 0.005933, time: 2.238 s\n",
      "Step  97980 (epoch   30.62), loss: 0.006749, time: 2.244 s\n",
      "Step  98000 (epoch   30.62), loss: 0.007496, time: 2.257 s, error: 0.013318\n",
      "\n",
      "Time since beginning  : 17027.445 s\n",
      "\n",
      "Step  98020 (epoch   30.63), loss: 0.010158, time: 15.051 s\n",
      "Step  98040 (epoch   30.64), loss: 0.006424, time: 2.241 s\n",
      "Step  98060 (epoch   30.64), loss: 0.005030, time: 2.239 s\n",
      "Step  98080 (epoch   30.65), loss: 0.008510, time: 2.241 s\n",
      "Step  98100 (epoch   30.66), loss: 0.006149, time: 2.243 s\n",
      "Step  98120 (epoch   30.66), loss: 0.009007, time: 2.240 s\n",
      "Step  98140 (epoch   30.67), loss: 0.007859, time: 2.243 s\n",
      "Step  98160 (epoch   30.68), loss: 0.009703, time: 2.233 s\n",
      "Step  98180 (epoch   30.68), loss: 0.004386, time: 2.248 s\n",
      "Step  98200 (epoch   30.69), loss: 0.005227, time: 2.250 s, error: 0.013893\n",
      "Step  98220 (epoch   30.69), loss: 0.007560, time: 15.241 s\n",
      "Step  98240 (epoch   30.70), loss: 0.006229, time: 2.244 s\n",
      "Step  98260 (epoch   30.71), loss: 0.007879, time: 2.244 s\n",
      "Step  98280 (epoch   30.71), loss: 0.008068, time: 2.246 s\n",
      "Step  98300 (epoch   30.72), loss: 0.006205, time: 2.247 s\n",
      "Step  98320 (epoch   30.73), loss: 0.005603, time: 2.235 s\n",
      "Step  98340 (epoch   30.73), loss: 0.005197, time: 2.239 s\n",
      "Step  98360 (epoch   30.74), loss: 0.007172, time: 2.239 s\n",
      "Step  98380 (epoch   30.74), loss: 0.007301, time: 2.227 s\n",
      "Step  98400 (epoch   30.75), loss: 0.006946, time: 2.232 s, error: 0.012894\n",
      "Step  98420 (epoch   30.76), loss: 0.003828, time: 15.124 s\n",
      "Step  98440 (epoch   30.76), loss: 0.007272, time: 2.243 s\n",
      "Step  98460 (epoch   30.77), loss: 0.010579, time: 2.246 s\n",
      "Step  98480 (epoch   30.77), loss: 0.008067, time: 2.243 s\n",
      "Step  98500 (epoch   30.78), loss: 0.012749, time: 2.237 s\n",
      "Step  98520 (epoch   30.79), loss: 0.005603, time: 2.241 s\n",
      "Step  98540 (epoch   30.79), loss: 0.008398, time: 2.245 s\n",
      "Step  98560 (epoch   30.80), loss: 0.009494, time: 2.249 s\n",
      "Step  98580 (epoch   30.81), loss: 0.007290, time: 2.242 s\n",
      "Step  98600 (epoch   30.81), loss: 0.008010, time: 2.242 s, error: 0.013383\n",
      "Step  98620 (epoch   30.82), loss: 0.006821, time: 15.079 s\n",
      "Step  98640 (epoch   30.82), loss: 0.005812, time: 2.237 s\n",
      "Step  98660 (epoch   30.83), loss: 0.021996, time: 2.247 s\n",
      "Step  98680 (epoch   30.84), loss: 0.005434, time: 2.239 s\n",
      "Step  98700 (epoch   30.84), loss: 0.006483, time: 2.232 s\n",
      "Step  98720 (epoch   30.85), loss: 0.006703, time: 2.231 s\n",
      "Step  98740 (epoch   30.86), loss: 0.005601, time: 2.240 s\n",
      "Step  98760 (epoch   30.86), loss: 0.007310, time: 2.240 s\n",
      "Step  98780 (epoch   30.87), loss: 0.010987, time: 2.236 s\n",
      "Step  98800 (epoch   30.88), loss: 0.006284, time: 2.232 s, error: 0.013067\n",
      "Step  98820 (epoch   30.88), loss: 0.009233, time: 15.007 s\n",
      "Step  98840 (epoch   30.89), loss: 0.006684, time: 2.242 s\n",
      "Step  98860 (epoch   30.89), loss: 0.004941, time: 2.255 s\n",
      "Step  98880 (epoch   30.90), loss: 0.012573, time: 2.245 s\n",
      "Step  98900 (epoch   30.91), loss: 0.007746, time: 2.254 s\n",
      "Step  98920 (epoch   30.91), loss: 0.007007, time: 2.257 s\n",
      "Step  98940 (epoch   30.92), loss: 0.007460, time: 2.250 s\n",
      "Step  98960 (epoch   30.93), loss: 0.008582, time: 2.254 s\n",
      "Step  98980 (epoch   30.93), loss: 0.005828, time: 2.254 s\n",
      "Step  99000 (epoch   30.94), loss: 0.004594, time: 2.254 s, error: 0.012927\n",
      "Step  99020 (epoch   30.94), loss: 0.005373, time: 15.153 s\n",
      "Step  99040 (epoch   30.95), loss: 0.005132, time: 2.236 s\n",
      "Step  99060 (epoch   30.96), loss: 0.007882, time: 2.244 s\n",
      "Step  99080 (epoch   30.96), loss: 0.013942, time: 2.254 s\n",
      "Step  99100 (epoch   30.97), loss: 0.014013, time: 2.241 s\n",
      "Step  99120 (epoch   30.98), loss: 0.005548, time: 2.243 s\n",
      "Step  99140 (epoch   30.98), loss: 0.006077, time: 2.242 s\n",
      "Step  99160 (epoch   30.99), loss: 0.005293, time: 2.252 s\n",
      "Step  99180 (epoch   30.99), loss: 0.009481, time: 2.256 s\n",
      "Step  99200 (epoch   31.00), loss: 0.006763, time: 2.253 s, error: 0.012954\n",
      "Step  99220 (epoch   31.01), loss: 0.006242, time: 15.221 s\n",
      "Step  99240 (epoch   31.01), loss: 0.005215, time: 2.260 s\n",
      "Step  99260 (epoch   31.02), loss: 0.005150, time: 2.258 s\n",
      "Step  99280 (epoch   31.02), loss: 0.006179, time: 2.249 s\n",
      "Step  99300 (epoch   31.03), loss: 0.006960, time: 2.256 s\n",
      "Step  99320 (epoch   31.04), loss: 0.004400, time: 2.253 s\n",
      "Step  99340 (epoch   31.04), loss: 0.006679, time: 2.251 s\n",
      "Step  99360 (epoch   31.05), loss: 0.007226, time: 2.254 s\n",
      "Step  99380 (epoch   31.06), loss: 0.007944, time: 2.246 s\n",
      "Step  99400 (epoch   31.06), loss: 0.006331, time: 2.226 s, error: 0.012921\n",
      "Step  99420 (epoch   31.07), loss: 0.007259, time: 15.242 s\n",
      "Step  99440 (epoch   31.07), loss: 0.006071, time: 2.249 s\n",
      "Step  99460 (epoch   31.08), loss: 0.005555, time: 2.246 s\n",
      "Step  99480 (epoch   31.09), loss: 0.008038, time: 2.244 s\n",
      "Step  99500 (epoch   31.09), loss: 0.007564, time: 2.254 s\n",
      "Step  99520 (epoch   31.10), loss: 0.008687, time: 2.246 s\n",
      "Step  99540 (epoch   31.11), loss: 0.006380, time: 2.233 s\n",
      "Step  99560 (epoch   31.11), loss: 0.008749, time: 2.252 s\n",
      "Step  99580 (epoch   31.12), loss: 0.004024, time: 2.245 s\n",
      "Step  99600 (epoch   31.12), loss: 0.007308, time: 2.239 s, error: 0.012949\n",
      "Step  99620 (epoch   31.13), loss: 0.005832, time: 15.225 s\n",
      "Step  99640 (epoch   31.14), loss: 0.007883, time: 2.228 s\n",
      "Step  99660 (epoch   31.14), loss: 0.009953, time: 2.238 s\n",
      "Step  99680 (epoch   31.15), loss: 0.004607, time: 2.231 s\n",
      "Step  99700 (epoch   31.16), loss: 0.005568, time: 2.245 s\n",
      "Step  99720 (epoch   31.16), loss: 0.008664, time: 2.229 s\n",
      "Step  99740 (epoch   31.17), loss: 0.013746, time: 2.250 s\n",
      "Step  99760 (epoch   31.18), loss: 0.007786, time: 2.237 s\n",
      "Step  99780 (epoch   31.18), loss: 0.007058, time: 2.228 s\n",
      "Step  99800 (epoch   31.19), loss: 0.011726, time: 2.239 s, error: 0.013467\n",
      "Step  99820 (epoch   31.19), loss: 0.004506, time: 15.089 s\n",
      "Step  99840 (epoch   31.20), loss: 0.010893, time: 2.245 s\n",
      "Step  99860 (epoch   31.21), loss: 0.005142, time: 2.253 s\n",
      "Step  99880 (epoch   31.21), loss: 0.005343, time: 2.231 s\n",
      "Step  99900 (epoch   31.22), loss: 0.011734, time: 2.228 s\n",
      "Step  99920 (epoch   31.23), loss: 0.009414, time: 2.235 s\n",
      "Step  99940 (epoch   31.23), loss: 0.011252, time: 2.239 s\n",
      "Step  99960 (epoch   31.24), loss: 0.009497, time: 2.235 s\n",
      "Step  99980 (epoch   31.24), loss: 0.006169, time: 2.242 s\n",
      "Step 100000 (epoch   31.25), loss: 0.005965, time: 2.237 s, error: 0.012909\n",
      "\n",
      "Time since beginning  : 17380.760 s\n",
      "\n",
      "Step 100020 (epoch   31.26), loss: 0.005567, time: 15.075 s\n",
      "Step 100040 (epoch   31.26), loss: 0.007256, time: 2.236 s\n",
      "Step 100060 (epoch   31.27), loss: 0.005675, time: 2.239 s\n",
      "Step 100080 (epoch   31.27), loss: 0.005735, time: 2.243 s\n",
      "Step 100100 (epoch   31.28), loss: 0.005980, time: 2.242 s\n",
      "Step 100120 (epoch   31.29), loss: 0.006753, time: 2.259 s\n",
      "Step 100140 (epoch   31.29), loss: 0.007998, time: 2.235 s\n",
      "Step 100160 (epoch   31.30), loss: 0.006105, time: 2.247 s\n",
      "Step 100180 (epoch   31.31), loss: 0.007029, time: 2.237 s\n",
      "Step 100200 (epoch   31.31), loss: 0.004982, time: 2.250 s, error: 0.013499\n",
      "Step 100220 (epoch   31.32), loss: 0.005573, time: 14.996 s\n",
      "Step 100240 (epoch   31.32), loss: 0.007279, time: 2.248 s\n",
      "Step 100260 (epoch   31.33), loss: 0.012356, time: 2.235 s\n",
      "Step 100280 (epoch   31.34), loss: 0.007610, time: 2.244 s\n",
      "Step 100300 (epoch   31.34), loss: 0.005737, time: 2.230 s\n",
      "Step 100320 (epoch   31.35), loss: 0.006094, time: 2.242 s\n",
      "Step 100340 (epoch   31.36), loss: 0.006524, time: 2.240 s\n",
      "Step 100360 (epoch   31.36), loss: 0.007390, time: 2.247 s\n",
      "Step 100380 (epoch   31.37), loss: 0.004734, time: 2.256 s\n",
      "Step 100400 (epoch   31.38), loss: 0.010800, time: 2.249 s, error: 0.013544\n",
      "Step 100420 (epoch   31.38), loss: 0.013221, time: 14.982 s\n",
      "Step 100440 (epoch   31.39), loss: 0.009636, time: 2.234 s\n",
      "Step 100460 (epoch   31.39), loss: 0.007990, time: 2.243 s\n",
      "Step 100480 (epoch   31.40), loss: 0.007992, time: 2.239 s\n",
      "Step 100500 (epoch   31.41), loss: 0.008002, time: 2.234 s\n",
      "Step 100520 (epoch   31.41), loss: 0.006427, time: 2.244 s\n",
      "Step 100540 (epoch   31.42), loss: 0.010110, time: 2.243 s\n",
      "Step 100560 (epoch   31.43), loss: 0.006398, time: 2.233 s\n",
      "Step 100580 (epoch   31.43), loss: 0.007130, time: 2.244 s\n",
      "Step 100600 (epoch   31.44), loss: 0.007216, time: 2.233 s, error: 0.013280\n",
      "Step 100620 (epoch   31.44), loss: 0.009225, time: 15.203 s\n",
      "Step 100640 (epoch   31.45), loss: 0.007619, time: 2.248 s\n",
      "Step 100660 (epoch   31.46), loss: 0.009043, time: 2.249 s\n",
      "Step 100680 (epoch   31.46), loss: 0.008068, time: 2.232 s\n",
      "Step 100700 (epoch   31.47), loss: 0.006570, time: 2.230 s\n",
      "Step 100720 (epoch   31.48), loss: 0.005690, time: 2.247 s\n",
      "Step 100740 (epoch   31.48), loss: 0.005434, time: 2.239 s\n",
      "Step 100760 (epoch   31.49), loss: 0.008789, time: 2.240 s\n",
      "Step 100780 (epoch   31.49), loss: 0.008997, time: 2.250 s\n",
      "Step 100800 (epoch   31.50), loss: 0.008298, time: 2.249 s, error: 0.012789\n",
      "Step 100820 (epoch   31.51), loss: 0.004893, time: 15.184 s\n",
      "Step 100840 (epoch   31.51), loss: 0.009482, time: 2.232 s\n",
      "Step 100860 (epoch   31.52), loss: 0.007125, time: 2.240 s\n",
      "Step 100880 (epoch   31.52), loss: 0.005351, time: 2.241 s\n",
      "Step 100900 (epoch   31.53), loss: 0.007526, time: 2.243 s\n",
      "Step 100920 (epoch   31.54), loss: 0.005041, time: 2.242 s\n",
      "Step 100940 (epoch   31.54), loss: 0.008057, time: 2.235 s\n",
      "Step 100960 (epoch   31.55), loss: 0.005343, time: 2.227 s\n",
      "Step 100980 (epoch   31.56), loss: 0.005502, time: 2.244 s\n",
      "Step 101000 (epoch   31.56), loss: 0.007290, time: 2.240 s, error: 0.013566\n",
      "Step 101020 (epoch   31.57), loss: 0.007310, time: 14.989 s\n",
      "Step 101040 (epoch   31.57), loss: 0.006484, time: 2.244 s\n",
      "Step 101060 (epoch   31.58), loss: 0.007492, time: 2.245 s\n",
      "Step 101080 (epoch   31.59), loss: 0.012457, time: 2.251 s\n",
      "Step 101100 (epoch   31.59), loss: 0.007458, time: 2.252 s\n",
      "Step 101120 (epoch   31.60), loss: 0.008894, time: 2.236 s\n",
      "Step 101140 (epoch   31.61), loss: 0.005660, time: 2.241 s\n",
      "Step 101160 (epoch   31.61), loss: 0.006053, time: 2.232 s\n",
      "Step 101180 (epoch   31.62), loss: 0.005408, time: 2.251 s\n",
      "Step 101200 (epoch   31.62), loss: 0.006434, time: 2.239 s, error: 0.013393\n",
      "Step 101220 (epoch   31.63), loss: 0.012735, time: 14.996 s\n",
      "Step 101240 (epoch   31.64), loss: 0.007087, time: 2.250 s\n",
      "Step 101260 (epoch   31.64), loss: 0.010627, time: 2.241 s\n",
      "Step 101280 (epoch   31.65), loss: 0.007594, time: 2.247 s\n",
      "Step 101300 (epoch   31.66), loss: 0.006299, time: 2.250 s\n",
      "Step 101320 (epoch   31.66), loss: 0.009607, time: 2.255 s\n",
      "Step 101340 (epoch   31.67), loss: 0.008513, time: 2.241 s\n",
      "Step 101360 (epoch   31.68), loss: 0.005680, time: 2.248 s\n",
      "Step 101380 (epoch   31.68), loss: 0.008560, time: 2.252 s\n",
      "Step 101400 (epoch   31.69), loss: 0.010423, time: 2.241 s, error: 0.013579\n",
      "Step 101420 (epoch   31.69), loss: 0.006580, time: 15.062 s\n",
      "Step 101440 (epoch   31.70), loss: 0.007188, time: 2.234 s\n",
      "Step 101460 (epoch   31.71), loss: 0.006481, time: 2.236 s\n",
      "Step 101480 (epoch   31.71), loss: 0.007377, time: 2.247 s\n",
      "Step 101500 (epoch   31.72), loss: 0.008354, time: 2.249 s\n",
      "Step 101520 (epoch   31.73), loss: 0.007663, time: 2.249 s\n",
      "Step 101540 (epoch   31.73), loss: 0.008449, time: 2.245 s\n",
      "Step 101560 (epoch   31.74), loss: 0.007492, time: 2.251 s\n",
      "Step 101580 (epoch   31.74), loss: 0.006514, time: 2.256 s\n",
      "Step 101600 (epoch   31.75), loss: 0.006142, time: 2.244 s, error: 0.012666\n",
      "Step 101620 (epoch   31.76), loss: 0.006504, time: 15.069 s\n",
      "Step 101640 (epoch   31.76), loss: 0.009023, time: 2.236 s\n",
      "Step 101660 (epoch   31.77), loss: 0.006467, time: 2.240 s\n",
      "Step 101680 (epoch   31.77), loss: 0.006714, time: 2.242 s\n",
      "Step 101700 (epoch   31.78), loss: 0.004543, time: 2.240 s\n",
      "Step 101720 (epoch   31.79), loss: 0.006844, time: 2.228 s\n",
      "Step 101740 (epoch   31.79), loss: 0.005233, time: 2.245 s\n",
      "Step 101760 (epoch   31.80), loss: 0.005348, time: 2.246 s\n",
      "Step 101780 (epoch   31.81), loss: 0.009636, time: 2.237 s\n",
      "Step 101800 (epoch   31.81), loss: 0.005705, time: 2.239 s, error: 0.013468\n",
      "Step 101820 (epoch   31.82), loss: 0.004960, time: 15.244 s\n",
      "Step 101840 (epoch   31.82), loss: 0.006182, time: 2.245 s\n",
      "Step 101860 (epoch   31.83), loss: 0.004816, time: 2.234 s\n",
      "Step 101880 (epoch   31.84), loss: 0.005151, time: 2.248 s\n",
      "Step 101900 (epoch   31.84), loss: 0.007086, time: 2.235 s\n",
      "Step 101920 (epoch   31.85), loss: 0.004266, time: 2.254 s\n",
      "Step 101940 (epoch   31.86), loss: 0.005771, time: 2.246 s\n",
      "Step 101960 (epoch   31.86), loss: 0.006637, time: 2.235 s\n",
      "Step 101980 (epoch   31.87), loss: 0.009847, time: 2.232 s\n",
      "Step 102000 (epoch   31.88), loss: 0.003797, time: 2.232 s, error: 0.012922\n",
      "\n",
      "Time since beginning  : 17733.583 s\n",
      "\n",
      "Step 102020 (epoch   31.88), loss: 0.007855, time: 15.300 s\n",
      "Step 102040 (epoch   31.89), loss: 0.005467, time: 2.233 s\n",
      "Step 102060 (epoch   31.89), loss: 0.006496, time: 2.244 s\n",
      "Step 102080 (epoch   31.90), loss: 0.007512, time: 2.241 s\n",
      "Step 102100 (epoch   31.91), loss: 0.010061, time: 2.242 s\n",
      "Step 102120 (epoch   31.91), loss: 0.008176, time: 2.236 s\n",
      "Step 102140 (epoch   31.92), loss: 0.008049, time: 2.240 s\n",
      "Step 102160 (epoch   31.93), loss: 0.007444, time: 2.245 s\n",
      "Step 102180 (epoch   31.93), loss: 0.005352, time: 2.245 s\n",
      "Step 102200 (epoch   31.94), loss: 0.006703, time: 2.246 s, error: 0.012836\n",
      "Step 102220 (epoch   31.94), loss: 0.008087, time: 15.100 s\n",
      "Step 102240 (epoch   31.95), loss: 0.006055, time: 2.251 s\n",
      "Step 102260 (epoch   31.96), loss: 0.004299, time: 2.374 s\n",
      "Step 102280 (epoch   31.96), loss: 0.007551, time: 2.253 s\n",
      "Step 102300 (epoch   31.97), loss: 0.006294, time: 2.234 s\n",
      "Step 102320 (epoch   31.98), loss: 0.014151, time: 2.235 s\n",
      "Step 102340 (epoch   31.98), loss: 0.005384, time: 2.230 s\n",
      "Step 102360 (epoch   31.99), loss: 0.006152, time: 2.245 s\n",
      "Step 102380 (epoch   31.99), loss: 0.006693, time: 2.235 s\n",
      "Step 102400 (epoch   32.00), loss: 0.005779, time: 2.239 s, error: 0.012932\n",
      "Step 102420 (epoch   32.01), loss: 0.009053, time: 14.992 s\n",
      "Step 102440 (epoch   32.01), loss: 0.006351, time: 2.219 s\n",
      "Step 102460 (epoch   32.02), loss: 0.008983, time: 2.245 s\n",
      "Step 102480 (epoch   32.02), loss: 0.007968, time: 2.234 s\n",
      "Step 102500 (epoch   32.03), loss: 0.006216, time: 2.257 s\n",
      "Step 102520 (epoch   32.04), loss: 0.006072, time: 2.236 s\n",
      "Step 102540 (epoch   32.04), loss: 0.003925, time: 2.246 s\n",
      "Step 102560 (epoch   32.05), loss: 0.009292, time: 2.220 s\n",
      "Step 102580 (epoch   32.06), loss: 0.009240, time: 2.245 s\n",
      "Step 102600 (epoch   32.06), loss: 0.006362, time: 2.228 s, error: 0.012760\n",
      "Step 102620 (epoch   32.07), loss: 0.006235, time: 15.013 s\n",
      "Step 102640 (epoch   32.08), loss: 0.007765, time: 2.252 s\n",
      "Step 102660 (epoch   32.08), loss: 0.006020, time: 2.250 s\n",
      "Step 102680 (epoch   32.09), loss: 0.005435, time: 2.241 s\n",
      "Step 102700 (epoch   32.09), loss: 0.005966, time: 2.254 s\n",
      "Step 102720 (epoch   32.10), loss: 0.006564, time: 2.246 s\n",
      "Step 102740 (epoch   32.11), loss: 0.005088, time: 2.241 s\n",
      "Step 102760 (epoch   32.11), loss: 0.015466, time: 2.256 s\n",
      "Step 102780 (epoch   32.12), loss: 0.008587, time: 2.249 s\n",
      "Step 102800 (epoch   32.12), loss: 0.006647, time: 2.237 s, error: 0.012857\n",
      "Step 102820 (epoch   32.13), loss: 0.008000, time: 14.999 s\n",
      "Step 102840 (epoch   32.14), loss: 0.006875, time: 2.244 s\n",
      "Step 102860 (epoch   32.14), loss: 0.007599, time: 2.238 s\n",
      "Step 102880 (epoch   32.15), loss: 0.013916, time: 2.246 s\n",
      "Step 102900 (epoch   32.16), loss: 0.007142, time: 2.247 s\n",
      "Step 102920 (epoch   32.16), loss: 0.005639, time: 2.243 s\n",
      "Step 102940 (epoch   32.17), loss: 0.005319, time: 2.237 s\n",
      "Step 102960 (epoch   32.17), loss: 0.005479, time: 2.244 s\n",
      "Step 102980 (epoch   32.18), loss: 0.007215, time: 2.230 s\n",
      "Step 103000 (epoch   32.19), loss: 0.006077, time: 2.237 s, error: 0.013476\n",
      "Step 103020 (epoch   32.19), loss: 0.004789, time: 15.175 s\n",
      "Step 103040 (epoch   32.20), loss: 0.007224, time: 2.222 s\n",
      "Step 103060 (epoch   32.21), loss: 0.007478, time: 2.246 s\n",
      "Step 103080 (epoch   32.21), loss: 0.011218, time: 2.235 s\n",
      "Step 103100 (epoch   32.22), loss: 0.005296, time: 2.232 s\n",
      "Step 103120 (epoch   32.23), loss: 0.006601, time: 2.230 s\n",
      "Step 103140 (epoch   32.23), loss: 0.008246, time: 2.245 s\n",
      "Step 103160 (epoch   32.24), loss: 0.010467, time: 2.234 s\n",
      "Step 103180 (epoch   32.24), loss: 0.007077, time: 2.242 s\n",
      "Step 103200 (epoch   32.25), loss: 0.006127, time: 2.242 s, error: 0.013011\n",
      "Step 103220 (epoch   32.26), loss: 0.005767, time: 15.208 s\n",
      "Step 103240 (epoch   32.26), loss: 0.005654, time: 2.241 s\n",
      "Step 103260 (epoch   32.27), loss: 0.007505, time: 2.229 s\n",
      "Step 103280 (epoch   32.27), loss: 0.007258, time: 2.245 s\n",
      "Step 103300 (epoch   32.28), loss: 0.004865, time: 2.240 s\n",
      "Step 103320 (epoch   32.29), loss: 0.011720, time: 2.253 s\n",
      "Step 103340 (epoch   32.29), loss: 0.005160, time: 2.237 s\n",
      "Step 103360 (epoch   32.30), loss: 0.005497, time: 2.236 s\n",
      "Step 103380 (epoch   32.31), loss: 0.009263, time: 2.252 s\n",
      "Step 103400 (epoch   32.31), loss: 0.006995, time: 2.235 s, error: 0.013425\n",
      "Step 103420 (epoch   32.32), loss: 0.005410, time: 15.077 s\n",
      "Step 103440 (epoch   32.33), loss: 0.010297, time: 2.251 s\n",
      "Step 103460 (epoch   32.33), loss: 0.007356, time: 2.242 s\n",
      "Step 103480 (epoch   32.34), loss: 0.007841, time: 2.238 s\n",
      "Step 103500 (epoch   32.34), loss: 0.007042, time: 2.224 s\n",
      "Step 103520 (epoch   32.35), loss: 0.005429, time: 2.237 s\n",
      "Step 103540 (epoch   32.36), loss: 0.007348, time: 2.243 s\n",
      "Step 103560 (epoch   32.36), loss: 0.006016, time: 2.241 s\n",
      "Step 103580 (epoch   32.37), loss: 0.004655, time: 2.237 s\n",
      "Step 103600 (epoch   32.38), loss: 0.008791, time: 2.246 s, error: 0.013563\n",
      "Step 103620 (epoch   32.38), loss: 0.006054, time: 15.017 s\n",
      "Step 103640 (epoch   32.39), loss: 0.009377, time: 2.219 s\n",
      "Step 103660 (epoch   32.39), loss: 0.006333, time: 2.243 s\n",
      "Step 103680 (epoch   32.40), loss: 0.006306, time: 2.256 s\n",
      "Step 103700 (epoch   32.41), loss: 0.010974, time: 2.243 s\n",
      "Step 103720 (epoch   32.41), loss: 0.005942, time: 2.245 s\n",
      "Step 103740 (epoch   32.42), loss: 0.006533, time: 2.235 s\n",
      "Step 103760 (epoch   32.42), loss: 0.005490, time: 2.247 s\n",
      "Step 103780 (epoch   32.43), loss: 0.010935, time: 2.242 s\n",
      "Step 103800 (epoch   32.44), loss: 0.005787, time: 2.231 s, error: 0.013464\n",
      "Step 103820 (epoch   32.44), loss: 0.006271, time: 15.042 s\n",
      "Step 103840 (epoch   32.45), loss: 0.008330, time: 2.238 s\n",
      "Step 103860 (epoch   32.46), loss: 0.005859, time: 2.231 s\n",
      "Step 103880 (epoch   32.46), loss: 0.004886, time: 2.235 s\n",
      "Step 103900 (epoch   32.47), loss: 0.007854, time: 2.247 s\n",
      "Step 103920 (epoch   32.48), loss: 0.020943, time: 2.239 s\n",
      "Step 103940 (epoch   32.48), loss: 0.005216, time: 2.248 s\n",
      "Step 103960 (epoch   32.49), loss: 0.005805, time: 2.257 s\n",
      "Step 103980 (epoch   32.49), loss: 0.005429, time: 2.232 s\n",
      "Step 104000 (epoch   32.50), loss: 0.005862, time: 2.234 s, error: 0.012716\n",
      "\n",
      "Time since beginning  : 18086.089 s\n",
      "\n",
      "Step 104020 (epoch   32.51), loss: 0.007100, time: 15.126 s\n",
      "Step 104040 (epoch   32.51), loss: 0.007109, time: 2.252 s\n",
      "Step 104060 (epoch   32.52), loss: 0.007865, time: 2.235 s\n",
      "Step 104080 (epoch   32.52), loss: 0.008343, time: 2.252 s\n",
      "Step 104100 (epoch   32.53), loss: 0.008264, time: 2.249 s\n",
      "Step 104120 (epoch   32.54), loss: 0.005351, time: 2.249 s\n",
      "Step 104140 (epoch   32.54), loss: 0.005290, time: 2.243 s\n",
      "Step 104160 (epoch   32.55), loss: 0.005522, time: 2.247 s\n",
      "Step 104180 (epoch   32.56), loss: 0.004387, time: 2.242 s\n",
      "Step 104200 (epoch   32.56), loss: 0.007130, time: 2.245 s, error: 0.014176\n",
      "Step 104220 (epoch   32.57), loss: 0.007441, time: 15.126 s\n",
      "Step 104240 (epoch   32.58), loss: 0.006913, time: 2.235 s\n",
      "Step 104260 (epoch   32.58), loss: 0.007528, time: 2.238 s\n",
      "Step 104280 (epoch   32.59), loss: 0.008673, time: 2.224 s\n",
      "Step 104300 (epoch   32.59), loss: 0.012187, time: 2.244 s\n",
      "Step 104320 (epoch   32.60), loss: 0.009300, time: 2.236 s\n",
      "Step 104340 (epoch   32.61), loss: 0.009279, time: 2.253 s\n",
      "Step 104360 (epoch   32.61), loss: 0.005538, time: 2.244 s\n",
      "Step 104380 (epoch   32.62), loss: 0.010284, time: 2.244 s\n",
      "Step 104400 (epoch   32.62), loss: 0.009590, time: 2.236 s, error: 0.013087\n",
      "Step 104420 (epoch   32.63), loss: 0.007879, time: 15.248 s\n",
      "Step 104440 (epoch   32.64), loss: 0.006317, time: 2.241 s\n",
      "Step 104460 (epoch   32.64), loss: 0.005274, time: 2.238 s\n",
      "Step 104480 (epoch   32.65), loss: 0.007543, time: 2.246 s\n",
      "Step 104500 (epoch   32.66), loss: 0.007028, time: 2.234 s\n",
      "Step 104520 (epoch   32.66), loss: 0.007922, time: 2.236 s\n",
      "Step 104540 (epoch   32.67), loss: 0.006879, time: 2.227 s\n",
      "Step 104560 (epoch   32.67), loss: 0.008213, time: 2.245 s\n",
      "Step 104580 (epoch   32.68), loss: 0.006196, time: 2.239 s\n",
      "Step 104600 (epoch   32.69), loss: 0.006520, time: 2.240 s, error: 0.013443\n",
      "Step 104620 (epoch   32.69), loss: 0.007160, time: 15.006 s\n",
      "Step 104640 (epoch   32.70), loss: 0.006218, time: 2.236 s\n",
      "Step 104660 (epoch   32.71), loss: 0.004808, time: 2.248 s\n",
      "Step 104680 (epoch   32.71), loss: 0.011899, time: 2.241 s\n",
      "Step 104700 (epoch   32.72), loss: 0.008679, time: 2.246 s\n",
      "Step 104720 (epoch   32.73), loss: 0.008341, time: 2.239 s\n",
      "Step 104740 (epoch   32.73), loss: 0.010758, time: 2.246 s\n",
      "Step 104760 (epoch   32.74), loss: 0.006764, time: 2.223 s\n",
      "Step 104780 (epoch   32.74), loss: 0.006922, time: 2.231 s\n",
      "Step 104800 (epoch   32.75), loss: 0.006366, time: 2.233 s, error: 0.012486\n",
      "Step 104820 (epoch   32.76), loss: 0.007677, time: 15.012 s\n",
      "Step 104840 (epoch   32.76), loss: 0.005791, time: 2.247 s\n",
      "Step 104860 (epoch   32.77), loss: 0.005191, time: 2.229 s\n",
      "Step 104880 (epoch   32.77), loss: 0.006551, time: 2.254 s\n",
      "Step 104900 (epoch   32.78), loss: 0.006443, time: 2.254 s\n",
      "Step 104920 (epoch   32.79), loss: 0.005043, time: 2.251 s\n",
      "Step 104940 (epoch   32.79), loss: 0.008093, time: 2.229 s\n",
      "Step 104960 (epoch   32.80), loss: 0.004707, time: 2.243 s\n",
      "Step 104980 (epoch   32.81), loss: 0.006720, time: 2.231 s\n",
      "Step 105000 (epoch   32.81), loss: 0.006094, time: 2.243 s, error: 0.013206\n",
      "Step 105020 (epoch   32.82), loss: 0.006610, time: 15.075 s\n",
      "Step 105040 (epoch   32.83), loss: 0.006621, time: 2.253 s\n",
      "Step 105060 (epoch   32.83), loss: 0.005873, time: 2.238 s\n",
      "Step 105080 (epoch   32.84), loss: 0.006739, time: 2.241 s\n",
      "Step 105100 (epoch   32.84), loss: 0.007822, time: 2.243 s\n",
      "Step 105120 (epoch   32.85), loss: 0.005031, time: 2.247 s\n",
      "Step 105140 (epoch   32.86), loss: 0.009336, time: 2.245 s\n",
      "Step 105160 (epoch   32.86), loss: 0.008405, time: 2.252 s\n",
      "Step 105180 (epoch   32.87), loss: 0.009293, time: 2.229 s\n",
      "Step 105200 (epoch   32.88), loss: 0.005599, time: 2.240 s, error: 0.012647\n",
      "Step 105220 (epoch   32.88), loss: 0.011051, time: 15.115 s\n",
      "Step 105240 (epoch   32.89), loss: 0.005968, time: 2.250 s\n",
      "Step 105260 (epoch   32.89), loss: 0.008876, time: 2.247 s\n",
      "Step 105280 (epoch   32.90), loss: 0.007137, time: 2.237 s\n",
      "Step 105300 (epoch   32.91), loss: 0.005793, time: 2.255 s\n",
      "Step 105320 (epoch   32.91), loss: 0.008304, time: 2.246 s\n",
      "Step 105340 (epoch   32.92), loss: 0.005528, time: 2.235 s\n",
      "Step 105360 (epoch   32.92), loss: 0.005176, time: 2.244 s\n",
      "Step 105380 (epoch   32.93), loss: 0.006849, time: 2.248 s\n",
      "Step 105400 (epoch   32.94), loss: 0.004087, time: 2.245 s, error: 0.012763\n",
      "Step 105420 (epoch   32.94), loss: 0.005347, time: 15.137 s\n",
      "Step 105440 (epoch   32.95), loss: 0.005516, time: 2.237 s\n",
      "Step 105460 (epoch   32.96), loss: 0.010612, time: 2.235 s\n",
      "Step 105480 (epoch   32.96), loss: 0.007600, time: 2.243 s\n",
      "Step 105500 (epoch   32.97), loss: 0.005345, time: 2.229 s\n",
      "Step 105520 (epoch   32.98), loss: 0.004796, time: 2.242 s\n",
      "Step 105540 (epoch   32.98), loss: 0.006415, time: 2.242 s\n",
      "Step 105560 (epoch   32.99), loss: 0.006866, time: 2.240 s\n",
      "Step 105580 (epoch   32.99), loss: 0.005783, time: 2.237 s\n",
      "Step 105600 (epoch   33.00), loss: 0.007143, time: 2.247 s, error: 0.012795\n",
      "Step 105620 (epoch   33.01), loss: 0.004421, time: 15.210 s\n",
      "Step 105640 (epoch   33.01), loss: 0.005664, time: 2.247 s\n",
      "Step 105660 (epoch   33.02), loss: 0.005663, time: 2.223 s\n",
      "Step 105680 (epoch   33.02), loss: 0.005235, time: 2.228 s\n",
      "Step 105700 (epoch   33.03), loss: 0.010086, time: 2.237 s\n",
      "Step 105720 (epoch   33.04), loss: 0.005432, time: 2.232 s\n",
      "Step 105740 (epoch   33.04), loss: 0.008052, time: 2.249 s\n",
      "Step 105760 (epoch   33.05), loss: 0.006088, time: 2.240 s\n",
      "Step 105780 (epoch   33.06), loss: 0.005190, time: 2.243 s\n",
      "Step 105800 (epoch   33.06), loss: 0.007256, time: 2.221 s, error: 0.012698\n",
      "Step 105820 (epoch   33.07), loss: 0.004502, time: 15.164 s\n",
      "Step 105840 (epoch   33.08), loss: 0.005616, time: 2.254 s\n",
      "Step 105860 (epoch   33.08), loss: 0.004437, time: 2.248 s\n",
      "Step 105880 (epoch   33.09), loss: 0.006081, time: 2.241 s\n",
      "Step 105900 (epoch   33.09), loss: 0.005658, time: 2.241 s\n",
      "Step 105920 (epoch   33.10), loss: 0.008173, time: 2.248 s\n",
      "Step 105940 (epoch   33.11), loss: 0.006299, time: 2.233 s\n",
      "Step 105960 (epoch   33.11), loss: 0.008844, time: 2.250 s\n",
      "Step 105980 (epoch   33.12), loss: 0.007142, time: 2.245 s\n",
      "Step 106000 (epoch   33.12), loss: 0.007280, time: 2.250 s, error: 0.012731\n",
      "\n",
      "Time since beginning  : 18439.027 s\n",
      "\n",
      "Step 106020 (epoch   33.13), loss: 0.007478, time: 15.140 s\n",
      "Step 106040 (epoch   33.14), loss: 0.007101, time: 2.237 s\n",
      "Step 106060 (epoch   33.14), loss: 0.007354, time: 2.239 s\n",
      "Step 106080 (epoch   33.15), loss: 0.012704, time: 2.255 s\n",
      "Step 106100 (epoch   33.16), loss: 0.006123, time: 2.238 s\n",
      "Step 106120 (epoch   33.16), loss: 0.009402, time: 2.239 s\n",
      "Step 106140 (epoch   33.17), loss: 0.006324, time: 2.248 s\n",
      "Step 106160 (epoch   33.17), loss: 0.006449, time: 2.230 s\n",
      "Step 106180 (epoch   33.18), loss: 0.006347, time: 2.250 s\n",
      "Step 106200 (epoch   33.19), loss: 0.007896, time: 2.242 s, error: 0.013317\n",
      "Step 106220 (epoch   33.19), loss: 0.008938, time: 15.044 s\n",
      "Step 106240 (epoch   33.20), loss: 0.012320, time: 2.233 s\n",
      "Step 106260 (epoch   33.21), loss: 0.006398, time: 2.247 s\n",
      "Step 106280 (epoch   33.21), loss: 0.006460, time: 2.234 s\n",
      "Step 106300 (epoch   33.22), loss: 0.005785, time: 2.244 s\n",
      "Step 106320 (epoch   33.23), loss: 0.006530, time: 2.239 s\n",
      "Step 106340 (epoch   33.23), loss: 0.006825, time: 2.254 s\n",
      "Step 106360 (epoch   33.24), loss: 0.006925, time: 2.223 s\n",
      "Step 106380 (epoch   33.24), loss: 0.008849, time: 2.245 s\n",
      "Step 106400 (epoch   33.25), loss: 0.014162, time: 2.249 s, error: 0.012943\n",
      "Step 106420 (epoch   33.26), loss: 0.006919, time: 14.985 s\n",
      "Step 106440 (epoch   33.26), loss: 0.005501, time: 2.243 s\n",
      "Step 106460 (epoch   33.27), loss: 0.006840, time: 2.242 s\n",
      "Step 106480 (epoch   33.27), loss: 0.015115, time: 2.251 s\n",
      "Step 106500 (epoch   33.28), loss: 0.005803, time: 2.245 s\n",
      "Step 106520 (epoch   33.29), loss: 0.011164, time: 2.244 s\n",
      "Step 106540 (epoch   33.29), loss: 0.005535, time: 2.247 s\n",
      "Step 106560 (epoch   33.30), loss: 0.009909, time: 2.230 s\n",
      "Step 106580 (epoch   33.31), loss: 0.008997, time: 2.236 s\n",
      "Step 106600 (epoch   33.31), loss: 0.007349, time: 2.258 s, error: 0.013159\n",
      "Step 106620 (epoch   33.32), loss: 0.006382, time: 15.019 s\n",
      "Step 106640 (epoch   33.33), loss: 0.009247, time: 2.238 s\n",
      "Step 106660 (epoch   33.33), loss: 0.004552, time: 2.235 s\n",
      "Step 106680 (epoch   33.34), loss: 0.009667, time: 2.228 s\n",
      "Step 106700 (epoch   33.34), loss: 0.008902, time: 2.241 s\n",
      "Step 106720 (epoch   33.35), loss: 0.006439, time: 2.245 s\n",
      "Step 106740 (epoch   33.36), loss: 0.004741, time: 2.239 s\n",
      "Step 106760 (epoch   33.36), loss: 0.006998, time: 2.244 s\n",
      "Step 106780 (epoch   33.37), loss: 0.004827, time: 2.238 s\n",
      "Step 106800 (epoch   33.38), loss: 0.007654, time: 2.239 s, error: 0.013083\n",
      "Step 106820 (epoch   33.38), loss: 0.007405, time: 15.185 s\n",
      "Step 106840 (epoch   33.39), loss: 0.005334, time: 2.245 s\n",
      "Step 106860 (epoch   33.39), loss: 0.006117, time: 2.238 s\n",
      "Step 106880 (epoch   33.40), loss: 0.006751, time: 2.235 s\n",
      "Step 106900 (epoch   33.41), loss: 0.006706, time: 2.236 s\n",
      "Step 106920 (epoch   33.41), loss: 0.005456, time: 2.235 s\n",
      "Step 106940 (epoch   33.42), loss: 0.004893, time: 2.245 s\n",
      "Step 106960 (epoch   33.42), loss: 0.003960, time: 2.242 s\n",
      "Step 106980 (epoch   33.43), loss: 0.010490, time: 2.238 s\n",
      "Step 107000 (epoch   33.44), loss: 0.007948, time: 2.232 s, error: 0.013254\n",
      "Step 107020 (epoch   33.44), loss: 0.010636, time: 15.115 s\n",
      "Step 107040 (epoch   33.45), loss: 0.010222, time: 2.246 s\n",
      "Step 107060 (epoch   33.46), loss: 0.015858, time: 2.237 s\n",
      "Step 107080 (epoch   33.46), loss: 0.005809, time: 2.236 s\n",
      "Step 107100 (epoch   33.47), loss: 0.009309, time: 2.234 s\n",
      "Step 107120 (epoch   33.48), loss: 0.007880, time: 2.241 s\n",
      "Step 107140 (epoch   33.48), loss: 0.006444, time: 2.233 s\n",
      "Step 107160 (epoch   33.49), loss: 0.012239, time: 2.239 s\n",
      "Step 107180 (epoch   33.49), loss: 0.006439, time: 2.242 s\n",
      "Step 107200 (epoch   33.50), loss: 0.005210, time: 2.239 s, error: 0.012679\n",
      "Step 107220 (epoch   33.51), loss: 0.006421, time: 15.019 s\n",
      "Step 107240 (epoch   33.51), loss: 0.009795, time: 2.243 s\n",
      "Step 107260 (epoch   33.52), loss: 0.007502, time: 2.253 s\n",
      "Step 107280 (epoch   33.52), loss: 0.006844, time: 2.249 s\n",
      "Step 107300 (epoch   33.53), loss: 0.006675, time: 2.250 s\n",
      "Step 107320 (epoch   33.54), loss: 0.007140, time: 2.245 s\n",
      "Step 107340 (epoch   33.54), loss: 0.005499, time: 2.243 s\n",
      "Step 107360 (epoch   33.55), loss: 0.006515, time: 2.225 s\n",
      "Step 107380 (epoch   33.56), loss: 0.007230, time: 2.240 s\n",
      "Step 107400 (epoch   33.56), loss: 0.007391, time: 2.233 s, error: 0.013518\n",
      "Step 107420 (epoch   33.57), loss: 0.012618, time: 15.070 s\n",
      "Step 107440 (epoch   33.58), loss: 0.004697, time: 2.248 s\n",
      "Step 107460 (epoch   33.58), loss: 0.006754, time: 2.231 s\n",
      "Step 107480 (epoch   33.59), loss: 0.009378, time: 2.226 s\n",
      "Step 107500 (epoch   33.59), loss: 0.006940, time: 2.239 s\n",
      "Step 107520 (epoch   33.60), loss: 0.006674, time: 2.255 s\n",
      "Step 107540 (epoch   33.61), loss: 0.005597, time: 2.256 s\n",
      "Step 107560 (epoch   33.61), loss: 0.008769, time: 2.249 s\n",
      "Step 107580 (epoch   33.62), loss: 0.007771, time: 2.247 s\n",
      "Step 107600 (epoch   33.62), loss: 0.007912, time: 2.247 s, error: 0.012887\n",
      "Step 107620 (epoch   33.63), loss: 0.011289, time: 15.062 s\n",
      "Step 107640 (epoch   33.64), loss: 0.004047, time: 2.241 s\n",
      "Step 107660 (epoch   33.64), loss: 0.008622, time: 2.238 s\n",
      "Step 107680 (epoch   33.65), loss: 0.006713, time: 2.240 s\n",
      "Step 107700 (epoch   33.66), loss: 0.005802, time: 2.235 s\n",
      "Step 107720 (epoch   33.66), loss: 0.009658, time: 2.227 s\n",
      "Step 107740 (epoch   33.67), loss: 0.006627, time: 2.244 s\n",
      "Step 107760 (epoch   33.67), loss: 0.006685, time: 2.231 s\n",
      "Step 107780 (epoch   33.68), loss: 0.007546, time: 2.244 s\n",
      "Step 107800 (epoch   33.69), loss: 0.005796, time: 2.254 s, error: 0.013243\n",
      "Step 107820 (epoch   33.69), loss: 0.004725, time: 15.100 s\n",
      "Step 107840 (epoch   33.70), loss: 0.008266, time: 2.241 s\n",
      "Step 107860 (epoch   33.71), loss: 0.006872, time: 2.244 s\n",
      "Step 107880 (epoch   33.71), loss: 0.006813, time: 2.230 s\n",
      "Step 107900 (epoch   33.72), loss: 0.008647, time: 2.247 s\n",
      "Step 107920 (epoch   33.73), loss: 0.009693, time: 2.244 s\n",
      "Step 107940 (epoch   33.73), loss: 0.010638, time: 2.246 s\n",
      "Step 107960 (epoch   33.74), loss: 0.008389, time: 2.225 s\n",
      "Step 107980 (epoch   33.74), loss: 0.006467, time: 2.250 s\n",
      "Step 108000 (epoch   33.75), loss: 0.005812, time: 2.251 s, error: 0.012370\n",
      "\n",
      "Time since beginning  : 18791.607 s\n",
      "\n",
      "Step 108020 (epoch   33.76), loss: 0.007417, time: 15.252 s\n",
      "Step 108040 (epoch   33.76), loss: 0.006716, time: 2.242 s\n",
      "Step 108060 (epoch   33.77), loss: 0.007447, time: 2.236 s\n",
      "Step 108080 (epoch   33.77), loss: 0.004746, time: 2.235 s\n",
      "Step 108100 (epoch   33.78), loss: 0.004725, time: 2.252 s\n",
      "Step 108120 (epoch   33.79), loss: 0.006514, time: 2.249 s\n",
      "Step 108140 (epoch   33.79), loss: 0.009786, time: 2.245 s\n",
      "Step 108160 (epoch   33.80), loss: 0.007419, time: 2.242 s\n",
      "Step 108180 (epoch   33.81), loss: 0.008976, time: 2.244 s\n",
      "Step 108200 (epoch   33.81), loss: 0.004299, time: 2.238 s, error: 0.012803\n",
      "Step 108220 (epoch   33.82), loss: 0.005302, time: 15.154 s\n",
      "Step 108240 (epoch   33.83), loss: 0.007681, time: 2.241 s\n",
      "Step 108260 (epoch   33.83), loss: 0.008030, time: 2.234 s\n",
      "Step 108280 (epoch   33.84), loss: 0.008444, time: 2.250 s\n",
      "Step 108300 (epoch   33.84), loss: 0.007348, time: 2.240 s\n",
      "Step 108320 (epoch   33.85), loss: 0.005284, time: 2.238 s\n",
      "Step 108340 (epoch   33.86), loss: 0.006134, time: 2.245 s\n",
      "Step 108360 (epoch   33.86), loss: 0.010854, time: 2.228 s\n",
      "Step 108380 (epoch   33.87), loss: 0.006335, time: 2.236 s\n",
      "Step 108400 (epoch   33.88), loss: 0.008274, time: 2.228 s, error: 0.012415\n",
      "Step 108420 (epoch   33.88), loss: 0.005946, time: 15.001 s\n",
      "Step 108440 (epoch   33.89), loss: 0.004531, time: 2.234 s\n",
      "Step 108460 (epoch   33.89), loss: 0.007821, time: 2.253 s\n",
      "Step 108480 (epoch   33.90), loss: 0.007068, time: 2.249 s\n",
      "Step 108500 (epoch   33.91), loss: 0.009685, time: 2.249 s\n",
      "Step 108520 (epoch   33.91), loss: 0.008182, time: 2.234 s\n",
      "Step 108540 (epoch   33.92), loss: 0.005007, time: 2.237 s\n",
      "Step 108560 (epoch   33.92), loss: 0.004672, time: 2.230 s\n",
      "Step 108580 (epoch   33.93), loss: 0.010283, time: 2.236 s\n",
      "Step 108600 (epoch   33.94), loss: 0.005820, time: 2.244 s, error: 0.012692\n",
      "Step 108620 (epoch   33.94), loss: 0.006398, time: 15.009 s\n",
      "Step 108640 (epoch   33.95), loss: 0.006819, time: 2.239 s\n",
      "Step 108660 (epoch   33.96), loss: 0.013698, time: 2.234 s\n",
      "Step 108680 (epoch   33.96), loss: 0.005838, time: 2.231 s\n",
      "Step 108700 (epoch   33.97), loss: 0.005960, time: 2.237 s\n",
      "Step 108720 (epoch   33.98), loss: 0.016737, time: 2.254 s\n",
      "Step 108740 (epoch   33.98), loss: 0.006951, time: 2.252 s\n",
      "Step 108760 (epoch   33.99), loss: 0.004906, time: 2.242 s\n",
      "Step 108780 (epoch   33.99), loss: 0.009229, time: 2.239 s\n",
      "Step 108800 (epoch   34.00), loss: 0.004919, time: 2.232 s, error: 0.012705\n",
      "Step 108820 (epoch   34.01), loss: 0.004873, time: 14.992 s\n",
      "Step 108840 (epoch   34.01), loss: 0.006187, time: 2.234 s\n",
      "Step 108860 (epoch   34.02), loss: 0.008788, time: 2.229 s\n",
      "Step 108880 (epoch   34.02), loss: 0.007568, time: 2.244 s\n",
      "Step 108900 (epoch   34.03), loss: 0.004910, time: 2.243 s\n",
      "Step 108920 (epoch   34.04), loss: 0.004216, time: 2.237 s\n",
      "Step 108940 (epoch   34.04), loss: 0.008001, time: 2.237 s\n",
      "Step 108960 (epoch   34.05), loss: 0.006664, time: 2.236 s\n",
      "Step 108980 (epoch   34.06), loss: 0.005553, time: 2.253 s\n",
      "Step 109000 (epoch   34.06), loss: 0.006064, time: 2.237 s, error: 0.012611\n",
      "Step 109020 (epoch   34.07), loss: 0.005875, time: 15.000 s\n",
      "Step 109040 (epoch   34.08), loss: 0.006856, time: 2.244 s\n",
      "Step 109060 (epoch   34.08), loss: 0.005034, time: 2.249 s\n",
      "Step 109080 (epoch   34.09), loss: 0.004592, time: 2.238 s\n",
      "Step 109100 (epoch   34.09), loss: 0.007104, time: 2.238 s\n",
      "Step 109120 (epoch   34.10), loss: 0.009544, time: 2.244 s\n",
      "Step 109140 (epoch   34.11), loss: 0.007805, time: 2.243 s\n",
      "Step 109160 (epoch   34.11), loss: 0.007407, time: 2.235 s\n",
      "Step 109180 (epoch   34.12), loss: 0.010199, time: 2.232 s\n",
      "Step 109200 (epoch   34.12), loss: 0.005588, time: 2.242 s, error: 0.012644\n",
      "Step 109220 (epoch   34.13), loss: 0.017127, time: 15.222 s\n",
      "Step 109240 (epoch   34.14), loss: 0.005553, time: 2.241 s\n",
      "Step 109260 (epoch   34.14), loss: 0.011083, time: 2.228 s\n",
      "Step 109280 (epoch   34.15), loss: 0.004714, time: 2.233 s\n",
      "Step 109300 (epoch   34.16), loss: 0.005500, time: 2.230 s\n",
      "Step 109320 (epoch   34.16), loss: 0.004445, time: 2.246 s\n",
      "Step 109340 (epoch   34.17), loss: 0.005409, time: 2.246 s\n",
      "Step 109360 (epoch   34.17), loss: 0.004826, time: 2.238 s\n",
      "Step 109380 (epoch   34.18), loss: 0.009650, time: 2.237 s\n",
      "Step 109400 (epoch   34.19), loss: 0.005456, time: 2.244 s, error: 0.013128\n",
      "Step 109420 (epoch   34.19), loss: 0.008199, time: 15.227 s\n",
      "Step 109440 (epoch   34.20), loss: 0.005116, time: 2.242 s\n",
      "Step 109460 (epoch   34.21), loss: 0.005914, time: 2.239 s\n",
      "Step 109480 (epoch   34.21), loss: 0.005830, time: 2.235 s\n",
      "Step 109500 (epoch   34.22), loss: 0.007503, time: 2.245 s\n",
      "Step 109520 (epoch   34.23), loss: 0.006980, time: 2.245 s\n",
      "Step 109540 (epoch   34.23), loss: 0.006255, time: 2.241 s\n",
      "Step 109560 (epoch   34.24), loss: 0.006705, time: 2.243 s\n",
      "Step 109580 (epoch   34.24), loss: 0.006816, time: 2.241 s\n",
      "Step 109600 (epoch   34.25), loss: 0.005321, time: 2.237 s, error: 0.012910\n",
      "Step 109620 (epoch   34.26), loss: 0.004544, time: 15.029 s\n",
      "Step 109640 (epoch   34.26), loss: 0.005794, time: 2.245 s\n",
      "Step 109660 (epoch   34.27), loss: 0.005397, time: 2.257 s\n",
      "Step 109680 (epoch   34.27), loss: 0.005770, time: 2.239 s\n",
      "Step 109700 (epoch   34.28), loss: 0.005976, time: 2.242 s\n",
      "Step 109720 (epoch   34.29), loss: 0.004694, time: 2.239 s\n",
      "Step 109740 (epoch   34.29), loss: 0.004987, time: 2.242 s\n",
      "Step 109760 (epoch   34.30), loss: 0.004072, time: 2.241 s\n",
      "Step 109780 (epoch   34.31), loss: 0.007759, time: 2.227 s\n",
      "Step 109800 (epoch   34.31), loss: 0.004419, time: 2.246 s, error: 0.012993\n",
      "Step 109820 (epoch   34.32), loss: 0.004931, time: 15.045 s\n",
      "Step 109840 (epoch   34.33), loss: 0.004254, time: 2.247 s\n",
      "Step 109860 (epoch   34.33), loss: 0.004483, time: 2.238 s\n",
      "Step 109880 (epoch   34.34), loss: 0.008526, time: 2.244 s\n",
      "Step 109900 (epoch   34.34), loss: 0.006583, time: 2.240 s\n",
      "Step 109920 (epoch   34.35), loss: 0.008347, time: 2.255 s\n",
      "Step 109940 (epoch   34.36), loss: 0.007307, time: 2.235 s\n",
      "Step 109960 (epoch   34.36), loss: 0.004872, time: 2.240 s\n",
      "Step 109980 (epoch   34.37), loss: 0.005892, time: 2.253 s\n",
      "Step 110000 (epoch   34.38), loss: 0.006967, time: 2.248 s, error: 0.013064\n",
      "\n",
      "Time since beginning  : 19144.043 s\n",
      "\n",
      "Step 110020 (epoch   34.38), loss: 0.007456, time: 15.112 s\n",
      "Step 110040 (epoch   34.39), loss: 0.008184, time: 2.255 s\n",
      "Step 110060 (epoch   34.39), loss: 0.005998, time: 2.253 s\n",
      "Step 110080 (epoch   34.40), loss: 0.009901, time: 2.245 s\n",
      "Step 110100 (epoch   34.41), loss: 0.007366, time: 2.259 s\n",
      "Step 110120 (epoch   34.41), loss: 0.006596, time: 2.250 s\n",
      "Step 110140 (epoch   34.42), loss: 0.006487, time: 2.255 s\n",
      "Step 110160 (epoch   34.42), loss: 0.004714, time: 2.254 s\n",
      "Step 110180 (epoch   34.43), loss: 0.005476, time: 2.255 s\n",
      "Step 110200 (epoch   34.44), loss: 0.005435, time: 2.250 s, error: 0.013033\n",
      "Step 110220 (epoch   34.44), loss: 0.004947, time: 15.045 s\n",
      "Step 110240 (epoch   34.45), loss: 0.005579, time: 2.252 s\n",
      "Step 110260 (epoch   34.46), loss: 0.007032, time: 2.254 s\n",
      "Step 110280 (epoch   34.46), loss: 0.006708, time: 2.255 s\n",
      "Step 110300 (epoch   34.47), loss: 0.005143, time: 2.254 s\n",
      "Step 110320 (epoch   34.48), loss: 0.007643, time: 2.253 s\n",
      "Step 110340 (epoch   34.48), loss: 0.005372, time: 2.254 s\n",
      "Step 110360 (epoch   34.49), loss: 0.005988, time: 2.248 s\n",
      "Step 110380 (epoch   34.49), loss: 0.009657, time: 2.249 s\n",
      "Step 110400 (epoch   34.50), loss: 0.004961, time: 2.253 s, error: 0.012787\n",
      "Step 110420 (epoch   34.51), loss: 0.006061, time: 15.255 s\n",
      "Step 110440 (epoch   34.51), loss: 0.006279, time: 2.254 s\n",
      "Step 110460 (epoch   34.52), loss: 0.005967, time: 2.237 s\n",
      "Step 110480 (epoch   34.52), loss: 0.009826, time: 2.256 s\n",
      "Step 110500 (epoch   34.53), loss: 0.008102, time: 2.256 s\n",
      "Step 110520 (epoch   34.54), loss: 0.006159, time: 2.241 s\n",
      "Step 110540 (epoch   34.54), loss: 0.008142, time: 2.246 s\n",
      "Step 110560 (epoch   34.55), loss: 0.006901, time: 2.252 s\n",
      "Step 110580 (epoch   34.56), loss: 0.006409, time: 2.248 s\n",
      "Step 110600 (epoch   34.56), loss: 0.006912, time: 2.255 s, error: 0.012690\n",
      "Step 110620 (epoch   34.57), loss: 0.011185, time: 15.200 s\n",
      "Step 110640 (epoch   34.58), loss: 0.009186, time: 2.237 s\n",
      "Step 110660 (epoch   34.58), loss: 0.011748, time: 2.244 s\n",
      "Step 110680 (epoch   34.59), loss: 0.006585, time: 2.232 s\n",
      "Step 110700 (epoch   34.59), loss: 0.007183, time: 2.250 s\n",
      "Step 110720 (epoch   34.60), loss: 0.011373, time: 2.243 s\n",
      "Step 110740 (epoch   34.61), loss: 0.003674, time: 2.254 s\n",
      "Step 110760 (epoch   34.61), loss: 0.008866, time: 2.243 s\n",
      "Step 110780 (epoch   34.62), loss: 0.006591, time: 2.247 s\n",
      "Step 110800 (epoch   34.62), loss: 0.005573, time: 2.251 s, error: 0.012778\n",
      "Step 110820 (epoch   34.63), loss: 0.006510, time: 15.000 s\n",
      "Step 110840 (epoch   34.64), loss: 0.010907, time: 2.250 s\n",
      "Step 110860 (epoch   34.64), loss: 0.005488, time: 2.257 s\n",
      "Step 110880 (epoch   34.65), loss: 0.005588, time: 2.236 s\n",
      "Step 110900 (epoch   34.66), loss: 0.004961, time: 2.242 s\n",
      "Step 110920 (epoch   34.66), loss: 0.004968, time: 2.244 s\n",
      "Step 110940 (epoch   34.67), loss: 0.007310, time: 2.254 s\n",
      "Step 110960 (epoch   34.67), loss: 0.011986, time: 2.235 s\n",
      "Step 110980 (epoch   34.68), loss: 0.004608, time: 2.235 s\n",
      "Step 111000 (epoch   34.69), loss: 0.006925, time: 2.241 s, error: 0.012966\n",
      "Step 111020 (epoch   34.69), loss: 0.005222, time: 15.057 s\n",
      "Step 111040 (epoch   34.70), loss: 0.005896, time: 2.250 s\n",
      "Step 111060 (epoch   34.71), loss: 0.004118, time: 2.244 s\n",
      "Step 111080 (epoch   34.71), loss: 0.006328, time: 2.241 s\n",
      "Step 111100 (epoch   34.72), loss: 0.005931, time: 2.252 s\n",
      "Step 111120 (epoch   34.73), loss: 0.006188, time: 2.257 s\n",
      "Step 111140 (epoch   34.73), loss: 0.006844, time: 2.244 s\n",
      "Step 111160 (epoch   34.74), loss: 0.006471, time: 2.243 s\n",
      "Step 111180 (epoch   34.74), loss: 0.006818, time: 2.248 s\n",
      "Step 111200 (epoch   34.75), loss: 0.007016, time: 2.240 s, error: 0.012491\n",
      "Step 111220 (epoch   34.76), loss: 0.007827, time: 15.012 s\n",
      "Step 111240 (epoch   34.76), loss: 0.004130, time: 2.252 s\n",
      "Step 111260 (epoch   34.77), loss: 0.007877, time: 2.251 s\n",
      "Step 111280 (epoch   34.77), loss: 0.007251, time: 2.255 s\n",
      "Step 111300 (epoch   34.78), loss: 0.004159, time: 2.246 s\n",
      "Step 111320 (epoch   34.79), loss: 0.009829, time: 2.240 s\n",
      "Step 111340 (epoch   34.79), loss: 0.005738, time: 2.234 s\n",
      "Step 111360 (epoch   34.80), loss: 0.005078, time: 2.237 s\n",
      "Step 111380 (epoch   34.81), loss: 0.006834, time: 2.261 s\n",
      "Step 111400 (epoch   34.81), loss: 0.006644, time: 2.253 s, error: 0.012627\n",
      "Step 111420 (epoch   34.82), loss: 0.007589, time: 15.088 s\n",
      "Step 111440 (epoch   34.83), loss: 0.007634, time: 2.239 s\n",
      "Step 111460 (epoch   34.83), loss: 0.005179, time: 2.252 s\n",
      "Step 111480 (epoch   34.84), loss: 0.004995, time: 2.249 s\n",
      "Step 111500 (epoch   34.84), loss: 0.012250, time: 2.248 s\n",
      "Step 111520 (epoch   34.85), loss: 0.005436, time: 2.249 s\n",
      "Step 111540 (epoch   34.86), loss: 0.004542, time: 2.245 s\n",
      "Step 111560 (epoch   34.86), loss: 0.006285, time: 2.242 s\n",
      "Step 111580 (epoch   34.87), loss: 0.004714, time: 2.251 s\n",
      "Step 111600 (epoch   34.88), loss: 0.011376, time: 2.249 s, error: 0.012371\n",
      "Step 111620 (epoch   34.88), loss: 0.006376, time: 15.210 s\n",
      "Step 111640 (epoch   34.89), loss: 0.007936, time: 2.238 s\n",
      "Step 111660 (epoch   34.89), loss: 0.009284, time: 2.236 s\n",
      "Step 111680 (epoch   34.90), loss: 0.012128, time: 2.250 s\n",
      "Step 111700 (epoch   34.91), loss: 0.005659, time: 2.240 s\n",
      "Step 111720 (epoch   34.91), loss: 0.004909, time: 2.251 s\n",
      "Step 111740 (epoch   34.92), loss: 0.005943, time: 2.247 s\n",
      "Step 111760 (epoch   34.92), loss: 0.005580, time: 2.249 s\n",
      "Step 111780 (epoch   34.93), loss: 0.006725, time: 2.238 s\n",
      "Step 111800 (epoch   34.94), loss: 0.006974, time: 2.245 s, error: 0.012683\n",
      "Step 111820 (epoch   34.94), loss: 0.004709, time: 15.189 s\n",
      "Step 111840 (epoch   34.95), loss: 0.007149, time: 2.250 s\n",
      "Step 111860 (epoch   34.96), loss: 0.006678, time: 2.246 s\n",
      "Step 111880 (epoch   34.96), loss: 0.007579, time: 2.237 s\n",
      "Step 111900 (epoch   34.97), loss: 0.006057, time: 2.249 s\n",
      "Step 111920 (epoch   34.98), loss: 0.005779, time: 2.238 s\n",
      "Step 111940 (epoch   34.98), loss: 0.005164, time: 2.251 s\n",
      "Step 111960 (epoch   34.99), loss: 0.007620, time: 2.244 s\n",
      "Step 111980 (epoch   34.99), loss: 0.010339, time: 2.249 s\n",
      "Step 112000 (epoch   35.00), loss: 0.008874, time: 2.238 s, error: 0.012616\n",
      "\n",
      "Time since beginning  : 19497.488 s\n",
      "\n",
      "Step 112020 (epoch   35.01), loss: 0.005205, time: 15.148 s\n",
      "Step 112040 (epoch   35.01), loss: 0.007843, time: 2.258 s\n",
      "Step 112060 (epoch   35.02), loss: 0.005751, time: 2.252 s\n",
      "Step 112080 (epoch   35.02), loss: 0.005650, time: 2.244 s\n",
      "Step 112100 (epoch   35.03), loss: 0.006772, time: 2.241 s\n",
      "Step 112120 (epoch   35.04), loss: 0.008644, time: 2.244 s\n",
      "Step 112140 (epoch   35.04), loss: 0.005456, time: 2.231 s\n",
      "Step 112160 (epoch   35.05), loss: 0.006849, time: 2.246 s\n",
      "Step 112180 (epoch   35.06), loss: 0.008914, time: 2.237 s\n",
      "Step 112200 (epoch   35.06), loss: 0.006775, time: 2.246 s, error: 0.012339\n",
      "Step 112220 (epoch   35.07), loss: 0.009389, time: 15.049 s\n",
      "Step 112240 (epoch   35.08), loss: 0.007701, time: 2.233 s\n",
      "Step 112260 (epoch   35.08), loss: 0.007175, time: 2.248 s\n",
      "Step 112280 (epoch   35.09), loss: 0.003979, time: 2.233 s\n",
      "Step 112300 (epoch   35.09), loss: 0.006562, time: 2.243 s\n",
      "Step 112320 (epoch   35.10), loss: 0.005327, time: 2.260 s\n",
      "Step 112340 (epoch   35.11), loss: 0.005777, time: 2.235 s\n",
      "Step 112360 (epoch   35.11), loss: 0.005602, time: 2.237 s\n",
      "Step 112380 (epoch   35.12), loss: 0.007892, time: 2.247 s\n",
      "Step 112400 (epoch   35.12), loss: 0.006679, time: 2.249 s, error: 0.012503\n",
      "Step 112420 (epoch   35.13), loss: 0.005841, time: 15.031 s\n",
      "Step 112440 (epoch   35.14), loss: 0.006866, time: 2.242 s\n",
      "Step 112460 (epoch   35.14), loss: 0.007201, time: 2.238 s\n",
      "Step 112480 (epoch   35.15), loss: 0.007766, time: 2.255 s\n",
      "Step 112500 (epoch   35.16), loss: 0.006639, time: 2.251 s\n",
      "Step 112520 (epoch   35.16), loss: 0.015659, time: 2.230 s\n",
      "Step 112540 (epoch   35.17), loss: 0.005763, time: 2.244 s\n",
      "Step 112560 (epoch   35.17), loss: 0.009348, time: 2.242 s\n",
      "Step 112580 (epoch   35.18), loss: 0.007444, time: 2.257 s\n",
      "Step 112600 (epoch   35.19), loss: 0.006213, time: 2.237 s, error: 0.012982\n",
      "Step 112620 (epoch   35.19), loss: 0.006479, time: 15.031 s\n",
      "Step 112640 (epoch   35.20), loss: 0.004121, time: 2.231 s\n",
      "Step 112660 (epoch   35.21), loss: 0.006476, time: 2.250 s\n",
      "Step 112680 (epoch   35.21), loss: 0.007211, time: 2.256 s\n",
      "Step 112700 (epoch   35.22), loss: 0.008635, time: 2.241 s\n",
      "Step 112720 (epoch   35.23), loss: 0.005661, time: 2.244 s\n",
      "Step 112740 (epoch   35.23), loss: 0.010581, time: 2.249 s\n",
      "Step 112760 (epoch   35.24), loss: 0.006029, time: 2.248 s\n",
      "Step 112780 (epoch   35.24), loss: 0.006933, time: 2.241 s\n",
      "Step 112800 (epoch   35.25), loss: 0.006842, time: 2.237 s, error: 0.013024\n",
      "Step 112820 (epoch   35.26), loss: 0.005873, time: 15.201 s\n",
      "Step 112840 (epoch   35.26), loss: 0.006613, time: 2.244 s\n",
      "Step 112860 (epoch   35.27), loss: 0.005190, time: 2.248 s\n",
      "Step 112880 (epoch   35.27), loss: 0.006599, time: 2.234 s\n",
      "Step 112900 (epoch   35.28), loss: 0.005486, time: 2.238 s\n",
      "Step 112920 (epoch   35.29), loss: 0.006573, time: 2.244 s\n",
      "Step 112940 (epoch   35.29), loss: 0.005211, time: 2.241 s\n",
      "Step 112960 (epoch   35.30), loss: 0.005271, time: 2.240 s\n",
      "Step 112980 (epoch   35.31), loss: 0.013075, time: 2.253 s\n",
      "Step 113000 (epoch   35.31), loss: 0.008496, time: 2.235 s, error: 0.012812\n",
      "Step 113020 (epoch   35.32), loss: 0.006441, time: 15.217 s\n",
      "Step 113040 (epoch   35.33), loss: 0.009629, time: 2.240 s\n",
      "Step 113060 (epoch   35.33), loss: 0.007359, time: 2.234 s\n",
      "Step 113080 (epoch   35.34), loss: 0.006028, time: 2.240 s\n",
      "Step 113100 (epoch   35.34), loss: 0.006077, time: 2.237 s\n",
      "Step 113120 (epoch   35.35), loss: 0.005099, time: 2.248 s\n",
      "Step 113140 (epoch   35.36), loss: 0.007983, time: 2.238 s\n",
      "Step 113160 (epoch   35.36), loss: 0.006001, time: 2.254 s\n",
      "Step 113180 (epoch   35.37), loss: 0.004685, time: 2.230 s\n",
      "Step 113200 (epoch   35.38), loss: 0.011861, time: 2.247 s, error: 0.012929\n",
      "Step 113220 (epoch   35.38), loss: 0.006292, time: 15.025 s\n",
      "Step 113240 (epoch   35.39), loss: 0.004868, time: 2.245 s\n",
      "Step 113260 (epoch   35.39), loss: 0.007813, time: 2.257 s\n",
      "Step 113280 (epoch   35.40), loss: 0.005720, time: 2.246 s\n",
      "Step 113300 (epoch   35.41), loss: 0.004500, time: 2.257 s\n",
      "Step 113320 (epoch   35.41), loss: 0.004878, time: 2.240 s\n",
      "Step 113340 (epoch   35.42), loss: 0.010003, time: 2.250 s\n",
      "Step 113360 (epoch   35.42), loss: 0.008327, time: 2.250 s\n",
      "Step 113380 (epoch   35.43), loss: 0.006231, time: 2.245 s\n",
      "Step 113400 (epoch   35.44), loss: 0.005073, time: 2.247 s, error: 0.012612\n",
      "Step 113420 (epoch   35.44), loss: 0.007285, time: 15.098 s\n",
      "Step 113440 (epoch   35.45), loss: 0.006462, time: 2.245 s\n",
      "Step 113460 (epoch   35.46), loss: 0.006283, time: 2.240 s\n",
      "Step 113480 (epoch   35.46), loss: 0.005280, time: 2.255 s\n",
      "Step 113500 (epoch   35.47), loss: 0.005198, time: 2.254 s\n",
      "Step 113520 (epoch   35.48), loss: 0.005038, time: 2.256 s\n",
      "Step 113540 (epoch   35.48), loss: 0.004959, time: 2.236 s\n",
      "Step 113560 (epoch   35.49), loss: 0.009243, time: 2.238 s\n",
      "Step 113580 (epoch   35.49), loss: 0.005661, time: 2.254 s\n",
      "Step 113600 (epoch   35.50), loss: 0.008648, time: 2.256 s, error: 0.012839\n",
      "Step 113620 (epoch   35.51), loss: 0.009831, time: 15.201 s\n",
      "Step 113640 (epoch   35.51), loss: 0.007116, time: 2.236 s\n",
      "Step 113660 (epoch   35.52), loss: 0.007511, time: 2.250 s\n",
      "Step 113680 (epoch   35.52), loss: 0.005693, time: 2.240 s\n",
      "Step 113700 (epoch   35.53), loss: 0.009888, time: 2.251 s\n",
      "Step 113720 (epoch   35.54), loss: 0.005235, time: 2.239 s\n",
      "Step 113740 (epoch   35.54), loss: 0.007074, time: 2.250 s\n",
      "Step 113760 (epoch   35.55), loss: 0.005320, time: 2.245 s\n",
      "Step 113780 (epoch   35.56), loss: 0.008428, time: 2.259 s\n",
      "Step 113800 (epoch   35.56), loss: 0.007139, time: 2.242 s, error: 0.012371\n",
      "Step 113820 (epoch   35.57), loss: 0.013198, time: 15.044 s\n",
      "Step 113840 (epoch   35.58), loss: 0.006909, time: 2.242 s\n",
      "Step 113860 (epoch   35.58), loss: 0.009614, time: 2.239 s\n",
      "Step 113880 (epoch   35.59), loss: 0.005247, time: 2.247 s\n",
      "Step 113900 (epoch   35.59), loss: 0.005995, time: 2.236 s\n",
      "Step 113920 (epoch   35.60), loss: 0.004190, time: 2.245 s\n",
      "Step 113940 (epoch   35.61), loss: 0.005280, time: 2.255 s\n",
      "Step 113960 (epoch   35.61), loss: 0.007630, time: 2.239 s\n",
      "Step 113980 (epoch   35.62), loss: 0.007720, time: 2.244 s\n",
      "Step 114000 (epoch   35.62), loss: 0.004771, time: 2.240 s, error: 0.012716\n",
      "\n",
      "Time since beginning  : 19850.749 s\n",
      "\n",
      "Step 114020 (epoch   35.63), loss: 0.008117, time: 15.386 s\n",
      "Step 114040 (epoch   35.64), loss: 0.012984, time: 2.252 s\n",
      "Step 114060 (epoch   35.64), loss: 0.005584, time: 2.249 s\n",
      "Step 114080 (epoch   35.65), loss: 0.006066, time: 2.253 s\n",
      "Step 114100 (epoch   35.66), loss: 0.005821, time: 2.252 s\n",
      "Step 114120 (epoch   35.66), loss: 0.005945, time: 2.240 s\n",
      "Step 114140 (epoch   35.67), loss: 0.010635, time: 2.241 s\n",
      "Step 114160 (epoch   35.67), loss: 0.004186, time: 2.239 s\n",
      "Step 114180 (epoch   35.68), loss: 0.004654, time: 2.230 s\n",
      "Step 114200 (epoch   35.69), loss: 0.009440, time: 2.248 s, error: 0.012832\n",
      "Step 114220 (epoch   35.69), loss: 0.009304, time: 15.241 s\n",
      "Step 114240 (epoch   35.70), loss: 0.007591, time: 2.242 s\n",
      "Step 114260 (epoch   35.71), loss: 0.005973, time: 2.239 s\n",
      "Step 114280 (epoch   35.71), loss: 0.005186, time: 2.252 s\n",
      "Step 114300 (epoch   35.72), loss: 0.005452, time: 2.244 s\n",
      "Step 114320 (epoch   35.73), loss: 0.010953, time: 2.244 s\n",
      "Step 114340 (epoch   35.73), loss: 0.005601, time: 2.249 s\n",
      "Step 114360 (epoch   35.74), loss: 0.004288, time: 2.248 s\n",
      "Step 114380 (epoch   35.74), loss: 0.007243, time: 2.251 s\n",
      "Step 114400 (epoch   35.75), loss: 0.010334, time: 2.232 s, error: 0.012372\n",
      "Step 114420 (epoch   35.76), loss: 0.009343, time: 15.198 s\n",
      "Step 114440 (epoch   35.76), loss: 0.004896, time: 2.256 s\n",
      "Step 114460 (epoch   35.77), loss: 0.007577, time: 2.250 s\n",
      "Step 114480 (epoch   35.77), loss: 0.007071, time: 2.242 s\n",
      "Step 114500 (epoch   35.78), loss: 0.005172, time: 2.251 s\n",
      "Step 114520 (epoch   35.79), loss: 0.004064, time: 2.239 s\n",
      "Step 114540 (epoch   35.79), loss: 0.015060, time: 2.243 s\n",
      "Step 114560 (epoch   35.80), loss: 0.008292, time: 2.233 s\n",
      "Step 114580 (epoch   35.81), loss: 0.005385, time: 2.252 s\n",
      "Step 114600 (epoch   35.81), loss: 0.005366, time: 2.229 s, error: 0.012703\n",
      "Step 114620 (epoch   35.82), loss: 0.008437, time: 15.042 s\n",
      "Step 114640 (epoch   35.83), loss: 0.006205, time: 2.235 s\n",
      "Step 114660 (epoch   35.83), loss: 0.005188, time: 2.241 s\n",
      "Step 114680 (epoch   35.84), loss: 0.006353, time: 2.251 s\n",
      "Step 114700 (epoch   35.84), loss: 0.010622, time: 2.256 s\n",
      "Step 114720 (epoch   35.85), loss: 0.006647, time: 2.241 s\n",
      "Step 114740 (epoch   35.86), loss: 0.005842, time: 2.244 s\n",
      "Step 114760 (epoch   35.86), loss: 0.005295, time: 2.249 s\n",
      "Step 114780 (epoch   35.87), loss: 0.005219, time: 2.251 s\n",
      "Step 114800 (epoch   35.88), loss: 0.007623, time: 2.241 s, error: 0.012552\n",
      "Step 114820 (epoch   35.88), loss: 0.004427, time: 15.100 s\n",
      "Step 114840 (epoch   35.89), loss: 0.005933, time: 2.239 s\n",
      "Step 114860 (epoch   35.89), loss: 0.007661, time: 2.243 s\n",
      "Step 114880 (epoch   35.90), loss: 0.006798, time: 2.252 s\n",
      "Step 114900 (epoch   35.91), loss: 0.016845, time: 2.251 s\n",
      "Step 114920 (epoch   35.91), loss: 0.005842, time: 2.245 s\n",
      "Step 114940 (epoch   35.92), loss: 0.004999, time: 2.241 s\n",
      "Step 114960 (epoch   35.92), loss: 0.007727, time: 2.245 s\n",
      "Step 114980 (epoch   35.93), loss: 0.005341, time: 2.229 s\n",
      "Step 115000 (epoch   35.94), loss: 0.010436, time: 2.253 s, error: 0.012617\n",
      "Step 115020 (epoch   35.94), loss: 0.005369, time: 14.996 s\n",
      "Step 115040 (epoch   35.95), loss: 0.005959, time: 2.243 s\n",
      "Step 115060 (epoch   35.96), loss: 0.006648, time: 2.246 s\n",
      "Step 115080 (epoch   35.96), loss: 0.009102, time: 2.252 s\n",
      "Step 115100 (epoch   35.97), loss: 0.006636, time: 2.250 s\n",
      "Step 115120 (epoch   35.98), loss: 0.007730, time: 2.247 s\n",
      "Step 115140 (epoch   35.98), loss: 0.005408, time: 2.235 s\n",
      "Step 115160 (epoch   35.99), loss: 0.004339, time: 2.235 s\n",
      "Step 115180 (epoch   35.99), loss: 0.005552, time: 2.244 s\n",
      "Step 115200 (epoch   36.00), loss: 0.009727, time: 2.252 s, error: 0.012419\n",
      "Step 115220 (epoch   36.01), loss: 0.004908, time: 15.150 s\n",
      "Step 115240 (epoch   36.01), loss: 0.006673, time: 2.239 s\n",
      "Step 115260 (epoch   36.02), loss: 0.006552, time: 2.249 s\n",
      "Step 115280 (epoch   36.02), loss: 0.005399, time: 2.239 s\n",
      "Step 115300 (epoch   36.03), loss: 0.006708, time: 2.245 s\n",
      "Step 115320 (epoch   36.04), loss: 0.007705, time: 2.231 s\n",
      "Step 115340 (epoch   36.04), loss: 0.006461, time: 2.236 s\n",
      "Step 115360 (epoch   36.05), loss: 0.011589, time: 2.236 s\n",
      "Step 115380 (epoch   36.06), loss: 0.006799, time: 2.247 s\n",
      "Step 115400 (epoch   36.06), loss: 0.006803, time: 2.247 s, error: 0.012220\n",
      "Step 115420 (epoch   36.07), loss: 0.005620, time: 15.224 s\n",
      "Step 115440 (epoch   36.08), loss: 0.026814, time: 2.235 s\n",
      "Step 115460 (epoch   36.08), loss: 0.005165, time: 2.244 s\n",
      "Step 115480 (epoch   36.09), loss: 0.005535, time: 2.239 s\n",
      "Step 115500 (epoch   36.09), loss: 0.008343, time: 2.248 s\n",
      "Step 115520 (epoch   36.10), loss: 0.004555, time: 2.237 s\n",
      "Step 115540 (epoch   36.11), loss: 0.006804, time: 2.244 s\n",
      "Step 115560 (epoch   36.11), loss: 0.007355, time: 2.236 s\n",
      "Step 115580 (epoch   36.12), loss: 0.008633, time: 2.247 s\n",
      "Step 115600 (epoch   36.12), loss: 0.005391, time: 2.250 s, error: 0.012420\n",
      "Step 115620 (epoch   36.13), loss: 0.008476, time: 15.131 s\n",
      "Step 115640 (epoch   36.14), loss: 0.005705, time: 2.254 s\n",
      "Step 115660 (epoch   36.14), loss: 0.007583, time: 2.233 s\n",
      "Step 115680 (epoch   36.15), loss: 0.009606, time: 2.245 s\n",
      "Step 115700 (epoch   36.16), loss: 0.005349, time: 2.234 s\n",
      "Step 115720 (epoch   36.16), loss: 0.007324, time: 2.244 s\n",
      "Step 115740 (epoch   36.17), loss: 0.006021, time: 2.235 s\n",
      "Step 115760 (epoch   36.17), loss: 0.005193, time: 2.241 s\n",
      "Step 115780 (epoch   36.18), loss: 0.004858, time: 2.226 s\n",
      "Step 115800 (epoch   36.19), loss: 0.004975, time: 2.249 s, error: 0.012708\n",
      "Step 115820 (epoch   36.19), loss: 0.005129, time: 15.029 s\n",
      "Step 115840 (epoch   36.20), loss: 0.005936, time: 2.247 s\n",
      "Step 115860 (epoch   36.21), loss: 0.006507, time: 2.230 s\n",
      "Step 115880 (epoch   36.21), loss: 0.009299, time: 2.249 s\n",
      "Step 115900 (epoch   36.22), loss: 0.008216, time: 2.250 s\n",
      "Step 115920 (epoch   36.23), loss: 0.009243, time: 2.250 s\n",
      "Step 115940 (epoch   36.23), loss: 0.006211, time: 2.250 s\n",
      "Step 115960 (epoch   36.24), loss: 0.004724, time: 2.230 s\n",
      "Step 115980 (epoch   36.24), loss: 0.005276, time: 2.237 s\n",
      "Step 116000 (epoch   36.25), loss: 0.004679, time: 2.244 s, error: 0.013316\n",
      "\n",
      "Time since beginning  : 20203.880 s\n",
      "\n",
      "Step 116020 (epoch   36.26), loss: 0.006848, time: 15.081 s\n",
      "Step 116040 (epoch   36.26), loss: 0.009153, time: 2.242 s\n",
      "Step 116060 (epoch   36.27), loss: 0.007434, time: 2.239 s\n",
      "Step 116080 (epoch   36.27), loss: 0.008757, time: 2.242 s\n",
      "Step 116100 (epoch   36.28), loss: 0.007961, time: 2.240 s\n",
      "Step 116120 (epoch   36.29), loss: 0.007559, time: 2.240 s\n",
      "Step 116140 (epoch   36.29), loss: 0.005436, time: 2.254 s\n",
      "Step 116160 (epoch   36.30), loss: 0.006217, time: 2.235 s\n",
      "Step 116180 (epoch   36.31), loss: 0.007721, time: 2.241 s\n",
      "Step 116200 (epoch   36.31), loss: 0.007853, time: 2.241 s, error: 0.012626\n",
      "Step 116220 (epoch   36.32), loss: 0.005819, time: 14.998 s\n",
      "Step 116240 (epoch   36.33), loss: 0.005084, time: 2.244 s\n",
      "Step 116260 (epoch   36.33), loss: 0.005234, time: 2.246 s\n",
      "Step 116280 (epoch   36.34), loss: 0.006501, time: 2.247 s\n",
      "Step 116300 (epoch   36.34), loss: 0.007717, time: 2.243 s\n",
      "Step 116320 (epoch   36.35), loss: 0.005857, time: 2.244 s\n",
      "Step 116340 (epoch   36.36), loss: 0.006628, time: 2.224 s\n",
      "Step 116360 (epoch   36.36), loss: 0.005654, time: 2.247 s\n",
      "Step 116380 (epoch   36.37), loss: 0.007785, time: 2.234 s\n",
      "Step 116400 (epoch   36.38), loss: 0.006277, time: 2.255 s, error: 0.012741\n",
      "Step 116420 (epoch   36.38), loss: 0.007453, time: 15.147 s\n",
      "Step 116440 (epoch   36.39), loss: 0.007904, time: 2.252 s\n",
      "Step 116460 (epoch   36.39), loss: 0.006348, time: 2.243 s\n",
      "Step 116480 (epoch   36.40), loss: 0.009315, time: 2.249 s\n",
      "Step 116500 (epoch   36.41), loss: 0.004667, time: 2.253 s\n",
      "Step 116520 (epoch   36.41), loss: 0.006018, time: 2.246 s\n",
      "Step 116540 (epoch   36.42), loss: 0.009007, time: 2.242 s\n",
      "Step 116560 (epoch   36.42), loss: 0.007801, time: 2.226 s\n",
      "Step 116580 (epoch   36.43), loss: 0.004548, time: 2.251 s\n",
      "Step 116600 (epoch   36.44), loss: 0.004719, time: 2.252 s, error: 0.012392\n",
      "Step 116620 (epoch   36.44), loss: 0.005339, time: 15.292 s\n",
      "Step 116640 (epoch   36.45), loss: 0.006790, time: 2.253 s\n",
      "Step 116660 (epoch   36.46), loss: 0.009404, time: 2.246 s\n",
      "Step 116680 (epoch   36.46), loss: 0.007350, time: 2.244 s\n",
      "Step 116700 (epoch   36.47), loss: 0.005631, time: 2.253 s\n",
      "Step 116720 (epoch   36.48), loss: 0.004600, time: 2.242 s\n",
      "Step 116740 (epoch   36.48), loss: 0.006940, time: 2.252 s\n",
      "Step 116760 (epoch   36.49), loss: 0.007370, time: 2.236 s\n",
      "Step 116780 (epoch   36.49), loss: 0.005952, time: 2.252 s\n",
      "Step 116800 (epoch   36.50), loss: 0.006583, time: 2.257 s, error: 0.012923\n",
      "Step 116820 (epoch   36.51), loss: 0.006397, time: 15.238 s\n",
      "Step 116840 (epoch   36.51), loss: 0.004113, time: 2.244 s\n",
      "Step 116860 (epoch   36.52), loss: 0.009648, time: 2.225 s\n",
      "Step 116880 (epoch   36.52), loss: 0.005675, time: 2.232 s\n",
      "Step 116900 (epoch   36.53), loss: 0.007533, time: 2.229 s\n",
      "Step 116920 (epoch   36.54), loss: 0.004022, time: 2.248 s\n",
      "Step 116940 (epoch   36.54), loss: 0.007546, time: 2.243 s\n",
      "Step 116960 (epoch   36.55), loss: 0.005320, time: 2.244 s\n",
      "Step 116980 (epoch   36.56), loss: 0.004924, time: 2.238 s\n",
      "Step 117000 (epoch   36.56), loss: 0.004687, time: 2.242 s, error: 0.012387\n",
      "Step 117020 (epoch   36.57), loss: 0.008328, time: 15.009 s\n",
      "Step 117040 (epoch   36.58), loss: 0.008543, time: 2.244 s\n",
      "Step 117060 (epoch   36.58), loss: 0.005838, time: 2.254 s\n",
      "Step 117080 (epoch   36.59), loss: 0.007531, time: 2.257 s\n",
      "Step 117100 (epoch   36.59), loss: 0.008296, time: 2.252 s\n",
      "Step 117120 (epoch   36.60), loss: 0.009328, time: 2.252 s\n",
      "Step 117140 (epoch   36.61), loss: 0.007989, time: 2.252 s\n",
      "Step 117160 (epoch   36.61), loss: 0.004174, time: 2.242 s\n",
      "Step 117180 (epoch   36.62), loss: 0.004813, time: 2.244 s\n",
      "Step 117200 (epoch   36.62), loss: 0.005614, time: 2.248 s, error: 0.012673\n",
      "Step 117220 (epoch   36.63), loss: 0.006897, time: 15.083 s\n",
      "Step 117240 (epoch   36.64), loss: 0.008236, time: 2.233 s\n",
      "Step 117260 (epoch   36.64), loss: 0.010839, time: 2.248 s\n",
      "Step 117280 (epoch   36.65), loss: 0.010139, time: 2.232 s\n",
      "Step 117300 (epoch   36.66), loss: 0.005674, time: 2.241 s\n",
      "Step 117320 (epoch   36.66), loss: 0.005672, time: 2.243 s\n",
      "Step 117340 (epoch   36.67), loss: 0.008741, time: 2.256 s\n",
      "Step 117360 (epoch   36.67), loss: 0.005076, time: 2.242 s\n",
      "Step 117380 (epoch   36.68), loss: 0.005436, time: 2.242 s\n",
      "Step 117400 (epoch   36.69), loss: 0.008825, time: 2.242 s, error: 0.012776\n",
      "Step 117420 (epoch   36.69), loss: 0.010856, time: 15.022 s\n",
      "Step 117440 (epoch   36.70), loss: 0.005874, time: 2.245 s\n",
      "Step 117460 (epoch   36.71), loss: 0.006109, time: 2.246 s\n",
      "Step 117480 (epoch   36.71), loss: 0.005429, time: 2.244 s\n",
      "Step 117500 (epoch   36.72), loss: 0.007678, time: 2.248 s\n",
      "Step 117520 (epoch   36.73), loss: 0.005960, time: 2.238 s\n",
      "Step 117540 (epoch   36.73), loss: 0.007182, time: 2.248 s\n",
      "Step 117560 (epoch   36.74), loss: 0.008913, time: 2.243 s\n",
      "Step 117580 (epoch   36.74), loss: 0.005910, time: 2.255 s\n",
      "Step 117600 (epoch   36.75), loss: 0.008917, time: 2.253 s, error: 0.012309\n",
      "Step 117620 (epoch   36.76), loss: 0.007736, time: 15.051 s\n",
      "Step 117640 (epoch   36.76), loss: 0.006338, time: 2.226 s\n",
      "Step 117660 (epoch   36.77), loss: 0.008184, time: 2.234 s\n",
      "Step 117680 (epoch   36.77), loss: 0.004621, time: 2.251 s\n",
      "Step 117700 (epoch   36.78), loss: 0.008710, time: 2.252 s\n",
      "Step 117720 (epoch   36.79), loss: 0.006021, time: 2.249 s\n",
      "Step 117740 (epoch   36.79), loss: 0.004186, time: 2.246 s\n",
      "Step 117760 (epoch   36.80), loss: 0.007694, time: 2.237 s\n",
      "Step 117780 (epoch   36.81), loss: 0.005858, time: 2.245 s\n",
      "Step 117800 (epoch   36.81), loss: 0.004759, time: 2.240 s, error: 0.012808\n",
      "Step 117820 (epoch   36.82), loss: 0.005524, time: 15.235 s\n",
      "Step 117840 (epoch   36.83), loss: 0.005548, time: 2.249 s\n",
      "Step 117860 (epoch   36.83), loss: 0.006090, time: 2.234 s\n",
      "Step 117880 (epoch   36.84), loss: 0.004071, time: 2.244 s\n",
      "Step 117900 (epoch   36.84), loss: 0.006090, time: 2.239 s\n",
      "Step 117920 (epoch   36.85), loss: 0.007565, time: 2.239 s\n",
      "Step 117940 (epoch   36.86), loss: 0.009108, time: 2.234 s\n",
      "Step 117960 (epoch   36.86), loss: 0.005906, time: 2.235 s\n",
      "Step 117980 (epoch   36.87), loss: 0.005696, time: 2.244 s\n",
      "Step 118000 (epoch   36.88), loss: 0.005788, time: 2.243 s, error: 0.012396\n",
      "\n",
      "Time since beginning  : 20557.170 s\n",
      "\n",
      "Step 118020 (epoch   36.88), loss: 0.005298, time: 15.281 s\n",
      "Step 118040 (epoch   36.89), loss: 0.004421, time: 2.230 s\n",
      "Step 118060 (epoch   36.89), loss: 0.009129, time: 2.250 s\n",
      "Step 118080 (epoch   36.90), loss: 0.005834, time: 2.239 s\n",
      "Step 118100 (epoch   36.91), loss: 0.010113, time: 2.249 s\n",
      "Step 118120 (epoch   36.91), loss: 0.006362, time: 2.242 s\n",
      "Step 118140 (epoch   36.92), loss: 0.007658, time: 2.245 s\n",
      "Step 118160 (epoch   36.92), loss: 0.005276, time: 2.244 s\n",
      "Step 118180 (epoch   36.93), loss: 0.007179, time: 2.253 s\n",
      "Step 118200 (epoch   36.94), loss: 0.006556, time: 2.235 s, error: 0.012401\n",
      "Step 118220 (epoch   36.94), loss: 0.005140, time: 15.056 s\n",
      "Step 118240 (epoch   36.95), loss: 0.005080, time: 2.242 s\n",
      "Step 118260 (epoch   36.96), loss: 0.006514, time: 2.258 s\n",
      "Step 118280 (epoch   36.96), loss: 0.007230, time: 2.250 s\n",
      "Step 118300 (epoch   36.97), loss: 0.003166, time: 2.238 s\n",
      "Step 118320 (epoch   36.98), loss: 0.009325, time: 2.238 s\n",
      "Step 118340 (epoch   36.98), loss: 0.004805, time: 2.241 s\n",
      "Step 118360 (epoch   36.99), loss: 0.004775, time: 2.227 s\n",
      "Step 118380 (epoch   36.99), loss: 0.007602, time: 2.230 s\n",
      "Step 118400 (epoch   37.00), loss: 0.004046, time: 2.251 s, error: 0.012366\n",
      "Step 118420 (epoch   37.01), loss: 0.005290, time: 15.006 s\n",
      "Step 118440 (epoch   37.01), loss: 0.004900, time: 2.232 s\n",
      "Step 118460 (epoch   37.02), loss: 0.004738, time: 2.249 s\n",
      "Step 118480 (epoch   37.02), loss: 0.006633, time: 2.240 s\n",
      "Step 118500 (epoch   37.03), loss: 0.008153, time: 2.247 s\n",
      "Step 118520 (epoch   37.04), loss: 0.006186, time: 2.254 s\n",
      "Step 118540 (epoch   37.04), loss: 0.006477, time: 2.243 s\n",
      "Step 118560 (epoch   37.05), loss: 0.006651, time: 2.242 s\n",
      "Step 118580 (epoch   37.06), loss: 0.007160, time: 2.247 s\n",
      "Step 118600 (epoch   37.06), loss: 0.007570, time: 2.225 s, error: 0.012123\n",
      "Step 118620 (epoch   37.07), loss: 0.006154, time: 15.084 s\n",
      "Step 118640 (epoch   37.08), loss: 0.007260, time: 2.251 s\n",
      "Step 118660 (epoch   37.08), loss: 0.007019, time: 2.245 s\n",
      "Step 118680 (epoch   37.09), loss: 0.011302, time: 2.254 s\n",
      "Step 118700 (epoch   37.09), loss: 0.006499, time: 2.249 s\n",
      "Step 118720 (epoch   37.10), loss: 0.006622, time: 2.247 s\n",
      "Step 118740 (epoch   37.11), loss: 0.006096, time: 2.235 s\n",
      "Step 118760 (epoch   37.11), loss: 0.005710, time: 2.251 s\n",
      "Step 118780 (epoch   37.12), loss: 0.005860, time: 2.246 s\n",
      "Step 118800 (epoch   37.12), loss: 0.005609, time: 2.241 s, error: 0.012302\n",
      "Step 118820 (epoch   37.13), loss: 0.013056, time: 15.003 s\n",
      "Step 118840 (epoch   37.14), loss: 0.008599, time: 2.234 s\n",
      "Step 118860 (epoch   37.14), loss: 0.006178, time: 2.247 s\n",
      "Step 118880 (epoch   37.15), loss: 0.008544, time: 2.226 s\n",
      "Step 118900 (epoch   37.16), loss: 0.007838, time: 2.240 s\n",
      "Step 118920 (epoch   37.16), loss: 0.012592, time: 2.235 s\n",
      "Step 118940 (epoch   37.17), loss: 0.007750, time: 2.251 s\n",
      "Step 118960 (epoch   37.17), loss: 0.006268, time: 2.225 s\n",
      "Step 118980 (epoch   37.18), loss: 0.005999, time: 2.245 s\n",
      "Step 119000 (epoch   37.19), loss: 0.005332, time: 2.232 s, error: 0.012470\n",
      "Step 119020 (epoch   37.19), loss: 0.005326, time: 15.202 s\n",
      "Step 119040 (epoch   37.20), loss: 0.007119, time: 2.244 s\n",
      "Step 119060 (epoch   37.21), loss: 0.004989, time: 2.237 s\n",
      "Step 119080 (epoch   37.21), loss: 0.008018, time: 2.234 s\n",
      "Step 119100 (epoch   37.22), loss: 0.006468, time: 2.236 s\n",
      "Step 119120 (epoch   37.23), loss: 0.006529, time: 2.245 s\n",
      "Step 119140 (epoch   37.23), loss: 0.006828, time: 2.231 s\n",
      "Step 119160 (epoch   37.24), loss: 0.007662, time: 2.241 s\n",
      "Step 119180 (epoch   37.24), loss: 0.003699, time: 2.232 s\n",
      "Step 119200 (epoch   37.25), loss: 0.007203, time: 2.239 s, error: 0.013353\n",
      "Step 119220 (epoch   37.26), loss: 0.010188, time: 15.200 s\n",
      "Step 119240 (epoch   37.26), loss: 0.005604, time: 2.235 s\n",
      "Step 119260 (epoch   37.27), loss: 0.005743, time: 2.237 s\n",
      "Step 119280 (epoch   37.27), loss: 0.004597, time: 2.243 s\n",
      "Step 119300 (epoch   37.28), loss: 0.007982, time: 2.243 s\n",
      "Step 119320 (epoch   37.29), loss: 0.005928, time: 2.244 s\n",
      "Step 119340 (epoch   37.29), loss: 0.006444, time: 2.239 s\n",
      "Step 119360 (epoch   37.30), loss: 0.004523, time: 2.248 s\n",
      "Step 119380 (epoch   37.31), loss: 0.006879, time: 2.244 s\n",
      "Step 119400 (epoch   37.31), loss: 0.006117, time: 2.231 s, error: 0.012659\n",
      "Step 119420 (epoch   37.32), loss: 0.006777, time: 15.069 s\n",
      "Step 119440 (epoch   37.33), loss: 0.011035, time: 2.250 s\n",
      "Step 119460 (epoch   37.33), loss: 0.005294, time: 2.251 s\n",
      "Step 119480 (epoch   37.34), loss: 0.009073, time: 2.249 s\n",
      "Step 119500 (epoch   37.34), loss: 0.010111, time: 2.250 s\n",
      "Step 119520 (epoch   37.35), loss: 0.009059, time: 2.246 s\n",
      "Step 119540 (epoch   37.36), loss: 0.004246, time: 2.232 s\n",
      "Step 119560 (epoch   37.36), loss: 0.005168, time: 2.243 s\n",
      "Step 119580 (epoch   37.37), loss: 0.005531, time: 2.238 s\n",
      "Step 119600 (epoch   37.38), loss: 0.006900, time: 2.235 s, error: 0.012574\n",
      "Step 119620 (epoch   37.38), loss: 0.005632, time: 15.023 s\n",
      "Step 119640 (epoch   37.39), loss: 0.008135, time: 2.245 s\n",
      "Step 119660 (epoch   37.39), loss: 0.007510, time: 2.241 s\n",
      "Step 119680 (epoch   37.40), loss: 0.011542, time: 2.251 s\n",
      "Step 119700 (epoch   37.41), loss: 0.005675, time: 2.239 s\n",
      "Step 119720 (epoch   37.41), loss: 0.009003, time: 2.255 s\n",
      "Step 119740 (epoch   37.42), loss: 0.004585, time: 2.243 s\n",
      "Step 119760 (epoch   37.42), loss: 0.004999, time: 2.240 s\n",
      "Step 119780 (epoch   37.43), loss: 0.009896, time: 2.236 s\n",
      "Step 119800 (epoch   37.44), loss: 0.007659, time: 2.251 s, error: 0.012778\n",
      "Step 119820 (epoch   37.44), loss: 0.004513, time: 15.094 s\n",
      "Step 119840 (epoch   37.45), loss: 0.005038, time: 2.254 s\n",
      "Step 119860 (epoch   37.46), loss: 0.008750, time: 2.247 s\n",
      "Step 119880 (epoch   37.46), loss: 0.005223, time: 2.240 s\n",
      "Step 119900 (epoch   37.47), loss: 0.006767, time: 2.248 s\n",
      "Step 119920 (epoch   37.48), loss: 0.005045, time: 2.235 s\n",
      "Step 119940 (epoch   37.48), loss: 0.005097, time: 2.251 s\n",
      "Step 119960 (epoch   37.49), loss: 0.004889, time: 2.255 s\n",
      "Step 119980 (epoch   37.49), loss: 0.004510, time: 2.256 s\n",
      "Step 120000 (epoch   37.50), loss: 0.004613, time: 2.245 s, error: 0.012709\n",
      "\n",
      "Time since beginning  : 20909.808 s\n",
      "\n",
      "Step 120020 (epoch   37.51), loss: 0.007360, time: 15.084 s\n",
      "Step 120040 (epoch   37.51), loss: 0.007444, time: 2.237 s\n",
      "Step 120060 (epoch   37.52), loss: 0.006899, time: 2.235 s\n",
      "Step 120080 (epoch   37.52), loss: 0.003998, time: 2.241 s\n",
      "Step 120100 (epoch   37.53), loss: 0.006340, time: 2.246 s\n",
      "Step 120120 (epoch   37.54), loss: 0.006310, time: 2.252 s\n",
      "Step 120140 (epoch   37.54), loss: 0.007488, time: 2.239 s\n",
      "Step 120160 (epoch   37.55), loss: 0.004934, time: 2.233 s\n",
      "Step 120180 (epoch   37.56), loss: 0.005808, time: 2.250 s\n",
      "Step 120200 (epoch   37.56), loss: 0.008023, time: 2.235 s, error: 0.012413\n",
      "Step 120220 (epoch   37.57), loss: 0.008713, time: 15.204 s\n",
      "Step 120240 (epoch   37.58), loss: 0.006042, time: 2.241 s\n",
      "Step 120260 (epoch   37.58), loss: 0.004207, time: 2.251 s\n",
      "Step 120280 (epoch   37.59), loss: 0.006633, time: 2.246 s\n",
      "Step 120300 (epoch   37.59), loss: 0.004455, time: 2.248 s\n",
      "Step 120320 (epoch   37.60), loss: 0.006740, time: 2.251 s\n",
      "Step 120340 (epoch   37.61), loss: 0.010376, time: 2.246 s\n",
      "Step 120360 (epoch   37.61), loss: 0.006372, time: 2.245 s\n",
      "Step 120380 (epoch   37.62), loss: 0.004453, time: 2.240 s\n",
      "Step 120400 (epoch   37.62), loss: 0.007069, time: 2.247 s, error: 0.012525\n",
      "Step 120420 (epoch   37.63), loss: 0.006557, time: 15.262 s\n",
      "Step 120440 (epoch   37.64), loss: 0.007360, time: 2.246 s\n",
      "Step 120460 (epoch   37.64), loss: 0.007852, time: 2.237 s\n",
      "Step 120480 (epoch   37.65), loss: 0.005978, time: 2.238 s\n",
      "Step 120500 (epoch   37.66), loss: 0.003962, time: 2.233 s\n",
      "Step 120520 (epoch   37.66), loss: 0.004925, time: 2.240 s\n",
      "Step 120540 (epoch   37.67), loss: 0.008730, time: 2.252 s\n",
      "Step 120560 (epoch   37.67), loss: 0.004537, time: 2.249 s\n",
      "Step 120580 (epoch   37.68), loss: 0.006518, time: 2.251 s\n",
      "Step 120600 (epoch   37.69), loss: 0.008946, time: 2.240 s, error: 0.012678\n",
      "Step 120620 (epoch   37.69), loss: 0.004527, time: 15.025 s\n",
      "Step 120640 (epoch   37.70), loss: 0.007722, time: 2.246 s\n",
      "Step 120660 (epoch   37.71), loss: 0.004861, time: 2.244 s\n",
      "Step 120680 (epoch   37.71), loss: 0.008000, time: 2.244 s\n",
      "Step 120700 (epoch   37.72), loss: 0.007731, time: 2.245 s\n",
      "Step 120720 (epoch   37.73), loss: 0.006478, time: 2.244 s\n",
      "Step 120740 (epoch   37.73), loss: 0.004555, time: 2.228 s\n",
      "Step 120760 (epoch   37.74), loss: 0.007311, time: 2.251 s\n",
      "Step 120780 (epoch   37.74), loss: 0.004465, time: 2.250 s\n",
      "Step 120800 (epoch   37.75), loss: 0.008125, time: 2.251 s, error: 0.012208\n",
      "Step 120820 (epoch   37.76), loss: 0.006947, time: 15.007 s\n",
      "Step 120840 (epoch   37.76), loss: 0.006007, time: 2.237 s\n",
      "Step 120860 (epoch   37.77), loss: 0.007836, time: 2.247 s\n",
      "Step 120880 (epoch   37.77), loss: 0.009351, time: 2.238 s\n",
      "Step 120900 (epoch   37.78), loss: 0.007109, time: 2.258 s\n",
      "Step 120920 (epoch   37.79), loss: 0.005881, time: 2.242 s\n",
      "Step 120940 (epoch   37.79), loss: 0.004665, time: 2.236 s\n",
      "Step 120960 (epoch   37.80), loss: 0.005055, time: 2.237 s\n",
      "Step 120980 (epoch   37.81), loss: 0.007365, time: 2.242 s\n",
      "Step 121000 (epoch   37.81), loss: 0.009131, time: 2.241 s, error: 0.012758\n",
      "Step 121020 (epoch   37.82), loss: 0.006534, time: 15.027 s\n",
      "Step 121040 (epoch   37.83), loss: 0.005437, time: 2.250 s\n",
      "Step 121060 (epoch   37.83), loss: 0.008777, time: 2.238 s\n",
      "Step 121080 (epoch   37.84), loss: 0.006799, time: 2.243 s\n",
      "Step 121100 (epoch   37.84), loss: 0.007045, time: 2.232 s\n",
      "Step 121120 (epoch   37.85), loss: 0.006169, time: 2.230 s\n",
      "Step 121140 (epoch   37.86), loss: 0.005063, time: 2.236 s\n",
      "Step 121160 (epoch   37.86), loss: 0.005620, time: 2.253 s\n",
      "Step 121180 (epoch   37.87), loss: 0.011758, time: 2.238 s\n",
      "Step 121200 (epoch   37.88), loss: 0.006567, time: 2.248 s, error: 0.011932\n",
      "Step 121220 (epoch   37.88), loss: 0.005643, time: 15.012 s\n",
      "Step 121240 (epoch   37.89), loss: 0.006733, time: 2.238 s\n",
      "Step 121260 (epoch   37.89), loss: 0.007012, time: 2.247 s\n",
      "Step 121280 (epoch   37.90), loss: 0.008173, time: 2.231 s\n",
      "Step 121300 (epoch   37.91), loss: 0.004314, time: 2.246 s\n",
      "Step 121320 (epoch   37.91), loss: 0.008533, time: 2.238 s\n",
      "Step 121340 (epoch   37.92), loss: 0.008578, time: 2.246 s\n",
      "Step 121360 (epoch   37.92), loss: 0.006022, time: 2.239 s\n",
      "Step 121380 (epoch   37.93), loss: 0.007004, time: 2.241 s\n",
      "Step 121400 (epoch   37.94), loss: 0.005323, time: 2.233 s, error: 0.012300\n",
      "Step 121420 (epoch   37.94), loss: 0.004935, time: 15.204 s\n",
      "Step 121440 (epoch   37.95), loss: 0.005104, time: 2.242 s\n",
      "Step 121460 (epoch   37.96), loss: 0.005027, time: 2.245 s\n",
      "Step 121480 (epoch   37.96), loss: 0.009813, time: 2.249 s\n",
      "Step 121500 (epoch   37.97), loss: 0.005133, time: 2.247 s\n",
      "Step 121520 (epoch   37.98), loss: 0.007231, time: 2.247 s\n",
      "Step 121540 (epoch   37.98), loss: 0.005902, time: 2.242 s\n",
      "Step 121560 (epoch   37.99), loss: 0.007896, time: 2.242 s\n",
      "Step 121580 (epoch   37.99), loss: 0.004879, time: 2.254 s\n",
      "Step 121600 (epoch   38.00), loss: 0.012767, time: 2.231 s, error: 0.012436\n",
      "Step 121620 (epoch   38.01), loss: 0.005918, time: 15.201 s\n",
      "Step 121640 (epoch   38.01), loss: 0.008923, time: 2.249 s\n",
      "Step 121660 (epoch   38.02), loss: 0.005449, time: 2.232 s\n",
      "Step 121680 (epoch   38.02), loss: 0.004071, time: 2.239 s\n",
      "Step 121700 (epoch   38.03), loss: 0.008307, time: 2.241 s\n",
      "Step 121720 (epoch   38.04), loss: 0.008458, time: 2.246 s\n",
      "Step 121740 (epoch   38.04), loss: 0.007087, time: 2.235 s\n",
      "Step 121760 (epoch   38.05), loss: 0.005984, time: 2.251 s\n",
      "Step 121780 (epoch   38.06), loss: 0.006471, time: 2.240 s\n",
      "Step 121800 (epoch   38.06), loss: 0.004228, time: 2.249 s, error: 0.012150\n",
      "Step 121820 (epoch   38.07), loss: 0.004810, time: 15.106 s\n",
      "Step 121840 (epoch   38.08), loss: 0.008193, time: 2.261 s\n",
      "Step 121860 (epoch   38.08), loss: 0.005424, time: 2.243 s\n",
      "Step 121880 (epoch   38.09), loss: 0.008193, time: 2.249 s\n",
      "Step 121900 (epoch   38.09), loss: 0.006604, time: 2.251 s\n",
      "Step 121920 (epoch   38.10), loss: 0.004461, time: 2.237 s\n",
      "Step 121940 (epoch   38.11), loss: 0.004474, time: 2.242 s\n",
      "Step 121960 (epoch   38.11), loss: 0.010428, time: 2.248 s\n",
      "Step 121980 (epoch   38.12), loss: 0.005938, time: 2.240 s\n",
      "Step 122000 (epoch   38.12), loss: 0.005944, time: 2.239 s, error: 0.012159\n",
      "\n",
      "Time since beginning  : 21262.821 s\n",
      "\n",
      "Step 122020 (epoch   38.13), loss: 0.006086, time: 15.090 s\n",
      "Step 122040 (epoch   38.14), loss: 0.004405, time: 2.243 s\n",
      "Step 122060 (epoch   38.14), loss: 0.004319, time: 2.234 s\n",
      "Step 122080 (epoch   38.15), loss: 0.005521, time: 2.244 s\n",
      "Step 122100 (epoch   38.16), loss: 0.005485, time: 2.258 s\n",
      "Step 122120 (epoch   38.16), loss: 0.006893, time: 2.250 s\n",
      "Step 122140 (epoch   38.17), loss: 0.004412, time: 2.255 s\n",
      "Step 122160 (epoch   38.17), loss: 0.005246, time: 2.241 s\n",
      "Step 122180 (epoch   38.18), loss: 0.005790, time: 2.245 s\n",
      "Step 122200 (epoch   38.19), loss: 0.005337, time: 2.247 s, error: 0.012344\n",
      "Step 122220 (epoch   38.19), loss: 0.006485, time: 15.060 s\n",
      "Step 122240 (epoch   38.20), loss: 0.008370, time: 2.237 s\n",
      "Step 122260 (epoch   38.21), loss: 0.005205, time: 2.223 s\n",
      "Step 122280 (epoch   38.21), loss: 0.004339, time: 2.247 s\n",
      "Step 122300 (epoch   38.22), loss: 0.007017, time: 2.231 s\n",
      "Step 122320 (epoch   38.23), loss: 0.004774, time: 2.247 s\n",
      "Step 122340 (epoch   38.23), loss: 0.007213, time: 2.244 s\n",
      "Step 122360 (epoch   38.24), loss: 0.005474, time: 2.258 s\n",
      "Step 122380 (epoch   38.24), loss: 0.004961, time: 2.251 s\n",
      "Step 122400 (epoch   38.25), loss: 0.006957, time: 2.248 s, error: 0.013204\n",
      "Step 122420 (epoch   38.26), loss: 0.005764, time: 15.005 s\n",
      "Step 122440 (epoch   38.26), loss: 0.004428, time: 2.238 s\n",
      "Step 122460 (epoch   38.27), loss: 0.007424, time: 2.244 s\n",
      "Step 122480 (epoch   38.27), loss: 0.004582, time: 2.241 s\n",
      "Step 122500 (epoch   38.28), loss: 0.008634, time: 2.248 s\n",
      "Step 122520 (epoch   38.29), loss: 0.006797, time: 2.236 s\n",
      "Step 122540 (epoch   38.29), loss: 0.007626, time: 2.243 s\n",
      "Step 122560 (epoch   38.30), loss: 0.005033, time: 2.251 s\n",
      "Step 122580 (epoch   38.31), loss: 0.005821, time: 2.235 s\n",
      "Step 122600 (epoch   38.31), loss: 0.006611, time: 2.248 s, error: 0.013008\n",
      "Step 122620 (epoch   38.32), loss: 0.004853, time: 15.257 s\n",
      "Step 122640 (epoch   38.33), loss: 0.005434, time: 2.252 s\n",
      "Step 122660 (epoch   38.33), loss: 0.007632, time: 2.236 s\n",
      "Step 122680 (epoch   38.34), loss: 0.005729, time: 2.245 s\n",
      "Step 122700 (epoch   38.34), loss: 0.007996, time: 2.234 s\n",
      "Step 122720 (epoch   38.35), loss: 0.006042, time: 2.248 s\n",
      "Step 122740 (epoch   38.36), loss: 0.005885, time: 2.240 s\n",
      "Step 122760 (epoch   38.36), loss: 0.006222, time: 2.246 s\n",
      "Step 122780 (epoch   38.37), loss: 0.003803, time: 2.241 s\n",
      "Step 122800 (epoch   38.38), loss: 0.007989, time: 2.232 s, error: 0.012464\n",
      "Step 122820 (epoch   38.38), loss: 0.011592, time: 15.214 s\n",
      "Step 122840 (epoch   38.39), loss: 0.007172, time: 2.246 s\n",
      "Step 122860 (epoch   38.39), loss: 0.009890, time: 2.241 s\n",
      "Step 122880 (epoch   38.40), loss: 0.006081, time: 2.246 s\n",
      "Step 122900 (epoch   38.41), loss: 0.007264, time: 2.256 s\n",
      "Step 122920 (epoch   38.41), loss: 0.004825, time: 2.251 s\n",
      "Step 122940 (epoch   38.42), loss: 0.006078, time: 2.238 s\n",
      "Step 122960 (epoch   38.42), loss: 0.009115, time: 2.238 s\n",
      "Step 122980 (epoch   38.43), loss: 0.005434, time: 2.245 s\n",
      "Step 123000 (epoch   38.44), loss: 0.007056, time: 2.244 s, error: 0.013613\n",
      "Step 123020 (epoch   38.44), loss: 0.008371, time: 15.045 s\n",
      "Step 123040 (epoch   38.45), loss: 0.005803, time: 2.256 s\n",
      "Step 123060 (epoch   38.46), loss: 0.005860, time: 2.244 s\n",
      "Step 123080 (epoch   38.46), loss: 0.006141, time: 2.245 s\n",
      "Step 123100 (epoch   38.47), loss: 0.005550, time: 2.240 s\n",
      "Step 123120 (epoch   38.48), loss: 0.009051, time: 2.245 s\n",
      "Step 123140 (epoch   38.48), loss: 0.005224, time: 2.245 s\n",
      "Step 123160 (epoch   38.49), loss: 0.019011, time: 2.246 s\n",
      "Step 123180 (epoch   38.49), loss: 0.004635, time: 2.243 s\n",
      "Step 123200 (epoch   38.50), loss: 0.005077, time: 2.242 s, error: 0.012400\n",
      "Step 123220 (epoch   38.51), loss: 0.009154, time: 15.064 s\n",
      "Step 123240 (epoch   38.51), loss: 0.016589, time: 2.240 s\n",
      "Step 123260 (epoch   38.52), loss: 0.006441, time: 2.240 s\n",
      "Step 123280 (epoch   38.52), loss: 0.005060, time: 2.258 s\n",
      "Step 123300 (epoch   38.53), loss: 0.008255, time: 2.255 s\n",
      "Step 123320 (epoch   38.54), loss: 0.007802, time: 2.250 s\n",
      "Step 123340 (epoch   38.54), loss: 0.016083, time: 2.235 s\n",
      "Step 123360 (epoch   38.55), loss: 0.009991, time: 2.245 s\n",
      "Step 123380 (epoch   38.56), loss: 0.005787, time: 2.241 s\n",
      "Step 123400 (epoch   38.56), loss: 0.006947, time: 2.230 s, error: 0.012362\n",
      "Step 123420 (epoch   38.57), loss: 0.006075, time: 15.030 s\n",
      "Step 123440 (epoch   38.58), loss: 0.007007, time: 2.243 s\n",
      "Step 123460 (epoch   38.58), loss: 0.005925, time: 2.241 s\n",
      "Step 123480 (epoch   38.59), loss: 0.006239, time: 2.249 s\n",
      "Step 123500 (epoch   38.59), loss: 0.009853, time: 2.238 s\n",
      "Step 123520 (epoch   38.60), loss: 0.004140, time: 2.228 s\n",
      "Step 123540 (epoch   38.61), loss: 0.005565, time: 2.255 s\n",
      "Step 123560 (epoch   38.61), loss: 0.008464, time: 2.253 s\n",
      "Step 123580 (epoch   38.62), loss: 0.006914, time: 2.240 s\n",
      "Step 123600 (epoch   38.62), loss: 0.004684, time: 2.229 s, error: 0.012378\n",
      "Step 123620 (epoch   38.63), loss: 0.005558, time: 15.194 s\n",
      "Step 123640 (epoch   38.64), loss: 0.004688, time: 2.244 s\n",
      "Step 123660 (epoch   38.64), loss: 0.007698, time: 2.244 s\n",
      "Step 123680 (epoch   38.65), loss: 0.006498, time: 2.242 s\n",
      "Step 123700 (epoch   38.66), loss: 0.004260, time: 2.232 s\n",
      "Step 123720 (epoch   38.66), loss: 0.004295, time: 2.244 s\n",
      "Step 123740 (epoch   38.67), loss: 0.006747, time: 2.232 s\n",
      "Step 123760 (epoch   38.67), loss: 0.007501, time: 2.234 s\n",
      "Step 123780 (epoch   38.68), loss: 0.007179, time: 2.234 s\n",
      "Step 123800 (epoch   38.69), loss: 0.006699, time: 2.255 s, error: 0.012408\n",
      "Step 123820 (epoch   38.69), loss: 0.007375, time: 15.136 s\n",
      "Step 123840 (epoch   38.70), loss: 0.005078, time: 2.243 s\n",
      "Step 123860 (epoch   38.71), loss: 0.006152, time: 2.239 s\n",
      "Step 123880 (epoch   38.71), loss: 0.004601, time: 2.233 s\n",
      "Step 123900 (epoch   38.72), loss: 0.006029, time: 2.250 s\n",
      "Step 123920 (epoch   38.73), loss: 0.005484, time: 2.249 s\n",
      "Step 123940 (epoch   38.73), loss: 0.006952, time: 2.241 s\n",
      "Step 123960 (epoch   38.74), loss: 0.007385, time: 2.228 s\n",
      "Step 123980 (epoch   38.74), loss: 0.006303, time: 2.252 s\n",
      "Step 124000 (epoch   38.75), loss: 0.005594, time: 2.241 s, error: 0.012162\n",
      "\n",
      "Time since beginning  : 21615.994 s\n",
      "\n",
      "Step 124020 (epoch   38.76), loss: 0.006410, time: 15.310 s\n",
      "Step 124040 (epoch   38.76), loss: 0.004984, time: 2.236 s\n",
      "Step 124060 (epoch   38.77), loss: 0.007064, time: 2.239 s\n",
      "Step 124080 (epoch   38.77), loss: 0.010147, time: 2.230 s\n",
      "Step 124100 (epoch   38.78), loss: 0.006711, time: 2.245 s\n",
      "Step 124120 (epoch   38.79), loss: 0.006823, time: 2.251 s\n",
      "Step 124140 (epoch   38.79), loss: 0.004727, time: 2.239 s\n",
      "Step 124160 (epoch   38.80), loss: 0.007208, time: 2.241 s\n",
      "Step 124180 (epoch   38.81), loss: 0.008628, time: 2.236 s\n",
      "Step 124200 (epoch   38.81), loss: 0.011031, time: 2.251 s, error: 0.012497\n",
      "Step 124220 (epoch   38.82), loss: 0.005250, time: 15.117 s\n",
      "Step 124240 (epoch   38.83), loss: 0.006212, time: 2.237 s\n",
      "Step 124260 (epoch   38.83), loss: 0.005258, time: 2.249 s\n",
      "Step 124280 (epoch   38.84), loss: 0.004336, time: 2.231 s\n",
      "Step 124300 (epoch   38.84), loss: 0.005289, time: 2.235 s\n",
      "Step 124320 (epoch   38.85), loss: 0.006255, time: 2.228 s\n",
      "Step 124340 (epoch   38.86), loss: 0.005727, time: 2.248 s\n",
      "Step 124360 (epoch   38.86), loss: 0.004323, time: 2.244 s\n",
      "Step 124380 (epoch   38.87), loss: 0.004915, time: 2.253 s\n",
      "Step 124400 (epoch   38.88), loss: 0.006813, time: 2.247 s, error: 0.011945\n",
      "Step 124420 (epoch   38.88), loss: 0.004715, time: 15.045 s\n",
      "Step 124440 (epoch   38.89), loss: 0.004986, time: 2.244 s\n",
      "Step 124460 (epoch   38.89), loss: 0.006412, time: 2.236 s\n",
      "Step 124480 (epoch   38.90), loss: 0.007160, time: 2.251 s\n",
      "Step 124500 (epoch   38.91), loss: 0.008367, time: 2.244 s\n",
      "Step 124520 (epoch   38.91), loss: 0.014921, time: 2.254 s\n",
      "Step 124540 (epoch   38.92), loss: 0.008377, time: 2.242 s\n",
      "Step 124560 (epoch   38.92), loss: 0.006917, time: 2.250 s\n",
      "Step 124580 (epoch   38.93), loss: 0.005340, time: 2.251 s\n",
      "Step 124600 (epoch   38.94), loss: 0.006607, time: 2.255 s, error: 0.012161\n",
      "Step 124620 (epoch   38.94), loss: 0.006284, time: 15.087 s\n",
      "Step 124640 (epoch   38.95), loss: 0.004807, time: 2.235 s\n",
      "Step 124660 (epoch   38.96), loss: 0.006945, time: 2.244 s\n",
      "Step 124680 (epoch   38.96), loss: 0.005183, time: 2.256 s\n",
      "Step 124700 (epoch   38.97), loss: 0.009708, time: 2.244 s\n",
      "Step 124720 (epoch   38.98), loss: 0.006241, time: 2.247 s\n",
      "Step 124740 (epoch   38.98), loss: 0.005103, time: 2.259 s\n",
      "Step 124760 (epoch   38.99), loss: 0.005277, time: 2.249 s\n",
      "Step 124780 (epoch   38.99), loss: 0.006829, time: 2.247 s\n",
      "Step 124800 (epoch   39.00), loss: 0.002991, time: 2.247 s, error: 0.012380\n",
      "Step 124820 (epoch   39.01), loss: 0.005857, time: 15.118 s\n",
      "Step 124840 (epoch   39.01), loss: 0.005728, time: 2.239 s\n",
      "Step 124860 (epoch   39.02), loss: 0.006488, time: 2.256 s\n",
      "Step 124880 (epoch   39.02), loss: 0.004695, time: 2.254 s\n",
      "Step 124900 (epoch   39.03), loss: 0.007075, time: 2.249 s\n",
      "Step 124920 (epoch   39.04), loss: 0.005170, time: 2.234 s\n",
      "Step 124940 (epoch   39.04), loss: 0.011650, time: 2.256 s\n",
      "Step 124960 (epoch   39.05), loss: 0.006807, time: 2.250 s\n",
      "Step 124980 (epoch   39.06), loss: 0.004210, time: 2.255 s\n",
      "Step 125000 (epoch   39.06), loss: 0.005907, time: 2.257 s, error: 0.012142\n",
      "Step 125020 (epoch   39.07), loss: 0.004803, time: 15.116 s\n",
      "Step 125040 (epoch   39.08), loss: 0.005440, time: 2.246 s\n",
      "Step 125060 (epoch   39.08), loss: 0.011768, time: 2.240 s\n",
      "Step 125080 (epoch   39.09), loss: 0.005361, time: 2.252 s\n",
      "Step 125100 (epoch   39.09), loss: 0.007906, time: 2.248 s\n",
      "Step 125120 (epoch   39.10), loss: 0.006467, time: 2.251 s\n",
      "Step 125140 (epoch   39.11), loss: 0.007061, time: 2.247 s\n",
      "Step 125160 (epoch   39.11), loss: 0.007287, time: 2.249 s\n",
      "Step 125180 (epoch   39.12), loss: 0.003487, time: 2.241 s\n",
      "Step 125200 (epoch   39.12), loss: 0.003671, time: 2.242 s, error: 0.012095\n",
      "Step 125220 (epoch   39.13), loss: 0.006379, time: 15.250 s\n",
      "Step 125240 (epoch   39.14), loss: 0.004515, time: 2.232 s\n",
      "Step 125260 (epoch   39.14), loss: 0.007514, time: 2.247 s\n",
      "Step 125280 (epoch   39.15), loss: 0.004799, time: 2.245 s\n",
      "Step 125300 (epoch   39.16), loss: 0.007290, time: 2.244 s\n",
      "Step 125320 (epoch   39.16), loss: 0.004537, time: 2.237 s\n",
      "Step 125340 (epoch   39.17), loss: 0.012528, time: 2.242 s\n",
      "Step 125360 (epoch   39.17), loss: 0.004686, time: 2.254 s\n",
      "Step 125380 (epoch   39.18), loss: 0.007780, time: 2.241 s\n",
      "Step 125400 (epoch   39.19), loss: 0.006128, time: 2.239 s, error: 0.012206\n",
      "Step 125420 (epoch   39.19), loss: 0.005792, time: 15.191 s\n",
      "Step 125440 (epoch   39.20), loss: 0.005472, time: 2.243 s\n",
      "Step 125460 (epoch   39.21), loss: 0.005543, time: 2.250 s\n",
      "Step 125480 (epoch   39.21), loss: 0.005865, time: 2.244 s\n",
      "Step 125500 (epoch   39.22), loss: 0.004398, time: 2.250 s\n",
      "Step 125520 (epoch   39.23), loss: 0.005201, time: 2.238 s\n",
      "Step 125540 (epoch   39.23), loss: 0.004985, time: 2.240 s\n",
      "Step 125560 (epoch   39.24), loss: 0.007154, time: 2.234 s\n",
      "Step 125580 (epoch   39.24), loss: 0.006242, time: 2.243 s\n",
      "Step 125600 (epoch   39.25), loss: 0.008199, time: 2.247 s, error: 0.012722\n",
      "Step 125620 (epoch   39.26), loss: 0.006673, time: 15.109 s\n",
      "Step 125640 (epoch   39.26), loss: 0.008999, time: 2.242 s\n",
      "Step 125660 (epoch   39.27), loss: 0.004982, time: 2.256 s\n",
      "Step 125680 (epoch   39.27), loss: 0.004768, time: 2.251 s\n",
      "Step 125700 (epoch   39.28), loss: 0.005618, time: 2.253 s\n",
      "Step 125720 (epoch   39.29), loss: 0.007411, time: 2.245 s\n",
      "Step 125740 (epoch   39.29), loss: 0.006395, time: 2.247 s\n",
      "Step 125760 (epoch   39.30), loss: 0.007078, time: 2.252 s\n",
      "Step 125780 (epoch   39.31), loss: 0.007713, time: 2.248 s\n",
      "Step 125800 (epoch   39.31), loss: 0.008311, time: 2.238 s, error: 0.013085\n",
      "Step 125820 (epoch   39.32), loss: 0.006084, time: 15.049 s\n",
      "Step 125840 (epoch   39.33), loss: 0.004502, time: 2.251 s\n",
      "Step 125860 (epoch   39.33), loss: 0.006244, time: 2.251 s\n",
      "Step 125880 (epoch   39.34), loss: 0.006611, time: 2.244 s\n",
      "Step 125900 (epoch   39.34), loss: 0.011647, time: 2.243 s\n",
      "Step 125920 (epoch   39.35), loss: 0.004690, time: 2.241 s\n",
      "Step 125940 (epoch   39.36), loss: 0.005896, time: 2.248 s\n",
      "Step 125960 (epoch   39.36), loss: 0.005729, time: 2.230 s\n",
      "Step 125980 (epoch   39.37), loss: 0.007178, time: 2.234 s\n",
      "Step 126000 (epoch   39.38), loss: 0.005238, time: 2.240 s, error: 0.012531\n",
      "\n",
      "Time since beginning  : 21969.269 s\n",
      "\n",
      "Step 126020 (epoch   39.38), loss: 0.004291, time: 15.141 s\n",
      "Step 126040 (epoch   39.39), loss: 0.004949, time: 2.246 s\n",
      "Step 126060 (epoch   39.39), loss: 0.008609, time: 2.230 s\n",
      "Step 126080 (epoch   39.40), loss: 0.007251, time: 2.243 s\n",
      "Step 126100 (epoch   39.41), loss: 0.009182, time: 2.246 s\n",
      "Step 126120 (epoch   39.41), loss: 0.006654, time: 2.248 s\n",
      "Step 126140 (epoch   39.42), loss: 0.007168, time: 2.236 s\n",
      "Step 126160 (epoch   39.42), loss: 0.009706, time: 2.247 s\n",
      "Step 126180 (epoch   39.43), loss: 0.007390, time: 2.250 s\n",
      "Step 126200 (epoch   39.44), loss: 0.007643, time: 2.260 s, error: 0.013271\n",
      "Step 126220 (epoch   39.44), loss: 0.005982, time: 15.125 s\n",
      "Step 126240 (epoch   39.45), loss: 0.003815, time: 2.229 s\n",
      "Step 126260 (epoch   39.46), loss: 0.016855, time: 2.253 s\n",
      "Step 126280 (epoch   39.46), loss: 0.005153, time: 2.234 s\n",
      "Step 126300 (epoch   39.47), loss: 0.004735, time: 2.248 s\n",
      "Step 126320 (epoch   39.48), loss: 0.003631, time: 2.242 s\n",
      "Step 126340 (epoch   39.48), loss: 0.004992, time: 2.231 s\n",
      "Step 126360 (epoch   39.49), loss: 0.005155, time: 2.245 s\n",
      "Step 126380 (epoch   39.49), loss: 0.006637, time: 2.252 s\n",
      "Step 126400 (epoch   39.50), loss: 0.010659, time: 2.243 s, error: 0.011969\n",
      "Step 126420 (epoch   39.51), loss: 0.009912, time: 15.213 s\n",
      "Step 126440 (epoch   39.51), loss: 0.005777, time: 2.241 s\n",
      "Step 126460 (epoch   39.52), loss: 0.004812, time: 2.235 s\n",
      "Step 126480 (epoch   39.52), loss: 0.006053, time: 2.240 s\n",
      "Step 126500 (epoch   39.53), loss: 0.004127, time: 2.252 s\n",
      "Step 126520 (epoch   39.54), loss: 0.003903, time: 2.241 s\n",
      "Step 126540 (epoch   39.54), loss: 0.005807, time: 2.249 s\n",
      "Step 126560 (epoch   39.55), loss: 0.017372, time: 2.243 s\n",
      "Step 126580 (epoch   39.56), loss: 0.005192, time: 2.245 s\n",
      "Step 126600 (epoch   39.56), loss: 0.004858, time: 2.248 s, error: 0.012086\n",
      "Step 126620 (epoch   39.57), loss: 0.006465, time: 15.261 s\n",
      "Step 126640 (epoch   39.58), loss: 0.004835, time: 2.255 s\n",
      "Step 126660 (epoch   39.58), loss: 0.004217, time: 2.233 s\n",
      "Step 126680 (epoch   39.59), loss: 0.006039, time: 2.255 s\n",
      "Step 126700 (epoch   39.59), loss: 0.005234, time: 2.241 s\n",
      "Step 126720 (epoch   39.60), loss: 0.009245, time: 2.248 s\n",
      "Step 126740 (epoch   39.61), loss: 0.006430, time: 2.243 s\n",
      "Step 126760 (epoch   39.61), loss: 0.005798, time: 2.245 s\n",
      "Step 126780 (epoch   39.62), loss: 0.010573, time: 2.237 s\n",
      "Step 126800 (epoch   39.62), loss: 0.005177, time: 2.240 s, error: 0.012409\n",
      "Step 126820 (epoch   39.63), loss: 0.006233, time: 15.164 s\n",
      "Step 126840 (epoch   39.64), loss: 0.006454, time: 2.252 s\n",
      "Step 126860 (epoch   39.64), loss: 0.005434, time: 2.262 s\n",
      "Step 126880 (epoch   39.65), loss: 0.005958, time: 2.238 s\n",
      "Step 126900 (epoch   39.66), loss: 0.007786, time: 2.250 s\n",
      "Step 126920 (epoch   39.66), loss: 0.006796, time: 2.249 s\n",
      "Step 126940 (epoch   39.67), loss: 0.004088, time: 2.245 s\n",
      "Step 126960 (epoch   39.67), loss: 0.006724, time: 2.239 s\n",
      "Step 126980 (epoch   39.68), loss: 0.005042, time: 2.243 s\n",
      "Step 127000 (epoch   39.69), loss: 0.006497, time: 2.250 s, error: 0.012248\n",
      "Step 127020 (epoch   39.69), loss: 0.004263, time: 15.063 s\n",
      "Step 127040 (epoch   39.70), loss: 0.008817, time: 2.242 s\n",
      "Step 127060 (epoch   39.71), loss: 0.004975, time: 2.248 s\n",
      "Step 127080 (epoch   39.71), loss: 0.006894, time: 2.241 s\n",
      "Step 127100 (epoch   39.72), loss: 0.004725, time: 2.255 s\n",
      "Step 127120 (epoch   39.73), loss: 0.004812, time: 2.263 s\n",
      "Step 127140 (epoch   39.73), loss: 0.004605, time: 2.245 s\n",
      "Step 127160 (epoch   39.74), loss: 0.005502, time: 2.248 s\n",
      "Step 127180 (epoch   39.74), loss: 0.005643, time: 2.255 s\n",
      "Step 127200 (epoch   39.75), loss: 0.004453, time: 2.250 s, error: 0.012214\n",
      "Step 127220 (epoch   39.76), loss: 0.006086, time: 15.104 s\n",
      "Step 127240 (epoch   39.76), loss: 0.008505, time: 2.246 s\n",
      "Step 127260 (epoch   39.77), loss: 0.005049, time: 2.239 s\n",
      "Step 127280 (epoch   39.77), loss: 0.006202, time: 2.252 s\n",
      "Step 127300 (epoch   39.78), loss: 0.007179, time: 2.233 s\n",
      "Step 127320 (epoch   39.79), loss: 0.006309, time: 2.240 s\n",
      "Step 127340 (epoch   39.79), loss: 0.007513, time: 2.245 s\n",
      "Step 127360 (epoch   39.80), loss: 0.006905, time: 2.255 s\n",
      "Step 127380 (epoch   39.81), loss: 0.005615, time: 2.257 s\n",
      "Step 127400 (epoch   39.81), loss: 0.004526, time: 2.247 s, error: 0.012367\n",
      "Step 127420 (epoch   39.82), loss: 0.004464, time: 15.020 s\n",
      "Step 127440 (epoch   39.83), loss: 0.004743, time: 2.251 s\n",
      "Step 127460 (epoch   39.83), loss: 0.006267, time: 2.243 s\n",
      "Step 127480 (epoch   39.84), loss: 0.003687, time: 2.253 s\n",
      "Step 127500 (epoch   39.84), loss: 0.005684, time: 2.247 s\n",
      "Step 127520 (epoch   39.85), loss: 0.007207, time: 2.253 s\n",
      "Step 127540 (epoch   39.86), loss: 0.006752, time: 2.250 s\n",
      "Step 127560 (epoch   39.86), loss: 0.004330, time: 2.250 s\n",
      "Step 127580 (epoch   39.87), loss: 0.005094, time: 2.238 s\n",
      "Step 127600 (epoch   39.88), loss: 0.004316, time: 2.254 s, error: 0.012447\n",
      "Step 127620 (epoch   39.88), loss: 0.006977, time: 15.259 s\n",
      "Step 127640 (epoch   39.89), loss: 0.004165, time: 2.248 s\n",
      "Step 127660 (epoch   39.89), loss: 0.004332, time: 2.250 s\n",
      "Step 127680 (epoch   39.90), loss: 0.015758, time: 2.240 s\n",
      "Step 127700 (epoch   39.91), loss: 0.004807, time: 2.258 s\n",
      "Step 127720 (epoch   39.91), loss: 0.007222, time: 2.245 s\n",
      "Step 127740 (epoch   39.92), loss: 0.007647, time: 2.257 s\n",
      "Step 127760 (epoch   39.92), loss: 0.004889, time: 2.258 s\n",
      "Step 127780 (epoch   39.93), loss: 0.006049, time: 2.255 s\n",
      "Step 127800 (epoch   39.94), loss: 0.005855, time: 2.257 s, error: 0.011986\n",
      "Step 127820 (epoch   39.94), loss: 0.007280, time: 15.390 s\n",
      "Step 127840 (epoch   39.95), loss: 0.005702, time: 2.253 s\n",
      "Step 127860 (epoch   39.96), loss: 0.010012, time: 2.240 s\n",
      "Step 127880 (epoch   39.96), loss: 0.005198, time: 2.233 s\n",
      "Step 127900 (epoch   39.97), loss: 0.004530, time: 2.233 s\n",
      "Step 127920 (epoch   39.98), loss: 0.004449, time: 2.242 s\n",
      "Step 127940 (epoch   39.98), loss: 0.009010, time: 2.248 s\n",
      "Step 127960 (epoch   39.99), loss: 0.013931, time: 2.245 s\n",
      "Step 127980 (epoch   39.99), loss: 0.006149, time: 2.243 s\n",
      "Step 128000 (epoch   40.00), loss: 0.004015, time: 2.239 s, error: 0.012252\n",
      "\n",
      "Time since beginning  : 22323.171 s\n",
      "\n",
      "Step 128020 (epoch   40.01), loss: 0.005793, time: 15.137 s\n",
      "Step 128040 (epoch   40.01), loss: 0.005569, time: 2.258 s\n",
      "Step 128060 (epoch   40.02), loss: 0.004247, time: 2.246 s\n",
      "Step 128080 (epoch   40.02), loss: 0.006327, time: 2.232 s\n",
      "Step 128100 (epoch   40.03), loss: 0.010725, time: 2.234 s\n",
      "Step 128120 (epoch   40.04), loss: 0.008696, time: 2.242 s\n",
      "Step 128140 (epoch   40.04), loss: 0.004855, time: 2.246 s\n",
      "Step 128160 (epoch   40.05), loss: 0.007034, time: 2.248 s\n",
      "Step 128180 (epoch   40.06), loss: 0.005695, time: 2.239 s\n",
      "Step 128200 (epoch   40.06), loss: 0.006520, time: 2.239 s, error: 0.011903\n",
      "Step 128220 (epoch   40.07), loss: 0.007244, time: 15.028 s\n",
      "Step 128240 (epoch   40.08), loss: 0.005844, time: 2.228 s\n",
      "Step 128260 (epoch   40.08), loss: 0.006958, time: 2.234 s\n",
      "Step 128280 (epoch   40.09), loss: 0.005912, time: 2.234 s\n",
      "Step 128300 (epoch   40.09), loss: 0.006049, time: 2.254 s\n",
      "Step 128320 (epoch   40.10), loss: 0.005396, time: 2.247 s\n",
      "Step 128340 (epoch   40.11), loss: 0.004160, time: 2.243 s\n",
      "Step 128360 (epoch   40.11), loss: 0.005384, time: 2.252 s\n",
      "Step 128380 (epoch   40.12), loss: 0.005623, time: 2.241 s\n",
      "Step 128400 (epoch   40.12), loss: 0.004059, time: 2.237 s, error: 0.012082\n",
      "Step 128420 (epoch   40.13), loss: 0.006545, time: 15.020 s\n",
      "Step 128440 (epoch   40.14), loss: 0.005860, time: 2.237 s\n",
      "Step 128460 (epoch   40.14), loss: 0.007313, time: 2.231 s\n",
      "Step 128480 (epoch   40.15), loss: 0.006301, time: 2.236 s\n",
      "Step 128500 (epoch   40.16), loss: 0.008146, time: 2.237 s\n",
      "Step 128520 (epoch   40.16), loss: 0.005803, time: 2.243 s\n",
      "Step 128540 (epoch   40.17), loss: 0.005425, time: 2.239 s\n",
      "Step 128560 (epoch   40.17), loss: 0.006796, time: 2.258 s\n",
      "Step 128580 (epoch   40.18), loss: 0.005051, time: 2.254 s\n",
      "Step 128600 (epoch   40.19), loss: 0.004763, time: 2.248 s, error: 0.012154\n",
      "Step 128620 (epoch   40.19), loss: 0.005894, time: 15.038 s\n",
      "Step 128640 (epoch   40.20), loss: 0.009101, time: 2.248 s\n",
      "Step 128660 (epoch   40.21), loss: 0.003604, time: 2.238 s\n",
      "Step 128680 (epoch   40.21), loss: 0.005520, time: 2.238 s\n",
      "Step 128700 (epoch   40.22), loss: 0.007669, time: 2.248 s\n",
      "Step 128720 (epoch   40.23), loss: 0.011307, time: 2.237 s\n",
      "Step 128740 (epoch   40.23), loss: 0.006609, time: 2.242 s\n",
      "Step 128760 (epoch   40.24), loss: 0.007205, time: 2.225 s\n",
      "Step 128780 (epoch   40.24), loss: 0.006212, time: 2.242 s\n",
      "Step 128800 (epoch   40.25), loss: 0.005334, time: 2.237 s, error: 0.012215\n",
      "Step 128820 (epoch   40.26), loss: 0.006705, time: 15.187 s\n",
      "Step 128840 (epoch   40.26), loss: 0.003984, time: 2.240 s\n",
      "Step 128860 (epoch   40.27), loss: 0.006413, time: 2.245 s\n",
      "Step 128880 (epoch   40.27), loss: 0.005235, time: 2.231 s\n",
      "Step 128900 (epoch   40.28), loss: 0.007122, time: 2.239 s\n",
      "Step 128920 (epoch   40.29), loss: 0.005830, time: 2.239 s\n",
      "Step 128940 (epoch   40.29), loss: 0.006248, time: 2.241 s\n",
      "Step 128960 (epoch   40.30), loss: 0.007505, time: 2.238 s\n",
      "Step 128980 (epoch   40.31), loss: 0.005694, time: 2.230 s\n",
      "Step 129000 (epoch   40.31), loss: 0.007259, time: 2.241 s, error: 0.012750\n",
      "Step 129020 (epoch   40.32), loss: 0.009063, time: 15.181 s\n",
      "Step 129040 (epoch   40.33), loss: 0.004790, time: 2.235 s\n",
      "Step 129060 (epoch   40.33), loss: 0.009773, time: 2.253 s\n",
      "Step 129080 (epoch   40.34), loss: 0.009822, time: 2.244 s\n",
      "Step 129100 (epoch   40.34), loss: 0.006332, time: 2.232 s\n",
      "Step 129120 (epoch   40.35), loss: 0.007167, time: 2.236 s\n",
      "Step 129140 (epoch   40.36), loss: 0.007711, time: 2.242 s\n",
      "Step 129160 (epoch   40.36), loss: 0.005150, time: 2.243 s\n",
      "Step 129180 (epoch   40.37), loss: 0.008322, time: 2.252 s\n",
      "Step 129200 (epoch   40.38), loss: 0.009582, time: 2.237 s, error: 0.012305\n",
      "Step 129220 (epoch   40.38), loss: 0.008201, time: 15.022 s\n",
      "Step 129240 (epoch   40.39), loss: 0.005983, time: 2.259 s\n",
      "Step 129260 (epoch   40.39), loss: 0.009653, time: 2.240 s\n",
      "Step 129280 (epoch   40.40), loss: 0.004559, time: 2.247 s\n",
      "Step 129300 (epoch   40.41), loss: 0.005988, time: 2.246 s\n",
      "Step 129320 (epoch   40.41), loss: 0.005841, time: 2.238 s\n",
      "Step 129340 (epoch   40.42), loss: 0.005613, time: 2.234 s\n",
      "Step 129360 (epoch   40.42), loss: 0.005222, time: 2.242 s\n",
      "Step 129380 (epoch   40.43), loss: 0.004647, time: 2.236 s\n",
      "Step 129400 (epoch   40.44), loss: 0.005811, time: 2.247 s, error: 0.012470\n",
      "Step 129420 (epoch   40.44), loss: 0.006878, time: 14.997 s\n",
      "Step 129440 (epoch   40.45), loss: 0.007839, time: 2.241 s\n",
      "Step 129460 (epoch   40.46), loss: 0.003404, time: 2.245 s\n",
      "Step 129480 (epoch   40.46), loss: 0.009728, time: 2.236 s\n",
      "Step 129500 (epoch   40.47), loss: 0.004873, time: 2.252 s\n",
      "Step 129520 (epoch   40.48), loss: 0.003459, time: 2.243 s\n",
      "Step 129540 (epoch   40.48), loss: 0.005649, time: 2.247 s\n",
      "Step 129560 (epoch   40.49), loss: 0.007662, time: 2.243 s\n",
      "Step 129580 (epoch   40.49), loss: 0.004548, time: 2.235 s\n",
      "Step 129600 (epoch   40.50), loss: 0.006409, time: 2.235 s, error: 0.011955\n",
      "Step 129620 (epoch   40.51), loss: 0.006792, time: 15.093 s\n",
      "Step 129640 (epoch   40.51), loss: 0.006367, time: 2.247 s\n",
      "Step 129660 (epoch   40.52), loss: 0.006094, time: 2.235 s\n",
      "Step 129680 (epoch   40.52), loss: 0.008663, time: 2.247 s\n",
      "Step 129700 (epoch   40.53), loss: 0.006514, time: 2.247 s\n",
      "Step 129720 (epoch   40.54), loss: 0.007662, time: 2.246 s\n",
      "Step 129740 (epoch   40.54), loss: 0.008205, time: 2.244 s\n",
      "Step 129760 (epoch   40.55), loss: 0.004890, time: 2.255 s\n",
      "Step 129780 (epoch   40.56), loss: 0.005852, time: 2.230 s\n",
      "Step 129800 (epoch   40.56), loss: 0.006159, time: 2.241 s, error: 0.012005\n",
      "Step 129820 (epoch   40.57), loss: 0.004555, time: 15.065 s\n",
      "Step 129840 (epoch   40.58), loss: 0.005961, time: 2.244 s\n",
      "Step 129860 (epoch   40.58), loss: 0.004342, time: 2.245 s\n",
      "Step 129880 (epoch   40.59), loss: 0.007789, time: 2.249 s\n",
      "Step 129900 (epoch   40.59), loss: 0.007315, time: 2.255 s\n",
      "Step 129920 (epoch   40.60), loss: 0.005460, time: 2.254 s\n",
      "Step 129940 (epoch   40.61), loss: 0.004970, time: 2.238 s\n",
      "Step 129960 (epoch   40.61), loss: 0.005498, time: 2.249 s\n",
      "Step 129980 (epoch   40.62), loss: 0.005939, time: 2.240 s\n",
      "Step 130000 (epoch   40.62), loss: 0.005995, time: 2.246 s, error: 0.012353\n",
      "\n",
      "Time since beginning  : 22675.839 s\n",
      "\n",
      "Step 130020 (epoch   40.63), loss: 0.007905, time: 15.274 s\n",
      "Step 130040 (epoch   40.64), loss: 0.005330, time: 2.244 s\n",
      "Step 130060 (epoch   40.64), loss: 0.006812, time: 2.245 s\n",
      "Step 130080 (epoch   40.65), loss: 0.005552, time: 2.245 s\n",
      "Step 130100 (epoch   40.66), loss: 0.005673, time: 2.241 s\n",
      "Step 130120 (epoch   40.66), loss: 0.009711, time: 2.240 s\n",
      "Step 130140 (epoch   40.67), loss: 0.009062, time: 2.243 s\n",
      "Step 130160 (epoch   40.67), loss: 0.009638, time: 2.231 s\n",
      "Step 130180 (epoch   40.68), loss: 0.008345, time: 2.250 s\n",
      "Step 130200 (epoch   40.69), loss: 0.006198, time: 2.238 s, error: 0.012236\n",
      "Step 130220 (epoch   40.69), loss: 0.008596, time: 15.266 s\n",
      "Step 130240 (epoch   40.70), loss: 0.004865, time: 2.241 s\n",
      "Step 130260 (epoch   40.71), loss: 0.005363, time: 2.236 s\n",
      "Step 130280 (epoch   40.71), loss: 0.004636, time: 2.246 s\n",
      "Step 130300 (epoch   40.72), loss: 0.022742, time: 2.235 s\n",
      "Step 130320 (epoch   40.73), loss: 0.007446, time: 2.231 s\n",
      "Step 130340 (epoch   40.73), loss: 0.005800, time: 2.230 s\n",
      "Step 130360 (epoch   40.74), loss: 0.006115, time: 2.245 s\n",
      "Step 130380 (epoch   40.74), loss: 0.007127, time: 2.244 s\n",
      "Step 130400 (epoch   40.75), loss: 0.005604, time: 2.240 s, error: 0.012029\n",
      "Step 130420 (epoch   40.76), loss: 0.010275, time: 15.183 s\n",
      "Step 130440 (epoch   40.76), loss: 0.005584, time: 2.239 s\n",
      "Step 130460 (epoch   40.77), loss: 0.005852, time: 2.236 s\n",
      "Step 130480 (epoch   40.77), loss: 0.005809, time: 2.239 s\n",
      "Step 130500 (epoch   40.78), loss: 0.005771, time: 2.251 s\n",
      "Step 130520 (epoch   40.79), loss: 0.008965, time: 2.256 s\n",
      "Step 130540 (epoch   40.79), loss: 0.006777, time: 2.245 s\n",
      "Step 130560 (epoch   40.80), loss: 0.007663, time: 2.252 s\n",
      "Step 130580 (epoch   40.81), loss: 0.006598, time: 2.240 s\n",
      "Step 130600 (epoch   40.81), loss: 0.004896, time: 2.243 s, error: 0.012191\n",
      "Step 130620 (epoch   40.82), loss: 0.006359, time: 15.188 s\n",
      "Step 130640 (epoch   40.83), loss: 0.005096, time: 2.254 s\n",
      "Step 130660 (epoch   40.83), loss: 0.006640, time: 2.258 s\n",
      "Step 130680 (epoch   40.84), loss: 0.008467, time: 2.252 s\n",
      "Step 130700 (epoch   40.84), loss: 0.004797, time: 2.228 s\n",
      "Step 130720 (epoch   40.85), loss: 0.006689, time: 2.258 s\n",
      "Step 130740 (epoch   40.86), loss: 0.004655, time: 2.233 s\n",
      "Step 130760 (epoch   40.86), loss: 0.003842, time: 2.255 s\n",
      "Step 130780 (epoch   40.87), loss: 0.004475, time: 2.249 s\n",
      "Step 130800 (epoch   40.88), loss: 0.007878, time: 2.253 s, error: 0.012700\n",
      "Step 130820 (epoch   40.88), loss: 0.004200, time: 15.100 s\n",
      "Step 130840 (epoch   40.89), loss: 0.005896, time: 2.246 s\n",
      "Step 130860 (epoch   40.89), loss: 0.005649, time: 2.245 s\n",
      "Step 130880 (epoch   40.90), loss: 0.005178, time: 2.240 s\n",
      "Step 130900 (epoch   40.91), loss: 0.009762, time: 2.250 s\n",
      "Step 130920 (epoch   40.91), loss: 0.009702, time: 2.242 s\n",
      "Step 130940 (epoch   40.92), loss: 0.007149, time: 2.254 s\n",
      "Step 130960 (epoch   40.92), loss: 0.009398, time: 2.245 s\n",
      "Step 130980 (epoch   40.93), loss: 0.003742, time: 2.250 s\n",
      "Step 131000 (epoch   40.94), loss: 0.005379, time: 2.245 s, error: 0.011968\n",
      "Step 131020 (epoch   40.94), loss: 0.005028, time: 15.106 s\n",
      "Step 131040 (epoch   40.95), loss: 0.006184, time: 2.254 s\n",
      "Step 131060 (epoch   40.96), loss: 0.004483, time: 2.249 s\n",
      "Step 131080 (epoch   40.96), loss: 0.004938, time: 2.251 s\n",
      "Step 131100 (epoch   40.97), loss: 0.005774, time: 2.245 s\n",
      "Step 131120 (epoch   40.98), loss: 0.009923, time: 2.229 s\n",
      "Step 131140 (epoch   40.98), loss: 0.008339, time: 2.231 s\n",
      "Step 131160 (epoch   40.99), loss: 0.005344, time: 2.240 s\n",
      "Step 131180 (epoch   40.99), loss: 0.006592, time: 2.246 s\n",
      "Step 131200 (epoch   41.00), loss: 0.004597, time: 2.249 s, error: 0.012084\n",
      "Step 131220 (epoch   41.01), loss: 0.006703, time: 15.217 s\n",
      "Step 131240 (epoch   41.01), loss: 0.004660, time: 2.247 s\n",
      "Step 131260 (epoch   41.02), loss: 0.008048, time: 2.227 s\n",
      "Step 131280 (epoch   41.02), loss: 0.008607, time: 2.235 s\n",
      "Step 131300 (epoch   41.03), loss: 0.005355, time: 2.244 s\n",
      "Step 131320 (epoch   41.04), loss: 0.005441, time: 2.244 s\n",
      "Step 131340 (epoch   41.04), loss: 0.005619, time: 2.239 s\n",
      "Step 131360 (epoch   41.05), loss: 0.005620, time: 2.253 s\n",
      "Step 131380 (epoch   41.06), loss: 0.009486, time: 2.230 s\n",
      "Step 131400 (epoch   41.06), loss: 0.005780, time: 2.250 s, error: 0.011771\n",
      "Step 131420 (epoch   41.07), loss: 0.004191, time: 15.365 s\n",
      "Step 131440 (epoch   41.08), loss: 0.007233, time: 2.245 s\n",
      "Step 131460 (epoch   41.08), loss: 0.004846, time: 2.251 s\n",
      "Step 131480 (epoch   41.09), loss: 0.004736, time: 2.244 s\n",
      "Step 131500 (epoch   41.09), loss: 0.005266, time: 2.249 s\n",
      "Step 131520 (epoch   41.10), loss: 0.004923, time: 2.239 s\n",
      "Step 131540 (epoch   41.11), loss: 0.004766, time: 2.250 s\n",
      "Step 131560 (epoch   41.11), loss: 0.005829, time: 2.233 s\n",
      "Step 131580 (epoch   41.12), loss: 0.006704, time: 2.251 s\n",
      "Step 131600 (epoch   41.12), loss: 0.012745, time: 2.235 s, error: 0.012111\n",
      "Step 131620 (epoch   41.13), loss: 0.006289, time: 15.319 s\n",
      "Step 131640 (epoch   41.14), loss: 0.003720, time: 2.246 s\n",
      "Step 131660 (epoch   41.14), loss: 0.010128, time: 2.248 s\n",
      "Step 131680 (epoch   41.15), loss: 0.004259, time: 2.256 s\n",
      "Step 131700 (epoch   41.16), loss: 0.006209, time: 2.252 s\n",
      "Step 131720 (epoch   41.16), loss: 0.006022, time: 2.244 s\n",
      "Step 131740 (epoch   41.17), loss: 0.005855, time: 2.239 s\n",
      "Step 131760 (epoch   41.17), loss: 0.007522, time: 2.253 s\n",
      "Step 131780 (epoch   41.18), loss: 0.004041, time: 2.238 s\n",
      "Step 131800 (epoch   41.19), loss: 0.005144, time: 2.237 s, error: 0.012006\n",
      "Step 131820 (epoch   41.19), loss: 0.005163, time: 15.136 s\n",
      "Step 131840 (epoch   41.20), loss: 0.004648, time: 2.239 s\n",
      "Step 131860 (epoch   41.21), loss: 0.006034, time: 2.262 s\n",
      "Step 131880 (epoch   41.21), loss: 0.003481, time: 2.251 s\n",
      "Step 131900 (epoch   41.22), loss: 0.014327, time: 2.248 s\n",
      "Step 131920 (epoch   41.23), loss: 0.005549, time: 2.244 s\n",
      "Step 131940 (epoch   41.23), loss: 0.009257, time: 2.244 s\n",
      "Step 131960 (epoch   41.24), loss: 0.004294, time: 2.238 s\n",
      "Step 131980 (epoch   41.24), loss: 0.005741, time: 2.245 s\n",
      "Step 132000 (epoch   41.25), loss: 0.007355, time: 2.253 s, error: 0.012068\n",
      "\n",
      "Time since beginning  : 23029.958 s\n",
      "\n",
      "Step 132020 (epoch   41.26), loss: 0.006883, time: 15.235 s\n",
      "Step 132040 (epoch   41.26), loss: 0.005442, time: 2.248 s\n",
      "Step 132060 (epoch   41.27), loss: 0.003901, time: 2.246 s\n",
      "Step 132080 (epoch   41.27), loss: 0.015380, time: 2.241 s\n",
      "Step 132100 (epoch   41.28), loss: 0.007743, time: 2.253 s\n",
      "Step 132120 (epoch   41.29), loss: 0.006736, time: 2.259 s\n",
      "Step 132140 (epoch   41.29), loss: 0.003645, time: 2.254 s\n",
      "Step 132160 (epoch   41.30), loss: 0.008530, time: 2.255 s\n",
      "Step 132180 (epoch   41.31), loss: 0.011067, time: 2.249 s\n",
      "Step 132200 (epoch   41.31), loss: 0.007187, time: 2.257 s, error: 0.012218\n",
      "Step 132220 (epoch   41.32), loss: 0.010775, time: 15.143 s\n",
      "Step 132240 (epoch   41.33), loss: 0.008365, time: 2.244 s\n",
      "Step 132260 (epoch   41.33), loss: 0.008002, time: 2.258 s\n",
      "Step 132280 (epoch   41.34), loss: 0.006164, time: 2.242 s\n",
      "Step 132300 (epoch   41.34), loss: 0.006188, time: 2.246 s\n",
      "Step 132320 (epoch   41.35), loss: 0.006578, time: 2.234 s\n",
      "Step 132340 (epoch   41.36), loss: 0.004731, time: 2.254 s\n",
      "Step 132360 (epoch   41.36), loss: 0.004356, time: 2.252 s\n",
      "Step 132380 (epoch   41.37), loss: 0.004809, time: 2.262 s\n",
      "Step 132400 (epoch   41.38), loss: 0.005563, time: 2.245 s, error: 0.012135\n",
      "Step 132420 (epoch   41.38), loss: 0.004287, time: 15.173 s\n",
      "Step 132440 (epoch   41.39), loss: 0.004765, time: 2.233 s\n",
      "Step 132460 (epoch   41.39), loss: 0.007005, time: 2.244 s\n",
      "Step 132480 (epoch   41.40), loss: 0.007662, time: 2.243 s\n",
      "Step 132500 (epoch   41.41), loss: 0.003864, time: 2.254 s\n",
      "Step 132520 (epoch   41.41), loss: 0.005699, time: 2.252 s\n",
      "Step 132540 (epoch   41.42), loss: 0.006399, time: 2.247 s\n",
      "Step 132560 (epoch   41.42), loss: 0.005386, time: 2.240 s\n",
      "Step 132580 (epoch   41.43), loss: 0.006503, time: 2.246 s\n",
      "Step 132600 (epoch   41.44), loss: 0.005297, time: 2.234 s, error: 0.012047\n",
      "Step 132620 (epoch   41.44), loss: 0.007247, time: 15.248 s\n",
      "Step 132640 (epoch   41.45), loss: 0.004387, time: 2.253 s\n",
      "Step 132660 (epoch   41.46), loss: 0.007567, time: 2.246 s\n",
      "Step 132680 (epoch   41.46), loss: 0.004204, time: 2.250 s\n",
      "Step 132700 (epoch   41.47), loss: 0.007952, time: 2.246 s\n",
      "Step 132720 (epoch   41.48), loss: 0.015957, time: 2.241 s\n",
      "Step 132740 (epoch   41.48), loss: 0.005060, time: 2.252 s\n",
      "Step 132760 (epoch   41.49), loss: 0.004802, time: 2.245 s\n",
      "Step 132780 (epoch   41.49), loss: 0.003802, time: 2.247 s\n",
      "Step 132800 (epoch   41.50), loss: 0.006124, time: 2.246 s, error: 0.012066\n",
      "Step 132820 (epoch   41.51), loss: 0.006749, time: 15.249 s\n",
      "Step 132840 (epoch   41.51), loss: 0.005470, time: 2.235 s\n",
      "Step 132860 (epoch   41.52), loss: 0.004472, time: 2.250 s\n",
      "Step 132880 (epoch   41.52), loss: 0.016100, time: 2.234 s\n",
      "Step 132900 (epoch   41.53), loss: 0.008098, time: 2.251 s\n",
      "Step 132920 (epoch   41.54), loss: 0.004749, time: 2.249 s\n",
      "Step 132940 (epoch   41.54), loss: 0.010759, time: 2.245 s\n",
      "Step 132960 (epoch   41.55), loss: 0.004575, time: 2.245 s\n",
      "Step 132980 (epoch   41.56), loss: 0.005997, time: 2.246 s\n",
      "Step 133000 (epoch   41.56), loss: 0.006476, time: 2.255 s, error: 0.011947\n",
      "Step 133020 (epoch   41.57), loss: 0.003914, time: 15.114 s\n",
      "Step 133040 (epoch   41.58), loss: 0.003675, time: 2.261 s\n",
      "Step 133060 (epoch   41.58), loss: 0.003771, time: 2.243 s\n",
      "Step 133080 (epoch   41.59), loss: 0.006074, time: 2.231 s\n",
      "Step 133100 (epoch   41.59), loss: 0.005540, time: 2.237 s\n",
      "Step 133120 (epoch   41.60), loss: 0.006046, time: 2.235 s\n",
      "Step 133140 (epoch   41.61), loss: 0.010665, time: 2.234 s\n",
      "Step 133160 (epoch   41.61), loss: 0.004326, time: 2.237 s\n",
      "Step 133180 (epoch   41.62), loss: 0.004032, time: 2.253 s\n",
      "Step 133200 (epoch   41.62), loss: 0.005340, time: 2.251 s, error: 0.012437\n",
      "Step 133220 (epoch   41.63), loss: 0.005843, time: 15.105 s\n",
      "Step 133240 (epoch   41.64), loss: 0.006303, time: 2.244 s\n",
      "Step 133260 (epoch   41.64), loss: 0.006963, time: 2.249 s\n",
      "Step 133280 (epoch   41.65), loss: 0.007926, time: 2.230 s\n",
      "Step 133300 (epoch   41.66), loss: 0.009464, time: 2.259 s\n",
      "Step 133320 (epoch   41.66), loss: 0.006006, time: 2.237 s\n",
      "Step 133340 (epoch   41.67), loss: 0.006177, time: 2.253 s\n",
      "Step 133360 (epoch   41.67), loss: 0.005543, time: 2.257 s\n",
      "Step 133380 (epoch   41.68), loss: 0.005061, time: 2.247 s\n",
      "Step 133400 (epoch   41.69), loss: 0.006172, time: 2.244 s, error: 0.012309\n",
      "Step 133420 (epoch   41.69), loss: 0.003854, time: 15.095 s\n",
      "Step 133440 (epoch   41.70), loss: 0.005271, time: 2.243 s\n",
      "Step 133460 (epoch   41.71), loss: 0.003735, time: 2.254 s\n",
      "Step 133480 (epoch   41.71), loss: 0.005760, time: 2.253 s\n",
      "Step 133500 (epoch   41.72), loss: 0.005507, time: 2.246 s\n",
      "Step 133520 (epoch   41.73), loss: 0.007469, time: 2.233 s\n",
      "Step 133540 (epoch   41.73), loss: 0.003783, time: 2.250 s\n",
      "Step 133560 (epoch   41.74), loss: 0.005978, time: 2.259 s\n",
      "Step 133580 (epoch   41.74), loss: 0.007179, time: 2.249 s\n",
      "Step 133600 (epoch   41.75), loss: 0.005583, time: 2.236 s, error: 0.012053\n",
      "Step 133620 (epoch   41.76), loss: 0.005270, time: 15.057 s\n",
      "Step 133640 (epoch   41.76), loss: 0.005688, time: 2.254 s\n",
      "Step 133660 (epoch   41.77), loss: 0.004368, time: 2.233 s\n",
      "Step 133680 (epoch   41.77), loss: 0.004501, time: 2.246 s\n",
      "Step 133700 (epoch   41.78), loss: 0.005222, time: 2.241 s\n",
      "Step 133720 (epoch   41.79), loss: 0.008048, time: 2.254 s\n",
      "Step 133740 (epoch   41.79), loss: 0.004639, time: 2.242 s\n",
      "Step 133760 (epoch   41.80), loss: 0.009862, time: 2.255 s\n",
      "Step 133780 (epoch   41.81), loss: 0.006404, time: 2.243 s\n",
      "Step 133800 (epoch   41.81), loss: 0.004916, time: 2.252 s, error: 0.012045\n",
      "Step 133820 (epoch   41.82), loss: 0.005446, time: 15.284 s\n",
      "Step 133840 (epoch   41.83), loss: 0.004997, time: 2.246 s\n",
      "Step 133860 (epoch   41.83), loss: 0.007847, time: 2.247 s\n",
      "Step 133880 (epoch   41.84), loss: 0.003574, time: 2.245 s\n",
      "Step 133900 (epoch   41.84), loss: 0.004826, time: 2.245 s\n",
      "Step 133920 (epoch   41.85), loss: 0.006180, time: 2.235 s\n",
      "Step 133940 (epoch   41.86), loss: 0.006401, time: 2.253 s\n",
      "Step 133960 (epoch   41.86), loss: 0.006377, time: 2.235 s\n",
      "Step 133980 (epoch   41.87), loss: 0.005805, time: 2.250 s\n",
      "Step 134000 (epoch   41.88), loss: 0.006871, time: 2.253 s, error: 0.012232\n",
      "\n",
      "Time since beginning  : 23383.959 s\n",
      "\n",
      "Step 134020 (epoch   41.88), loss: 0.006276, time: 15.351 s\n",
      "Step 134040 (epoch   41.89), loss: 0.005850, time: 2.241 s\n",
      "Step 134060 (epoch   41.89), loss: 0.005035, time: 2.241 s\n",
      "Step 134080 (epoch   41.90), loss: 0.004195, time: 2.244 s\n",
      "Step 134100 (epoch   41.91), loss: 0.007184, time: 2.258 s\n",
      "Step 134120 (epoch   41.91), loss: 0.008475, time: 2.255 s\n",
      "Step 134140 (epoch   41.92), loss: 0.004663, time: 2.256 s\n",
      "Step 134160 (epoch   41.92), loss: 0.007896, time: 2.242 s\n",
      "Step 134180 (epoch   41.93), loss: 0.004441, time: 2.232 s\n",
      "Step 134200 (epoch   41.94), loss: 0.003998, time: 2.242 s, error: 0.011902\n",
      "Step 134220 (epoch   41.94), loss: 0.005285, time: 15.035 s\n",
      "Step 134240 (epoch   41.95), loss: 0.004984, time: 2.247 s\n",
      "Step 134260 (epoch   41.96), loss: 0.006054, time: 2.244 s\n",
      "Step 134280 (epoch   41.96), loss: 0.004323, time: 2.238 s\n",
      "Step 134300 (epoch   41.97), loss: 0.005871, time: 2.239 s\n",
      "Step 134320 (epoch   41.98), loss: 0.004195, time: 2.245 s\n",
      "Step 134340 (epoch   41.98), loss: 0.006914, time: 2.239 s\n",
      "Step 134360 (epoch   41.99), loss: 0.004931, time: 2.236 s\n",
      "Step 134380 (epoch   41.99), loss: 0.009314, time: 2.245 s\n",
      "Step 134400 (epoch   42.00), loss: 0.010516, time: 2.231 s, error: 0.011980\n",
      "Step 134420 (epoch   42.01), loss: 0.007534, time: 15.079 s\n",
      "Step 134440 (epoch   42.01), loss: 0.004437, time: 2.237 s\n",
      "Step 134460 (epoch   42.02), loss: 0.007675, time: 2.245 s\n",
      "Step 134480 (epoch   42.02), loss: 0.004259, time: 2.255 s\n",
      "Step 134500 (epoch   42.03), loss: 0.006235, time: 2.246 s\n",
      "Step 134520 (epoch   42.04), loss: 0.010901, time: 2.246 s\n",
      "Step 134540 (epoch   42.04), loss: 0.004671, time: 2.240 s\n",
      "Step 134560 (epoch   42.05), loss: 0.005894, time: 2.242 s\n",
      "Step 134580 (epoch   42.06), loss: 0.006641, time: 2.242 s\n",
      "Step 134600 (epoch   42.06), loss: 0.004813, time: 2.232 s, error: 0.011828\n",
      "Step 134620 (epoch   42.07), loss: 0.005757, time: 15.074 s\n",
      "Step 134640 (epoch   42.08), loss: 0.005799, time: 2.240 s\n",
      "Step 134660 (epoch   42.08), loss: 0.006417, time: 2.241 s\n",
      "Step 134680 (epoch   42.09), loss: 0.005057, time: 2.234 s\n",
      "Step 134700 (epoch   42.09), loss: 0.005968, time: 2.253 s\n",
      "Step 134720 (epoch   42.10), loss: 0.004677, time: 2.226 s\n",
      "Step 134740 (epoch   42.11), loss: 0.008568, time: 2.253 s\n",
      "Step 134760 (epoch   42.11), loss: 0.006058, time: 2.249 s\n",
      "Step 134780 (epoch   42.12), loss: 0.004679, time: 2.250 s\n",
      "Step 134800 (epoch   42.12), loss: 0.007441, time: 2.244 s, error: 0.012041\n",
      "Step 134820 (epoch   42.13), loss: 0.005144, time: 15.095 s\n",
      "Step 134840 (epoch   42.14), loss: 0.005108, time: 2.243 s\n",
      "Step 134860 (epoch   42.14), loss: 0.006956, time: 2.228 s\n",
      "Step 134880 (epoch   42.15), loss: 0.008656, time: 2.244 s\n",
      "Step 134900 (epoch   42.16), loss: 0.005710, time: 2.233 s\n",
      "Step 134920 (epoch   42.16), loss: 0.010038, time: 2.243 s\n",
      "Step 134940 (epoch   42.17), loss: 0.005673, time: 2.240 s\n",
      "Step 134960 (epoch   42.17), loss: 0.006241, time: 2.239 s\n",
      "Step 134980 (epoch   42.18), loss: 0.003386, time: 2.244 s\n",
      "Step 135000 (epoch   42.19), loss: 0.005003, time: 2.249 s, error: 0.011895\n",
      "Step 135020 (epoch   42.19), loss: 0.008306, time: 15.127 s\n",
      "Step 135040 (epoch   42.20), loss: 0.005265, time: 2.244 s\n",
      "Step 135060 (epoch   42.21), loss: 0.005700, time: 2.244 s\n",
      "Step 135080 (epoch   42.21), loss: 0.005027, time: 2.244 s\n",
      "Step 135100 (epoch   42.22), loss: 0.010961, time: 2.256 s\n",
      "Step 135120 (epoch   42.23), loss: 0.006117, time: 2.254 s\n",
      "Step 135140 (epoch   42.23), loss: 0.008223, time: 2.260 s\n",
      "Step 135160 (epoch   42.24), loss: 0.003433, time: 2.256 s\n",
      "Step 135180 (epoch   42.24), loss: 0.004386, time: 2.248 s\n",
      "Step 135200 (epoch   42.25), loss: 0.005709, time: 2.246 s, error: 0.012056\n",
      "Step 135220 (epoch   42.26), loss: 0.008595, time: 15.375 s\n",
      "Step 135240 (epoch   42.26), loss: 0.007111, time: 2.253 s\n",
      "Step 135260 (epoch   42.27), loss: 0.004682, time: 2.236 s\n",
      "Step 135280 (epoch   42.27), loss: 0.005923, time: 2.255 s\n",
      "Step 135300 (epoch   42.28), loss: 0.004824, time: 2.243 s\n",
      "Step 135320 (epoch   42.29), loss: 0.004609, time: 2.242 s\n",
      "Step 135340 (epoch   42.29), loss: 0.004333, time: 2.246 s\n",
      "Step 135360 (epoch   42.30), loss: 0.005715, time: 2.245 s\n",
      "Step 135380 (epoch   42.31), loss: 0.007558, time: 2.227 s\n",
      "Step 135400 (epoch   42.31), loss: 0.008894, time: 2.242 s, error: 0.011940\n",
      "Step 135420 (epoch   42.32), loss: 0.004466, time: 15.189 s\n",
      "Step 135440 (epoch   42.33), loss: 0.005564, time: 2.245 s\n",
      "Step 135460 (epoch   42.33), loss: 0.005484, time: 2.237 s\n",
      "Step 135480 (epoch   42.34), loss: 0.003580, time: 2.250 s\n",
      "Step 135500 (epoch   42.34), loss: 0.005101, time: 2.255 s\n",
      "Step 135520 (epoch   42.35), loss: 0.004490, time: 2.259 s\n",
      "Step 135540 (epoch   42.36), loss: 0.005225, time: 2.242 s\n",
      "Step 135560 (epoch   42.36), loss: 0.008232, time: 2.241 s\n",
      "Step 135580 (epoch   42.37), loss: 0.005415, time: 2.242 s\n",
      "Step 135600 (epoch   42.38), loss: 0.007159, time: 2.237 s, error: 0.012070\n",
      "Step 135620 (epoch   42.38), loss: 0.004600, time: 15.068 s\n",
      "Step 135640 (epoch   42.39), loss: 0.005580, time: 2.242 s\n",
      "Step 135660 (epoch   42.39), loss: 0.005703, time: 2.252 s\n",
      "Step 135680 (epoch   42.40), loss: 0.005343, time: 2.250 s\n",
      "Step 135700 (epoch   42.41), loss: 0.005166, time: 2.230 s\n",
      "Step 135720 (epoch   42.41), loss: 0.004370, time: 2.245 s\n",
      "Step 135740 (epoch   42.42), loss: 0.004618, time: 2.244 s\n",
      "Step 135760 (epoch   42.42), loss: 0.005015, time: 2.248 s\n",
      "Step 135780 (epoch   42.43), loss: 0.005279, time: 2.241 s\n",
      "Step 135800 (epoch   42.44), loss: 0.006684, time: 2.229 s, error: 0.012113\n",
      "Step 135820 (epoch   42.44), loss: 0.007009, time: 15.074 s\n",
      "Step 135840 (epoch   42.45), loss: 0.006004, time: 2.241 s\n",
      "Step 135860 (epoch   42.46), loss: 0.007602, time: 2.236 s\n",
      "Step 135880 (epoch   42.46), loss: 0.004469, time: 2.249 s\n",
      "Step 135900 (epoch   42.47), loss: 0.003925, time: 2.245 s\n",
      "Step 135920 (epoch   42.48), loss: 0.005678, time: 2.253 s\n",
      "Step 135940 (epoch   42.48), loss: 0.007665, time: 2.255 s\n",
      "Step 135960 (epoch   42.49), loss: 0.005831, time: 2.234 s\n",
      "Step 135980 (epoch   42.49), loss: 0.006186, time: 2.244 s\n",
      "Step 136000 (epoch   42.50), loss: 0.005248, time: 2.240 s, error: 0.012097\n",
      "\n",
      "Time since beginning  : 23737.229 s\n",
      "\n",
      "Step 136020 (epoch   42.51), loss: 0.013801, time: 15.234 s\n",
      "Step 136040 (epoch   42.51), loss: 0.006935, time: 2.240 s\n",
      "Step 136060 (epoch   42.52), loss: 0.007153, time: 2.240 s\n",
      "Step 136080 (epoch   42.52), loss: 0.007590, time: 2.252 s\n",
      "Step 136100 (epoch   42.53), loss: 0.008683, time: 2.236 s\n",
      "Step 136120 (epoch   42.54), loss: 0.004415, time: 2.238 s\n",
      "Step 136140 (epoch   42.54), loss: 0.011216, time: 2.230 s\n",
      "Step 136160 (epoch   42.55), loss: 0.006947, time: 2.253 s\n",
      "Step 136180 (epoch   42.56), loss: 0.005404, time: 2.261 s\n",
      "Step 136200 (epoch   42.56), loss: 0.003743, time: 2.259 s, error: 0.011832\n",
      "Step 136220 (epoch   42.57), loss: 0.005067, time: 15.102 s\n",
      "Step 136240 (epoch   42.58), loss: 0.005393, time: 2.239 s\n",
      "Step 136260 (epoch   42.58), loss: 0.006134, time: 2.247 s\n",
      "Step 136280 (epoch   42.59), loss: 0.006304, time: 2.238 s\n",
      "Step 136300 (epoch   42.59), loss: 0.009030, time: 2.232 s\n",
      "Step 136320 (epoch   42.60), loss: 0.003729, time: 2.244 s\n",
      "Step 136340 (epoch   42.61), loss: 0.005431, time: 2.239 s\n",
      "Step 136360 (epoch   42.61), loss: 0.003847, time: 2.236 s\n",
      "Step 136380 (epoch   42.62), loss: 0.003657, time: 2.242 s\n",
      "Step 136400 (epoch   42.62), loss: 0.007712, time: 2.244 s, error: 0.012348\n",
      "Step 136420 (epoch   42.63), loss: 0.004884, time: 15.272 s\n",
      "Step 136440 (epoch   42.64), loss: 0.005098, time: 2.248 s\n",
      "Step 136460 (epoch   42.64), loss: 0.005489, time: 2.241 s\n",
      "Step 136480 (epoch   42.65), loss: 0.005640, time: 2.243 s\n",
      "Step 136500 (epoch   42.66), loss: 0.004123, time: 2.255 s\n",
      "Step 136520 (epoch   42.66), loss: 0.010834, time: 2.248 s\n",
      "Step 136540 (epoch   42.67), loss: 0.007361, time: 2.250 s\n",
      "Step 136560 (epoch   42.67), loss: 0.006627, time: 2.255 s\n",
      "Step 136580 (epoch   42.68), loss: 0.010630, time: 2.255 s\n",
      "Step 136600 (epoch   42.69), loss: 0.008786, time: 2.256 s, error: 0.012479\n",
      "Step 136620 (epoch   42.69), loss: 0.005069, time: 15.331 s\n",
      "Step 136640 (epoch   42.70), loss: 0.004913, time: 2.248 s\n",
      "Step 136660 (epoch   42.71), loss: 0.025858, time: 2.242 s\n",
      "Step 136680 (epoch   42.71), loss: 0.003996, time: 2.247 s\n",
      "Step 136700 (epoch   42.72), loss: 0.010191, time: 2.253 s\n",
      "Step 136720 (epoch   42.73), loss: 0.004843, time: 2.242 s\n",
      "Step 136740 (epoch   42.73), loss: 0.007984, time: 2.249 s\n",
      "Step 136760 (epoch   42.74), loss: 0.006783, time: 2.244 s\n",
      "Step 136780 (epoch   42.74), loss: 0.006236, time: 2.245 s\n",
      "Step 136800 (epoch   42.75), loss: 0.005070, time: 2.228 s, error: 0.012065\n",
      "Step 136820 (epoch   42.76), loss: 0.005476, time: 15.123 s\n",
      "Step 136840 (epoch   42.76), loss: 0.006969, time: 2.246 s\n",
      "Step 136860 (epoch   42.77), loss: 0.004370, time: 2.257 s\n",
      "Step 136880 (epoch   42.77), loss: 0.005348, time: 2.236 s\n",
      "Step 136900 (epoch   42.78), loss: 0.011393, time: 2.231 s\n",
      "Step 136920 (epoch   42.79), loss: 0.005774, time: 2.253 s\n",
      "Step 136940 (epoch   42.79), loss: 0.005882, time: 2.228 s\n",
      "Step 136960 (epoch   42.80), loss: 0.006973, time: 2.252 s\n",
      "Step 136980 (epoch   42.81), loss: 0.004933, time: 2.245 s\n",
      "Step 137000 (epoch   42.81), loss: 0.003998, time: 2.258 s, error: 0.011924\n",
      "Step 137020 (epoch   42.82), loss: 0.005718, time: 15.108 s\n",
      "Step 137040 (epoch   42.83), loss: 0.005714, time: 2.238 s\n",
      "Step 137060 (epoch   42.83), loss: 0.005701, time: 2.245 s\n",
      "Step 137080 (epoch   42.84), loss: 0.006130, time: 2.246 s\n",
      "Step 137100 (epoch   42.84), loss: 0.007413, time: 2.235 s\n",
      "Step 137120 (epoch   42.85), loss: 0.005740, time: 2.260 s\n",
      "Step 137140 (epoch   42.86), loss: 0.004888, time: 2.245 s\n",
      "Step 137160 (epoch   42.86), loss: 0.006454, time: 2.245 s\n",
      "Step 137180 (epoch   42.87), loss: 0.006828, time: 2.243 s\n",
      "Step 137200 (epoch   42.88), loss: 0.004792, time: 2.242 s, error: 0.011785\n",
      "Step 137220 (epoch   42.88), loss: 0.004035, time: 15.074 s\n",
      "Step 137240 (epoch   42.89), loss: 0.005332, time: 2.254 s\n",
      "Step 137260 (epoch   42.89), loss: 0.007514, time: 2.241 s\n",
      "Step 137280 (epoch   42.90), loss: 0.009506, time: 2.251 s\n",
      "Step 137300 (epoch   42.91), loss: 0.006018, time: 2.236 s\n",
      "Step 137320 (epoch   42.91), loss: 0.006591, time: 2.251 s\n",
      "Step 137340 (epoch   42.92), loss: 0.006607, time: 2.235 s\n",
      "Step 137360 (epoch   42.92), loss: 0.008995, time: 2.244 s\n",
      "Step 137380 (epoch   42.93), loss: 0.007397, time: 2.262 s\n",
      "Step 137400 (epoch   42.94), loss: 0.005705, time: 2.251 s, error: 0.011673\n",
      "Step 137420 (epoch   42.94), loss: 0.005509, time: 15.096 s\n",
      "Step 137440 (epoch   42.95), loss: 0.004588, time: 2.240 s\n",
      "Step 137460 (epoch   42.96), loss: 0.011468, time: 2.247 s\n",
      "Step 137480 (epoch   42.96), loss: 0.005386, time: 2.243 s\n",
      "Step 137500 (epoch   42.97), loss: 0.004976, time: 2.245 s\n",
      "Step 137520 (epoch   42.98), loss: 0.003078, time: 2.238 s\n",
      "Step 137540 (epoch   42.98), loss: 0.005828, time: 2.247 s\n",
      "Step 137560 (epoch   42.99), loss: 0.008191, time: 2.237 s\n",
      "Step 137580 (epoch   42.99), loss: 0.004833, time: 2.235 s\n",
      "Step 137600 (epoch   43.00), loss: 0.003509, time: 2.231 s, error: 0.011941\n",
      "Step 137620 (epoch   43.01), loss: 0.004887, time: 15.235 s\n",
      "Step 137640 (epoch   43.01), loss: 0.006642, time: 2.256 s\n",
      "Step 137660 (epoch   43.02), loss: 0.004226, time: 2.236 s\n",
      "Step 137680 (epoch   43.02), loss: 0.003915, time: 2.236 s\n",
      "Step 137700 (epoch   43.03), loss: 0.005762, time: 2.237 s\n",
      "Step 137720 (epoch   43.04), loss: 0.006392, time: 2.233 s\n",
      "Step 137740 (epoch   43.04), loss: 0.005778, time: 2.237 s\n",
      "Step 137760 (epoch   43.05), loss: 0.004280, time: 2.254 s\n",
      "Step 137780 (epoch   43.06), loss: 0.005952, time: 2.253 s\n",
      "Step 137800 (epoch   43.06), loss: 0.010104, time: 2.255 s, error: 0.012160\n",
      "Step 137820 (epoch   43.07), loss: 0.015587, time: 15.367 s\n",
      "Step 137840 (epoch   43.08), loss: 0.008595, time: 2.234 s\n",
      "Step 137860 (epoch   43.08), loss: 0.004156, time: 2.235 s\n",
      "Step 137880 (epoch   43.09), loss: 0.005442, time: 2.241 s\n",
      "Step 137900 (epoch   43.09), loss: 0.004391, time: 2.258 s\n",
      "Step 137920 (epoch   43.10), loss: 0.006676, time: 2.251 s\n",
      "Step 137940 (epoch   43.11), loss: 0.004334, time: 2.254 s\n",
      "Step 137960 (epoch   43.11), loss: 0.003640, time: 2.238 s\n",
      "Step 137980 (epoch   43.12), loss: 0.005503, time: 2.247 s\n",
      "Step 138000 (epoch   43.12), loss: 0.004185, time: 2.247 s, error: 0.011935\n",
      "\n",
      "Time since beginning  : 24091.172 s\n",
      "\n",
      "Step 138020 (epoch   43.13), loss: 0.007966, time: 15.217 s\n",
      "Step 138040 (epoch   43.14), loss: 0.004576, time: 2.251 s\n",
      "Step 138060 (epoch   43.14), loss: 0.004729, time: 2.258 s\n",
      "Step 138080 (epoch   43.15), loss: 0.007631, time: 2.246 s\n",
      "Step 138100 (epoch   43.16), loss: 0.005474, time: 2.236 s\n",
      "Step 138120 (epoch   43.16), loss: 0.008514, time: 2.232 s\n",
      "Step 138140 (epoch   43.17), loss: 0.007425, time: 2.247 s\n",
      "Step 138160 (epoch   43.17), loss: 0.013617, time: 2.248 s\n",
      "Step 138180 (epoch   43.18), loss: 0.004942, time: 2.242 s\n",
      "Step 138200 (epoch   43.19), loss: 0.005858, time: 2.247 s, error: 0.011842\n",
      "Step 138220 (epoch   43.19), loss: 0.006941, time: 15.061 s\n",
      "Step 138240 (epoch   43.20), loss: 0.005856, time: 2.247 s\n",
      "Step 138260 (epoch   43.21), loss: 0.007196, time: 2.240 s\n",
      "Step 138280 (epoch   43.21), loss: 0.008088, time: 2.241 s\n",
      "Step 138300 (epoch   43.22), loss: 0.004486, time: 2.258 s\n",
      "Step 138320 (epoch   43.23), loss: 0.007433, time: 2.247 s\n",
      "Step 138340 (epoch   43.23), loss: 0.005209, time: 2.251 s\n",
      "Step 138360 (epoch   43.24), loss: 0.016021, time: 2.244 s\n",
      "Step 138380 (epoch   43.24), loss: 0.007994, time: 2.247 s\n",
      "Step 138400 (epoch   43.25), loss: 0.006882, time: 2.247 s, error: 0.011944\n",
      "Step 138420 (epoch   43.26), loss: 0.007096, time: 15.060 s\n",
      "Step 138440 (epoch   43.26), loss: 0.007538, time: 2.244 s\n",
      "Step 138460 (epoch   43.27), loss: 0.023527, time: 2.233 s\n",
      "Step 138480 (epoch   43.27), loss: 0.011512, time: 2.239 s\n",
      "Step 138500 (epoch   43.28), loss: 0.006596, time: 2.241 s\n",
      "Step 138520 (epoch   43.29), loss: 0.008108, time: 2.247 s\n",
      "Step 138540 (epoch   43.29), loss: 0.004271, time: 2.256 s\n",
      "Step 138560 (epoch   43.30), loss: 0.005066, time: 2.261 s\n",
      "Step 138580 (epoch   43.31), loss: 0.004868, time: 2.257 s\n",
      "Step 138600 (epoch   43.31), loss: 0.009874, time: 2.254 s, error: 0.011960\n",
      "Step 138620 (epoch   43.32), loss: 0.005563, time: 15.075 s\n",
      "Step 138640 (epoch   43.33), loss: 0.005405, time: 2.256 s\n",
      "Step 138660 (epoch   43.33), loss: 0.005251, time: 2.241 s\n",
      "Step 138680 (epoch   43.34), loss: 0.009512, time: 2.244 s\n",
      "Step 138700 (epoch   43.34), loss: 0.005752, time: 2.238 s\n",
      "Step 138720 (epoch   43.35), loss: 0.005646, time: 2.245 s\n",
      "Step 138740 (epoch   43.36), loss: 0.004464, time: 2.241 s\n",
      "Step 138760 (epoch   43.36), loss: 0.005343, time: 2.237 s\n",
      "Step 138780 (epoch   43.37), loss: 0.005204, time: 2.234 s\n",
      "Step 138800 (epoch   43.38), loss: 0.005404, time: 2.230 s, error: 0.012021\n",
      "Step 138820 (epoch   43.38), loss: 0.005888, time: 15.257 s\n",
      "Step 138840 (epoch   43.39), loss: 0.003724, time: 2.249 s\n",
      "Step 138860 (epoch   43.39), loss: 0.008116, time: 2.241 s\n",
      "Step 138880 (epoch   43.40), loss: 0.005642, time: 2.234 s\n",
      "Step 138900 (epoch   43.41), loss: 0.006363, time: 2.255 s\n",
      "Step 138920 (epoch   43.41), loss: 0.004741, time: 2.237 s\n",
      "Step 138940 (epoch   43.42), loss: 0.006266, time: 2.258 s\n",
      "Step 138960 (epoch   43.42), loss: 0.004476, time: 2.240 s\n",
      "Step 138980 (epoch   43.43), loss: 0.006894, time: 2.251 s\n",
      "Step 139000 (epoch   43.44), loss: 0.010805, time: 2.249 s, error: 0.012068\n",
      "Step 139020 (epoch   43.44), loss: 0.004036, time: 15.296 s\n",
      "Step 139040 (epoch   43.45), loss: 0.006203, time: 2.242 s\n",
      "Step 139060 (epoch   43.46), loss: 0.005715, time: 2.243 s\n",
      "Step 139080 (epoch   43.46), loss: 0.004456, time: 2.239 s\n",
      "Step 139100 (epoch   43.47), loss: 0.006271, time: 2.246 s\n",
      "Step 139120 (epoch   43.48), loss: 0.005706, time: 2.256 s\n",
      "Step 139140 (epoch   43.48), loss: 0.006546, time: 2.245 s\n",
      "Step 139160 (epoch   43.49), loss: 0.005576, time: 2.249 s\n",
      "Step 139180 (epoch   43.49), loss: 0.006475, time: 2.249 s\n",
      "Step 139200 (epoch   43.50), loss: 0.006948, time: 2.248 s, error: 0.012066\n",
      "Step 139220 (epoch   43.51), loss: 0.006422, time: 15.123 s\n",
      "Step 139240 (epoch   43.51), loss: 0.005358, time: 2.247 s\n",
      "Step 139260 (epoch   43.52), loss: 0.005280, time: 2.247 s\n",
      "Step 139280 (epoch   43.52), loss: 0.004717, time: 2.237 s\n",
      "Step 139300 (epoch   43.53), loss: 0.006955, time: 2.252 s\n",
      "Step 139320 (epoch   43.54), loss: 0.004541, time: 2.257 s\n",
      "Step 139340 (epoch   43.54), loss: 0.005309, time: 2.245 s\n",
      "Step 139360 (epoch   43.55), loss: 0.004929, time: 2.245 s\n",
      "Step 139380 (epoch   43.56), loss: 0.008677, time: 2.241 s\n",
      "Step 139400 (epoch   43.56), loss: 0.005174, time: 2.249 s, error: 0.011558\n",
      "Step 139420 (epoch   43.57), loss: 0.006410, time: 15.080 s\n",
      "Step 139440 (epoch   43.58), loss: 0.006677, time: 2.243 s\n",
      "Step 139460 (epoch   43.58), loss: 0.004789, time: 2.246 s\n",
      "Step 139480 (epoch   43.59), loss: 0.003410, time: 2.244 s\n",
      "Step 139500 (epoch   43.59), loss: 0.006875, time: 2.260 s\n",
      "Step 139520 (epoch   43.60), loss: 0.005199, time: 2.239 s\n",
      "Step 139540 (epoch   43.61), loss: 0.005115, time: 2.245 s\n",
      "Step 139560 (epoch   43.61), loss: 0.005137, time: 2.257 s\n",
      "Step 139580 (epoch   43.62), loss: 0.007075, time: 2.244 s\n",
      "Step 139600 (epoch   43.62), loss: 0.006675, time: 2.245 s, error: 0.012182\n",
      "Step 139620 (epoch   43.63), loss: 0.007165, time: 15.069 s\n",
      "Step 139640 (epoch   43.64), loss: 0.006734, time: 2.251 s\n",
      "Step 139660 (epoch   43.64), loss: 0.007811, time: 2.253 s\n",
      "Step 139680 (epoch   43.65), loss: 0.004458, time: 2.240 s\n",
      "Step 139700 (epoch   43.66), loss: 0.005351, time: 2.243 s\n",
      "Step 139720 (epoch   43.66), loss: 0.012156, time: 2.244 s\n",
      "Step 139740 (epoch   43.67), loss: 0.004407, time: 2.252 s\n",
      "Step 139760 (epoch   43.67), loss: 0.006291, time: 2.253 s\n",
      "Step 139780 (epoch   43.68), loss: 0.006898, time: 2.255 s\n",
      "Step 139800 (epoch   43.69), loss: 0.006244, time: 2.251 s, error: 0.012310\n",
      "Step 139820 (epoch   43.69), loss: 0.006219, time: 15.161 s\n",
      "Step 139840 (epoch   43.70), loss: 0.009118, time: 2.239 s\n",
      "Step 139860 (epoch   43.71), loss: 0.007923, time: 2.259 s\n",
      "Step 139880 (epoch   43.71), loss: 0.004356, time: 2.250 s\n",
      "Step 139900 (epoch   43.72), loss: 0.008533, time: 2.241 s\n",
      "Step 139920 (epoch   43.73), loss: 0.007509, time: 2.238 s\n",
      "Step 139940 (epoch   43.73), loss: 0.004613, time: 2.225 s\n",
      "Step 139960 (epoch   43.74), loss: 0.006315, time: 2.246 s\n",
      "Step 139980 (epoch   43.74), loss: 0.005681, time: 2.242 s\n",
      "Step 140000 (epoch   43.75), loss: 0.004178, time: 2.259 s, error: 0.011928\n",
      "\n",
      "Time since beginning  : 24444.793 s\n",
      "\n",
      "Step 140020 (epoch   43.76), loss: 0.005385, time: 15.316 s\n",
      "Step 140040 (epoch   43.76), loss: 0.008032, time: 2.236 s\n",
      "Step 140060 (epoch   43.77), loss: 0.005490, time: 2.244 s\n",
      "Step 140080 (epoch   43.77), loss: 0.005546, time: 2.252 s\n",
      "Step 140100 (epoch   43.78), loss: 0.005389, time: 2.246 s\n",
      "Step 140120 (epoch   43.79), loss: 0.005871, time: 2.244 s\n",
      "Step 140140 (epoch   43.79), loss: 0.006082, time: 2.249 s\n",
      "Step 140160 (epoch   43.80), loss: 0.007683, time: 2.246 s\n",
      "Step 140180 (epoch   43.81), loss: 0.005403, time: 2.237 s\n",
      "Step 140200 (epoch   43.81), loss: 0.003860, time: 2.237 s, error: 0.011885\n",
      "Step 140220 (epoch   43.82), loss: 0.006757, time: 15.273 s\n",
      "Step 140240 (epoch   43.83), loss: 0.004424, time: 2.240 s\n",
      "Step 140260 (epoch   43.83), loss: 0.005174, time: 2.244 s\n",
      "Step 140280 (epoch   43.84), loss: 0.005098, time: 2.245 s\n",
      "Step 140300 (epoch   43.84), loss: 0.006346, time: 2.243 s\n",
      "Step 140320 (epoch   43.85), loss: 0.004522, time: 2.239 s\n",
      "Step 140340 (epoch   43.86), loss: 0.010124, time: 2.238 s\n",
      "Step 140360 (epoch   43.86), loss: 0.004486, time: 2.238 s\n",
      "Step 140380 (epoch   43.87), loss: 0.004100, time: 2.231 s\n",
      "Step 140400 (epoch   43.88), loss: 0.007285, time: 2.245 s, error: 0.011530\n",
      "Step 140420 (epoch   43.88), loss: 0.004241, time: 15.072 s\n",
      "Step 140440 (epoch   43.89), loss: 0.006079, time: 2.250 s\n",
      "Step 140460 (epoch   43.89), loss: 0.004281, time: 2.243 s\n",
      "Step 140480 (epoch   43.90), loss: 0.004875, time: 2.241 s\n",
      "Step 140500 (epoch   43.91), loss: 0.005111, time: 2.246 s\n",
      "Step 140520 (epoch   43.91), loss: 0.005940, time: 2.242 s\n",
      "Step 140540 (epoch   43.92), loss: 0.006060, time: 2.240 s\n",
      "Step 140560 (epoch   43.92), loss: 0.006009, time: 2.244 s\n",
      "Step 140580 (epoch   43.93), loss: 0.004742, time: 2.257 s\n",
      "Step 140600 (epoch   43.94), loss: 0.004911, time: 2.239 s, error: 0.011566\n",
      "Step 140620 (epoch   43.94), loss: 0.004741, time: 15.064 s\n",
      "Step 140640 (epoch   43.95), loss: 0.008438, time: 2.233 s\n",
      "Step 140660 (epoch   43.96), loss: 0.004926, time: 2.246 s\n",
      "Step 140680 (epoch   43.96), loss: 0.004992, time: 2.259 s\n",
      "Step 140700 (epoch   43.97), loss: 0.007155, time: 2.240 s\n",
      "Step 140720 (epoch   43.98), loss: 0.004302, time: 2.251 s\n",
      "Step 140740 (epoch   43.98), loss: 0.004812, time: 2.238 s\n",
      "Step 140760 (epoch   43.99), loss: 0.010433, time: 2.238 s\n",
      "Step 140780 (epoch   43.99), loss: 0.005720, time: 2.240 s\n",
      "Step 140800 (epoch   44.00), loss: 0.006076, time: 2.241 s, error: 0.011830\n",
      "Step 140820 (epoch   44.01), loss: 0.008936, time: 15.051 s\n",
      "Step 140840 (epoch   44.01), loss: 0.005114, time: 2.237 s\n",
      "Step 140860 (epoch   44.02), loss: 0.005528, time: 2.250 s\n",
      "Step 140880 (epoch   44.02), loss: 0.004353, time: 2.242 s\n",
      "Step 140900 (epoch   44.03), loss: 0.006543, time: 2.253 s\n",
      "Step 140920 (epoch   44.04), loss: 0.005451, time: 2.243 s\n",
      "Step 140940 (epoch   44.04), loss: 0.003994, time: 2.256 s\n",
      "Step 140960 (epoch   44.05), loss: 0.004788, time: 2.251 s\n",
      "Step 140980 (epoch   44.06), loss: 0.006160, time: 2.244 s\n",
      "Step 141000 (epoch   44.06), loss: 0.006546, time: 2.235 s, error: 0.012307\n",
      "Step 141020 (epoch   44.07), loss: 0.004786, time: 15.079 s\n",
      "Step 141040 (epoch   44.08), loss: 0.005325, time: 2.251 s\n",
      "Step 141060 (epoch   44.08), loss: 0.006909, time: 2.235 s\n",
      "Step 141080 (epoch   44.09), loss: 0.004892, time: 2.239 s\n",
      "Step 141100 (epoch   44.09), loss: 0.005547, time: 2.236 s\n",
      "Step 141120 (epoch   44.10), loss: 0.005981, time: 2.236 s\n",
      "Step 141140 (epoch   44.11), loss: 0.009289, time: 2.233 s\n",
      "Step 141160 (epoch   44.11), loss: 0.008256, time: 2.249 s\n",
      "Step 141180 (epoch   44.12), loss: 0.003826, time: 2.255 s\n",
      "Step 141200 (epoch   44.12), loss: 0.010246, time: 2.256 s, error: 0.011792\n",
      "Step 141220 (epoch   44.13), loss: 0.005707, time: 15.172 s\n",
      "Step 141240 (epoch   44.14), loss: 0.004164, time: 2.252 s\n",
      "Step 141260 (epoch   44.14), loss: 0.005644, time: 2.252 s\n",
      "Step 141280 (epoch   44.15), loss: 0.005629, time: 2.249 s\n",
      "Step 141300 (epoch   44.16), loss: 0.005484, time: 2.248 s\n",
      "Step 141320 (epoch   44.16), loss: 0.005005, time: 2.248 s\n",
      "Step 141340 (epoch   44.17), loss: 0.005898, time: 2.239 s\n",
      "Step 141360 (epoch   44.17), loss: 0.005563, time: 2.249 s\n",
      "Step 141380 (epoch   44.18), loss: 0.007484, time: 2.243 s\n",
      "Step 141400 (epoch   44.19), loss: 0.005557, time: 2.255 s, error: 0.011815\n",
      "Step 141420 (epoch   44.19), loss: 0.004866, time: 15.325 s\n",
      "Step 141440 (epoch   44.20), loss: 0.005670, time: 2.250 s\n",
      "Step 141460 (epoch   44.21), loss: 0.008221, time: 2.242 s\n",
      "Step 141480 (epoch   44.21), loss: 0.004807, time: 2.245 s\n",
      "Step 141500 (epoch   44.22), loss: 0.007395, time: 2.239 s\n",
      "Step 141520 (epoch   44.23), loss: 0.004183, time: 2.252 s\n",
      "Step 141540 (epoch   44.23), loss: 0.007145, time: 2.250 s\n",
      "Step 141560 (epoch   44.24), loss: 0.007575, time: 2.252 s\n",
      "Step 141580 (epoch   44.24), loss: 0.007448, time: 2.243 s\n",
      "Step 141600 (epoch   44.25), loss: 0.007669, time: 2.255 s, error: 0.011789\n",
      "Step 141620 (epoch   44.26), loss: 0.006620, time: 15.117 s\n",
      "Step 141640 (epoch   44.26), loss: 0.005374, time: 2.257 s\n",
      "Step 141660 (epoch   44.27), loss: 0.005793, time: 2.250 s\n",
      "Step 141680 (epoch   44.27), loss: 0.008252, time: 2.246 s\n",
      "Step 141700 (epoch   44.28), loss: 0.007387, time: 2.249 s\n",
      "Step 141720 (epoch   44.29), loss: 0.004407, time: 2.237 s\n",
      "Step 141740 (epoch   44.29), loss: 0.004511, time: 2.254 s\n",
      "Step 141760 (epoch   44.30), loss: 0.005600, time: 2.238 s\n",
      "Step 141780 (epoch   44.31), loss: 0.003467, time: 2.249 s\n",
      "Step 141800 (epoch   44.31), loss: 0.007080, time: 2.242 s, error: 0.012008\n",
      "Step 141820 (epoch   44.32), loss: 0.010207, time: 15.085 s\n",
      "Step 141840 (epoch   44.33), loss: 0.004532, time: 2.253 s\n",
      "Step 141860 (epoch   44.33), loss: 0.006914, time: 2.245 s\n",
      "Step 141880 (epoch   44.34), loss: 0.005334, time: 2.254 s\n",
      "Step 141900 (epoch   44.34), loss: 0.007851, time: 2.240 s\n",
      "Step 141920 (epoch   44.35), loss: 0.004497, time: 2.247 s\n",
      "Step 141940 (epoch   44.36), loss: 0.003349, time: 2.241 s\n",
      "Step 141960 (epoch   44.36), loss: 0.011975, time: 2.234 s\n",
      "Step 141980 (epoch   44.37), loss: 0.005942, time: 2.243 s\n",
      "Step 142000 (epoch   44.38), loss: 0.007111, time: 2.243 s, error: 0.011964\n",
      "\n",
      "Time since beginning  : 24798.191 s\n",
      "\n",
      "Step 142020 (epoch   44.38), loss: 0.004410, time: 15.123 s\n",
      "Step 142040 (epoch   44.39), loss: 0.009668, time: 2.249 s\n",
      "Step 142060 (epoch   44.39), loss: 0.004587, time: 2.248 s\n",
      "Step 142080 (epoch   44.40), loss: 0.004469, time: 2.238 s\n",
      "Step 142100 (epoch   44.41), loss: 0.007619, time: 2.237 s\n",
      "Step 142120 (epoch   44.41), loss: 0.007998, time: 2.254 s\n",
      "Step 142140 (epoch   44.42), loss: 0.008662, time: 2.266 s\n",
      "Step 142160 (epoch   44.42), loss: 0.006506, time: 2.241 s\n",
      "Step 142180 (epoch   44.43), loss: 0.005483, time: 2.246 s\n",
      "Step 142200 (epoch   44.44), loss: 0.005974, time: 2.241 s, error: 0.012019\n",
      "Step 142220 (epoch   44.44), loss: 0.003891, time: 15.140 s\n",
      "Step 142240 (epoch   44.45), loss: 0.006071, time: 2.259 s\n",
      "Step 142260 (epoch   44.46), loss: 0.008069, time: 2.239 s\n",
      "Step 142280 (epoch   44.46), loss: 0.008222, time: 2.249 s\n",
      "Step 142300 (epoch   44.47), loss: 0.003898, time: 2.246 s\n",
      "Step 142320 (epoch   44.48), loss: 0.004126, time: 2.245 s\n",
      "Step 142340 (epoch   44.48), loss: 0.004815, time: 2.241 s\n",
      "Step 142360 (epoch   44.49), loss: 0.007080, time: 2.242 s\n",
      "Step 142380 (epoch   44.49), loss: 0.006319, time: 2.236 s\n",
      "Step 142400 (epoch   44.50), loss: 0.005767, time: 2.261 s, error: 0.011672\n",
      "Step 142420 (epoch   44.51), loss: 0.006597, time: 15.140 s\n",
      "Step 142440 (epoch   44.51), loss: 0.006502, time: 2.238 s\n",
      "Step 142460 (epoch   44.52), loss: 0.005740, time: 2.244 s\n",
      "Step 142480 (epoch   44.52), loss: 0.005896, time: 2.239 s\n",
      "Step 142500 (epoch   44.53), loss: 0.005023, time: 2.237 s\n",
      "Step 142520 (epoch   44.54), loss: 0.005812, time: 2.230 s\n",
      "Step 142540 (epoch   44.54), loss: 0.003578, time: 2.250 s\n",
      "Step 142560 (epoch   44.55), loss: 0.005310, time: 2.232 s\n",
      "Step 142580 (epoch   44.56), loss: 0.004330, time: 2.250 s\n",
      "Step 142600 (epoch   44.56), loss: 0.004476, time: 2.236 s, error: 0.011497\n",
      "Step 142620 (epoch   44.57), loss: 0.005291, time: 15.291 s\n",
      "Step 142640 (epoch   44.58), loss: 0.007916, time: 2.250 s\n",
      "Step 142660 (epoch   44.58), loss: 0.005727, time: 2.253 s\n",
      "Step 142680 (epoch   44.59), loss: 0.004582, time: 2.249 s\n",
      "Step 142700 (epoch   44.59), loss: 0.005495, time: 2.239 s\n",
      "Step 142720 (epoch   44.60), loss: 0.004280, time: 2.257 s\n",
      "Step 142740 (epoch   44.61), loss: 0.004014, time: 2.248 s\n",
      "Step 142760 (epoch   44.61), loss: 0.007423, time: 2.253 s\n",
      "Step 142780 (epoch   44.62), loss: 0.005725, time: 2.247 s\n",
      "Step 142800 (epoch   44.62), loss: 0.006762, time: 2.247 s, error: 0.011827\n",
      "Step 142820 (epoch   44.63), loss: 0.007188, time: 15.372 s\n",
      "Step 142840 (epoch   44.64), loss: 0.005267, time: 2.238 s\n",
      "Step 142860 (epoch   44.64), loss: 0.004516, time: 2.239 s\n",
      "Step 142880 (epoch   44.65), loss: 0.013975, time: 2.233 s\n",
      "Step 142900 (epoch   44.66), loss: 0.004706, time: 2.242 s\n",
      "Step 142920 (epoch   44.66), loss: 0.005812, time: 2.246 s\n",
      "Step 142940 (epoch   44.67), loss: 0.004837, time: 2.237 s\n",
      "Step 142960 (epoch   44.67), loss: 0.005702, time: 2.241 s\n",
      "Step 142980 (epoch   44.68), loss: 0.005531, time: 2.242 s\n",
      "Step 143000 (epoch   44.69), loss: 0.005780, time: 2.246 s, error: 0.012162\n",
      "Step 143020 (epoch   44.69), loss: 0.006216, time: 15.090 s\n",
      "Step 143040 (epoch   44.70), loss: 0.005789, time: 2.239 s\n",
      "Step 143060 (epoch   44.71), loss: 0.006167, time: 2.263 s\n",
      "Step 143080 (epoch   44.71), loss: 0.005467, time: 2.253 s\n",
      "Step 143100 (epoch   44.72), loss: 0.005473, time: 2.248 s\n",
      "Step 143120 (epoch   44.73), loss: 0.005058, time: 2.246 s\n",
      "Step 143140 (epoch   44.73), loss: 0.004106, time: 2.255 s\n",
      "Step 143160 (epoch   44.74), loss: 0.006055, time: 2.245 s\n",
      "Step 143180 (epoch   44.74), loss: 0.004684, time: 2.261 s\n",
      "Step 143200 (epoch   44.75), loss: 0.007086, time: 2.250 s, error: 0.011821\n",
      "Step 143220 (epoch   44.76), loss: 0.008704, time: 15.136 s\n",
      "Step 143240 (epoch   44.76), loss: 0.005095, time: 2.255 s\n",
      "Step 143260 (epoch   44.77), loss: 0.006125, time: 2.244 s\n",
      "Step 143280 (epoch   44.77), loss: 0.007104, time: 2.258 s\n",
      "Step 143300 (epoch   44.78), loss: 0.009350, time: 2.259 s\n",
      "Step 143320 (epoch   44.79), loss: 0.007100, time: 2.265 s\n",
      "Step 143340 (epoch   44.79), loss: 0.005155, time: 2.256 s\n",
      "Step 143360 (epoch   44.80), loss: 0.007789, time: 2.257 s\n",
      "Step 143380 (epoch   44.81), loss: 0.003958, time: 2.238 s\n",
      "Step 143400 (epoch   44.81), loss: 0.004178, time: 2.250 s, error: 0.011824\n",
      "Step 143420 (epoch   44.82), loss: 0.007577, time: 15.050 s\n",
      "Step 143440 (epoch   44.83), loss: 0.005492, time: 2.249 s\n",
      "Step 143460 (epoch   44.83), loss: 0.005609, time: 2.241 s\n",
      "Step 143480 (epoch   44.84), loss: 0.003184, time: 2.242 s\n",
      "Step 143500 (epoch   44.84), loss: 0.004789, time: 2.237 s\n",
      "Step 143520 (epoch   44.85), loss: 0.008268, time: 2.233 s\n",
      "Step 143540 (epoch   44.86), loss: 0.009394, time: 2.251 s\n",
      "Step 143560 (epoch   44.86), loss: 0.005009, time: 2.236 s\n",
      "Step 143580 (epoch   44.87), loss: 0.004437, time: 2.263 s\n",
      "Step 143600 (epoch   44.88), loss: 0.003496, time: 2.254 s, error: 0.011745\n",
      "Step 143620 (epoch   44.88), loss: 0.004470, time: 15.220 s\n",
      "Step 143640 (epoch   44.89), loss: 0.004362, time: 2.245 s\n",
      "Step 143660 (epoch   44.89), loss: 0.007035, time: 2.241 s\n",
      "Step 143680 (epoch   44.90), loss: 0.008121, time: 2.241 s\n",
      "Step 143700 (epoch   44.91), loss: 0.005131, time: 2.251 s\n",
      "Step 143720 (epoch   44.91), loss: 0.009202, time: 2.247 s\n",
      "Step 143740 (epoch   44.92), loss: 0.005859, time: 2.249 s\n",
      "Step 143760 (epoch   44.92), loss: 0.004426, time: 2.227 s\n",
      "Step 143780 (epoch   44.93), loss: 0.006007, time: 2.253 s\n",
      "Step 143800 (epoch   44.94), loss: 0.014330, time: 2.242 s, error: 0.011450\n",
      "Step 143820 (epoch   44.94), loss: 0.005664, time: 15.294 s\n",
      "Step 143840 (epoch   44.95), loss: 0.004007, time: 2.249 s\n",
      "Step 143860 (epoch   44.96), loss: 0.005656, time: 2.242 s\n",
      "Step 143880 (epoch   44.96), loss: 0.007116, time: 2.237 s\n",
      "Step 143900 (epoch   44.97), loss: 0.006521, time: 2.246 s\n",
      "Step 143920 (epoch   44.98), loss: 0.005927, time: 2.234 s\n",
      "Step 143940 (epoch   44.98), loss: 0.006270, time: 2.252 s\n",
      "Step 143960 (epoch   44.99), loss: 0.005830, time: 2.245 s\n",
      "Step 143980 (epoch   44.99), loss: 0.005374, time: 2.241 s\n",
      "Step 144000 (epoch   45.00), loss: 0.004381, time: 2.251 s, error: 0.011777\n",
      "\n",
      "Time since beginning  : 25152.392 s\n",
      "\n",
      "Step 144020 (epoch   45.01), loss: 0.005666, time: 15.310 s\n",
      "Step 144040 (epoch   45.01), loss: 0.005905, time: 2.228 s\n",
      "Step 144060 (epoch   45.02), loss: 0.007352, time: 2.244 s\n",
      "Step 144080 (epoch   45.02), loss: 0.004254, time: 2.242 s\n",
      "Step 144100 (epoch   45.03), loss: 0.007806, time: 2.239 s\n",
      "Step 144120 (epoch   45.04), loss: 0.003918, time: 2.250 s\n",
      "Step 144140 (epoch   45.04), loss: 0.005840, time: 2.250 s\n",
      "Step 144160 (epoch   45.05), loss: 0.007113, time: 2.252 s\n",
      "Step 144180 (epoch   45.06), loss: 0.005211, time: 2.255 s\n",
      "Step 144200 (epoch   45.06), loss: 0.005339, time: 2.247 s, error: 0.012280\n",
      "Step 144220 (epoch   45.07), loss: 0.007279, time: 15.163 s\n",
      "Step 144240 (epoch   45.08), loss: 0.004672, time: 2.248 s\n",
      "Step 144260 (epoch   45.08), loss: 0.007516, time: 2.248 s\n",
      "Step 144280 (epoch   45.09), loss: 0.004631, time: 2.244 s\n",
      "Step 144300 (epoch   45.09), loss: 0.003098, time: 2.248 s\n",
      "Step 144320 (epoch   45.10), loss: 0.006474, time: 2.243 s\n",
      "Step 144340 (epoch   45.11), loss: 0.004526, time: 2.250 s\n",
      "Step 144360 (epoch   45.11), loss: 0.005474, time: 2.228 s\n",
      "Step 144380 (epoch   45.12), loss: 0.005669, time: 2.241 s\n",
      "Step 144400 (epoch   45.12), loss: 0.007612, time: 2.243 s, error: 0.011713\n",
      "Step 144420 (epoch   45.13), loss: 0.008487, time: 15.090 s\n",
      "Step 144440 (epoch   45.14), loss: 0.008777, time: 2.250 s\n",
      "Step 144460 (epoch   45.14), loss: 0.005811, time: 2.248 s\n",
      "Step 144480 (epoch   45.15), loss: 0.008815, time: 2.249 s\n",
      "Step 144500 (epoch   45.16), loss: 0.007333, time: 2.252 s\n",
      "Step 144520 (epoch   45.16), loss: 0.008655, time: 2.261 s\n",
      "Step 144540 (epoch   45.17), loss: 0.005253, time: 2.256 s\n",
      "Step 144560 (epoch   45.17), loss: 0.004411, time: 2.248 s\n",
      "Step 144580 (epoch   45.18), loss: 0.004435, time: 2.254 s\n",
      "Step 144600 (epoch   45.19), loss: 0.007159, time: 2.247 s, error: 0.011831\n",
      "Step 144620 (epoch   45.19), loss: 0.006399, time: 15.024 s\n",
      "Step 144640 (epoch   45.20), loss: 0.005820, time: 2.235 s\n",
      "Step 144660 (epoch   45.21), loss: 0.004809, time: 2.249 s\n",
      "Step 144680 (epoch   45.21), loss: 0.007067, time: 2.243 s\n",
      "Step 144700 (epoch   45.22), loss: 0.006270, time: 2.251 s\n",
      "Step 144720 (epoch   45.23), loss: 0.003938, time: 2.244 s\n",
      "Step 144740 (epoch   45.23), loss: 0.006314, time: 2.243 s\n",
      "Step 144760 (epoch   45.24), loss: 0.003181, time: 2.259 s\n",
      "Step 144780 (epoch   45.24), loss: 0.007972, time: 2.255 s\n",
      "Step 144800 (epoch   45.25), loss: 0.005593, time: 2.253 s, error: 0.011669\n",
      "Step 144820 (epoch   45.26), loss: 0.008317, time: 15.053 s\n",
      "Step 144840 (epoch   45.26), loss: 0.003280, time: 2.246 s\n",
      "Step 144860 (epoch   45.27), loss: 0.006088, time: 2.247 s\n",
      "Step 144880 (epoch   45.27), loss: 0.004062, time: 2.244 s\n",
      "Step 144900 (epoch   45.28), loss: 0.004599, time: 2.262 s\n",
      "Step 144920 (epoch   45.29), loss: 0.005441, time: 2.241 s\n",
      "Step 144940 (epoch   45.29), loss: 0.006082, time: 2.238 s\n",
      "Step 144960 (epoch   45.30), loss: 0.005278, time: 2.234 s\n",
      "Step 144980 (epoch   45.31), loss: 0.008641, time: 2.249 s\n",
      "Step 145000 (epoch   45.31), loss: 0.007263, time: 2.238 s, error: 0.011709\n",
      "Step 145020 (epoch   45.32), loss: 0.005285, time: 15.264 s\n",
      "Step 145040 (epoch   45.33), loss: 0.007020, time: 2.258 s\n",
      "Step 145060 (epoch   45.33), loss: 0.005044, time: 2.256 s\n",
      "Step 145080 (epoch   45.34), loss: 0.002744, time: 2.251 s\n",
      "Step 145100 (epoch   45.34), loss: 0.005891, time: 2.240 s\n",
      "Step 145120 (epoch   45.35), loss: 0.006160, time: 2.254 s\n",
      "Step 145140 (epoch   45.36), loss: 0.004522, time: 2.247 s\n",
      "Step 145160 (epoch   45.36), loss: 0.007829, time: 2.254 s\n",
      "Step 145180 (epoch   45.37), loss: 0.006023, time: 2.243 s\n",
      "Step 145200 (epoch   45.38), loss: 0.005529, time: 2.252 s, error: 0.011984\n",
      "Step 145220 (epoch   45.38), loss: 0.006347, time: 15.241 s\n",
      "Step 145240 (epoch   45.39), loss: 0.004040, time: 2.252 s\n",
      "Step 145260 (epoch   45.39), loss: 0.005827, time: 2.244 s\n",
      "Step 145280 (epoch   45.40), loss: 0.007240, time: 2.240 s\n",
      "Step 145300 (epoch   45.41), loss: 0.009191, time: 2.256 s\n",
      "Step 145320 (epoch   45.41), loss: 0.006560, time: 2.254 s\n",
      "Step 145340 (epoch   45.42), loss: 0.004625, time: 2.255 s\n",
      "Step 145360 (epoch   45.42), loss: 0.003919, time: 2.250 s\n",
      "Step 145380 (epoch   45.43), loss: 0.005799, time: 2.252 s\n",
      "Step 145400 (epoch   45.44), loss: 0.003772, time: 2.230 s, error: 0.012159\n",
      "Step 145420 (epoch   45.44), loss: 0.004575, time: 15.024 s\n",
      "Step 145440 (epoch   45.45), loss: 0.005329, time: 2.265 s\n",
      "Step 145460 (epoch   45.46), loss: 0.006251, time: 2.251 s\n",
      "Step 145480 (epoch   45.46), loss: 0.004788, time: 2.247 s\n",
      "Step 145500 (epoch   45.47), loss: 0.005957, time: 2.246 s\n",
      "Step 145520 (epoch   45.48), loss: 0.005196, time: 2.233 s\n",
      "Step 145540 (epoch   45.48), loss: 0.004890, time: 2.245 s\n",
      "Step 145560 (epoch   45.49), loss: 0.004733, time: 2.235 s\n",
      "Step 145580 (epoch   45.49), loss: 0.004321, time: 2.249 s\n",
      "Step 145600 (epoch   45.50), loss: 0.007631, time: 2.250 s, error: 0.011469\n",
      "Step 145620 (epoch   45.51), loss: 0.006394, time: 15.108 s\n",
      "Step 145640 (epoch   45.51), loss: 0.006182, time: 2.257 s\n",
      "Step 145660 (epoch   45.52), loss: 0.007290, time: 2.241 s\n",
      "Step 145680 (epoch   45.52), loss: 0.003945, time: 2.251 s\n",
      "Step 145700 (epoch   45.53), loss: 0.004525, time: 2.257 s\n",
      "Step 145720 (epoch   45.54), loss: 0.005105, time: 2.259 s\n",
      "Step 145740 (epoch   45.54), loss: 0.004237, time: 2.241 s\n",
      "Step 145760 (epoch   45.55), loss: 0.003711, time: 2.249 s\n",
      "Step 145780 (epoch   45.56), loss: 0.006420, time: 2.241 s\n",
      "Step 145800 (epoch   45.56), loss: 0.006635, time: 2.243 s, error: 0.011513\n",
      "Step 145820 (epoch   45.57), loss: 0.008966, time: 15.062 s\n",
      "Step 145840 (epoch   45.58), loss: 0.005571, time: 2.247 s\n",
      "Step 145860 (epoch   45.58), loss: 0.005664, time: 2.250 s\n",
      "Step 145880 (epoch   45.59), loss: 0.004166, time: 2.254 s\n",
      "Step 145900 (epoch   45.59), loss: 0.004792, time: 2.236 s\n",
      "Step 145920 (epoch   45.60), loss: 0.006444, time: 2.255 s\n",
      "Step 145940 (epoch   45.61), loss: 0.006373, time: 2.234 s\n",
      "Step 145960 (epoch   45.61), loss: 0.004700, time: 2.251 s\n",
      "Step 145980 (epoch   45.62), loss: 0.006312, time: 2.260 s\n",
      "Step 146000 (epoch   45.62), loss: 0.011497, time: 2.237 s, error: 0.011711\n",
      "\n",
      "Time since beginning  : 25505.858 s\n",
      "\n",
      "Step 146020 (epoch   45.63), loss: 0.005126, time: 15.187 s\n",
      "Step 146040 (epoch   45.64), loss: 0.005324, time: 2.246 s\n",
      "Step 146060 (epoch   45.64), loss: 0.004905, time: 2.249 s\n",
      "Step 146080 (epoch   45.65), loss: 0.005444, time: 2.252 s\n",
      "Step 146100 (epoch   45.66), loss: 0.004935, time: 2.247 s\n",
      "Step 146120 (epoch   45.66), loss: 0.005034, time: 2.257 s\n",
      "Step 146140 (epoch   45.67), loss: 0.003672, time: 2.232 s\n",
      "Step 146160 (epoch   45.67), loss: 0.005284, time: 2.249 s\n",
      "Step 146180 (epoch   45.68), loss: 0.009265, time: 2.238 s\n",
      "Step 146200 (epoch   45.69), loss: 0.005073, time: 2.248 s, error: 0.012098\n",
      "Step 146220 (epoch   45.69), loss: 0.005315, time: 15.321 s\n",
      "Step 146240 (epoch   45.70), loss: 0.005276, time: 2.241 s\n",
      "Step 146260 (epoch   45.71), loss: 0.005275, time: 2.238 s\n",
      "Step 146280 (epoch   45.71), loss: 0.004622, time: 2.251 s\n",
      "Step 146300 (epoch   45.72), loss: 0.004318, time: 2.237 s\n",
      "Step 146320 (epoch   45.73), loss: 0.005606, time: 2.249 s\n",
      "Step 146340 (epoch   45.73), loss: 0.005609, time: 2.245 s\n",
      "Step 146360 (epoch   45.74), loss: 0.004460, time: 2.249 s\n",
      "Step 146380 (epoch   45.74), loss: 0.004858, time: 2.249 s\n",
      "Step 146400 (epoch   45.75), loss: 0.004216, time: 2.250 s, error: 0.011837\n",
      "Step 146420 (epoch   45.76), loss: 0.004134, time: 15.267 s\n",
      "Step 146440 (epoch   45.76), loss: 0.005040, time: 2.247 s\n",
      "Step 146460 (epoch   45.77), loss: 0.005247, time: 2.240 s\n",
      "Step 146480 (epoch   45.77), loss: 0.004825, time: 2.228 s\n",
      "Step 146500 (epoch   45.78), loss: 0.006689, time: 2.251 s\n",
      "Step 146520 (epoch   45.79), loss: 0.004536, time: 2.238 s\n",
      "Step 146540 (epoch   45.79), loss: 0.004656, time: 2.251 s\n",
      "Step 146560 (epoch   45.80), loss: 0.009567, time: 2.238 s\n",
      "Step 146580 (epoch   45.81), loss: 0.005570, time: 2.241 s\n",
      "Step 146600 (epoch   45.81), loss: 0.004356, time: 2.245 s, error: 0.011729\n",
      "Step 146620 (epoch   45.82), loss: 0.003712, time: 15.096 s\n",
      "Step 146640 (epoch   45.83), loss: 0.005603, time: 2.263 s\n",
      "Step 146660 (epoch   45.83), loss: 0.007523, time: 2.246 s\n",
      "Step 146680 (epoch   45.84), loss: 0.003590, time: 2.254 s\n",
      "Step 146700 (epoch   45.84), loss: 0.004683, time: 2.246 s\n",
      "Step 146720 (epoch   45.85), loss: 0.006211, time: 2.253 s\n",
      "Step 146740 (epoch   45.86), loss: 0.004215, time: 2.251 s\n",
      "Step 146760 (epoch   45.86), loss: 0.007029, time: 2.246 s\n",
      "Step 146780 (epoch   45.87), loss: 0.004768, time: 2.256 s\n",
      "Step 146800 (epoch   45.88), loss: 0.005357, time: 2.245 s, error: 0.011833\n",
      "Step 146820 (epoch   45.88), loss: 0.005209, time: 15.149 s\n",
      "Step 146840 (epoch   45.89), loss: 0.004658, time: 2.251 s\n",
      "Step 146860 (epoch   45.89), loss: 0.008101, time: 2.255 s\n",
      "Step 146880 (epoch   45.90), loss: 0.005920, time: 2.239 s\n",
      "Step 146900 (epoch   45.91), loss: 0.006485, time: 2.261 s\n",
      "Step 146920 (epoch   45.91), loss: 0.006405, time: 2.242 s\n",
      "Step 146940 (epoch   45.92), loss: 0.004328, time: 2.261 s\n",
      "Step 146960 (epoch   45.92), loss: 0.004840, time: 2.250 s\n",
      "Step 146980 (epoch   45.93), loss: 0.004145, time: 2.247 s\n",
      "Step 147000 (epoch   45.94), loss: 0.004421, time: 2.246 s, error: 0.011366\n",
      "Step 147020 (epoch   45.94), loss: 0.005401, time: 15.187 s\n",
      "Step 147040 (epoch   45.95), loss: 0.005237, time: 2.240 s\n",
      "Step 147060 (epoch   45.96), loss: 0.005481, time: 2.245 s\n",
      "Step 147080 (epoch   45.96), loss: 0.006423, time: 2.260 s\n",
      "Step 147100 (epoch   45.97), loss: 0.008466, time: 2.238 s\n",
      "Step 147120 (epoch   45.98), loss: 0.004979, time: 2.249 s\n",
      "Step 147140 (epoch   45.98), loss: 0.004599, time: 2.249 s\n",
      "Step 147160 (epoch   45.99), loss: 0.006253, time: 2.261 s\n",
      "Step 147180 (epoch   45.99), loss: 0.005341, time: 2.241 s\n",
      "Step 147200 (epoch   46.00), loss: 0.005313, time: 2.251 s, error: 0.011634\n",
      "Step 147220 (epoch   46.01), loss: 0.004459, time: 15.106 s\n",
      "Step 147240 (epoch   46.01), loss: 0.006119, time: 2.238 s\n",
      "Step 147260 (epoch   46.02), loss: 0.005421, time: 2.249 s\n",
      "Step 147280 (epoch   46.02), loss: 0.005330, time: 2.241 s\n",
      "Step 147300 (epoch   46.03), loss: 0.005190, time: 2.248 s\n",
      "Step 147320 (epoch   46.04), loss: 0.003677, time: 2.243 s\n",
      "Step 147340 (epoch   46.04), loss: 0.005622, time: 2.247 s\n",
      "Step 147360 (epoch   46.05), loss: 0.008084, time: 2.247 s\n",
      "Step 147380 (epoch   46.06), loss: 0.008345, time: 2.256 s\n",
      "Step 147400 (epoch   46.06), loss: 0.003998, time: 2.259 s, error: 0.011974\n",
      "Step 147420 (epoch   46.07), loss: 0.008755, time: 15.309 s\n",
      "Step 147440 (epoch   46.08), loss: 0.004051, time: 2.250 s\n",
      "Step 147460 (epoch   46.08), loss: 0.005807, time: 2.250 s\n",
      "Step 147480 (epoch   46.09), loss: 0.007651, time: 2.256 s\n",
      "Step 147500 (epoch   46.09), loss: 0.005410, time: 2.254 s\n",
      "Step 147520 (epoch   46.10), loss: 0.004529, time: 2.255 s\n",
      "Step 147540 (epoch   46.11), loss: 0.004117, time: 2.240 s\n",
      "Step 147560 (epoch   46.11), loss: 0.004602, time: 2.249 s\n",
      "Step 147580 (epoch   46.12), loss: 0.004450, time: 2.237 s\n",
      "Step 147600 (epoch   46.12), loss: 0.004849, time: 2.252 s, error: 0.011630\n",
      "Step 147620 (epoch   46.13), loss: 0.005930, time: 15.246 s\n",
      "Step 147640 (epoch   46.14), loss: 0.003826, time: 2.243 s\n",
      "Step 147660 (epoch   46.14), loss: 0.005749, time: 2.256 s\n",
      "Step 147680 (epoch   46.15), loss: 0.007296, time: 2.241 s\n",
      "Step 147700 (epoch   46.16), loss: 0.008242, time: 2.248 s\n",
      "Step 147720 (epoch   46.16), loss: 0.004211, time: 2.230 s\n",
      "Step 147740 (epoch   46.17), loss: 0.005878, time: 2.246 s\n",
      "Step 147760 (epoch   46.17), loss: 0.008351, time: 2.242 s\n",
      "Step 147780 (epoch   46.18), loss: 0.005226, time: 2.250 s\n",
      "Step 147800 (epoch   46.19), loss: 0.005295, time: 2.242 s, error: 0.011955\n",
      "Step 147820 (epoch   46.19), loss: 0.004589, time: 15.156 s\n",
      "Step 147840 (epoch   46.20), loss: 0.004977, time: 2.256 s\n",
      "Step 147860 (epoch   46.21), loss: 0.003826, time: 2.240 s\n",
      "Step 147880 (epoch   46.21), loss: 0.009211, time: 2.252 s\n",
      "Step 147900 (epoch   46.22), loss: 0.006818, time: 2.245 s\n",
      "Step 147920 (epoch   46.23), loss: 0.005012, time: 2.239 s\n",
      "Step 147940 (epoch   46.23), loss: 0.010392, time: 2.233 s\n",
      "Step 147960 (epoch   46.24), loss: 0.006183, time: 2.247 s\n",
      "Step 147980 (epoch   46.24), loss: 0.006377, time: 2.233 s\n",
      "Step 148000 (epoch   46.25), loss: 0.004966, time: 2.248 s, error: 0.011693\n",
      "\n",
      "Time since beginning  : 25860.073 s\n",
      "\n",
      "Step 148020 (epoch   46.26), loss: 0.006988, time: 15.162 s\n",
      "Step 148040 (epoch   46.26), loss: 0.006163, time: 2.242 s\n",
      "Step 148060 (epoch   46.27), loss: 0.006757, time: 2.255 s\n",
      "Step 148080 (epoch   46.27), loss: 0.006661, time: 2.262 s\n",
      "Step 148100 (epoch   46.28), loss: 0.006811, time: 2.243 s\n",
      "Step 148120 (epoch   46.29), loss: 0.005307, time: 2.254 s\n",
      "Step 148140 (epoch   46.29), loss: 0.005499, time: 2.248 s\n",
      "Step 148160 (epoch   46.30), loss: 0.004992, time: 2.245 s\n",
      "Step 148180 (epoch   46.31), loss: 0.003638, time: 2.246 s\n",
      "Step 148200 (epoch   46.31), loss: 0.004382, time: 2.257 s, error: 0.011477\n",
      "Step 148220 (epoch   46.32), loss: 0.004342, time: 15.156 s\n",
      "Step 148240 (epoch   46.33), loss: 0.006560, time: 2.248 s\n",
      "Step 148260 (epoch   46.33), loss: 0.003685, time: 2.239 s\n",
      "Step 148280 (epoch   46.34), loss: 0.004338, time: 2.239 s\n",
      "Step 148300 (epoch   46.34), loss: 0.006471, time: 2.243 s\n",
      "Step 148320 (epoch   46.35), loss: 0.004002, time: 2.245 s\n",
      "Step 148340 (epoch   46.36), loss: 0.009380, time: 2.260 s\n",
      "Step 148360 (epoch   46.36), loss: 0.006230, time: 2.233 s\n",
      "Step 148380 (epoch   46.37), loss: 0.006058, time: 2.253 s\n",
      "Step 148400 (epoch   46.38), loss: 0.006279, time: 2.246 s, error: 0.012059\n",
      "Step 148420 (epoch   46.38), loss: 0.005278, time: 15.150 s\n",
      "Step 148440 (epoch   46.39), loss: 0.007163, time: 2.252 s\n",
      "Step 148460 (epoch   46.39), loss: 0.005259, time: 2.247 s\n",
      "Step 148480 (epoch   46.40), loss: 0.003929, time: 2.249 s\n",
      "Step 148500 (epoch   46.41), loss: 0.006530, time: 2.249 s\n",
      "Step 148520 (epoch   46.41), loss: 0.004007, time: 2.238 s\n",
      "Step 148540 (epoch   46.42), loss: 0.005734, time: 2.240 s\n",
      "Step 148560 (epoch   46.42), loss: 0.004396, time: 2.242 s\n",
      "Step 148580 (epoch   46.43), loss: 0.003552, time: 2.247 s\n",
      "Step 148600 (epoch   46.44), loss: 0.005753, time: 2.260 s, error: 0.012125\n",
      "Step 148620 (epoch   46.44), loss: 0.007367, time: 15.090 s\n",
      "Step 148640 (epoch   46.45), loss: 0.016126, time: 2.250 s\n",
      "Step 148660 (epoch   46.46), loss: 0.008417, time: 2.228 s\n",
      "Step 148680 (epoch   46.46), loss: 0.004929, time: 2.245 s\n",
      "Step 148700 (epoch   46.47), loss: 0.008287, time: 2.246 s\n",
      "Step 148720 (epoch   46.48), loss: 0.004520, time: 2.253 s\n",
      "Step 148740 (epoch   46.48), loss: 0.004919, time: 2.230 s\n",
      "Step 148760 (epoch   46.49), loss: 0.003325, time: 2.242 s\n",
      "Step 148780 (epoch   46.49), loss: 0.006357, time: 2.236 s\n",
      "Step 148800 (epoch   46.50), loss: 0.005832, time: 2.235 s, error: 0.011419\n",
      "Step 148820 (epoch   46.51), loss: 0.004381, time: 15.269 s\n",
      "Step 148840 (epoch   46.51), loss: 0.005717, time: 2.247 s\n",
      "Step 148860 (epoch   46.52), loss: 0.003260, time: 2.242 s\n",
      "Step 148880 (epoch   46.52), loss: 0.004613, time: 2.243 s\n",
      "Step 148900 (epoch   46.53), loss: 0.007168, time: 2.252 s\n",
      "Step 148920 (epoch   46.54), loss: 0.005958, time: 2.239 s\n",
      "Step 148940 (epoch   46.54), loss: 0.004788, time: 2.245 s\n",
      "Step 148960 (epoch   46.55), loss: 0.006336, time: 2.257 s\n",
      "Step 148980 (epoch   46.56), loss: 0.005376, time: 2.230 s\n",
      "Step 149000 (epoch   46.56), loss: 0.004935, time: 2.251 s, error: 0.011553\n",
      "Step 149020 (epoch   46.57), loss: 0.006944, time: 15.285 s\n",
      "Step 149040 (epoch   46.58), loss: 0.010799, time: 2.239 s\n",
      "Step 149060 (epoch   46.58), loss: 0.012689, time: 2.230 s\n",
      "Step 149080 (epoch   46.59), loss: 0.006061, time: 2.249 s\n",
      "Step 149100 (epoch   46.59), loss: 0.004651, time: 2.232 s\n",
      "Step 149120 (epoch   46.60), loss: 0.006697, time: 2.252 s\n",
      "Step 149140 (epoch   46.61), loss: 0.006730, time: 2.255 s\n",
      "Step 149160 (epoch   46.61), loss: 0.006925, time: 2.245 s\n",
      "Step 149180 (epoch   46.62), loss: 0.007602, time: 2.243 s\n",
      "Step 149200 (epoch   46.62), loss: 0.004075, time: 2.246 s, error: 0.011648\n",
      "Step 149220 (epoch   46.63), loss: 0.004747, time: 15.093 s\n",
      "Step 149240 (epoch   46.64), loss: 0.004687, time: 2.255 s\n",
      "Step 149260 (epoch   46.64), loss: 0.008262, time: 2.247 s\n",
      "Step 149280 (epoch   46.65), loss: 0.004249, time: 2.256 s\n",
      "Step 149300 (epoch   46.66), loss: 0.003808, time: 2.233 s\n",
      "Step 149320 (epoch   46.66), loss: 0.005736, time: 2.254 s\n",
      "Step 149340 (epoch   46.67), loss: 0.004329, time: 2.244 s\n",
      "Step 149360 (epoch   46.67), loss: 0.008120, time: 2.252 s\n",
      "Step 149380 (epoch   46.68), loss: 0.005237, time: 2.243 s\n",
      "Step 149400 (epoch   46.69), loss: 0.005945, time: 2.256 s, error: 0.011981\n",
      "Step 149420 (epoch   46.69), loss: 0.004647, time: 15.246 s\n",
      "Step 149440 (epoch   46.70), loss: 0.007025, time: 2.246 s\n",
      "Step 149460 (epoch   46.71), loss: 0.004856, time: 2.235 s\n",
      "Step 149480 (epoch   46.71), loss: 0.006575, time: 2.242 s\n",
      "Step 149500 (epoch   46.72), loss: 0.005489, time: 2.254 s\n",
      "Step 149520 (epoch   46.73), loss: 0.005759, time: 2.245 s\n",
      "Step 149540 (epoch   46.73), loss: 0.005390, time: 2.252 s\n",
      "Step 149560 (epoch   46.74), loss: 0.004931, time: 2.239 s\n",
      "Step 149580 (epoch   46.74), loss: 0.004878, time: 2.253 s\n",
      "Step 149600 (epoch   46.75), loss: 0.004526, time: 2.244 s, error: 0.011774\n",
      "Step 149620 (epoch   46.76), loss: 0.004348, time: 15.042 s\n",
      "Step 149640 (epoch   46.76), loss: 0.006331, time: 2.249 s\n",
      "Step 149660 (epoch   46.77), loss: 0.009159, time: 2.237 s\n",
      "Step 149680 (epoch   46.77), loss: 0.004995, time: 2.251 s\n",
      "Step 149700 (epoch   46.78), loss: 0.005994, time: 2.247 s\n",
      "Step 149720 (epoch   46.79), loss: 0.004165, time: 2.243 s\n",
      "Step 149740 (epoch   46.79), loss: 0.002520, time: 2.243 s\n",
      "Step 149760 (epoch   46.80), loss: 0.007507, time: 2.252 s\n",
      "Step 149780 (epoch   46.81), loss: 0.008571, time: 2.255 s\n",
      "Step 149800 (epoch   46.81), loss: 0.003723, time: 2.258 s, error: 0.011639\n",
      "Step 149820 (epoch   46.82), loss: 0.004559, time: 15.167 s\n",
      "Step 149840 (epoch   46.83), loss: 0.007356, time: 2.257 s\n",
      "Step 149860 (epoch   46.83), loss: 0.009354, time: 2.252 s\n",
      "Step 149880 (epoch   46.84), loss: 0.007376, time: 2.247 s\n",
      "Step 149900 (epoch   46.84), loss: 0.004534, time: 2.235 s\n",
      "Step 149920 (epoch   46.85), loss: 0.009543, time: 2.241 s\n",
      "Step 149940 (epoch   46.86), loss: 0.007187, time: 2.250 s\n",
      "Step 149960 (epoch   46.86), loss: 0.004400, time: 2.246 s\n",
      "Step 149980 (epoch   46.87), loss: 0.004501, time: 2.249 s\n",
      "Step 150000 (epoch   46.88), loss: 0.004208, time: 2.246 s, error: 0.011610\n",
      "\n",
      "Time since beginning  : 26214.065 s\n",
      "\n",
      "Step 150020 (epoch   46.88), loss: 0.003652, time: 15.361 s\n",
      "Step 150040 (epoch   46.89), loss: 0.006854, time: 2.249 s\n",
      "Step 150060 (epoch   46.89), loss: 0.005385, time: 2.248 s\n",
      "Step 150080 (epoch   46.90), loss: 0.007416, time: 2.244 s\n",
      "Step 150100 (epoch   46.91), loss: 0.005037, time: 2.249 s\n",
      "Step 150120 (epoch   46.91), loss: 0.006829, time: 2.252 s\n",
      "Step 150140 (epoch   46.92), loss: 0.008498, time: 2.250 s\n",
      "Step 150160 (epoch   46.92), loss: 0.008153, time: 2.251 s\n",
      "Step 150180 (epoch   46.93), loss: 0.005955, time: 2.236 s\n",
      "Step 150200 (epoch   46.94), loss: 0.005700, time: 2.241 s, error: 0.011324\n",
      "Step 150220 (epoch   46.94), loss: 0.004702, time: 15.248 s\n",
      "Step 150240 (epoch   46.95), loss: 0.005212, time: 2.246 s\n",
      "Step 150260 (epoch   46.96), loss: 0.004869, time: 2.243 s\n",
      "Step 150280 (epoch   46.96), loss: 0.003260, time: 2.248 s\n",
      "Step 150300 (epoch   46.97), loss: 0.007936, time: 2.247 s\n",
      "Step 150320 (epoch   46.98), loss: 0.005736, time: 2.248 s\n",
      "Step 150340 (epoch   46.98), loss: 0.010520, time: 2.255 s\n",
      "Step 150360 (epoch   46.99), loss: 0.004903, time: 2.245 s\n",
      "Step 150380 (epoch   46.99), loss: 0.009845, time: 2.256 s\n",
      "Step 150400 (epoch   47.00), loss: 0.005229, time: 2.254 s, error: 0.011477\n",
      "Step 150420 (epoch   47.01), loss: 0.004886, time: 15.212 s\n",
      "Step 150440 (epoch   47.01), loss: 0.006028, time: 2.260 s\n",
      "Step 150460 (epoch   47.02), loss: 0.004684, time: 2.249 s\n",
      "Step 150480 (epoch   47.02), loss: 0.004513, time: 2.238 s\n",
      "Step 150500 (epoch   47.03), loss: 0.007572, time: 2.258 s\n",
      "Step 150520 (epoch   47.04), loss: 0.006173, time: 2.248 s\n",
      "Step 150540 (epoch   47.04), loss: 0.005870, time: 2.246 s\n",
      "Step 150560 (epoch   47.05), loss: 0.009729, time: 2.232 s\n",
      "Step 150580 (epoch   47.06), loss: 0.004605, time: 2.249 s\n",
      "Step 150600 (epoch   47.06), loss: 0.004996, time: 2.249 s, error: 0.011724\n",
      "Step 150620 (epoch   47.07), loss: 0.006771, time: 15.063 s\n",
      "Step 150640 (epoch   47.08), loss: 0.005360, time: 2.245 s\n",
      "Step 150660 (epoch   47.08), loss: 0.003751, time: 2.242 s\n",
      "Step 150680 (epoch   47.09), loss: 0.005768, time: 2.228 s\n",
      "Step 150700 (epoch   47.09), loss: 0.007597, time: 2.249 s\n",
      "Step 150720 (epoch   47.10), loss: 0.005669, time: 2.253 s\n",
      "Step 150740 (epoch   47.11), loss: 0.009468, time: 2.247 s\n",
      "Step 150760 (epoch   47.11), loss: 0.013279, time: 2.257 s\n",
      "Step 150780 (epoch   47.12), loss: 0.005092, time: 2.255 s\n",
      "Step 150800 (epoch   47.12), loss: 0.004649, time: 2.246 s, error: 0.011555\n",
      "Step 150820 (epoch   47.13), loss: 0.008440, time: 15.059 s\n",
      "Step 150840 (epoch   47.14), loss: 0.007351, time: 2.250 s\n",
      "Step 150860 (epoch   47.14), loss: 0.005866, time: 2.237 s\n",
      "Step 150880 (epoch   47.15), loss: 0.005598, time: 2.232 s\n",
      "Step 150900 (epoch   47.16), loss: 0.004702, time: 2.257 s\n",
      "Step 150920 (epoch   47.16), loss: 0.006685, time: 2.245 s\n",
      "Step 150940 (epoch   47.17), loss: 0.005384, time: 2.230 s\n",
      "Step 150960 (epoch   47.17), loss: 0.005988, time: 2.255 s\n",
      "Step 150980 (epoch   47.18), loss: 0.008794, time: 2.246 s\n",
      "Step 151000 (epoch   47.19), loss: 0.008206, time: 2.252 s, error: 0.011990\n",
      "Step 151020 (epoch   47.19), loss: 0.004118, time: 15.126 s\n",
      "Step 151040 (epoch   47.20), loss: 0.005056, time: 2.250 s\n",
      "Step 151060 (epoch   47.21), loss: 0.006226, time: 2.250 s\n",
      "Step 151080 (epoch   47.21), loss: 0.005920, time: 2.249 s\n",
      "Step 151100 (epoch   47.22), loss: 0.004499, time: 2.257 s\n",
      "Step 151120 (epoch   47.23), loss: 0.006746, time: 2.243 s\n",
      "Step 151140 (epoch   47.23), loss: 0.004780, time: 2.254 s\n",
      "Step 151160 (epoch   47.24), loss: 0.006404, time: 2.246 s\n",
      "Step 151180 (epoch   47.24), loss: 0.006189, time: 2.246 s\n",
      "Step 151200 (epoch   47.25), loss: 0.004291, time: 2.246 s, error: 0.011870\n",
      "Step 151220 (epoch   47.26), loss: 0.004518, time: 15.286 s\n",
      "Step 151240 (epoch   47.26), loss: 0.005154, time: 2.257 s\n",
      "Step 151260 (epoch   47.27), loss: 0.007035, time: 2.245 s\n",
      "Step 151280 (epoch   47.27), loss: 0.007244, time: 2.248 s\n",
      "Step 151300 (epoch   47.28), loss: 0.007185, time: 2.236 s\n",
      "Step 151320 (epoch   47.29), loss: 0.004391, time: 2.254 s\n",
      "Step 151340 (epoch   47.29), loss: 0.005439, time: 2.255 s\n",
      "Step 151360 (epoch   47.30), loss: 0.009653, time: 2.250 s\n",
      "Step 151380 (epoch   47.31), loss: 0.005358, time: 2.241 s\n",
      "Step 151400 (epoch   47.31), loss: 0.004098, time: 2.241 s, error: 0.011431\n",
      "Step 151420 (epoch   47.32), loss: 0.004571, time: 15.258 s\n",
      "Step 151440 (epoch   47.33), loss: 0.011083, time: 2.243 s\n",
      "Step 151460 (epoch   47.33), loss: 0.005396, time: 2.229 s\n",
      "Step 151480 (epoch   47.34), loss: 0.004401, time: 2.225 s\n",
      "Step 151500 (epoch   47.34), loss: 0.003953, time: 2.241 s\n",
      "Step 151520 (epoch   47.35), loss: 0.005611, time: 2.243 s\n",
      "Step 151540 (epoch   47.36), loss: 0.005589, time: 2.243 s\n",
      "Step 151560 (epoch   47.36), loss: 0.004147, time: 2.247 s\n",
      "Step 151580 (epoch   47.37), loss: 0.005033, time: 2.248 s\n",
      "Step 151600 (epoch   47.38), loss: 0.004238, time: 2.236 s, error: 0.012049\n",
      "Step 151620 (epoch   47.38), loss: 0.005252, time: 15.088 s\n",
      "Step 151640 (epoch   47.39), loss: 0.006734, time: 2.259 s\n",
      "Step 151660 (epoch   47.39), loss: 0.011580, time: 2.242 s\n",
      "Step 151680 (epoch   47.40), loss: 0.006153, time: 2.242 s\n",
      "Step 151700 (epoch   47.41), loss: 0.007550, time: 2.248 s\n",
      "Step 151720 (epoch   47.41), loss: 0.006598, time: 2.248 s\n",
      "Step 151740 (epoch   47.42), loss: 0.005036, time: 2.243 s\n",
      "Step 151760 (epoch   47.42), loss: 0.004836, time: 2.240 s\n",
      "Step 151780 (epoch   47.43), loss: 0.003869, time: 2.235 s\n",
      "Step 151800 (epoch   47.44), loss: 0.006590, time: 2.247 s, error: 0.011604\n",
      "Step 151820 (epoch   47.44), loss: 0.006146, time: 15.044 s\n",
      "Step 151840 (epoch   47.45), loss: 0.011293, time: 2.243 s\n",
      "Step 151860 (epoch   47.46), loss: 0.008562, time: 2.234 s\n",
      "Step 151880 (epoch   47.46), loss: 0.003421, time: 2.256 s\n",
      "Step 151900 (epoch   47.47), loss: 0.003145, time: 2.262 s\n",
      "Step 151920 (epoch   47.48), loss: 0.005348, time: 2.246 s\n",
      "Step 151940 (epoch   47.48), loss: 0.005066, time: 2.248 s\n",
      "Step 151960 (epoch   47.49), loss: 0.005505, time: 2.237 s\n",
      "Step 151980 (epoch   47.49), loss: 0.004124, time: 2.238 s\n",
      "Step 152000 (epoch   47.50), loss: 0.006020, time: 2.237 s, error: 0.011444\n",
      "\n",
      "Time since beginning  : 26567.766 s\n",
      "\n",
      "Step 152020 (epoch   47.51), loss: 0.004686, time: 15.143 s\n",
      "Step 152040 (epoch   47.51), loss: 0.005912, time: 2.242 s\n",
      "Step 152060 (epoch   47.52), loss: 0.004024, time: 2.243 s\n",
      "Step 152080 (epoch   47.52), loss: 0.008199, time: 2.252 s\n",
      "Step 152100 (epoch   47.53), loss: 0.007504, time: 2.243 s\n",
      "Step 152120 (epoch   47.54), loss: 0.004593, time: 2.244 s\n",
      "Step 152140 (epoch   47.54), loss: 0.003414, time: 2.257 s\n",
      "Step 152160 (epoch   47.55), loss: 0.005990, time: 2.257 s\n",
      "Step 152180 (epoch   47.56), loss: 0.006630, time: 2.239 s\n",
      "Step 152200 (epoch   47.56), loss: 0.004113, time: 2.246 s, error: 0.011449\n",
      "Step 152220 (epoch   47.57), loss: 0.005213, time: 15.074 s\n",
      "Step 152240 (epoch   47.58), loss: 0.007445, time: 2.238 s\n",
      "Step 152260 (epoch   47.58), loss: 0.007088, time: 2.237 s\n",
      "Step 152280 (epoch   47.59), loss: 0.008889, time: 2.246 s\n",
      "Step 152300 (epoch   47.59), loss: 0.003883, time: 2.247 s\n",
      "Step 152320 (epoch   47.60), loss: 0.003281, time: 2.220 s\n",
      "Step 152340 (epoch   47.61), loss: 0.005839, time: 2.251 s\n",
      "Step 152360 (epoch   47.61), loss: 0.007991, time: 2.241 s\n",
      "Step 152380 (epoch   47.62), loss: 0.004636, time: 2.247 s\n",
      "Step 152400 (epoch   47.62), loss: 0.007553, time: 2.249 s, error: 0.011545\n",
      "Step 152420 (epoch   47.63), loss: 0.006582, time: 15.203 s\n",
      "Step 152440 (epoch   47.64), loss: 0.004711, time: 2.239 s\n",
      "Step 152460 (epoch   47.64), loss: 0.006550, time: 2.245 s\n",
      "Step 152480 (epoch   47.65), loss: 0.007832, time: 2.234 s\n",
      "Step 152500 (epoch   47.66), loss: 0.005067, time: 2.240 s\n",
      "Step 152520 (epoch   47.66), loss: 0.005449, time: 2.246 s\n",
      "Step 152540 (epoch   47.67), loss: 0.004093, time: 2.245 s\n",
      "Step 152560 (epoch   47.67), loss: 0.020644, time: 2.240 s\n",
      "Step 152580 (epoch   47.68), loss: 0.006291, time: 2.229 s\n",
      "Step 152600 (epoch   47.69), loss: 0.005867, time: 2.248 s, error: 0.011866\n",
      "Step 152620 (epoch   47.69), loss: 0.006470, time: 15.340 s\n",
      "Step 152640 (epoch   47.70), loss: 0.005807, time: 2.240 s\n",
      "Step 152660 (epoch   47.71), loss: 0.007297, time: 2.253 s\n",
      "Step 152680 (epoch   47.71), loss: 0.004705, time: 2.230 s\n",
      "Step 152700 (epoch   47.72), loss: 0.005602, time: 2.238 s\n",
      "Step 152720 (epoch   47.73), loss: 0.005284, time: 2.239 s\n",
      "Step 152740 (epoch   47.73), loss: 0.004380, time: 2.243 s\n",
      "Step 152760 (epoch   47.74), loss: 0.008126, time: 2.243 s\n",
      "Step 152780 (epoch   47.74), loss: 0.006737, time: 2.231 s\n",
      "Step 152800 (epoch   47.75), loss: 0.007672, time: 2.240 s, error: 0.011680\n",
      "Step 152820 (epoch   47.76), loss: 0.006165, time: 15.124 s\n",
      "Step 152840 (epoch   47.76), loss: 0.005465, time: 2.249 s\n",
      "Step 152860 (epoch   47.77), loss: 0.003769, time: 2.231 s\n",
      "Step 152880 (epoch   47.77), loss: 0.004066, time: 2.245 s\n",
      "Step 152900 (epoch   47.78), loss: 0.005351, time: 2.241 s\n",
      "Step 152920 (epoch   47.79), loss: 0.006628, time: 2.246 s\n",
      "Step 152940 (epoch   47.79), loss: 0.004470, time: 2.236 s\n",
      "Step 152960 (epoch   47.80), loss: 0.008390, time: 2.240 s\n",
      "Step 152980 (epoch   47.81), loss: 0.005758, time: 2.247 s\n",
      "Step 153000 (epoch   47.81), loss: 0.004435, time: 2.235 s, error: 0.011642\n",
      "Step 153020 (epoch   47.82), loss: 0.005659, time: 15.065 s\n",
      "Step 153040 (epoch   47.83), loss: 0.007985, time: 2.255 s\n",
      "Step 153060 (epoch   47.83), loss: 0.007782, time: 2.256 s\n",
      "Step 153080 (epoch   47.84), loss: 0.003152, time: 2.261 s\n",
      "Step 153100 (epoch   47.84), loss: 0.003463, time: 2.237 s\n",
      "Step 153120 (epoch   47.85), loss: 0.005115, time: 2.233 s\n",
      "Step 153140 (epoch   47.86), loss: 0.006606, time: 2.220 s\n",
      "Step 153160 (epoch   47.86), loss: 0.002908, time: 2.247 s\n",
      "Step 153180 (epoch   47.87), loss: 0.005042, time: 2.226 s\n",
      "Step 153200 (epoch   47.88), loss: 0.005170, time: 2.251 s, error: 0.011384\n",
      "Step 153220 (epoch   47.88), loss: 0.006237, time: 15.118 s\n",
      "Step 153240 (epoch   47.89), loss: 0.004441, time: 2.248 s\n",
      "Step 153260 (epoch   47.89), loss: 0.004736, time: 2.243 s\n",
      "Step 153280 (epoch   47.90), loss: 0.008451, time: 2.246 s\n",
      "Step 153300 (epoch   47.91), loss: 0.006313, time: 2.243 s\n",
      "Step 153320 (epoch   47.91), loss: 0.005829, time: 2.255 s\n",
      "Step 153340 (epoch   47.92), loss: 0.005811, time: 2.250 s\n",
      "Step 153360 (epoch   47.92), loss: 0.003937, time: 2.248 s\n",
      "Step 153380 (epoch   47.93), loss: 0.004028, time: 2.248 s\n",
      "Step 153400 (epoch   47.94), loss: 0.005255, time: 2.238 s, error: 0.011279\n",
      "Step 153420 (epoch   47.94), loss: 0.005247, time: 15.044 s\n",
      "Step 153440 (epoch   47.95), loss: 0.006100, time: 2.239 s\n",
      "Step 153460 (epoch   47.96), loss: 0.007452, time: 2.247 s\n",
      "Step 153480 (epoch   47.96), loss: 0.004364, time: 2.247 s\n",
      "Step 153500 (epoch   47.97), loss: 0.004381, time: 2.251 s\n",
      "Step 153520 (epoch   47.98), loss: 0.008897, time: 2.238 s\n",
      "Step 153540 (epoch   47.98), loss: 0.004342, time: 2.246 s\n",
      "Step 153560 (epoch   47.99), loss: 0.006528, time: 2.237 s\n",
      "Step 153580 (epoch   47.99), loss: 0.003645, time: 2.254 s\n",
      "Step 153600 (epoch   48.00), loss: 0.007790, time: 2.256 s, error: 0.011373\n",
      "Step 153620 (epoch   48.01), loss: 0.011047, time: 15.069 s\n",
      "Step 153640 (epoch   48.01), loss: 0.008746, time: 2.246 s\n",
      "Step 153660 (epoch   48.02), loss: 0.004013, time: 2.233 s\n",
      "Step 153680 (epoch   48.02), loss: 0.004138, time: 2.235 s\n",
      "Step 153700 (epoch   48.03), loss: 0.006212, time: 2.243 s\n",
      "Step 153720 (epoch   48.04), loss: 0.006432, time: 2.244 s\n",
      "Step 153740 (epoch   48.04), loss: 0.004162, time: 2.238 s\n",
      "Step 153760 (epoch   48.05), loss: 0.008450, time: 2.250 s\n",
      "Step 153780 (epoch   48.06), loss: 0.004385, time: 2.229 s\n",
      "Step 153800 (epoch   48.06), loss: 0.005343, time: 2.240 s, error: 0.011799\n",
      "Step 153820 (epoch   48.07), loss: 0.008510, time: 15.281 s\n",
      "Step 153840 (epoch   48.08), loss: 0.007291, time: 2.239 s\n",
      "Step 153860 (epoch   48.08), loss: 0.005409, time: 2.242 s\n",
      "Step 153880 (epoch   48.09), loss: 0.004080, time: 2.235 s\n",
      "Step 153900 (epoch   48.09), loss: 0.004807, time: 2.242 s\n",
      "Step 153920 (epoch   48.10), loss: 0.004641, time: 2.234 s\n",
      "Step 153940 (epoch   48.11), loss: 0.004328, time: 2.252 s\n",
      "Step 153960 (epoch   48.11), loss: 0.008883, time: 2.224 s\n",
      "Step 153980 (epoch   48.12), loss: 0.004761, time: 2.252 s\n",
      "Step 154000 (epoch   48.12), loss: 0.003347, time: 2.240 s, error: 0.011503\n",
      "\n",
      "Time since beginning  : 26921.249 s\n",
      "\n",
      "Step 154020 (epoch   48.13), loss: 0.005879, time: 15.346 s\n",
      "Step 154040 (epoch   48.14), loss: 0.005148, time: 2.241 s\n",
      "Step 154060 (epoch   48.14), loss: 0.004997, time: 2.241 s\n",
      "Step 154080 (epoch   48.15), loss: 0.009797, time: 2.241 s\n",
      "Step 154100 (epoch   48.16), loss: 0.006063, time: 2.239 s\n",
      "Step 154120 (epoch   48.16), loss: 0.004907, time: 2.253 s\n",
      "Step 154140 (epoch   48.17), loss: 0.006080, time: 2.240 s\n",
      "Step 154160 (epoch   48.17), loss: 0.005135, time: 2.249 s\n",
      "Step 154180 (epoch   48.18), loss: 0.004418, time: 2.243 s\n",
      "Step 154200 (epoch   48.19), loss: 0.010226, time: 2.250 s, error: 0.012057\n",
      "Step 154220 (epoch   48.19), loss: 0.008062, time: 15.078 s\n",
      "Step 154240 (epoch   48.20), loss: 0.005159, time: 2.239 s\n",
      "Step 154260 (epoch   48.21), loss: 0.003683, time: 2.252 s\n",
      "Step 154280 (epoch   48.21), loss: 0.005603, time: 2.241 s\n",
      "Step 154300 (epoch   48.22), loss: 0.006319, time: 2.238 s\n",
      "Step 154320 (epoch   48.23), loss: 0.003847, time: 2.249 s\n",
      "Step 154340 (epoch   48.23), loss: 0.006322, time: 2.247 s\n",
      "Step 154360 (epoch   48.24), loss: 0.003264, time: 2.239 s\n",
      "Step 154380 (epoch   48.24), loss: 0.005096, time: 2.251 s\n",
      "Step 154400 (epoch   48.25), loss: 0.004302, time: 2.236 s, error: 0.011720\n",
      "Step 154420 (epoch   48.26), loss: 0.004882, time: 15.116 s\n",
      "Step 154440 (epoch   48.26), loss: 0.006736, time: 2.241 s\n",
      "Step 154460 (epoch   48.27), loss: 0.006456, time: 2.223 s\n",
      "Step 154480 (epoch   48.27), loss: 0.004950, time: 2.247 s\n",
      "Step 154500 (epoch   48.28), loss: 0.005064, time: 2.245 s\n",
      "Step 154520 (epoch   48.29), loss: 0.004384, time: 2.250 s\n",
      "Step 154540 (epoch   48.29), loss: 0.005827, time: 2.233 s\n",
      "Step 154560 (epoch   48.30), loss: 0.006575, time: 2.242 s\n",
      "Step 154580 (epoch   48.31), loss: 0.005227, time: 2.239 s\n",
      "Step 154600 (epoch   48.31), loss: 0.005913, time: 2.250 s, error: 0.011404\n",
      "Step 154620 (epoch   48.32), loss: 0.005247, time: 15.117 s\n",
      "Step 154640 (epoch   48.33), loss: 0.004491, time: 2.249 s\n",
      "Step 154660 (epoch   48.33), loss: 0.005979, time: 2.245 s\n",
      "Step 154680 (epoch   48.34), loss: 0.010171, time: 2.245 s\n",
      "Step 154700 (epoch   48.34), loss: 0.003424, time: 2.250 s\n",
      "Step 154720 (epoch   48.35), loss: 0.003650, time: 2.244 s\n",
      "Step 154740 (epoch   48.36), loss: 0.005341, time: 2.250 s\n",
      "Step 154760 (epoch   48.36), loss: 0.005088, time: 2.247 s\n",
      "Step 154780 (epoch   48.37), loss: 0.006820, time: 2.261 s\n",
      "Step 154800 (epoch   48.38), loss: 0.004776, time: 2.244 s, error: 0.011928\n",
      "Step 154820 (epoch   48.38), loss: 0.005581, time: 15.150 s\n",
      "Step 154840 (epoch   48.39), loss: 0.005149, time: 2.242 s\n",
      "Step 154860 (epoch   48.39), loss: 0.003704, time: 2.234 s\n",
      "Step 154880 (epoch   48.40), loss: 0.008809, time: 2.249 s\n",
      "Step 154900 (epoch   48.41), loss: 0.009807, time: 2.253 s\n",
      "Step 154920 (epoch   48.41), loss: 0.007664, time: 2.249 s\n",
      "Step 154940 (epoch   48.42), loss: 0.005642, time: 2.236 s\n",
      "Step 154960 (epoch   48.42), loss: 0.008266, time: 2.240 s\n",
      "Step 154980 (epoch   48.43), loss: 0.005710, time: 2.223 s\n",
      "Step 155000 (epoch   48.44), loss: 0.004923, time: 2.246 s, error: 0.011368\n",
      "Step 155020 (epoch   48.44), loss: 0.005172, time: 15.306 s\n",
      "Step 155040 (epoch   48.45), loss: 0.005576, time: 2.247 s\n",
      "Step 155060 (epoch   48.46), loss: 0.004296, time: 2.251 s\n",
      "Step 155080 (epoch   48.46), loss: 0.003864, time: 2.246 s\n",
      "Step 155100 (epoch   48.47), loss: 0.006074, time: 2.247 s\n",
      "Step 155120 (epoch   48.48), loss: 0.005549, time: 2.243 s\n",
      "Step 155140 (epoch   48.48), loss: 0.010961, time: 2.248 s\n",
      "Step 155160 (epoch   48.49), loss: 0.006563, time: 2.230 s\n",
      "Step 155180 (epoch   48.49), loss: 0.003881, time: 2.236 s\n",
      "Step 155200 (epoch   48.50), loss: 0.004086, time: 2.238 s, error: 0.011469\n",
      "Step 155220 (epoch   48.51), loss: 0.004621, time: 15.304 s\n",
      "Step 155240 (epoch   48.51), loss: 0.005763, time: 2.230 s\n",
      "Step 155260 (epoch   48.52), loss: 0.004636, time: 2.247 s\n",
      "Step 155280 (epoch   48.52), loss: 0.014300, time: 2.246 s\n",
      "Step 155300 (epoch   48.53), loss: 0.005211, time: 2.246 s\n",
      "Step 155320 (epoch   48.54), loss: 0.004655, time: 2.245 s\n",
      "Step 155340 (epoch   48.54), loss: 0.009249, time: 2.239 s\n",
      "Step 155360 (epoch   48.55), loss: 0.006026, time: 2.243 s\n",
      "Step 155380 (epoch   48.56), loss: 0.006634, time: 2.235 s\n",
      "Step 155400 (epoch   48.56), loss: 0.004866, time: 2.238 s, error: 0.011352\n",
      "Step 155420 (epoch   48.57), loss: 0.006587, time: 15.189 s\n",
      "Step 155440 (epoch   48.58), loss: 0.005223, time: 2.257 s\n",
      "Step 155460 (epoch   48.58), loss: 0.007287, time: 2.254 s\n",
      "Step 155480 (epoch   48.59), loss: 0.004077, time: 2.258 s\n",
      "Step 155500 (epoch   48.59), loss: 0.006221, time: 2.248 s\n",
      "Step 155520 (epoch   48.60), loss: 0.004258, time: 2.256 s\n",
      "Step 155540 (epoch   48.61), loss: 0.006145, time: 2.254 s\n",
      "Step 155560 (epoch   48.61), loss: 0.005887, time: 2.249 s\n",
      "Step 155580 (epoch   48.62), loss: 0.009389, time: 2.243 s\n",
      "Step 155600 (epoch   48.62), loss: 0.004741, time: 2.240 s, error: 0.011474\n",
      "Step 155620 (epoch   48.63), loss: 0.006729, time: 15.106 s\n",
      "Step 155640 (epoch   48.64), loss: 0.004801, time: 2.259 s\n",
      "Step 155660 (epoch   48.64), loss: 0.005571, time: 2.251 s\n",
      "Step 155680 (epoch   48.65), loss: 0.003856, time: 2.244 s\n",
      "Step 155700 (epoch   48.66), loss: 0.004892, time: 2.262 s\n",
      "Step 155720 (epoch   48.66), loss: 0.008911, time: 2.254 s\n",
      "Step 155740 (epoch   48.67), loss: 0.004560, time: 2.256 s\n",
      "Step 155760 (epoch   48.67), loss: 0.005376, time: 2.243 s\n",
      "Step 155780 (epoch   48.68), loss: 0.004402, time: 2.255 s\n",
      "Step 155800 (epoch   48.69), loss: 0.006462, time: 2.235 s, error: 0.011745\n",
      "Step 155820 (epoch   48.69), loss: 0.004504, time: 15.102 s\n",
      "Step 155840 (epoch   48.70), loss: 0.007036, time: 2.257 s\n",
      "Step 155860 (epoch   48.71), loss: 0.011272, time: 2.243 s\n",
      "Step 155880 (epoch   48.71), loss: 0.004137, time: 2.239 s\n",
      "Step 155900 (epoch   48.72), loss: 0.004679, time: 2.250 s\n",
      "Step 155920 (epoch   48.73), loss: 0.007716, time: 2.250 s\n",
      "Step 155940 (epoch   48.73), loss: 0.005874, time: 2.247 s\n",
      "Step 155960 (epoch   48.74), loss: 0.005577, time: 2.262 s\n",
      "Step 155980 (epoch   48.74), loss: 0.005346, time: 2.225 s\n",
      "Step 156000 (epoch   48.75), loss: 0.005697, time: 2.251 s, error: 0.011532\n",
      "\n",
      "Time since beginning  : 27275.005 s\n",
      "\n",
      "Step 156020 (epoch   48.76), loss: 0.004542, time: 15.232 s\n",
      "Step 156040 (epoch   48.76), loss: 0.004729, time: 2.244 s\n",
      "Step 156060 (epoch   48.77), loss: 0.004272, time: 2.249 s\n",
      "Step 156080 (epoch   48.77), loss: 0.006361, time: 2.246 s\n",
      "Step 156100 (epoch   48.78), loss: 0.004061, time: 2.248 s\n",
      "Step 156120 (epoch   48.79), loss: 0.005962, time: 2.232 s\n",
      "Step 156140 (epoch   48.79), loss: 0.006787, time: 2.250 s\n",
      "Step 156160 (epoch   48.80), loss: 0.008922, time: 2.244 s\n",
      "Step 156180 (epoch   48.81), loss: 0.004222, time: 2.247 s\n",
      "Step 156200 (epoch   48.81), loss: 0.007832, time: 2.247 s, error: 0.011585\n",
      "Step 156220 (epoch   48.82), loss: 0.004219, time: 15.177 s\n",
      "Step 156240 (epoch   48.83), loss: 0.004441, time: 2.247 s\n",
      "Step 156260 (epoch   48.83), loss: 0.008355, time: 2.238 s\n",
      "Step 156280 (epoch   48.84), loss: 0.003136, time: 2.248 s\n",
      "Step 156300 (epoch   48.84), loss: 0.005641, time: 2.242 s\n",
      "Step 156320 (epoch   48.85), loss: 0.005733, time: 2.248 s\n",
      "Step 156340 (epoch   48.86), loss: 0.003342, time: 2.236 s\n",
      "Step 156360 (epoch   48.86), loss: 0.004662, time: 2.241 s\n",
      "Step 156380 (epoch   48.87), loss: 0.005322, time: 2.233 s\n",
      "Step 156400 (epoch   48.88), loss: 0.004925, time: 2.249 s, error: 0.011191\n",
      "Step 156420 (epoch   48.88), loss: 0.004763, time: 15.354 s\n",
      "Step 156440 (epoch   48.89), loss: 0.005185, time: 2.243 s\n",
      "Step 156460 (epoch   48.89), loss: 0.006292, time: 2.258 s\n",
      "Step 156480 (epoch   48.90), loss: 0.010631, time: 2.256 s\n",
      "Step 156500 (epoch   48.91), loss: 0.003451, time: 2.247 s\n",
      "Step 156520 (epoch   48.91), loss: 0.006823, time: 2.246 s\n",
      "Step 156540 (epoch   48.92), loss: 0.005253, time: 2.236 s\n",
      "Step 156560 (epoch   48.92), loss: 0.007151, time: 2.243 s\n",
      "Step 156580 (epoch   48.93), loss: 0.005145, time: 2.221 s\n",
      "Step 156600 (epoch   48.94), loss: 0.004400, time: 2.246 s, error: 0.011244\n",
      "Step 156620 (epoch   48.94), loss: 0.004447, time: 15.130 s\n",
      "Step 156640 (epoch   48.95), loss: 0.005075, time: 2.248 s\n",
      "Step 156660 (epoch   48.96), loss: 0.009879, time: 2.255 s\n",
      "Step 156680 (epoch   48.96), loss: 0.005043, time: 2.246 s\n",
      "Step 156700 (epoch   48.97), loss: 0.005837, time: 2.230 s\n",
      "Step 156720 (epoch   48.98), loss: 0.004934, time: 2.240 s\n",
      "Step 156740 (epoch   48.98), loss: 0.008646, time: 2.248 s\n",
      "Step 156760 (epoch   48.99), loss: 0.005635, time: 2.248 s\n",
      "Step 156780 (epoch   48.99), loss: 0.004711, time: 2.252 s\n",
      "Step 156800 (epoch   49.00), loss: 0.002946, time: 2.241 s, error: 0.011328\n",
      "Step 156820 (epoch   49.01), loss: 0.006248, time: 15.346 s\n",
      "Step 156840 (epoch   49.01), loss: 0.004196, time: 2.257 s\n",
      "Step 156860 (epoch   49.02), loss: 0.004765, time: 2.258 s\n",
      "Step 156880 (epoch   49.02), loss: 0.006699, time: 2.262 s\n",
      "Step 156900 (epoch   49.03), loss: 0.005226, time: 2.248 s\n",
      "Step 156920 (epoch   49.04), loss: 0.005888, time: 2.255 s\n",
      "Step 156940 (epoch   49.04), loss: 0.004652, time: 2.257 s\n",
      "Step 156960 (epoch   49.05), loss: 0.008836, time: 2.259 s\n",
      "Step 156980 (epoch   49.06), loss: 0.004127, time: 2.244 s\n",
      "Step 157000 (epoch   49.06), loss: 0.008218, time: 2.240 s, error: 0.011748\n",
      "Step 157020 (epoch   49.07), loss: 0.006467, time: 15.085 s\n",
      "Step 157040 (epoch   49.08), loss: 0.005447, time: 2.251 s\n",
      "Step 157060 (epoch   49.08), loss: 0.004530, time: 2.256 s\n",
      "Step 157080 (epoch   49.09), loss: 0.003843, time: 2.245 s\n",
      "Step 157100 (epoch   49.09), loss: 0.005211, time: 2.250 s\n",
      "Step 157120 (epoch   49.10), loss: 0.005735, time: 2.250 s\n",
      "Step 157140 (epoch   49.11), loss: 0.005196, time: 2.263 s\n",
      "Step 157160 (epoch   49.11), loss: 0.003821, time: 2.257 s\n",
      "Step 157180 (epoch   49.12), loss: 0.005975, time: 2.244 s\n",
      "Step 157200 (epoch   49.12), loss: 0.008536, time: 2.255 s, error: 0.011462\n",
      "Step 157220 (epoch   49.13), loss: 0.004264, time: 15.096 s\n",
      "Step 157240 (epoch   49.14), loss: 0.007230, time: 2.248 s\n",
      "Step 157260 (epoch   49.14), loss: 0.005461, time: 2.250 s\n",
      "Step 157280 (epoch   49.15), loss: 0.005262, time: 2.249 s\n",
      "Step 157300 (epoch   49.16), loss: 0.005764, time: 2.237 s\n",
      "Step 157320 (epoch   49.16), loss: 0.003780, time: 2.242 s\n",
      "Step 157340 (epoch   49.17), loss: 0.006809, time: 2.241 s\n",
      "Step 157360 (epoch   49.17), loss: 0.004678, time: 2.227 s\n",
      "Step 157380 (epoch   49.18), loss: 0.007537, time: 2.230 s\n",
      "Step 157400 (epoch   49.19), loss: 0.006817, time: 2.247 s, error: 0.011844\n",
      "Step 157420 (epoch   49.19), loss: 0.008720, time: 15.114 s\n",
      "Step 157440 (epoch   49.20), loss: 0.007479, time: 2.230 s\n",
      "Step 157460 (epoch   49.21), loss: 0.003500, time: 2.242 s\n",
      "Step 157480 (epoch   49.21), loss: 0.008549, time: 2.239 s\n",
      "Step 157500 (epoch   49.22), loss: 0.006172, time: 2.248 s\n",
      "Step 157520 (epoch   49.23), loss: 0.007026, time: 2.252 s\n",
      "Step 157540 (epoch   49.23), loss: 0.004700, time: 2.238 s\n",
      "Step 157560 (epoch   49.24), loss: 0.003235, time: 2.246 s\n",
      "Step 157580 (epoch   49.24), loss: 0.004001, time: 2.242 s\n",
      "Step 157600 (epoch   49.25), loss: 0.004946, time: 2.240 s, error: 0.011442\n",
      "Step 157620 (epoch   49.26), loss: 0.005409, time: 15.370 s\n",
      "Step 157640 (epoch   49.26), loss: 0.005404, time: 2.240 s\n",
      "Step 157660 (epoch   49.27), loss: 0.003670, time: 2.233 s\n",
      "Step 157680 (epoch   49.27), loss: 0.008465, time: 2.235 s\n",
      "Step 157700 (epoch   49.28), loss: 0.004505, time: 2.228 s\n",
      "Step 157720 (epoch   49.29), loss: 0.005558, time: 2.250 s\n",
      "Step 157740 (epoch   49.29), loss: 0.003738, time: 2.241 s\n",
      "Step 157760 (epoch   49.30), loss: 0.005986, time: 2.242 s\n",
      "Step 157780 (epoch   49.31), loss: 0.004560, time: 2.253 s\n",
      "Step 157800 (epoch   49.31), loss: 0.005255, time: 2.242 s, error: 0.011409\n",
      "Step 157820 (epoch   49.32), loss: 0.004958, time: 15.247 s\n",
      "Step 157840 (epoch   49.33), loss: 0.004688, time: 2.253 s\n",
      "Step 157860 (epoch   49.33), loss: 0.004415, time: 2.243 s\n",
      "Step 157880 (epoch   49.34), loss: 0.005143, time: 2.246 s\n",
      "Step 157900 (epoch   49.34), loss: 0.005111, time: 2.235 s\n",
      "Step 157920 (epoch   49.35), loss: 0.005200, time: 2.251 s\n",
      "Step 157940 (epoch   49.36), loss: 0.005142, time: 2.255 s\n",
      "Step 157960 (epoch   49.36), loss: 0.004423, time: 2.259 s\n",
      "Step 157980 (epoch   49.37), loss: 0.007462, time: 2.254 s\n",
      "Step 158000 (epoch   49.38), loss: 0.006741, time: 2.237 s, error: 0.011743\n",
      "\n",
      "Time since beginning  : 27629.278 s\n",
      "\n",
      "Step 158020 (epoch   49.38), loss: 0.004827, time: 15.255 s\n",
      "Step 158040 (epoch   49.39), loss: 0.004219, time: 2.247 s\n",
      "Step 158060 (epoch   49.39), loss: 0.006194, time: 2.258 s\n",
      "Step 158080 (epoch   49.40), loss: 0.012065, time: 2.254 s\n",
      "Step 158100 (epoch   49.41), loss: 0.005359, time: 2.232 s\n",
      "Step 158120 (epoch   49.41), loss: 0.006237, time: 2.242 s\n",
      "Step 158140 (epoch   49.42), loss: 0.006321, time: 2.236 s\n",
      "Step 158160 (epoch   49.42), loss: 0.005696, time: 2.235 s\n",
      "Step 158180 (epoch   49.43), loss: 0.008101, time: 2.252 s\n",
      "Step 158200 (epoch   49.44), loss: 0.007319, time: 2.238 s, error: 0.011264\n",
      "Step 158220 (epoch   49.44), loss: 0.003879, time: 15.150 s\n",
      "Step 158240 (epoch   49.45), loss: 0.004147, time: 2.252 s\n",
      "Step 158260 (epoch   49.46), loss: 0.007950, time: 2.245 s\n",
      "Step 158280 (epoch   49.46), loss: 0.004337, time: 2.253 s\n",
      "Step 158300 (epoch   49.47), loss: 0.004533, time: 2.232 s\n",
      "Step 158320 (epoch   49.48), loss: 0.008328, time: 2.258 s\n",
      "Step 158340 (epoch   49.48), loss: 0.008071, time: 2.253 s\n",
      "Step 158360 (epoch   49.49), loss: 0.004880, time: 2.242 s\n",
      "Step 158380 (epoch   49.49), loss: 0.006072, time: 2.241 s\n",
      "Step 158400 (epoch   49.50), loss: 0.004144, time: 2.253 s, error: 0.011436\n",
      "Step 158420 (epoch   49.51), loss: 0.005742, time: 15.089 s\n",
      "Step 158440 (epoch   49.51), loss: 0.004443, time: 2.246 s\n",
      "Step 158460 (epoch   49.52), loss: 0.005579, time: 2.249 s\n",
      "Step 158480 (epoch   49.52), loss: 0.007391, time: 2.235 s\n",
      "Step 158500 (epoch   49.53), loss: 0.004766, time: 2.246 s\n",
      "Step 158520 (epoch   49.54), loss: 0.004716, time: 2.234 s\n",
      "Step 158540 (epoch   49.54), loss: 0.004609, time: 2.257 s\n",
      "Step 158560 (epoch   49.55), loss: 0.004982, time: 2.256 s\n",
      "Step 158580 (epoch   49.56), loss: 0.004623, time: 2.260 s\n",
      "Step 158600 (epoch   49.56), loss: 0.004815, time: 2.253 s, error: 0.011274\n",
      "Step 158620 (epoch   49.57), loss: 0.007103, time: 15.106 s\n",
      "Step 158640 (epoch   49.58), loss: 0.007868, time: 2.242 s\n",
      "Step 158660 (epoch   49.58), loss: 0.005269, time: 2.244 s\n",
      "Step 158680 (epoch   49.59), loss: 0.006286, time: 2.253 s\n",
      "Step 158700 (epoch   49.59), loss: 0.006233, time: 2.251 s\n",
      "Step 158720 (epoch   49.60), loss: 0.005393, time: 2.244 s\n",
      "Step 158740 (epoch   49.61), loss: 0.003849, time: 2.240 s\n",
      "Step 158760 (epoch   49.61), loss: 0.004022, time: 2.244 s\n",
      "Step 158780 (epoch   49.62), loss: 0.004632, time: 2.247 s\n",
      "Step 158800 (epoch   49.62), loss: 0.005884, time: 2.228 s, error: 0.011381\n",
      "Step 158820 (epoch   49.63), loss: 0.006397, time: 15.301 s\n",
      "Step 158840 (epoch   49.64), loss: 0.005956, time: 2.251 s\n",
      "Step 158860 (epoch   49.64), loss: 0.003794, time: 2.248 s\n",
      "Step 158880 (epoch   49.65), loss: 0.003589, time: 2.251 s\n",
      "Step 158900 (epoch   49.66), loss: 0.004459, time: 2.256 s\n",
      "Step 158920 (epoch   49.66), loss: 0.005241, time: 2.255 s\n",
      "Step 158940 (epoch   49.67), loss: 0.006441, time: 2.256 s\n",
      "Step 158960 (epoch   49.67), loss: 0.006336, time: 2.255 s\n",
      "Step 158980 (epoch   49.68), loss: 0.008558, time: 2.244 s\n",
      "Step 159000 (epoch   49.69), loss: 0.006130, time: 2.257 s, error: 0.011602\n",
      "Step 159020 (epoch   49.69), loss: 0.004566, time: 15.402 s\n",
      "Step 159040 (epoch   49.70), loss: 0.005871, time: 2.246 s\n",
      "Step 159060 (epoch   49.71), loss: 0.005780, time: 2.251 s\n",
      "Step 159080 (epoch   49.71), loss: 0.005978, time: 2.230 s\n",
      "Step 159100 (epoch   49.72), loss: 0.005007, time: 2.244 s\n",
      "Step 159120 (epoch   49.73), loss: 0.003869, time: 2.245 s\n",
      "Step 159140 (epoch   49.73), loss: 0.008105, time: 2.251 s\n",
      "Step 159160 (epoch   49.74), loss: 0.004661, time: 2.243 s\n",
      "Step 159180 (epoch   49.74), loss: 0.004593, time: 2.246 s\n",
      "Step 159200 (epoch   49.75), loss: 0.004943, time: 2.241 s, error: 0.011481\n",
      "Step 159220 (epoch   49.76), loss: 0.004204, time: 15.194 s\n",
      "Step 159240 (epoch   49.76), loss: 0.004368, time: 2.256 s\n",
      "Step 159260 (epoch   49.77), loss: 0.004910, time: 2.264 s\n",
      "Step 159280 (epoch   49.77), loss: 0.004947, time: 2.252 s\n",
      "Step 159300 (epoch   49.78), loss: 0.003991, time: 2.247 s\n",
      "Step 159320 (epoch   49.79), loss: 0.003917, time: 2.254 s\n",
      "Step 159340 (epoch   49.79), loss: 0.006269, time: 2.238 s\n",
      "Step 159360 (epoch   49.80), loss: 0.005014, time: 2.256 s\n",
      "Step 159380 (epoch   49.81), loss: 0.007289, time: 2.243 s\n",
      "Step 159400 (epoch   49.81), loss: 0.004451, time: 2.248 s, error: 0.011548\n",
      "Step 159420 (epoch   49.82), loss: 0.005909, time: 15.062 s\n",
      "Step 159440 (epoch   49.83), loss: 0.005729, time: 2.232 s\n",
      "Step 159460 (epoch   49.83), loss: 0.004287, time: 2.246 s\n",
      "Step 159480 (epoch   49.84), loss: 0.005086, time: 2.239 s\n",
      "Step 159500 (epoch   49.84), loss: 0.004713, time: 2.248 s\n",
      "Step 159520 (epoch   49.85), loss: 0.003943, time: 2.260 s\n",
      "Step 159540 (epoch   49.86), loss: 0.004184, time: 2.237 s\n",
      "Step 159560 (epoch   49.86), loss: 0.008062, time: 2.252 s\n",
      "Step 159580 (epoch   49.87), loss: 0.006109, time: 2.251 s\n",
      "Step 159600 (epoch   49.88), loss: 0.004799, time: 2.248 s, error: 0.011330\n",
      "Step 159620 (epoch   49.88), loss: 0.003208, time: 15.128 s\n",
      "Step 159640 (epoch   49.89), loss: 0.004924, time: 2.245 s\n",
      "Step 159660 (epoch   49.89), loss: 0.008515, time: 2.247 s\n",
      "Step 159680 (epoch   49.90), loss: 0.006710, time: 2.248 s\n",
      "Step 159700 (epoch   49.91), loss: 0.006011, time: 2.242 s\n",
      "Step 159720 (epoch   49.91), loss: 0.008221, time: 2.244 s\n",
      "Step 159740 (epoch   49.92), loss: 0.006045, time: 2.239 s\n",
      "Step 159760 (epoch   49.92), loss: 0.006056, time: 2.247 s\n",
      "Step 159780 (epoch   49.93), loss: 0.007112, time: 2.262 s\n",
      "Step 159800 (epoch   49.94), loss: 0.009530, time: 2.245 s, error: 0.011185\n",
      "Step 159820 (epoch   49.94), loss: 0.006878, time: 15.110 s\n",
      "Step 159840 (epoch   49.95), loss: 0.005717, time: 2.249 s\n",
      "Step 159860 (epoch   49.96), loss: 0.005370, time: 2.244 s\n",
      "Step 159880 (epoch   49.96), loss: 0.006596, time: 2.235 s\n",
      "Step 159900 (epoch   49.97), loss: 0.004511, time: 2.243 s\n",
      "Step 159920 (epoch   49.98), loss: 0.008165, time: 2.246 s\n",
      "Step 159940 (epoch   49.98), loss: 0.003461, time: 2.238 s\n",
      "Step 159960 (epoch   49.99), loss: 0.016127, time: 2.244 s\n",
      "Step 159980 (epoch   49.99), loss: 0.009432, time: 2.232 s\n",
      "Step 160000 (epoch   50.00), loss: 0.005175, time: 2.255 s, error: 0.011462\n",
      "\n",
      "Time since beginning  : 27983.366 s\n",
      "\n",
      "Step 160020 (epoch   50.01), loss: 0.005357, time: 15.356 s\n",
      "Step 160040 (epoch   50.01), loss: 0.005125, time: 2.246 s\n",
      "Step 160060 (epoch   50.02), loss: 0.004664, time: 2.245 s\n",
      "Step 160080 (epoch   50.02), loss: 0.007127, time: 2.246 s\n",
      "Step 160100 (epoch   50.03), loss: 0.007354, time: 2.240 s\n",
      "Step 160120 (epoch   50.04), loss: 0.009730, time: 2.242 s\n",
      "Step 160140 (epoch   50.04), loss: 0.005127, time: 2.246 s\n",
      "Step 160160 (epoch   50.05), loss: 0.004894, time: 2.239 s\n",
      "Step 160180 (epoch   50.06), loss: 0.006315, time: 2.245 s\n",
      "Step 160200 (epoch   50.06), loss: 0.006649, time: 2.243 s, error: 0.011686\n",
      "Step 160220 (epoch   50.07), loss: 0.008692, time: 15.330 s\n",
      "Step 160240 (epoch   50.08), loss: 0.006408, time: 2.246 s\n",
      "Step 160260 (epoch   50.08), loss: 0.009388, time: 2.247 s\n",
      "Step 160280 (epoch   50.09), loss: 0.004905, time: 2.256 s\n",
      "Step 160300 (epoch   50.09), loss: 0.004231, time: 2.252 s\n",
      "Step 160320 (epoch   50.10), loss: 0.005004, time: 2.252 s\n",
      "Step 160340 (epoch   50.11), loss: 0.008166, time: 2.239 s\n",
      "Step 160360 (epoch   50.11), loss: 0.006314, time: 2.243 s\n",
      "Step 160380 (epoch   50.12), loss: 0.009597, time: 2.240 s\n",
      "Step 160400 (epoch   50.12), loss: 0.008681, time: 2.237 s, error: 0.011523\n",
      "Step 160420 (epoch   50.13), loss: 0.005376, time: 15.059 s\n",
      "Step 160440 (epoch   50.14), loss: 0.006829, time: 2.256 s\n",
      "Step 160460 (epoch   50.14), loss: 0.005955, time: 2.250 s\n",
      "Step 160480 (epoch   50.15), loss: 0.003321, time: 2.255 s\n",
      "Step 160500 (epoch   50.16), loss: 0.011361, time: 2.253 s\n",
      "Step 160520 (epoch   50.16), loss: 0.006135, time: 2.248 s\n",
      "Step 160540 (epoch   50.17), loss: 0.005393, time: 2.233 s\n",
      "Step 160560 (epoch   50.17), loss: 0.009558, time: 2.241 s\n",
      "Step 160580 (epoch   50.18), loss: 0.005110, time: 2.230 s\n",
      "Step 160600 (epoch   50.19), loss: 0.004312, time: 2.244 s, error: 0.011561\n",
      "Step 160620 (epoch   50.19), loss: 0.004688, time: 15.093 s\n",
      "Step 160640 (epoch   50.20), loss: 0.004614, time: 2.241 s\n",
      "Step 160660 (epoch   50.21), loss: 0.003756, time: 2.231 s\n",
      "Step 160680 (epoch   50.21), loss: 0.003175, time: 2.247 s\n",
      "Step 160700 (epoch   50.22), loss: 0.006066, time: 2.256 s\n",
      "Step 160720 (epoch   50.23), loss: 0.004631, time: 2.258 s\n",
      "Step 160740 (epoch   50.23), loss: 0.007237, time: 2.238 s\n",
      "Step 160760 (epoch   50.24), loss: 0.004765, time: 2.248 s\n",
      "Step 160780 (epoch   50.24), loss: 0.005156, time: 2.240 s\n",
      "Step 160800 (epoch   50.25), loss: 0.005412, time: 2.223 s, error: 0.011312\n",
      "Step 160820 (epoch   50.26), loss: 0.003804, time: 15.060 s\n",
      "Step 160840 (epoch   50.26), loss: 0.004468, time: 2.243 s\n",
      "Step 160860 (epoch   50.27), loss: 0.004484, time: 2.236 s\n",
      "Step 160880 (epoch   50.27), loss: 0.004180, time: 2.252 s\n",
      "Step 160900 (epoch   50.28), loss: 0.005083, time: 2.237 s\n",
      "Step 160920 (epoch   50.29), loss: 0.005549, time: 2.252 s\n",
      "Step 160940 (epoch   50.29), loss: 0.004341, time: 2.235 s\n",
      "Step 160960 (epoch   50.30), loss: 0.005441, time: 2.260 s\n",
      "Step 160980 (epoch   50.31), loss: 0.006189, time: 2.258 s\n",
      "Step 161000 (epoch   50.31), loss: 0.005375, time: 2.246 s, error: 0.011309\n",
      "Step 161020 (epoch   50.32), loss: 0.006248, time: 15.275 s\n",
      "Step 161040 (epoch   50.33), loss: 0.004983, time: 2.254 s\n",
      "Step 161060 (epoch   50.33), loss: 0.005459, time: 2.256 s\n",
      "Step 161080 (epoch   50.34), loss: 0.008155, time: 2.247 s\n",
      "Step 161100 (epoch   50.34), loss: 0.005229, time: 2.251 s\n",
      "Step 161120 (epoch   50.35), loss: 0.005092, time: 2.248 s\n",
      "Step 161140 (epoch   50.36), loss: 0.003357, time: 2.244 s\n",
      "Step 161160 (epoch   50.36), loss: 0.006185, time: 2.224 s\n",
      "Step 161180 (epoch   50.37), loss: 0.005844, time: 2.240 s\n",
      "Step 161200 (epoch   50.38), loss: 0.007186, time: 2.229 s, error: 0.011704\n",
      "Step 161220 (epoch   50.38), loss: 0.004371, time: 15.253 s\n",
      "Step 161240 (epoch   50.39), loss: 0.008037, time: 2.257 s\n",
      "Step 161260 (epoch   50.39), loss: 0.004561, time: 2.246 s\n",
      "Step 161280 (epoch   50.40), loss: 0.004724, time: 2.257 s\n",
      "Step 161300 (epoch   50.41), loss: 0.003713, time: 2.251 s\n",
      "Step 161320 (epoch   50.41), loss: 0.005638, time: 2.247 s\n",
      "Step 161340 (epoch   50.42), loss: 0.007162, time: 2.256 s\n",
      "Step 161360 (epoch   50.42), loss: 0.004474, time: 2.253 s\n",
      "Step 161380 (epoch   50.43), loss: 0.004203, time: 2.256 s\n",
      "Step 161400 (epoch   50.44), loss: 0.004225, time: 2.243 s, error: 0.011239\n",
      "Step 161420 (epoch   50.44), loss: 0.002874, time: 15.260 s\n",
      "Step 161440 (epoch   50.45), loss: 0.008533, time: 2.248 s\n",
      "Step 161460 (epoch   50.46), loss: 0.005174, time: 2.250 s\n",
      "Step 161480 (epoch   50.46), loss: 0.003849, time: 2.248 s\n",
      "Step 161500 (epoch   50.47), loss: 0.007540, time: 2.247 s\n",
      "Step 161520 (epoch   50.48), loss: 0.005181, time: 2.253 s\n",
      "Step 161540 (epoch   50.48), loss: 0.004217, time: 2.247 s\n",
      "Step 161560 (epoch   50.49), loss: 0.005590, time: 2.247 s\n",
      "Step 161580 (epoch   50.49), loss: 0.004182, time: 2.238 s\n",
      "Step 161600 (epoch   50.50), loss: 0.004799, time: 2.254 s, error: 0.011510\n",
      "Step 161620 (epoch   50.51), loss: 0.005421, time: 15.107 s\n",
      "Step 161640 (epoch   50.51), loss: 0.005589, time: 2.258 s\n",
      "Step 161660 (epoch   50.52), loss: 0.003280, time: 2.256 s\n",
      "Step 161680 (epoch   50.52), loss: 0.003962, time: 2.254 s\n",
      "Step 161700 (epoch   50.53), loss: 0.003869, time: 2.241 s\n",
      "Step 161720 (epoch   50.54), loss: 0.007944, time: 2.230 s\n",
      "Step 161740 (epoch   50.54), loss: 0.004095, time: 2.245 s\n",
      "Step 161760 (epoch   50.55), loss: 0.003472, time: 2.236 s\n",
      "Step 161780 (epoch   50.56), loss: 0.007682, time: 2.239 s\n",
      "Step 161800 (epoch   50.56), loss: 0.003441, time: 2.238 s, error: 0.011419\n",
      "Step 161820 (epoch   50.57), loss: 0.005620, time: 15.052 s\n",
      "Step 161840 (epoch   50.58), loss: 0.005722, time: 2.238 s\n",
      "Step 161860 (epoch   50.58), loss: 0.004774, time: 2.245 s\n",
      "Step 161880 (epoch   50.59), loss: 0.005322, time: 2.253 s\n",
      "Step 161900 (epoch   50.59), loss: 0.005630, time: 2.251 s\n",
      "Step 161920 (epoch   50.60), loss: 0.010932, time: 2.237 s\n",
      "Step 161940 (epoch   50.61), loss: 0.005190, time: 2.241 s\n",
      "Step 161960 (epoch   50.61), loss: 0.005928, time: 2.247 s\n",
      "Step 161980 (epoch   50.62), loss: 0.005711, time: 2.237 s\n",
      "Step 162000 (epoch   50.62), loss: 0.006606, time: 2.249 s, error: 0.011335\n",
      "\n",
      "Time since beginning  : 28337.066 s\n",
      "\n",
      "Step 162020 (epoch   50.63), loss: 0.005614, time: 15.118 s\n",
      "Step 162040 (epoch   50.64), loss: 0.004184, time: 2.240 s\n",
      "Step 162060 (epoch   50.64), loss: 0.005678, time: 2.248 s\n",
      "Step 162080 (epoch   50.65), loss: 0.004279, time: 2.243 s\n",
      "Step 162100 (epoch   50.66), loss: 0.006660, time: 2.243 s\n",
      "Step 162120 (epoch   50.66), loss: 0.005085, time: 2.239 s\n",
      "Step 162140 (epoch   50.67), loss: 0.006336, time: 2.249 s\n",
      "Step 162160 (epoch   50.67), loss: 0.003123, time: 2.260 s\n",
      "Step 162180 (epoch   50.68), loss: 0.003647, time: 2.250 s\n",
      "Step 162200 (epoch   50.69), loss: 0.006140, time: 2.247 s, error: 0.011482\n",
      "Step 162220 (epoch   50.69), loss: 0.004602, time: 15.090 s\n",
      "Step 162240 (epoch   50.70), loss: 0.006846, time: 2.259 s\n",
      "Step 162260 (epoch   50.71), loss: 0.003698, time: 2.245 s\n",
      "Step 162280 (epoch   50.71), loss: 0.005268, time: 2.242 s\n",
      "Step 162300 (epoch   50.72), loss: 0.005602, time: 2.238 s\n",
      "Step 162320 (epoch   50.73), loss: 0.003470, time: 2.252 s\n",
      "Step 162340 (epoch   50.73), loss: 0.004240, time: 2.247 s\n",
      "Step 162360 (epoch   50.74), loss: 0.005113, time: 2.248 s\n",
      "Step 162380 (epoch   50.74), loss: 0.005226, time: 2.246 s\n",
      "Step 162400 (epoch   50.75), loss: 0.003556, time: 2.241 s, error: 0.011486\n",
      "Step 162420 (epoch   50.76), loss: 0.004912, time: 15.401 s\n",
      "Step 162440 (epoch   50.76), loss: 0.006636, time: 2.251 s\n",
      "Step 162460 (epoch   50.77), loss: 0.004781, time: 2.251 s\n",
      "Step 162480 (epoch   50.77), loss: 0.008734, time: 2.238 s\n",
      "Step 162500 (epoch   50.78), loss: 0.004139, time: 2.250 s\n",
      "Step 162520 (epoch   50.79), loss: 0.005959, time: 2.244 s\n",
      "Step 162540 (epoch   50.79), loss: 0.008776, time: 2.241 s\n",
      "Step 162560 (epoch   50.80), loss: 0.004146, time: 2.239 s\n",
      "Step 162580 (epoch   50.81), loss: 0.004662, time: 2.246 s\n",
      "Step 162600 (epoch   50.81), loss: 0.004874, time: 2.240 s, error: 0.011479\n",
      "Step 162620 (epoch   50.82), loss: 0.003708, time: 15.250 s\n",
      "Step 162640 (epoch   50.83), loss: 0.018899, time: 2.254 s\n",
      "Step 162660 (epoch   50.83), loss: 0.004416, time: 2.249 s\n",
      "Step 162680 (epoch   50.84), loss: 0.006852, time: 2.244 s\n",
      "Step 162700 (epoch   50.84), loss: 0.005931, time: 2.236 s\n",
      "Step 162720 (epoch   50.85), loss: 0.004985, time: 2.239 s\n",
      "Step 162740 (epoch   50.86), loss: 0.004673, time: 2.239 s\n",
      "Step 162760 (epoch   50.86), loss: 0.009073, time: 2.253 s\n",
      "Step 162780 (epoch   50.87), loss: 0.005072, time: 2.236 s\n",
      "Step 162800 (epoch   50.88), loss: 0.007186, time: 2.251 s, error: 0.011653\n",
      "Step 162820 (epoch   50.88), loss: 0.005763, time: 15.157 s\n",
      "Step 162840 (epoch   50.89), loss: 0.003645, time: 2.234 s\n",
      "Step 162860 (epoch   50.89), loss: 0.009389, time: 2.250 s\n",
      "Step 162880 (epoch   50.90), loss: 0.005665, time: 2.258 s\n",
      "Step 162900 (epoch   50.91), loss: 0.004549, time: 2.250 s\n",
      "Step 162920 (epoch   50.91), loss: 0.004795, time: 2.236 s\n",
      "Step 162940 (epoch   50.92), loss: 0.005782, time: 2.234 s\n",
      "Step 162960 (epoch   50.92), loss: 0.004798, time: 2.247 s\n",
      "Step 162980 (epoch   50.93), loss: 0.004652, time: 2.241 s\n",
      "Step 163000 (epoch   50.94), loss: 0.003688, time: 2.251 s, error: 0.011126\n",
      "Step 163020 (epoch   50.94), loss: 0.004375, time: 15.087 s\n",
      "Step 163040 (epoch   50.95), loss: 0.004660, time: 2.234 s\n",
      "Step 163060 (epoch   50.96), loss: 0.011727, time: 2.237 s\n",
      "Step 163080 (epoch   50.96), loss: 0.013160, time: 2.255 s\n",
      "Step 163100 (epoch   50.97), loss: 0.003981, time: 2.251 s\n",
      "Step 163120 (epoch   50.98), loss: 0.005077, time: 2.234 s\n",
      "Step 163140 (epoch   50.98), loss: 0.003306, time: 2.251 s\n",
      "Step 163160 (epoch   50.99), loss: 0.006883, time: 2.246 s\n",
      "Step 163180 (epoch   50.99), loss: 0.005447, time: 2.239 s\n",
      "Step 163200 (epoch   51.00), loss: 0.004146, time: 2.238 s, error: 0.011744\n",
      "Step 163220 (epoch   51.01), loss: 0.004279, time: 15.038 s\n",
      "Step 163240 (epoch   51.01), loss: 0.003372, time: 2.241 s\n",
      "Step 163260 (epoch   51.02), loss: 0.004711, time: 2.251 s\n",
      "Step 163280 (epoch   51.02), loss: 0.006431, time: 2.240 s\n",
      "Step 163300 (epoch   51.03), loss: 0.003371, time: 2.240 s\n",
      "Step 163320 (epoch   51.04), loss: 0.005800, time: 2.231 s\n",
      "Step 163340 (epoch   51.04), loss: 0.006204, time: 2.252 s\n",
      "Step 163360 (epoch   51.05), loss: 0.005705, time: 2.249 s\n",
      "Step 163380 (epoch   51.06), loss: 0.005523, time: 2.239 s\n",
      "Step 163400 (epoch   51.06), loss: 0.007065, time: 2.242 s, error: 0.011565\n",
      "Step 163420 (epoch   51.07), loss: 0.004380, time: 15.060 s\n",
      "Step 163440 (epoch   51.08), loss: 0.004118, time: 2.248 s\n",
      "Step 163460 (epoch   51.08), loss: 0.005623, time: 2.238 s\n",
      "Step 163480 (epoch   51.09), loss: 0.008977, time: 2.235 s\n",
      "Step 163500 (epoch   51.09), loss: 0.007756, time: 2.244 s\n",
      "Step 163520 (epoch   51.10), loss: 0.005595, time: 2.257 s\n",
      "Step 163540 (epoch   51.11), loss: 0.006533, time: 2.246 s\n",
      "Step 163560 (epoch   51.11), loss: 0.003324, time: 2.247 s\n",
      "Step 163580 (epoch   51.12), loss: 0.004447, time: 2.234 s\n",
      "Step 163600 (epoch   51.12), loss: 0.004349, time: 2.256 s, error: 0.011673\n",
      "Step 163620 (epoch   51.13), loss: 0.004306, time: 15.147 s\n",
      "Step 163640 (epoch   51.14), loss: 0.006037, time: 2.243 s\n",
      "Step 163660 (epoch   51.14), loss: 0.003430, time: 2.247 s\n",
      "Step 163680 (epoch   51.15), loss: 0.004291, time: 2.246 s\n",
      "Step 163700 (epoch   51.16), loss: 0.007007, time: 2.243 s\n",
      "Step 163720 (epoch   51.16), loss: 0.009795, time: 2.234 s\n",
      "Step 163740 (epoch   51.17), loss: 0.005717, time: 2.245 s\n",
      "Step 163760 (epoch   51.17), loss: 0.004292, time: 2.239 s\n",
      "Step 163780 (epoch   51.18), loss: 0.010760, time: 2.242 s\n",
      "Step 163800 (epoch   51.19), loss: 0.004296, time: 2.234 s, error: 0.011588\n",
      "Step 163820 (epoch   51.19), loss: 0.008720, time: 15.259 s\n",
      "Step 163840 (epoch   51.20), loss: 0.003741, time: 2.245 s\n",
      "Step 163860 (epoch   51.21), loss: 0.004301, time: 2.238 s\n",
      "Step 163880 (epoch   51.21), loss: 0.009775, time: 2.243 s\n",
      "Step 163900 (epoch   51.22), loss: 0.007389, time: 2.234 s\n",
      "Step 163920 (epoch   51.23), loss: 0.008340, time: 2.243 s\n",
      "Step 163940 (epoch   51.23), loss: 0.005595, time: 2.239 s\n",
      "Step 163960 (epoch   51.24), loss: 0.005516, time: 2.244 s\n",
      "Step 163980 (epoch   51.24), loss: 0.004822, time: 2.226 s\n",
      "Step 164000 (epoch   51.25), loss: 0.004196, time: 2.238 s, error: 0.011264\n",
      "\n",
      "Time since beginning  : 28690.744 s\n",
      "\n",
      "Step 164020 (epoch   51.26), loss: 0.005535, time: 15.266 s\n",
      "Step 164040 (epoch   51.26), loss: 0.003594, time: 2.247 s\n",
      "Step 164060 (epoch   51.27), loss: 0.004103, time: 2.240 s\n",
      "Step 164080 (epoch   51.27), loss: 0.004303, time: 2.248 s\n",
      "Step 164100 (epoch   51.28), loss: 0.005143, time: 2.253 s\n",
      "Step 164120 (epoch   51.29), loss: 0.005689, time: 2.246 s\n",
      "Step 164140 (epoch   51.29), loss: 0.006752, time: 2.255 s\n",
      "Step 164160 (epoch   51.30), loss: 0.005230, time: 2.245 s\n",
      "Step 164180 (epoch   51.31), loss: 0.004283, time: 2.225 s\n",
      "Step 164200 (epoch   51.31), loss: 0.003788, time: 2.240 s, error: 0.011233\n",
      "Step 164220 (epoch   51.32), loss: 0.004191, time: 15.045 s\n",
      "Step 164240 (epoch   51.33), loss: 0.007560, time: 2.237 s\n",
      "Step 164260 (epoch   51.33), loss: 0.004316, time: 2.255 s\n",
      "Step 164280 (epoch   51.34), loss: 0.005456, time: 2.255 s\n",
      "Step 164300 (epoch   51.34), loss: 0.003576, time: 2.248 s\n",
      "Step 164320 (epoch   51.35), loss: 0.004888, time: 2.242 s\n",
      "Step 164340 (epoch   51.36), loss: 0.006541, time: 2.237 s\n",
      "Step 164360 (epoch   51.36), loss: 0.003601, time: 2.246 s\n",
      "Step 164380 (epoch   51.37), loss: 0.007707, time: 2.241 s\n",
      "Step 164400 (epoch   51.38), loss: 0.007368, time: 2.236 s, error: 0.011772\n",
      "Step 164420 (epoch   51.38), loss: 0.007577, time: 15.030 s\n",
      "Step 164440 (epoch   51.39), loss: 0.005856, time: 2.241 s\n",
      "Step 164460 (epoch   51.39), loss: 0.006117, time: 2.246 s\n",
      "Step 164480 (epoch   51.40), loss: 0.006319, time: 2.229 s\n",
      "Step 164500 (epoch   51.41), loss: 0.005814, time: 2.249 s\n",
      "Step 164520 (epoch   51.41), loss: 0.009695, time: 2.252 s\n",
      "Step 164540 (epoch   51.42), loss: 0.004605, time: 2.244 s\n",
      "Step 164560 (epoch   51.42), loss: 0.005559, time: 2.238 s\n",
      "Step 164580 (epoch   51.43), loss: 0.004847, time: 2.246 s\n",
      "Step 164600 (epoch   51.44), loss: 0.007747, time: 2.238 s, error: 0.011252\n",
      "Step 164620 (epoch   51.44), loss: 0.007031, time: 15.069 s\n",
      "Step 164640 (epoch   51.45), loss: 0.010346, time: 2.243 s\n",
      "Step 164660 (epoch   51.46), loss: 0.005575, time: 2.228 s\n",
      "Step 164680 (epoch   51.46), loss: 0.005201, time: 2.251 s\n",
      "Step 164700 (epoch   51.47), loss: 0.004225, time: 2.236 s\n",
      "Step 164720 (epoch   51.48), loss: 0.004955, time: 2.233 s\n",
      "Step 164740 (epoch   51.48), loss: 0.007815, time: 2.232 s\n",
      "Step 164760 (epoch   51.49), loss: 0.006604, time: 2.243 s\n",
      "Step 164780 (epoch   51.49), loss: 0.006497, time: 2.254 s\n",
      "Step 164800 (epoch   51.50), loss: 0.004714, time: 2.247 s, error: 0.011482\n",
      "Step 164820 (epoch   51.51), loss: 0.007497, time: 15.123 s\n",
      "Step 164840 (epoch   51.51), loss: 0.006412, time: 2.240 s\n",
      "Step 164860 (epoch   51.52), loss: 0.004188, time: 2.248 s\n",
      "Step 164880 (epoch   51.52), loss: 0.005580, time: 2.238 s\n",
      "Step 164900 (epoch   51.53), loss: 0.004403, time: 2.246 s\n",
      "Step 164920 (epoch   51.54), loss: 0.005205, time: 2.237 s\n",
      "Step 164940 (epoch   51.54), loss: 0.003793, time: 2.239 s\n",
      "Step 164960 (epoch   51.55), loss: 0.004267, time: 2.233 s\n",
      "Step 164980 (epoch   51.56), loss: 0.006222, time: 2.241 s\n",
      "Step 165000 (epoch   51.56), loss: 0.006004, time: 2.249 s, error: 0.011657\n",
      "Step 165020 (epoch   51.57), loss: 0.005349, time: 15.256 s\n",
      "Step 165040 (epoch   51.58), loss: 0.005358, time: 2.246 s\n",
      "Step 165060 (epoch   51.58), loss: 0.008704, time: 2.234 s\n",
      "Step 165080 (epoch   51.59), loss: 0.006149, time: 2.249 s\n",
      "Step 165100 (epoch   51.59), loss: 0.006047, time: 2.233 s\n",
      "Step 165120 (epoch   51.60), loss: 0.004093, time: 2.234 s\n",
      "Step 165140 (epoch   51.61), loss: 0.005035, time: 2.243 s\n",
      "Step 165160 (epoch   51.61), loss: 0.004072, time: 2.249 s\n",
      "Step 165180 (epoch   51.62), loss: 0.005168, time: 2.238 s\n",
      "Step 165200 (epoch   51.62), loss: 0.012869, time: 2.243 s, error: 0.011394\n",
      "Step 165220 (epoch   51.63), loss: 0.005715, time: 15.246 s\n",
      "Step 165240 (epoch   51.64), loss: 0.006997, time: 2.231 s\n",
      "Step 165260 (epoch   51.64), loss: 0.005525, time: 2.242 s\n",
      "Step 165280 (epoch   51.65), loss: 0.005107, time: 2.226 s\n",
      "Step 165300 (epoch   51.66), loss: 0.008302, time: 2.241 s\n",
      "Step 165320 (epoch   51.66), loss: 0.005332, time: 2.227 s\n",
      "Step 165340 (epoch   51.67), loss: 0.003343, time: 2.237 s\n",
      "Step 165360 (epoch   51.67), loss: 0.005788, time: 2.245 s\n",
      "Step 165380 (epoch   51.68), loss: 0.008435, time: 2.253 s\n",
      "Step 165400 (epoch   51.69), loss: 0.004449, time: 2.238 s, error: 0.011371\n",
      "Step 165420 (epoch   51.69), loss: 0.005300, time: 15.046 s\n",
      "Step 165440 (epoch   51.70), loss: 0.003800, time: 2.257 s\n",
      "Step 165460 (epoch   51.71), loss: 0.007215, time: 2.259 s\n",
      "Step 165480 (epoch   51.71), loss: 0.006448, time: 2.239 s\n",
      "Step 165500 (epoch   51.72), loss: 0.006956, time: 2.231 s\n",
      "Step 165520 (epoch   51.73), loss: 0.005337, time: 2.238 s\n",
      "Step 165540 (epoch   51.73), loss: 0.005487, time: 2.236 s\n",
      "Step 165560 (epoch   51.74), loss: 0.004935, time: 2.235 s\n",
      "Step 165580 (epoch   51.74), loss: 0.004565, time: 2.225 s\n",
      "Step 165600 (epoch   51.75), loss: 0.003703, time: 2.241 s, error: 0.011559\n",
      "Step 165620 (epoch   51.76), loss: 0.006780, time: 15.104 s\n",
      "Step 165640 (epoch   51.76), loss: 0.005877, time: 2.256 s\n",
      "Step 165660 (epoch   51.77), loss: 0.004901, time: 2.248 s\n",
      "Step 165680 (epoch   51.77), loss: 0.003005, time: 2.230 s\n",
      "Step 165700 (epoch   51.78), loss: 0.005520, time: 2.248 s\n",
      "Step 165720 (epoch   51.79), loss: 0.004796, time: 2.255 s\n",
      "Step 165740 (epoch   51.79), loss: 0.003927, time: 2.229 s\n",
      "Step 165760 (epoch   51.80), loss: 0.005660, time: 2.244 s\n",
      "Step 165780 (epoch   51.81), loss: 0.004527, time: 2.246 s\n",
      "Step 165800 (epoch   51.81), loss: 0.003543, time: 2.241 s, error: 0.011406\n",
      "Step 165820 (epoch   51.82), loss: 0.004982, time: 15.099 s\n",
      "Step 165840 (epoch   51.83), loss: 0.003620, time: 2.243 s\n",
      "Step 165860 (epoch   51.83), loss: 0.003429, time: 2.255 s\n",
      "Step 165880 (epoch   51.84), loss: 0.005709, time: 2.255 s\n",
      "Step 165900 (epoch   51.84), loss: 0.003611, time: 2.246 s\n",
      "Step 165920 (epoch   51.85), loss: 0.004763, time: 2.244 s\n",
      "Step 165940 (epoch   51.86), loss: 0.003860, time: 2.235 s\n",
      "Step 165960 (epoch   51.86), loss: 0.007161, time: 2.244 s\n",
      "Step 165980 (epoch   51.87), loss: 0.003252, time: 2.252 s\n",
      "Step 166000 (epoch   51.88), loss: 0.005143, time: 2.238 s, error: 0.011864\n",
      "\n",
      "Time since beginning  : 29043.696 s\n",
      "\n",
      "Step 166020 (epoch   51.88), loss: 0.004685, time: 15.109 s\n",
      "Step 166040 (epoch   51.89), loss: 0.004586, time: 2.240 s\n",
      "Step 166060 (epoch   51.89), loss: 0.005275, time: 2.247 s\n",
      "Step 166080 (epoch   51.90), loss: 0.010666, time: 2.247 s\n",
      "Step 166100 (epoch   51.91), loss: 0.007886, time: 2.244 s\n",
      "Step 166120 (epoch   51.91), loss: 0.005990, time: 2.236 s\n",
      "Step 166140 (epoch   51.92), loss: 0.005056, time: 2.250 s\n",
      "Step 166160 (epoch   51.92), loss: 0.003427, time: 2.227 s\n",
      "Step 166180 (epoch   51.93), loss: 0.005283, time: 2.244 s\n",
      "Step 166200 (epoch   51.94), loss: 0.007895, time: 2.239 s, error: 0.011067\n",
      "Step 166220 (epoch   51.94), loss: 0.005784, time: 15.262 s\n",
      "Step 166240 (epoch   51.95), loss: 0.003569, time: 2.255 s\n",
      "Step 166260 (epoch   51.96), loss: 0.005174, time: 2.243 s\n",
      "Step 166280 (epoch   51.96), loss: 0.005687, time: 2.246 s\n",
      "Step 166300 (epoch   51.97), loss: 0.014139, time: 2.237 s\n",
      "Step 166320 (epoch   51.98), loss: 0.003958, time: 2.252 s\n",
      "Step 166340 (epoch   51.98), loss: 0.004834, time: 2.252 s\n",
      "Step 166360 (epoch   51.99), loss: 0.005235, time: 2.255 s\n",
      "Step 166380 (epoch   51.99), loss: 0.003926, time: 2.238 s\n",
      "Step 166400 (epoch   52.00), loss: 0.006946, time: 2.247 s, error: 0.011634\n",
      "Step 166420 (epoch   52.01), loss: 0.005005, time: 15.266 s\n",
      "Step 166440 (epoch   52.01), loss: 0.006637, time: 2.232 s\n",
      "Step 166460 (epoch   52.02), loss: 0.006836, time: 2.247 s\n",
      "Step 166480 (epoch   52.02), loss: 0.003811, time: 2.235 s\n",
      "Step 166500 (epoch   52.03), loss: 0.004659, time: 2.247 s\n",
      "Step 166520 (epoch   52.04), loss: 0.003316, time: 2.241 s\n",
      "Step 166540 (epoch   52.04), loss: 0.007207, time: 2.244 s\n",
      "Step 166560 (epoch   52.05), loss: 0.007233, time: 2.246 s\n",
      "Step 166580 (epoch   52.06), loss: 0.004586, time: 2.247 s\n",
      "Step 166600 (epoch   52.06), loss: 0.005370, time: 2.250 s, error: 0.011479\n",
      "Step 166620 (epoch   52.07), loss: 0.006348, time: 15.211 s\n",
      "Step 166640 (epoch   52.08), loss: 0.003974, time: 2.250 s\n",
      "Step 166660 (epoch   52.08), loss: 0.003294, time: 2.239 s\n",
      "Step 166680 (epoch   52.09), loss: 0.004585, time: 2.236 s\n",
      "Step 166700 (epoch   52.09), loss: 0.006067, time: 2.240 s\n",
      "Step 166720 (epoch   52.10), loss: 0.003249, time: 2.234 s\n",
      "Step 166740 (epoch   52.11), loss: 0.011405, time: 2.248 s\n",
      "Step 166760 (epoch   52.11), loss: 0.006578, time: 2.240 s\n",
      "Step 166780 (epoch   52.12), loss: 0.007373, time: 2.242 s\n",
      "Step 166800 (epoch   52.12), loss: 0.007850, time: 2.229 s, error: 0.011713\n",
      "Step 166820 (epoch   52.13), loss: 0.004732, time: 15.107 s\n",
      "Step 166840 (epoch   52.14), loss: 0.006469, time: 2.244 s\n",
      "Step 166860 (epoch   52.14), loss: 0.009406, time: 2.248 s\n",
      "Step 166880 (epoch   52.15), loss: 0.004163, time: 2.253 s\n",
      "Step 166900 (epoch   52.16), loss: 0.004780, time: 2.257 s\n",
      "Step 166920 (epoch   52.16), loss: 0.005782, time: 2.246 s\n",
      "Step 166940 (epoch   52.17), loss: 0.003623, time: 2.225 s\n",
      "Step 166960 (epoch   52.17), loss: 0.005756, time: 2.252 s\n",
      "Step 166980 (epoch   52.18), loss: 0.005082, time: 2.239 s\n",
      "Step 167000 (epoch   52.19), loss: 0.003760, time: 2.252 s, error: 0.011665\n",
      "Step 167020 (epoch   52.19), loss: 0.006306, time: 15.150 s\n",
      "Step 167040 (epoch   52.20), loss: 0.005894, time: 2.228 s\n",
      "Step 167060 (epoch   52.21), loss: 0.009160, time: 2.243 s\n",
      "Step 167080 (epoch   52.21), loss: 0.003470, time: 2.242 s\n",
      "Step 167100 (epoch   52.22), loss: 0.005082, time: 2.236 s\n",
      "Step 167120 (epoch   52.23), loss: 0.005838, time: 2.248 s\n",
      "Step 167140 (epoch   52.23), loss: 0.008301, time: 2.249 s\n",
      "Step 167160 (epoch   52.24), loss: 0.005647, time: 2.261 s\n",
      "Step 167180 (epoch   52.24), loss: 0.004712, time: 2.241 s\n",
      "Step 167200 (epoch   52.25), loss: 0.005688, time: 2.248 s, error: 0.011222\n",
      "Step 167220 (epoch   52.26), loss: 0.004567, time: 15.154 s\n",
      "Step 167240 (epoch   52.26), loss: 0.006247, time: 2.226 s\n",
      "Step 167260 (epoch   52.27), loss: 0.005185, time: 2.250 s\n",
      "Step 167280 (epoch   52.27), loss: 0.003453, time: 2.234 s\n",
      "Step 167300 (epoch   52.28), loss: 0.007732, time: 2.236 s\n",
      "Step 167320 (epoch   52.29), loss: 0.004623, time: 2.226 s\n",
      "Step 167340 (epoch   52.29), loss: 0.003931, time: 2.235 s\n",
      "Step 167360 (epoch   52.30), loss: 0.006254, time: 2.238 s\n",
      "Step 167380 (epoch   52.31), loss: 0.005597, time: 2.256 s\n",
      "Step 167400 (epoch   52.31), loss: 0.003671, time: 2.259 s, error: 0.011146\n",
      "Step 167420 (epoch   52.32), loss: 0.009338, time: 15.268 s\n",
      "Step 167440 (epoch   52.33), loss: 0.005490, time: 2.260 s\n",
      "Step 167460 (epoch   52.33), loss: 0.005529, time: 2.254 s\n",
      "Step 167480 (epoch   52.34), loss: 0.005465, time: 2.247 s\n",
      "Step 167500 (epoch   52.34), loss: 0.003982, time: 2.244 s\n",
      "Step 167520 (epoch   52.35), loss: 0.003731, time: 2.252 s\n",
      "Step 167540 (epoch   52.36), loss: 0.004041, time: 2.236 s\n",
      "Step 167560 (epoch   52.36), loss: 0.003879, time: 2.232 s\n",
      "Step 167580 (epoch   52.37), loss: 0.007956, time: 2.240 s\n",
      "Step 167600 (epoch   52.38), loss: 0.004004, time: 2.231 s, error: 0.011890\n",
      "Step 167620 (epoch   52.38), loss: 0.007420, time: 15.257 s\n",
      "Step 167640 (epoch   52.39), loss: 0.004219, time: 2.253 s\n",
      "Step 167660 (epoch   52.39), loss: 0.004889, time: 2.237 s\n",
      "Step 167680 (epoch   52.40), loss: 0.007179, time: 2.244 s\n",
      "Step 167700 (epoch   52.41), loss: 0.004371, time: 2.243 s\n",
      "Step 167720 (epoch   52.41), loss: 0.005474, time: 2.242 s\n",
      "Step 167740 (epoch   52.42), loss: 0.003870, time: 2.240 s\n",
      "Step 167760 (epoch   52.42), loss: 0.007657, time: 2.232 s\n",
      "Step 167780 (epoch   52.43), loss: 0.005190, time: 2.245 s\n",
      "Step 167800 (epoch   52.44), loss: 0.006491, time: 2.242 s, error: 0.011227\n",
      "Step 167820 (epoch   52.44), loss: 0.006040, time: 15.194 s\n",
      "Step 167840 (epoch   52.45), loss: 0.004708, time: 2.247 s\n",
      "Step 167860 (epoch   52.46), loss: 0.004042, time: 2.224 s\n",
      "Step 167880 (epoch   52.46), loss: 0.006581, time: 2.236 s\n",
      "Step 167900 (epoch   52.47), loss: 0.012734, time: 2.234 s\n",
      "Step 167920 (epoch   52.48), loss: 0.003711, time: 2.245 s\n",
      "Step 167940 (epoch   52.48), loss: 0.005079, time: 2.234 s\n",
      "Step 167960 (epoch   52.49), loss: 0.004630, time: 2.250 s\n",
      "Step 167980 (epoch   52.49), loss: 0.005369, time: 2.241 s\n",
      "Step 168000 (epoch   52.50), loss: 0.004715, time: 2.246 s, error: 0.011350\n",
      "\n",
      "Time since beginning  : 29397.583 s\n",
      "\n",
      "Step 168020 (epoch   52.51), loss: 0.005228, time: 15.184 s\n",
      "Step 168040 (epoch   52.51), loss: 0.004869, time: 2.237 s\n",
      "Step 168060 (epoch   52.52), loss: 0.006077, time: 2.254 s\n",
      "Step 168080 (epoch   52.52), loss: 0.005901, time: 2.254 s\n",
      "Step 168100 (epoch   52.53), loss: 0.004382, time: 2.246 s\n",
      "Step 168120 (epoch   52.54), loss: 0.005069, time: 2.219 s\n",
      "Step 168140 (epoch   52.54), loss: 0.005238, time: 2.236 s\n",
      "Step 168160 (epoch   52.55), loss: 0.003446, time: 2.237 s\n",
      "Step 168180 (epoch   52.56), loss: 0.004837, time: 2.251 s\n",
      "Step 168200 (epoch   52.56), loss: 0.005734, time: 2.238 s, error: 0.011903\n",
      "Step 168220 (epoch   52.57), loss: 0.006522, time: 15.042 s\n",
      "Step 168240 (epoch   52.58), loss: 0.006845, time: 2.243 s\n",
      "Step 168260 (epoch   52.58), loss: 0.005156, time: 2.236 s\n",
      "Step 168280 (epoch   52.59), loss: 0.008470, time: 2.248 s\n",
      "Step 168300 (epoch   52.59), loss: 0.006577, time: 2.236 s\n",
      "Step 168320 (epoch   52.60), loss: 0.007926, time: 2.255 s\n",
      "Step 168340 (epoch   52.61), loss: 0.003713, time: 2.254 s\n",
      "Step 168360 (epoch   52.61), loss: 0.009015, time: 2.243 s\n",
      "Step 168380 (epoch   52.62), loss: 0.006571, time: 2.238 s\n",
      "Step 168400 (epoch   52.62), loss: 0.006271, time: 2.245 s, error: 0.011547\n",
      "Step 168420 (epoch   52.63), loss: 0.005091, time: 15.087 s\n",
      "Step 168440 (epoch   52.64), loss: 0.003744, time: 2.238 s\n",
      "Step 168460 (epoch   52.64), loss: 0.005590, time: 2.247 s\n",
      "Step 168480 (epoch   52.65), loss: 0.005405, time: 2.237 s\n",
      "Step 168500 (epoch   52.66), loss: 0.005617, time: 2.233 s\n",
      "Step 168520 (epoch   52.66), loss: 0.005619, time: 2.231 s\n",
      "Step 168540 (epoch   52.67), loss: 0.004603, time: 2.246 s\n",
      "Step 168560 (epoch   52.67), loss: 0.004965, time: 2.224 s\n",
      "Step 168580 (epoch   52.68), loss: 0.005185, time: 2.250 s\n",
      "Step 168600 (epoch   52.69), loss: 0.005274, time: 2.256 s, error: 0.011324\n",
      "Step 168620 (epoch   52.69), loss: 0.005378, time: 15.180 s\n",
      "Step 168640 (epoch   52.70), loss: 0.003610, time: 2.237 s\n",
      "Step 168660 (epoch   52.71), loss: 0.008627, time: 2.247 s\n",
      "Step 168680 (epoch   52.71), loss: 0.005821, time: 2.229 s\n",
      "Step 168700 (epoch   52.72), loss: 0.007559, time: 2.249 s\n",
      "Step 168720 (epoch   52.73), loss: 0.006637, time: 2.241 s\n",
      "Step 168740 (epoch   52.73), loss: 0.004600, time: 2.239 s\n",
      "Step 168760 (epoch   52.74), loss: 0.004838, time: 2.242 s\n",
      "Step 168780 (epoch   52.74), loss: 0.005061, time: 2.244 s\n",
      "Step 168800 (epoch   52.75), loss: 0.003998, time: 2.237 s, error: 0.011564\n",
      "Step 168820 (epoch   52.76), loss: 0.004565, time: 15.266 s\n",
      "Step 168840 (epoch   52.76), loss: 0.003554, time: 2.242 s\n",
      "Step 168860 (epoch   52.77), loss: 0.005584, time: 2.250 s\n",
      "Step 168880 (epoch   52.77), loss: 0.004991, time: 2.242 s\n",
      "Step 168900 (epoch   52.78), loss: 0.003867, time: 2.244 s\n",
      "Step 168920 (epoch   52.79), loss: 0.006331, time: 2.240 s\n",
      "Step 168940 (epoch   52.79), loss: 0.004193, time: 2.241 s\n",
      "Step 168960 (epoch   52.80), loss: 0.004390, time: 2.249 s\n",
      "Step 168980 (epoch   52.81), loss: 0.006707, time: 2.240 s\n",
      "Step 169000 (epoch   52.81), loss: 0.005786, time: 2.236 s, error: 0.011298\n",
      "Step 169020 (epoch   52.82), loss: 0.005712, time: 15.346 s\n",
      "Step 169040 (epoch   52.83), loss: 0.004420, time: 2.249 s\n",
      "Step 169060 (epoch   52.83), loss: 0.005632, time: 2.245 s\n",
      "Step 169080 (epoch   52.84), loss: 0.005912, time: 2.241 s\n",
      "Step 169100 (epoch   52.84), loss: 0.003459, time: 2.244 s\n",
      "Step 169120 (epoch   52.85), loss: 0.006620, time: 2.246 s\n",
      "Step 169140 (epoch   52.86), loss: 0.006115, time: 2.232 s\n",
      "Step 169160 (epoch   52.86), loss: 0.005576, time: 2.240 s\n",
      "Step 169180 (epoch   52.87), loss: 0.004197, time: 2.248 s\n",
      "Step 169200 (epoch   52.88), loss: 0.008107, time: 2.238 s, error: 0.011728\n",
      "Step 169220 (epoch   52.88), loss: 0.003800, time: 15.136 s\n",
      "Step 169240 (epoch   52.89), loss: 0.007275, time: 2.255 s\n",
      "Step 169260 (epoch   52.89), loss: 0.004505, time: 2.258 s\n",
      "Step 169280 (epoch   52.90), loss: 0.004441, time: 2.252 s\n",
      "Step 169300 (epoch   52.91), loss: 0.007208, time: 2.243 s\n",
      "Step 169320 (epoch   52.91), loss: 0.003603, time: 2.241 s\n",
      "Step 169340 (epoch   52.92), loss: 0.004080, time: 2.230 s\n",
      "Step 169360 (epoch   52.92), loss: 0.005650, time: 2.244 s\n",
      "Step 169380 (epoch   52.93), loss: 0.003226, time: 2.240 s\n",
      "Step 169400 (epoch   52.94), loss: 0.003684, time: 2.249 s, error: 0.011128\n",
      "Step 169420 (epoch   52.94), loss: 0.004605, time: 15.075 s\n",
      "Step 169440 (epoch   52.95), loss: 0.009080, time: 2.239 s\n",
      "Step 169460 (epoch   52.96), loss: 0.005942, time: 2.245 s\n",
      "Step 169480 (epoch   52.96), loss: 0.005255, time: 2.238 s\n",
      "Step 169500 (epoch   52.97), loss: 0.003782, time: 2.245 s\n",
      "Step 169520 (epoch   52.98), loss: 0.004408, time: 2.248 s\n",
      "Step 169540 (epoch   52.98), loss: 0.005866, time: 2.249 s\n",
      "Step 169560 (epoch   52.99), loss: 0.005136, time: 2.258 s\n",
      "Step 169580 (epoch   52.99), loss: 0.005267, time: 2.240 s\n",
      "Step 169600 (epoch   53.00), loss: 0.003924, time: 2.256 s, error: 0.011424\n",
      "Step 169620 (epoch   53.01), loss: 0.004755, time: 15.093 s\n",
      "Step 169640 (epoch   53.01), loss: 0.004141, time: 2.233 s\n",
      "Step 169660 (epoch   53.02), loss: 0.003499, time: 2.250 s\n",
      "Step 169680 (epoch   53.02), loss: 0.006184, time: 2.254 s\n",
      "Step 169700 (epoch   53.03), loss: 0.004840, time: 2.254 s\n",
      "Step 169720 (epoch   53.04), loss: 0.005302, time: 2.237 s\n",
      "Step 169740 (epoch   53.04), loss: 0.005650, time: 2.234 s\n",
      "Step 169760 (epoch   53.05), loss: 0.004459, time: 2.255 s\n",
      "Step 169780 (epoch   53.06), loss: 0.005034, time: 2.260 s\n",
      "Step 169800 (epoch   53.06), loss: 0.003074, time: 2.256 s, error: 0.011440\n",
      "Step 169820 (epoch   53.07), loss: 0.003680, time: 15.123 s\n",
      "Step 169840 (epoch   53.08), loss: 0.003907, time: 2.251 s\n",
      "Step 169860 (epoch   53.08), loss: 0.005998, time: 2.244 s\n",
      "Step 169880 (epoch   53.09), loss: 0.004232, time: 2.237 s\n",
      "Step 169900 (epoch   53.09), loss: 0.008412, time: 2.247 s\n",
      "Step 169920 (epoch   53.10), loss: 0.005443, time: 2.241 s\n",
      "Step 169940 (epoch   53.11), loss: 0.005584, time: 2.246 s\n",
      "Step 169960 (epoch   53.11), loss: 0.004150, time: 2.253 s\n",
      "Step 169980 (epoch   53.12), loss: 0.005226, time: 2.227 s\n",
      "Step 170000 (epoch   53.12), loss: 0.005918, time: 2.235 s, error: 0.011706\n",
      "\n",
      "Time since beginning  : 29751.150 s\n",
      "\n",
      "Step 170020 (epoch   53.13), loss: 0.005643, time: 15.304 s\n",
      "Step 170040 (epoch   53.14), loss: 0.005432, time: 2.241 s\n",
      "Step 170060 (epoch   53.14), loss: 0.006281, time: 2.240 s\n",
      "Step 170080 (epoch   53.15), loss: 0.005809, time: 2.250 s\n",
      "Step 170100 (epoch   53.16), loss: 0.008098, time: 2.242 s\n",
      "Step 170120 (epoch   53.16), loss: 0.003796, time: 2.246 s\n",
      "Step 170140 (epoch   53.17), loss: 0.005623, time: 2.250 s\n",
      "Step 170160 (epoch   53.17), loss: 0.004853, time: 2.232 s\n",
      "Step 170180 (epoch   53.18), loss: 0.006607, time: 2.239 s\n",
      "Step 170200 (epoch   53.19), loss: 0.007927, time: 2.238 s, error: 0.011493\n",
      "Step 170220 (epoch   53.19), loss: 0.009339, time: 15.351 s\n",
      "Step 170240 (epoch   53.20), loss: 0.004876, time: 2.239 s\n",
      "Step 170260 (epoch   53.21), loss: 0.004939, time: 2.245 s\n",
      "Step 170280 (epoch   53.21), loss: 0.004337, time: 2.251 s\n",
      "Step 170300 (epoch   53.22), loss: 0.003618, time: 2.244 s\n",
      "Step 170320 (epoch   53.23), loss: 0.005773, time: 2.241 s\n",
      "Step 170340 (epoch   53.23), loss: 0.005330, time: 2.228 s\n",
      "Step 170360 (epoch   53.24), loss: 0.007080, time: 2.251 s\n",
      "Step 170380 (epoch   53.24), loss: 0.008633, time: 2.256 s\n",
      "Step 170400 (epoch   53.25), loss: 0.005408, time: 2.244 s, error: 0.011192\n",
      "Step 170420 (epoch   53.26), loss: 0.004492, time: 15.098 s\n",
      "Step 170440 (epoch   53.26), loss: 0.004211, time: 2.253 s\n",
      "Step 170460 (epoch   53.27), loss: 0.011697, time: 2.227 s\n",
      "Step 170480 (epoch   53.27), loss: 0.006078, time: 2.248 s\n",
      "Step 170500 (epoch   53.28), loss: 0.007428, time: 2.235 s\n",
      "Step 170520 (epoch   53.29), loss: 0.005030, time: 2.243 s\n",
      "Step 170540 (epoch   53.29), loss: 0.008600, time: 2.239 s\n",
      "Step 170560 (epoch   53.30), loss: 0.007639, time: 2.250 s\n",
      "Step 170580 (epoch   53.31), loss: 0.004996, time: 2.241 s\n",
      "Step 170600 (epoch   53.31), loss: 0.004796, time: 2.231 s, error: 0.011129\n",
      "Step 170620 (epoch   53.32), loss: 0.006270, time: 15.062 s\n",
      "Step 170640 (epoch   53.33), loss: 0.003775, time: 2.229 s\n",
      "Step 170660 (epoch   53.33), loss: 0.006798, time: 2.245 s\n",
      "Step 170680 (epoch   53.34), loss: 0.005970, time: 2.240 s\n",
      "Step 170700 (epoch   53.34), loss: 0.005517, time: 2.263 s\n",
      "Step 170720 (epoch   53.35), loss: 0.003460, time: 2.237 s\n",
      "Step 170740 (epoch   53.36), loss: 0.005045, time: 2.237 s\n",
      "Step 170760 (epoch   53.36), loss: 0.004720, time: 2.256 s\n",
      "Step 170780 (epoch   53.37), loss: 0.005850, time: 2.254 s\n",
      "Step 170800 (epoch   53.38), loss: 0.004622, time: 2.235 s, error: 0.012004\n",
      "Step 170820 (epoch   53.38), loss: 0.003402, time: 15.082 s\n",
      "Step 170840 (epoch   53.39), loss: 0.005539, time: 2.239 s\n",
      "Step 170860 (epoch   53.39), loss: 0.004928, time: 2.244 s\n",
      "Step 170880 (epoch   53.40), loss: 0.005301, time: 2.251 s\n",
      "Step 170900 (epoch   53.41), loss: 0.004247, time: 2.243 s\n",
      "Step 170920 (epoch   53.41), loss: 0.003732, time: 2.230 s\n",
      "Step 170940 (epoch   53.42), loss: 0.002649, time: 2.246 s\n",
      "Step 170960 (epoch   53.42), loss: 0.006917, time: 2.262 s\n",
      "Step 170980 (epoch   53.43), loss: 0.007416, time: 2.232 s\n",
      "Step 171000 (epoch   53.44), loss: 0.006935, time: 2.240 s, error: 0.011291\n",
      "Step 171020 (epoch   53.44), loss: 0.007717, time: 15.061 s\n",
      "Step 171040 (epoch   53.45), loss: 0.011891, time: 2.234 s\n",
      "Step 171060 (epoch   53.46), loss: 0.006249, time: 2.244 s\n",
      "Step 171080 (epoch   53.46), loss: 0.007940, time: 2.247 s\n",
      "Step 171100 (epoch   53.47), loss: 0.004868, time: 2.232 s\n",
      "Step 171120 (epoch   53.48), loss: 0.004766, time: 2.236 s\n",
      "Step 171140 (epoch   53.48), loss: 0.007855, time: 2.246 s\n",
      "Step 171160 (epoch   53.49), loss: 0.003993, time: 2.239 s\n",
      "Step 171180 (epoch   53.49), loss: 0.003749, time: 2.234 s\n",
      "Step 171200 (epoch   53.50), loss: 0.005333, time: 2.237 s, error: 0.011087\n",
      "Step 171220 (epoch   53.51), loss: 0.008521, time: 15.220 s\n",
      "Step 171240 (epoch   53.51), loss: 0.005527, time: 2.231 s\n",
      "Step 171260 (epoch   53.52), loss: 0.004042, time: 2.227 s\n",
      "Step 171280 (epoch   53.52), loss: 0.005125, time: 2.250 s\n",
      "Step 171300 (epoch   53.53), loss: 0.005727, time: 2.233 s\n",
      "Step 171320 (epoch   53.54), loss: 0.004045, time: 2.251 s\n",
      "Step 171340 (epoch   53.54), loss: 0.005101, time: 2.235 s\n",
      "Step 171360 (epoch   53.55), loss: 0.005875, time: 2.251 s\n",
      "Step 171380 (epoch   53.56), loss: 0.005349, time: 2.244 s\n",
      "Step 171400 (epoch   53.56), loss: 0.010513, time: 2.249 s, error: 0.011562\n",
      "Step 171420 (epoch   53.57), loss: 0.004084, time: 15.273 s\n",
      "Step 171440 (epoch   53.58), loss: 0.004947, time: 2.246 s\n",
      "Step 171460 (epoch   53.58), loss: 0.007020, time: 2.246 s\n",
      "Step 171480 (epoch   53.59), loss: 0.005569, time: 2.247 s\n",
      "Step 171500 (epoch   53.59), loss: 0.005482, time: 2.237 s\n",
      "Step 171520 (epoch   53.60), loss: 0.004499, time: 2.247 s\n",
      "Step 171540 (epoch   53.61), loss: 0.008392, time: 2.223 s\n",
      "Step 171560 (epoch   53.61), loss: 0.004973, time: 2.235 s\n",
      "Step 171580 (epoch   53.62), loss: 0.004701, time: 2.233 s\n",
      "Step 171600 (epoch   53.62), loss: 0.007095, time: 2.242 s, error: 0.011634\n",
      "Step 171620 (epoch   53.63), loss: 0.003390, time: 15.056 s\n",
      "Step 171640 (epoch   53.64), loss: 0.007060, time: 2.245 s\n",
      "Step 171660 (epoch   53.64), loss: 0.006155, time: 2.247 s\n",
      "Step 171680 (epoch   53.65), loss: 0.006229, time: 2.248 s\n",
      "Step 171700 (epoch   53.66), loss: 0.005535, time: 2.243 s\n",
      "Step 171720 (epoch   53.66), loss: 0.005422, time: 2.243 s\n",
      "Step 171740 (epoch   53.67), loss: 0.006349, time: 2.226 s\n",
      "Step 171760 (epoch   53.67), loss: 0.006443, time: 2.245 s\n",
      "Step 171780 (epoch   53.68), loss: 0.004484, time: 2.226 s\n",
      "Step 171800 (epoch   53.69), loss: 0.003879, time: 2.245 s, error: 0.011293\n",
      "Step 171820 (epoch   53.69), loss: 0.006812, time: 15.056 s\n",
      "Step 171840 (epoch   53.70), loss: 0.005724, time: 2.238 s\n",
      "Step 171860 (epoch   53.71), loss: 0.005713, time: 2.238 s\n",
      "Step 171880 (epoch   53.71), loss: 0.006569, time: 2.254 s\n",
      "Step 171900 (epoch   53.72), loss: 0.005201, time: 2.249 s\n",
      "Step 171920 (epoch   53.73), loss: 0.008596, time: 2.225 s\n",
      "Step 171940 (epoch   53.73), loss: 0.005831, time: 2.243 s\n",
      "Step 171960 (epoch   53.74), loss: 0.003725, time: 2.231 s\n",
      "Step 171980 (epoch   53.74), loss: 0.004923, time: 2.240 s\n",
      "Step 172000 (epoch   53.75), loss: 0.005643, time: 2.245 s, error: 0.011546\n",
      "\n",
      "Time since beginning  : 30104.289 s\n",
      "\n",
      "Step 172020 (epoch   53.76), loss: 0.005212, time: 15.152 s\n",
      "Step 172040 (epoch   53.76), loss: 0.005421, time: 2.243 s\n",
      "Step 172060 (epoch   53.77), loss: 0.003664, time: 2.239 s\n",
      "Step 172080 (epoch   53.77), loss: 0.003886, time: 2.247 s\n",
      "Step 172100 (epoch   53.78), loss: 0.003915, time: 2.248 s\n",
      "Step 172120 (epoch   53.79), loss: 0.008752, time: 2.255 s\n",
      "Step 172140 (epoch   53.79), loss: 0.005594, time: 2.249 s\n",
      "Step 172160 (epoch   53.80), loss: 0.005415, time: 2.253 s\n",
      "Step 172180 (epoch   53.81), loss: 0.004508, time: 2.237 s\n",
      "Step 172200 (epoch   53.81), loss: 0.003725, time: 2.236 s, error: 0.011264\n",
      "Step 172220 (epoch   53.82), loss: 0.004895, time: 15.084 s\n",
      "Step 172240 (epoch   53.83), loss: 0.007137, time: 2.244 s\n",
      "Step 172260 (epoch   53.83), loss: 0.005850, time: 2.230 s\n",
      "Step 172280 (epoch   53.84), loss: 0.006255, time: 2.238 s\n",
      "Step 172300 (epoch   53.84), loss: 0.004834, time: 2.234 s\n",
      "Step 172320 (epoch   53.85), loss: 0.005119, time: 2.238 s\n",
      "Step 172340 (epoch   53.86), loss: 0.009588, time: 2.239 s\n",
      "Step 172360 (epoch   53.86), loss: 0.003937, time: 2.230 s\n",
      "Step 172380 (epoch   53.87), loss: 0.006308, time: 2.246 s\n",
      "Step 172400 (epoch   53.88), loss: 0.003920, time: 2.258 s, error: 0.011269\n",
      "Step 172420 (epoch   53.88), loss: 0.003855, time: 15.184 s\n",
      "Step 172440 (epoch   53.89), loss: 0.005658, time: 2.251 s\n",
      "Step 172460 (epoch   53.89), loss: 0.004972, time: 2.245 s\n",
      "Step 172480 (epoch   53.90), loss: 0.006169, time: 2.239 s\n",
      "Step 172500 (epoch   53.91), loss: 0.005179, time: 2.228 s\n",
      "Step 172520 (epoch   53.91), loss: 0.004622, time: 2.239 s\n",
      "Step 172540 (epoch   53.92), loss: 0.003908, time: 2.243 s\n",
      "Step 172560 (epoch   53.92), loss: 0.006406, time: 2.245 s\n",
      "Step 172580 (epoch   53.93), loss: 0.003825, time: 2.231 s\n",
      "Step 172600 (epoch   53.94), loss: 0.004683, time: 2.247 s, error: 0.011155\n",
      "Step 172620 (epoch   53.94), loss: 0.005071, time: 15.297 s\n",
      "Step 172640 (epoch   53.95), loss: 0.008180, time: 2.243 s\n",
      "Step 172660 (epoch   53.96), loss: 0.004276, time: 2.247 s\n",
      "Step 172680 (epoch   53.96), loss: 0.004742, time: 2.242 s\n",
      "Step 172700 (epoch   53.97), loss: 0.010756, time: 2.248 s\n",
      "Step 172720 (epoch   53.98), loss: 0.004800, time: 2.244 s\n",
      "Step 172740 (epoch   53.98), loss: 0.004967, time: 2.241 s\n",
      "Step 172760 (epoch   53.99), loss: 0.008820, time: 2.235 s\n",
      "Step 172780 (epoch   53.99), loss: 0.002764, time: 2.244 s\n",
      "Step 172800 (epoch   54.00), loss: 0.004345, time: 2.242 s, error: 0.011192\n",
      "Step 172820 (epoch   54.01), loss: 0.004222, time: 15.202 s\n",
      "Step 172840 (epoch   54.01), loss: 0.006169, time: 2.251 s\n",
      "Step 172860 (epoch   54.02), loss: 0.007245, time: 2.234 s\n",
      "Step 172880 (epoch   54.02), loss: 0.003018, time: 2.242 s\n",
      "Step 172900 (epoch   54.03), loss: 0.002440, time: 2.238 s\n",
      "Step 172920 (epoch   54.04), loss: 0.006549, time: 2.247 s\n",
      "Step 172940 (epoch   54.04), loss: 0.005652, time: 2.245 s\n",
      "Step 172960 (epoch   54.05), loss: 0.004429, time: 2.248 s\n",
      "Step 172980 (epoch   54.06), loss: 0.004596, time: 2.235 s\n",
      "Step 173000 (epoch   54.06), loss: 0.003960, time: 2.245 s, error: 0.011387\n",
      "Step 173020 (epoch   54.07), loss: 0.004956, time: 15.053 s\n",
      "Step 173040 (epoch   54.08), loss: 0.004339, time: 2.244 s\n",
      "Step 173060 (epoch   54.08), loss: 0.003037, time: 2.249 s\n",
      "Step 173080 (epoch   54.09), loss: 0.006014, time: 2.247 s\n",
      "Step 173100 (epoch   54.09), loss: 0.006907, time: 2.243 s\n",
      "Step 173120 (epoch   54.10), loss: 0.005359, time: 2.240 s\n",
      "Step 173140 (epoch   54.11), loss: 0.005561, time: 2.247 s\n",
      "Step 173160 (epoch   54.11), loss: 0.005371, time: 2.238 s\n",
      "Step 173180 (epoch   54.12), loss: 0.004484, time: 2.243 s\n",
      "Step 173200 (epoch   54.12), loss: 0.015309, time: 2.238 s, error: 0.011304\n",
      "Step 173220 (epoch   54.13), loss: 0.003506, time: 15.010 s\n",
      "Step 173240 (epoch   54.14), loss: 0.007057, time: 2.247 s\n",
      "Step 173260 (epoch   54.14), loss: 0.004147, time: 2.239 s\n",
      "Step 173280 (epoch   54.15), loss: 0.005460, time: 2.235 s\n",
      "Step 173300 (epoch   54.16), loss: 0.004803, time: 2.236 s\n",
      "Step 173320 (epoch   54.16), loss: 0.003037, time: 2.253 s\n",
      "Step 173340 (epoch   54.17), loss: 0.004855, time: 2.257 s\n",
      "Step 173360 (epoch   54.17), loss: 0.007799, time: 2.252 s\n",
      "Step 173380 (epoch   54.18), loss: 0.004294, time: 2.244 s\n",
      "Step 173400 (epoch   54.19), loss: 0.006316, time: 2.254 s, error: 0.011131\n",
      "Step 173420 (epoch   54.19), loss: 0.004074, time: 15.119 s\n",
      "Step 173440 (epoch   54.20), loss: 0.004241, time: 2.241 s\n",
      "Step 173460 (epoch   54.21), loss: 0.004631, time: 2.246 s\n",
      "Step 173480 (epoch   54.21), loss: 0.005218, time: 2.233 s\n",
      "Step 173500 (epoch   54.22), loss: 0.004446, time: 2.242 s\n",
      "Step 173520 (epoch   54.23), loss: 0.004151, time: 2.251 s\n",
      "Step 173540 (epoch   54.23), loss: 0.005149, time: 2.237 s\n",
      "Step 173560 (epoch   54.24), loss: 0.005433, time: 2.234 s\n",
      "Step 173580 (epoch   54.24), loss: 0.004737, time: 2.254 s\n",
      "Step 173600 (epoch   54.25), loss: 0.003395, time: 2.255 s, error: 0.011109\n",
      "Step 173620 (epoch   54.26), loss: 0.005089, time: 15.082 s\n",
      "Step 173640 (epoch   54.26), loss: 0.003763, time: 2.241 s\n",
      "Step 173660 (epoch   54.27), loss: 0.004132, time: 2.242 s\n",
      "Step 173680 (epoch   54.27), loss: 0.003604, time: 2.232 s\n",
      "Step 173700 (epoch   54.28), loss: 0.004137, time: 2.239 s\n",
      "Step 173720 (epoch   54.29), loss: 0.003270, time: 2.253 s\n",
      "Step 173740 (epoch   54.29), loss: 0.003661, time: 2.225 s\n",
      "Step 173760 (epoch   54.30), loss: 0.005808, time: 2.247 s\n",
      "Step 173780 (epoch   54.31), loss: 0.003163, time: 2.235 s\n",
      "Step 173800 (epoch   54.31), loss: 0.003268, time: 2.242 s, error: 0.011163\n",
      "Step 173820 (epoch   54.32), loss: 0.003355, time: 15.249 s\n",
      "Step 173840 (epoch   54.33), loss: 0.003272, time: 2.242 s\n",
      "Step 173860 (epoch   54.33), loss: 0.007389, time: 2.250 s\n",
      "Step 173880 (epoch   54.34), loss: 0.004343, time: 2.236 s\n",
      "Step 173900 (epoch   54.34), loss: 0.007758, time: 2.242 s\n",
      "Step 173920 (epoch   54.35), loss: 0.005400, time: 2.234 s\n",
      "Step 173940 (epoch   54.36), loss: 0.003934, time: 2.244 s\n",
      "Step 173960 (epoch   54.36), loss: 0.004315, time: 2.234 s\n",
      "Step 173980 (epoch   54.37), loss: 0.005095, time: 2.250 s\n",
      "Step 174000 (epoch   54.38), loss: 0.007642, time: 2.248 s, error: 0.011953\n",
      "\n",
      "Time since beginning  : 30457.727 s\n",
      "\n",
      "Step 174020 (epoch   54.38), loss: 0.006091, time: 15.311 s\n",
      "Step 174040 (epoch   54.39), loss: 0.006236, time: 2.232 s\n",
      "Step 174060 (epoch   54.39), loss: 0.006693, time: 2.224 s\n",
      "Step 174080 (epoch   54.40), loss: 0.005761, time: 2.250 s\n",
      "Step 174100 (epoch   54.41), loss: 0.005138, time: 2.239 s\n",
      "Step 174120 (epoch   54.41), loss: 0.005495, time: 2.244 s\n",
      "Step 174140 (epoch   54.42), loss: 0.003734, time: 2.227 s\n",
      "Step 174160 (epoch   54.42), loss: 0.003217, time: 2.249 s\n",
      "Step 174180 (epoch   54.43), loss: 0.004230, time: 2.232 s\n",
      "Step 174200 (epoch   54.44), loss: 0.004274, time: 2.246 s, error: 0.011159\n",
      "Step 174220 (epoch   54.44), loss: 0.005457, time: 15.080 s\n",
      "Step 174240 (epoch   54.45), loss: 0.005924, time: 2.258 s\n",
      "Step 174260 (epoch   54.46), loss: 0.004878, time: 2.251 s\n",
      "Step 174280 (epoch   54.46), loss: 0.004171, time: 2.252 s\n",
      "Step 174300 (epoch   54.47), loss: 0.005595, time: 2.245 s\n",
      "Step 174320 (epoch   54.48), loss: 0.005551, time: 2.255 s\n",
      "Step 174340 (epoch   54.48), loss: 0.003964, time: 2.239 s\n",
      "Step 174360 (epoch   54.49), loss: 0.006899, time: 2.246 s\n",
      "Step 174380 (epoch   54.49), loss: 0.003834, time: 2.239 s\n",
      "Step 174400 (epoch   54.50), loss: 0.003976, time: 2.242 s, error: 0.011025\n",
      "Step 174420 (epoch   54.51), loss: 0.004435, time: 15.015 s\n",
      "Step 174440 (epoch   54.51), loss: 0.004000, time: 2.242 s\n",
      "Step 174460 (epoch   54.52), loss: 0.005824, time: 2.232 s\n",
      "Step 174480 (epoch   54.52), loss: 0.006043, time: 2.240 s\n",
      "Step 174500 (epoch   54.53), loss: 0.004624, time: 2.256 s\n",
      "Step 174520 (epoch   54.54), loss: 0.004541, time: 2.250 s\n",
      "Step 174540 (epoch   54.54), loss: 0.005667, time: 2.245 s\n",
      "Step 174560 (epoch   54.55), loss: 0.004237, time: 2.233 s\n",
      "Step 174580 (epoch   54.56), loss: 0.004171, time: 2.253 s\n",
      "Step 174600 (epoch   54.56), loss: 0.009307, time: 2.235 s, error: 0.011118\n",
      "Step 174620 (epoch   54.57), loss: 0.009446, time: 15.073 s\n",
      "Step 174640 (epoch   54.58), loss: 0.006732, time: 2.247 s\n",
      "Step 174660 (epoch   54.58), loss: 0.006166, time: 2.242 s\n",
      "Step 174680 (epoch   54.59), loss: 0.006234, time: 2.249 s\n",
      "Step 174700 (epoch   54.59), loss: 0.005498, time: 2.238 s\n",
      "Step 174720 (epoch   54.60), loss: 0.003718, time: 2.238 s\n",
      "Step 174740 (epoch   54.61), loss: 0.006669, time: 2.237 s\n",
      "Step 174760 (epoch   54.61), loss: 0.005745, time: 2.243 s\n",
      "Step 174780 (epoch   54.62), loss: 0.004662, time: 2.246 s\n",
      "Step 174800 (epoch   54.62), loss: 0.005508, time: 2.239 s, error: 0.011735\n",
      "Step 174820 (epoch   54.63), loss: 0.011046, time: 15.028 s\n",
      "Step 174840 (epoch   54.64), loss: 0.004935, time: 2.232 s\n",
      "Step 174860 (epoch   54.64), loss: 0.004825, time: 2.252 s\n",
      "Step 174880 (epoch   54.65), loss: 0.003857, time: 2.242 s\n",
      "Step 174900 (epoch   54.66), loss: 0.004388, time: 2.245 s\n",
      "Step 174920 (epoch   54.66), loss: 0.006032, time: 2.225 s\n",
      "Step 174940 (epoch   54.67), loss: 0.007942, time: 2.240 s\n",
      "Step 174960 (epoch   54.67), loss: 0.003320, time: 2.256 s\n",
      "Step 174980 (epoch   54.68), loss: 0.005611, time: 2.241 s\n",
      "Step 175000 (epoch   54.69), loss: 0.003492, time: 2.241 s, error: 0.011162\n",
      "Step 175020 (epoch   54.69), loss: 0.004277, time: 15.352 s\n",
      "Step 175040 (epoch   54.70), loss: 0.003003, time: 2.251 s\n",
      "Step 175060 (epoch   54.71), loss: 0.004606, time: 2.220 s\n",
      "Step 175080 (epoch   54.71), loss: 0.004343, time: 2.242 s\n",
      "Step 175100 (epoch   54.72), loss: 0.004113, time: 2.242 s\n",
      "Step 175120 (epoch   54.73), loss: 0.005741, time: 2.244 s\n",
      "Step 175140 (epoch   54.73), loss: 0.004897, time: 2.239 s\n",
      "Step 175160 (epoch   54.74), loss: 0.004899, time: 2.244 s\n",
      "Step 175180 (epoch   54.74), loss: 0.005812, time: 2.246 s\n",
      "Step 175200 (epoch   54.75), loss: 0.006145, time: 2.242 s, error: 0.011357\n",
      "Step 175220 (epoch   54.76), loss: 0.004026, time: 15.285 s\n",
      "Step 175240 (epoch   54.76), loss: 0.007546, time: 2.248 s\n",
      "Step 175260 (epoch   54.77), loss: 0.005501, time: 2.246 s\n",
      "Step 175280 (epoch   54.77), loss: 0.003347, time: 2.236 s\n",
      "Step 175300 (epoch   54.78), loss: 0.006000, time: 2.236 s\n",
      "Step 175320 (epoch   54.79), loss: 0.005761, time: 2.250 s\n",
      "Step 175340 (epoch   54.79), loss: 0.004279, time: 2.233 s\n",
      "Step 175360 (epoch   54.80), loss: 0.004709, time: 2.247 s\n",
      "Step 175380 (epoch   54.81), loss: 0.005521, time: 2.229 s\n",
      "Step 175400 (epoch   54.81), loss: 0.005431, time: 2.237 s, error: 0.011252\n",
      "Step 175420 (epoch   54.82), loss: 0.006708, time: 15.100 s\n",
      "Step 175440 (epoch   54.83), loss: 0.004956, time: 2.249 s\n",
      "Step 175460 (epoch   54.83), loss: 0.003448, time: 2.250 s\n",
      "Step 175480 (epoch   54.84), loss: 0.010575, time: 2.250 s\n",
      "Step 175500 (epoch   54.84), loss: 0.004951, time: 2.243 s\n",
      "Step 175520 (epoch   54.85), loss: 0.003566, time: 2.233 s\n",
      "Step 175540 (epoch   54.86), loss: 0.004263, time: 2.242 s\n",
      "Step 175560 (epoch   54.86), loss: 0.003699, time: 2.227 s\n",
      "Step 175580 (epoch   54.87), loss: 0.009701, time: 2.243 s\n",
      "Step 175600 (epoch   54.88), loss: 0.004431, time: 2.246 s, error: 0.010989\n",
      "Step 175620 (epoch   54.88), loss: 0.005567, time: 15.078 s\n",
      "Step 175640 (epoch   54.89), loss: 0.007217, time: 2.242 s\n",
      "Step 175660 (epoch   54.89), loss: 0.010276, time: 2.241 s\n",
      "Step 175680 (epoch   54.90), loss: 0.003918, time: 2.255 s\n",
      "Step 175700 (epoch   54.91), loss: 0.003393, time: 2.251 s\n",
      "Step 175720 (epoch   54.91), loss: 0.004915, time: 2.239 s\n",
      "Step 175740 (epoch   54.92), loss: 0.004400, time: 2.245 s\n",
      "Step 175760 (epoch   54.92), loss: 0.003722, time: 2.241 s\n",
      "Step 175780 (epoch   54.93), loss: 0.004675, time: 2.252 s\n",
      "Step 175800 (epoch   54.94), loss: 0.003795, time: 2.237 s, error: 0.011295\n",
      "Step 175820 (epoch   54.94), loss: 0.006327, time: 15.069 s\n",
      "Step 175840 (epoch   54.95), loss: 0.006470, time: 2.242 s\n",
      "Step 175860 (epoch   54.96), loss: 0.006671, time: 2.236 s\n",
      "Step 175880 (epoch   54.96), loss: 0.003952, time: 2.241 s\n",
      "Step 175900 (epoch   54.97), loss: 0.004004, time: 2.236 s\n",
      "Step 175920 (epoch   54.98), loss: 0.003854, time: 2.246 s\n",
      "Step 175940 (epoch   54.98), loss: 0.006050, time: 2.253 s\n",
      "Step 175960 (epoch   54.99), loss: 0.006895, time: 2.254 s\n",
      "Step 175980 (epoch   54.99), loss: 0.008375, time: 2.239 s\n",
      "Step 176000 (epoch   55.00), loss: 0.004229, time: 2.247 s, error: 0.011204\n",
      "\n",
      "Time since beginning  : 30810.761 s\n",
      "\n",
      "Step 176020 (epoch   55.01), loss: 0.007853, time: 15.127 s\n",
      "Step 176040 (epoch   55.01), loss: 0.003910, time: 2.242 s\n",
      "Step 176060 (epoch   55.02), loss: 0.004128, time: 2.228 s\n",
      "Step 176080 (epoch   55.02), loss: 0.004911, time: 2.245 s\n",
      "Step 176100 (epoch   55.03), loss: 0.009230, time: 2.237 s\n",
      "Step 176120 (epoch   55.04), loss: 0.003767, time: 2.244 s\n",
      "Step 176140 (epoch   55.04), loss: 0.005109, time: 2.238 s\n",
      "Step 176160 (epoch   55.05), loss: 0.006814, time: 2.244 s\n",
      "Step 176180 (epoch   55.06), loss: 0.006167, time: 2.250 s\n",
      "Step 176200 (epoch   55.06), loss: 0.006762, time: 2.259 s, error: 0.011268\n",
      "Step 176220 (epoch   55.07), loss: 0.006227, time: 15.233 s\n",
      "Step 176240 (epoch   55.08), loss: 0.005206, time: 2.238 s\n",
      "Step 176260 (epoch   55.08), loss: 0.003128, time: 2.242 s\n",
      "Step 176280 (epoch   55.09), loss: 0.005868, time: 2.247 s\n",
      "Step 176300 (epoch   55.09), loss: 0.003558, time: 2.248 s\n",
      "Step 176320 (epoch   55.10), loss: 0.004648, time: 2.243 s\n",
      "Step 176340 (epoch   55.11), loss: 0.004263, time: 2.245 s\n",
      "Step 176360 (epoch   55.11), loss: 0.005991, time: 2.240 s\n",
      "Step 176380 (epoch   55.12), loss: 0.005082, time: 2.249 s\n",
      "Step 176400 (epoch   55.12), loss: 0.003800, time: 2.237 s, error: 0.010999\n",
      "Step 176420 (epoch   55.13), loss: 0.006838, time: 15.336 s\n",
      "Step 176440 (epoch   55.14), loss: 0.005763, time: 2.236 s\n",
      "Step 176460 (epoch   55.14), loss: 0.006257, time: 2.232 s\n",
      "Step 176480 (epoch   55.15), loss: 0.006156, time: 2.241 s\n",
      "Step 176500 (epoch   55.16), loss: 0.011684, time: 2.243 s\n",
      "Step 176520 (epoch   55.16), loss: 0.004445, time: 2.236 s\n",
      "Step 176540 (epoch   55.17), loss: 0.006932, time: 2.237 s\n",
      "Step 176560 (epoch   55.17), loss: 0.006409, time: 2.247 s\n",
      "Step 176580 (epoch   55.18), loss: 0.004356, time: 2.238 s\n",
      "Step 176600 (epoch   55.19), loss: 0.005664, time: 2.248 s, error: 0.011085\n",
      "Step 176620 (epoch   55.19), loss: 0.004200, time: 15.138 s\n",
      "Step 176640 (epoch   55.20), loss: 0.005207, time: 2.243 s\n",
      "Step 176660 (epoch   55.21), loss: 0.005226, time: 2.244 s\n",
      "Step 176680 (epoch   55.21), loss: 0.005708, time: 2.243 s\n",
      "Step 176700 (epoch   55.22), loss: 0.004297, time: 2.251 s\n",
      "Step 176720 (epoch   55.23), loss: 0.006832, time: 2.222 s\n",
      "Step 176740 (epoch   55.23), loss: 0.006009, time: 2.245 s\n",
      "Step 176760 (epoch   55.24), loss: 0.005886, time: 2.238 s\n",
      "Step 176780 (epoch   55.24), loss: 0.004712, time: 2.239 s\n",
      "Step 176800 (epoch   55.25), loss: 0.004399, time: 2.248 s, error: 0.011040\n",
      "Step 176820 (epoch   55.26), loss: 0.006006, time: 15.042 s\n",
      "Step 176840 (epoch   55.26), loss: 0.003604, time: 2.235 s\n",
      "Step 176860 (epoch   55.27), loss: 0.005602, time: 2.250 s\n",
      "Step 176880 (epoch   55.27), loss: 0.004462, time: 2.257 s\n",
      "Step 176900 (epoch   55.28), loss: 0.004961, time: 2.239 s\n",
      "Step 176920 (epoch   55.29), loss: 0.004230, time: 2.249 s\n",
      "Step 176940 (epoch   55.29), loss: 0.004159, time: 2.231 s\n",
      "Step 176960 (epoch   55.30), loss: 0.010326, time: 2.248 s\n",
      "Step 176980 (epoch   55.31), loss: 0.006601, time: 2.236 s\n",
      "Step 177000 (epoch   55.31), loss: 0.004616, time: 2.236 s, error: 0.011247\n",
      "Step 177020 (epoch   55.32), loss: 0.006620, time: 15.058 s\n",
      "Step 177040 (epoch   55.33), loss: 0.006502, time: 2.246 s\n",
      "Step 177060 (epoch   55.33), loss: 0.005367, time: 2.240 s\n",
      "Step 177080 (epoch   55.34), loss: 0.004324, time: 2.243 s\n",
      "Step 177100 (epoch   55.34), loss: 0.004765, time: 2.247 s\n",
      "Step 177120 (epoch   55.35), loss: 0.005493, time: 2.256 s\n",
      "Step 177140 (epoch   55.36), loss: 0.004913, time: 2.253 s\n",
      "Step 177160 (epoch   55.36), loss: 0.003838, time: 2.251 s\n",
      "Step 177180 (epoch   55.37), loss: 0.006376, time: 2.246 s\n",
      "Step 177200 (epoch   55.38), loss: 0.006355, time: 2.258 s, error: 0.011793\n",
      "Step 177220 (epoch   55.38), loss: 0.004733, time: 15.097 s\n",
      "Step 177240 (epoch   55.39), loss: 0.007413, time: 2.253 s\n",
      "Step 177260 (epoch   55.39), loss: 0.005088, time: 2.235 s\n",
      "Step 177280 (epoch   55.40), loss: 0.003165, time: 2.250 s\n",
      "Step 177300 (epoch   55.41), loss: 0.003075, time: 2.252 s\n",
      "Step 177320 (epoch   55.41), loss: 0.006529, time: 2.240 s\n",
      "Step 177340 (epoch   55.42), loss: 0.005815, time: 2.237 s\n",
      "Step 177360 (epoch   55.42), loss: 0.004416, time: 2.238 s\n",
      "Step 177380 (epoch   55.43), loss: 0.003606, time: 2.247 s\n",
      "Step 177400 (epoch   55.44), loss: 0.006015, time: 2.246 s, error: 0.011051\n",
      "Step 177420 (epoch   55.44), loss: 0.004727, time: 15.047 s\n",
      "Step 177440 (epoch   55.45), loss: 0.004778, time: 2.240 s\n",
      "Step 177460 (epoch   55.46), loss: 0.004342, time: 2.244 s\n",
      "Step 177480 (epoch   55.46), loss: 0.004977, time: 2.230 s\n",
      "Step 177500 (epoch   55.47), loss: 0.003573, time: 2.240 s\n",
      "Step 177520 (epoch   55.48), loss: 0.004219, time: 2.229 s\n",
      "Step 177540 (epoch   55.48), loss: 0.007595, time: 2.247 s\n",
      "Step 177560 (epoch   55.49), loss: 0.004477, time: 2.233 s\n",
      "Step 177580 (epoch   55.49), loss: 0.006260, time: 2.247 s\n",
      "Step 177600 (epoch   55.50), loss: 0.006137, time: 2.253 s, error: 0.011571\n",
      "Step 177620 (epoch   55.51), loss: 0.005661, time: 15.306 s\n",
      "Step 177640 (epoch   55.51), loss: 0.005951, time: 2.241 s\n",
      "Step 177660 (epoch   55.52), loss: 0.004277, time: 2.239 s\n",
      "Step 177680 (epoch   55.52), loss: 0.008123, time: 2.243 s\n",
      "Step 177700 (epoch   55.53), loss: 0.003685, time: 2.238 s\n",
      "Step 177720 (epoch   55.54), loss: 0.005317, time: 2.241 s\n",
      "Step 177740 (epoch   55.54), loss: 0.005050, time: 2.239 s\n",
      "Step 177760 (epoch   55.55), loss: 0.004166, time: 2.241 s\n",
      "Step 177780 (epoch   55.56), loss: 0.006076, time: 2.234 s\n",
      "Step 177800 (epoch   55.56), loss: 0.008835, time: 2.238 s, error: 0.011090\n",
      "Step 177820 (epoch   55.57), loss: 0.005660, time: 15.263 s\n",
      "Step 177840 (epoch   55.58), loss: 0.005857, time: 2.237 s\n",
      "Step 177860 (epoch   55.58), loss: 0.004244, time: 2.235 s\n",
      "Step 177880 (epoch   55.59), loss: 0.004092, time: 2.230 s\n",
      "Step 177900 (epoch   55.59), loss: 0.003673, time: 2.251 s\n",
      "Step 177920 (epoch   55.60), loss: 0.004016, time: 2.240 s\n",
      "Step 177940 (epoch   55.61), loss: 0.008237, time: 2.235 s\n",
      "Step 177960 (epoch   55.61), loss: 0.006211, time: 2.226 s\n",
      "Step 177980 (epoch   55.62), loss: 0.003748, time: 2.245 s\n",
      "Step 178000 (epoch   55.62), loss: 0.005908, time: 2.227 s, error: 0.011525\n",
      "\n",
      "Time since beginning  : 31164.203 s\n",
      "\n",
      "Step 178020 (epoch   55.63), loss: 0.009993, time: 15.155 s\n",
      "Step 178040 (epoch   55.64), loss: 0.004225, time: 2.256 s\n",
      "Step 178060 (epoch   55.64), loss: 0.004302, time: 2.259 s\n",
      "Step 178080 (epoch   55.65), loss: 0.005533, time: 2.250 s\n",
      "Step 178100 (epoch   55.66), loss: 0.003648, time: 2.242 s\n",
      "Step 178120 (epoch   55.66), loss: 0.008128, time: 2.243 s\n",
      "Step 178140 (epoch   55.67), loss: 0.003690, time: 2.225 s\n",
      "Step 178160 (epoch   55.67), loss: 0.003541, time: 2.245 s\n",
      "Step 178180 (epoch   55.68), loss: 0.005960, time: 2.241 s\n",
      "Step 178200 (epoch   55.69), loss: 0.006960, time: 2.248 s, error: 0.011067\n",
      "Step 178220 (epoch   55.69), loss: 0.005100, time: 15.126 s\n",
      "Step 178240 (epoch   55.70), loss: 0.005462, time: 2.226 s\n",
      "Step 178260 (epoch   55.71), loss: 0.004091, time: 2.247 s\n",
      "Step 178280 (epoch   55.71), loss: 0.004922, time: 2.229 s\n",
      "Step 178300 (epoch   55.72), loss: 0.007729, time: 2.256 s\n",
      "Step 178320 (epoch   55.73), loss: 0.004495, time: 2.253 s\n",
      "Step 178340 (epoch   55.73), loss: 0.002639, time: 2.239 s\n",
      "Step 178360 (epoch   55.74), loss: 0.005065, time: 2.234 s\n",
      "Step 178380 (epoch   55.74), loss: 0.009727, time: 2.239 s\n",
      "Step 178400 (epoch   55.75), loss: 0.004886, time: 2.250 s, error: 0.011232\n",
      "Step 178420 (epoch   55.76), loss: 0.004093, time: 15.209 s\n",
      "Step 178440 (epoch   55.76), loss: 0.004448, time: 2.249 s\n",
      "Step 178460 (epoch   55.77), loss: 0.005702, time: 2.237 s\n",
      "Step 178480 (epoch   55.77), loss: 0.004732, time: 2.247 s\n",
      "Step 178500 (epoch   55.78), loss: 0.003436, time: 2.223 s\n",
      "Step 178520 (epoch   55.79), loss: 0.013012, time: 2.251 s\n",
      "Step 178540 (epoch   55.79), loss: 0.005901, time: 2.233 s\n",
      "Step 178560 (epoch   55.80), loss: 0.003878, time: 2.244 s\n",
      "Step 178580 (epoch   55.81), loss: 0.005036, time: 2.256 s\n",
      "Step 178600 (epoch   55.81), loss: 0.007325, time: 2.241 s, error: 0.011180\n",
      "Step 178620 (epoch   55.82), loss: 0.004640, time: 15.016 s\n",
      "Step 178640 (epoch   55.83), loss: 0.004374, time: 2.228 s\n",
      "Step 178660 (epoch   55.83), loss: 0.005642, time: 2.247 s\n",
      "Step 178680 (epoch   55.84), loss: 0.010364, time: 2.240 s\n",
      "Step 178700 (epoch   55.84), loss: 0.004774, time: 2.241 s\n",
      "Step 178720 (epoch   55.85), loss: 0.003837, time: 2.237 s\n",
      "Step 178740 (epoch   55.86), loss: 0.004905, time: 2.230 s\n",
      "Step 178760 (epoch   55.86), loss: 0.004020, time: 2.231 s\n",
      "Step 178780 (epoch   55.87), loss: 0.005946, time: 2.240 s\n",
      "Step 178800 (epoch   55.88), loss: 0.002810, time: 2.254 s, error: 0.010854\n",
      "Step 178820 (epoch   55.88), loss: 0.004176, time: 15.273 s\n",
      "Step 178840 (epoch   55.89), loss: 0.004935, time: 2.244 s\n",
      "Step 178860 (epoch   55.89), loss: 0.005195, time: 2.236 s\n",
      "Step 178880 (epoch   55.90), loss: 0.014069, time: 2.252 s\n",
      "Step 178900 (epoch   55.91), loss: 0.005046, time: 2.240 s\n",
      "Step 178920 (epoch   55.91), loss: 0.004772, time: 2.252 s\n",
      "Step 178940 (epoch   55.92), loss: 0.005728, time: 2.247 s\n",
      "Step 178960 (epoch   55.92), loss: 0.003655, time: 2.254 s\n",
      "Step 178980 (epoch   55.93), loss: 0.007957, time: 2.253 s\n",
      "Step 179000 (epoch   55.94), loss: 0.003700, time: 2.250 s, error: 0.011238\n",
      "Step 179020 (epoch   55.94), loss: 0.003403, time: 15.325 s\n",
      "Step 179040 (epoch   55.95), loss: 0.004578, time: 2.244 s\n",
      "Step 179060 (epoch   55.96), loss: 0.005255, time: 2.238 s\n",
      "Step 179080 (epoch   55.96), loss: 0.006160, time: 2.248 s\n",
      "Step 179100 (epoch   55.97), loss: 0.005313, time: 2.240 s\n",
      "Step 179120 (epoch   55.98), loss: 0.003514, time: 2.243 s\n",
      "Step 179140 (epoch   55.98), loss: 0.003008, time: 2.220 s\n",
      "Step 179160 (epoch   55.99), loss: 0.004709, time: 2.232 s\n",
      "Step 179180 (epoch   55.99), loss: 0.007769, time: 2.240 s\n",
      "Step 179200 (epoch   56.00), loss: 0.003145, time: 2.242 s, error: 0.011402\n",
      "Step 179220 (epoch   56.01), loss: 0.006825, time: 15.063 s\n",
      "Step 179240 (epoch   56.01), loss: 0.004054, time: 2.255 s\n",
      "Step 179260 (epoch   56.02), loss: 0.003964, time: 2.242 s\n",
      "Step 179280 (epoch   56.02), loss: 0.004893, time: 2.236 s\n",
      "Step 179300 (epoch   56.03), loss: 0.006030, time: 2.246 s\n",
      "Step 179320 (epoch   56.04), loss: 0.004955, time: 2.239 s\n",
      "Step 179340 (epoch   56.04), loss: 0.008490, time: 2.243 s\n",
      "Step 179360 (epoch   56.05), loss: 0.005583, time: 2.244 s\n",
      "Step 179380 (epoch   56.06), loss: 0.005268, time: 2.240 s\n",
      "Step 179400 (epoch   56.06), loss: 0.004330, time: 2.234 s, error: 0.011159\n",
      "Step 179420 (epoch   56.07), loss: 0.023051, time: 15.081 s\n",
      "Step 179440 (epoch   56.08), loss: 0.003556, time: 2.245 s\n",
      "Step 179460 (epoch   56.08), loss: 0.005132, time: 2.251 s\n",
      "Step 179480 (epoch   56.09), loss: 0.005646, time: 2.251 s\n",
      "Step 179500 (epoch   56.09), loss: 0.003209, time: 2.261 s\n",
      "Step 179520 (epoch   56.10), loss: 0.006425, time: 2.231 s\n",
      "Step 179540 (epoch   56.11), loss: 0.006140, time: 2.249 s\n",
      "Step 179560 (epoch   56.11), loss: 0.007260, time: 2.248 s\n",
      "Step 179580 (epoch   56.12), loss: 0.003348, time: 2.246 s\n",
      "Step 179600 (epoch   56.12), loss: 0.005939, time: 2.239 s, error: 0.010984\n",
      "Step 179620 (epoch   56.13), loss: 0.003776, time: 15.042 s\n",
      "Step 179640 (epoch   56.14), loss: 0.005088, time: 2.238 s\n",
      "Step 179660 (epoch   56.14), loss: 0.007954, time: 2.240 s\n",
      "Step 179680 (epoch   56.15), loss: 0.004274, time: 2.250 s\n",
      "Step 179700 (epoch   56.16), loss: 0.005029, time: 2.233 s\n",
      "Step 179720 (epoch   56.16), loss: 0.003146, time: 2.246 s\n",
      "Step 179740 (epoch   56.17), loss: 0.003928, time: 2.236 s\n",
      "Step 179760 (epoch   56.17), loss: 0.005045, time: 2.251 s\n",
      "Step 179780 (epoch   56.18), loss: 0.003353, time: 2.242 s\n",
      "Step 179800 (epoch   56.19), loss: 0.003914, time: 2.236 s, error: 0.011477\n",
      "Step 179820 (epoch   56.19), loss: 0.004479, time: 15.088 s\n",
      "Step 179840 (epoch   56.20), loss: 0.005039, time: 2.234 s\n",
      "Step 179860 (epoch   56.21), loss: 0.007412, time: 2.246 s\n",
      "Step 179880 (epoch   56.21), loss: 0.006912, time: 2.228 s\n",
      "Step 179900 (epoch   56.22), loss: 0.005882, time: 2.243 s\n",
      "Step 179920 (epoch   56.23), loss: 0.007162, time: 2.245 s\n",
      "Step 179940 (epoch   56.23), loss: 0.003742, time: 2.245 s\n",
      "Step 179960 (epoch   56.24), loss: 0.003833, time: 2.240 s\n",
      "Step 179980 (epoch   56.24), loss: 0.003577, time: 2.227 s\n",
      "Step 180000 (epoch   56.25), loss: 0.005420, time: 2.245 s, error: 0.011054\n",
      "\n",
      "Time since beginning  : 31517.611 s\n",
      "\n",
      "Step 180020 (epoch   56.26), loss: 0.007868, time: 15.354 s\n",
      "Step 180040 (epoch   56.26), loss: 0.005479, time: 2.225 s\n",
      "Step 180060 (epoch   56.27), loss: 0.005624, time: 2.232 s\n",
      "Step 180080 (epoch   56.27), loss: 0.006127, time: 2.237 s\n",
      "Step 180100 (epoch   56.28), loss: 0.006085, time: 2.238 s\n",
      "Step 180120 (epoch   56.29), loss: 0.005019, time: 2.241 s\n",
      "Step 180140 (epoch   56.29), loss: 0.004125, time: 2.240 s\n",
      "Step 180160 (epoch   56.30), loss: 0.006174, time: 2.228 s\n",
      "Step 180180 (epoch   56.31), loss: 0.004386, time: 2.249 s\n",
      "Step 180200 (epoch   56.31), loss: 0.004187, time: 2.241 s, error: 0.011322\n",
      "Step 180220 (epoch   56.32), loss: 0.003598, time: 15.291 s\n",
      "Step 180240 (epoch   56.33), loss: 0.004053, time: 2.241 s\n",
      "Step 180260 (epoch   56.33), loss: 0.004503, time: 2.241 s\n",
      "Step 180280 (epoch   56.34), loss: 0.005569, time: 2.241 s\n",
      "Step 180300 (epoch   56.34), loss: 0.004940, time: 2.238 s\n",
      "Step 180320 (epoch   56.35), loss: 0.004742, time: 2.253 s\n",
      "Step 180340 (epoch   56.36), loss: 0.003396, time: 2.250 s\n",
      "Step 180360 (epoch   56.36), loss: 0.005177, time: 2.239 s\n",
      "Step 180380 (epoch   56.37), loss: 0.003330, time: 2.224 s\n",
      "Step 180400 (epoch   56.38), loss: 0.005606, time: 2.243 s, error: 0.011473\n",
      "Step 180420 (epoch   56.38), loss: 0.006439, time: 15.082 s\n",
      "Step 180440 (epoch   56.39), loss: 0.005126, time: 2.235 s\n",
      "Step 180460 (epoch   56.39), loss: 0.007459, time: 2.239 s\n",
      "Step 180480 (epoch   56.40), loss: 0.003778, time: 2.233 s\n",
      "Step 180500 (epoch   56.41), loss: 0.005344, time: 2.250 s\n",
      "Step 180520 (epoch   56.41), loss: 0.008149, time: 2.231 s\n",
      "Step 180540 (epoch   56.42), loss: 0.006787, time: 2.248 s\n",
      "Step 180560 (epoch   56.42), loss: 0.003325, time: 2.224 s\n",
      "Step 180580 (epoch   56.43), loss: 0.003654, time: 2.236 s\n",
      "Step 180600 (epoch   56.44), loss: 0.004694, time: 2.237 s, error: 0.011123\n",
      "Step 180620 (epoch   56.44), loss: 0.005651, time: 15.032 s\n",
      "Step 180640 (epoch   56.45), loss: 0.007886, time: 2.230 s\n",
      "Step 180660 (epoch   56.46), loss: 0.004714, time: 2.235 s\n",
      "Step 180680 (epoch   56.46), loss: 0.005092, time: 2.256 s\n",
      "Step 180700 (epoch   56.47), loss: 0.003407, time: 2.244 s\n",
      "Step 180720 (epoch   56.48), loss: 0.007010, time: 2.247 s\n",
      "Step 180740 (epoch   56.48), loss: 0.006364, time: 2.239 s\n",
      "Step 180760 (epoch   56.49), loss: 0.004812, time: 2.248 s\n",
      "Step 180780 (epoch   56.49), loss: 0.005151, time: 2.248 s\n",
      "Step 180800 (epoch   56.50), loss: 0.005282, time: 2.240 s, error: 0.012743\n",
      "Step 180820 (epoch   56.51), loss: 0.004080, time: 15.228 s\n",
      "Step 180840 (epoch   56.51), loss: 0.007653, time: 2.253 s\n",
      "Step 180860 (epoch   56.52), loss: 0.004916, time: 2.253 s\n",
      "Step 180880 (epoch   56.52), loss: 0.005411, time: 2.248 s\n",
      "Step 180900 (epoch   56.53), loss: 0.003214, time: 2.241 s\n",
      "Step 180920 (epoch   56.54), loss: 0.005342, time: 2.247 s\n",
      "Step 180940 (epoch   56.54), loss: 0.003816, time: 2.259 s\n",
      "Step 180960 (epoch   56.55), loss: 0.004844, time: 2.250 s\n",
      "Step 180980 (epoch   56.56), loss: 0.003813, time: 2.226 s\n",
      "Step 181000 (epoch   56.56), loss: 0.006425, time: 2.251 s, error: 0.011687\n",
      "Step 181020 (epoch   56.57), loss: 0.007134, time: 15.080 s\n",
      "Step 181040 (epoch   56.58), loss: 0.004524, time: 2.244 s\n",
      "Step 181060 (epoch   56.58), loss: 0.004802, time: 2.230 s\n",
      "Step 181080 (epoch   56.59), loss: 0.004829, time: 2.237 s\n",
      "Step 181100 (epoch   56.59), loss: 0.007647, time: 2.250 s\n",
      "Step 181120 (epoch   56.60), loss: 0.005666, time: 2.240 s\n",
      "Step 181140 (epoch   56.61), loss: 0.003540, time: 2.252 s\n",
      "Step 181160 (epoch   56.61), loss: 0.004526, time: 2.243 s\n",
      "Step 181180 (epoch   56.62), loss: 0.004432, time: 2.247 s\n",
      "Step 181200 (epoch   56.62), loss: 0.006440, time: 2.260 s, error: 0.011170\n",
      "Step 181220 (epoch   56.63), loss: 0.007158, time: 15.091 s\n",
      "Step 181240 (epoch   56.64), loss: 0.007627, time: 2.229 s\n",
      "Step 181260 (epoch   56.64), loss: 0.005527, time: 2.230 s\n",
      "Step 181280 (epoch   56.65), loss: 0.004868, time: 2.240 s\n",
      "Step 181300 (epoch   56.66), loss: 0.004406, time: 2.234 s\n",
      "Step 181320 (epoch   56.66), loss: 0.007013, time: 2.233 s\n",
      "Step 181340 (epoch   56.67), loss: 0.004214, time: 2.238 s\n",
      "Step 181360 (epoch   56.67), loss: 0.003357, time: 2.237 s\n",
      "Step 181380 (epoch   56.68), loss: 0.007616, time: 2.240 s\n",
      "Step 181400 (epoch   56.69), loss: 0.009214, time: 2.228 s, error: 0.011003\n",
      "Step 181420 (epoch   56.69), loss: 0.005346, time: 15.278 s\n",
      "Step 181440 (epoch   56.70), loss: 0.003583, time: 2.241 s\n",
      "Step 181460 (epoch   56.71), loss: 0.005148, time: 2.241 s\n",
      "Step 181480 (epoch   56.71), loss: 0.006593, time: 2.249 s\n",
      "Step 181500 (epoch   56.72), loss: 0.004554, time: 2.240 s\n",
      "Step 181520 (epoch   56.73), loss: 0.004888, time: 2.223 s\n",
      "Step 181540 (epoch   56.73), loss: 0.005349, time: 2.247 s\n",
      "Step 181560 (epoch   56.74), loss: 0.004143, time: 2.235 s\n",
      "Step 181580 (epoch   56.74), loss: 0.007570, time: 2.245 s\n",
      "Step 181600 (epoch   56.75), loss: 0.005540, time: 2.232 s, error: 0.011074\n",
      "Step 181620 (epoch   56.76), loss: 0.005027, time: 15.243 s\n",
      "Step 181640 (epoch   56.76), loss: 0.006569, time: 2.246 s\n",
      "Step 181660 (epoch   56.77), loss: 0.003487, time: 2.240 s\n",
      "Step 181680 (epoch   56.77), loss: 0.005931, time: 2.246 s\n",
      "Step 181700 (epoch   56.78), loss: 0.004137, time: 2.227 s\n",
      "Step 181720 (epoch   56.79), loss: 0.003198, time: 2.234 s\n",
      "Step 181740 (epoch   56.79), loss: 0.005528, time: 2.232 s\n",
      "Step 181760 (epoch   56.80), loss: 0.003355, time: 2.237 s\n",
      "Step 181780 (epoch   56.81), loss: 0.004337, time: 2.233 s\n",
      "Step 181800 (epoch   56.81), loss: 0.004462, time: 2.241 s, error: 0.011141\n",
      "Step 181820 (epoch   56.82), loss: 0.005139, time: 15.046 s\n",
      "Step 181840 (epoch   56.83), loss: 0.004830, time: 2.238 s\n",
      "Step 181860 (epoch   56.83), loss: 0.003535, time: 2.259 s\n",
      "Step 181880 (epoch   56.84), loss: 0.004866, time: 2.251 s\n",
      "Step 181900 (epoch   56.84), loss: 0.005229, time: 2.245 s\n",
      "Step 181920 (epoch   56.85), loss: 0.005930, time: 2.239 s\n",
      "Step 181940 (epoch   56.86), loss: 0.004740, time: 2.245 s\n",
      "Step 181960 (epoch   56.86), loss: 0.003573, time: 2.241 s\n",
      "Step 181980 (epoch   56.87), loss: 0.004851, time: 2.245 s\n",
      "Step 182000 (epoch   56.88), loss: 0.003287, time: 2.237 s, error: 0.010833\n",
      "\n",
      "Time since beginning  : 31870.790 s\n",
      "\n",
      "Step 182020 (epoch   56.88), loss: 0.003456, time: 15.174 s\n",
      "Step 182040 (epoch   56.89), loss: 0.007264, time: 2.242 s\n",
      "Step 182060 (epoch   56.89), loss: 0.005315, time: 2.227 s\n",
      "Step 182080 (epoch   56.90), loss: 0.008066, time: 2.245 s\n",
      "Step 182100 (epoch   56.91), loss: 0.005772, time: 2.243 s\n",
      "Step 182120 (epoch   56.91), loss: 0.005707, time: 2.259 s\n",
      "Step 182140 (epoch   56.92), loss: 0.003921, time: 2.229 s\n",
      "Step 182160 (epoch   56.92), loss: 0.006515, time: 2.244 s\n",
      "Step 182180 (epoch   56.93), loss: 0.004539, time: 2.235 s\n",
      "Step 182200 (epoch   56.94), loss: 0.003550, time: 2.243 s, error: 0.011097\n",
      "Step 182220 (epoch   56.94), loss: 0.003966, time: 15.072 s\n",
      "Step 182240 (epoch   56.95), loss: 0.004960, time: 2.248 s\n",
      "Step 182260 (epoch   56.96), loss: 0.006604, time: 2.241 s\n",
      "Step 182280 (epoch   56.96), loss: 0.002686, time: 2.256 s\n",
      "Step 182300 (epoch   56.97), loss: 0.005262, time: 2.246 s\n",
      "Step 182320 (epoch   56.98), loss: 0.003373, time: 2.247 s\n",
      "Step 182340 (epoch   56.98), loss: 0.003768, time: 2.234 s\n",
      "Step 182360 (epoch   56.99), loss: 0.005778, time: 2.246 s\n",
      "Step 182380 (epoch   56.99), loss: 0.003230, time: 2.250 s\n",
      "Step 182400 (epoch   57.00), loss: 0.004370, time: 2.238 s, error: 0.011588\n",
      "Step 182420 (epoch   57.01), loss: 0.004218, time: 15.072 s\n",
      "Step 182440 (epoch   57.01), loss: 0.004199, time: 2.243 s\n",
      "Step 182460 (epoch   57.02), loss: 0.005866, time: 2.248 s\n",
      "Step 182480 (epoch   57.02), loss: 0.006621, time: 2.237 s\n",
      "Step 182500 (epoch   57.03), loss: 0.004048, time: 2.239 s\n",
      "Step 182520 (epoch   57.04), loss: 0.006470, time: 2.241 s\n",
      "Step 182540 (epoch   57.04), loss: 0.004440, time: 2.244 s\n",
      "Step 182560 (epoch   57.05), loss: 0.005735, time: 2.247 s\n",
      "Step 182580 (epoch   57.06), loss: 0.005241, time: 2.248 s\n",
      "Step 182600 (epoch   57.06), loss: 0.004692, time: 2.242 s, error: 0.011101\n",
      "Step 182620 (epoch   57.07), loss: 0.005327, time: 15.252 s\n",
      "Step 182640 (epoch   57.08), loss: 0.004591, time: 2.246 s\n",
      "Step 182660 (epoch   57.08), loss: 0.007098, time: 2.237 s\n",
      "Step 182680 (epoch   57.09), loss: 0.004604, time: 2.244 s\n",
      "Step 182700 (epoch   57.09), loss: 0.004614, time: 2.243 s\n",
      "Step 182720 (epoch   57.10), loss: 0.004817, time: 2.243 s\n",
      "Step 182740 (epoch   57.11), loss: 0.003336, time: 2.238 s\n",
      "Step 182760 (epoch   57.11), loss: 0.004705, time: 2.241 s\n",
      "Step 182780 (epoch   57.12), loss: 0.003788, time: 2.255 s\n",
      "Step 182800 (epoch   57.12), loss: 0.010361, time: 2.251 s, error: 0.010986\n",
      "Step 182820 (epoch   57.13), loss: 0.007107, time: 15.360 s\n",
      "Step 182840 (epoch   57.14), loss: 0.004249, time: 2.243 s\n",
      "Step 182860 (epoch   57.14), loss: 0.005868, time: 2.231 s\n",
      "Step 182880 (epoch   57.15), loss: 0.005935, time: 2.245 s\n",
      "Step 182900 (epoch   57.16), loss: 0.008384, time: 2.241 s\n",
      "Step 182920 (epoch   57.16), loss: 0.005582, time: 2.248 s\n",
      "Step 182940 (epoch   57.17), loss: 0.004411, time: 2.245 s\n",
      "Step 182960 (epoch   57.17), loss: 0.004665, time: 2.257 s\n",
      "Step 182980 (epoch   57.18), loss: 0.005449, time: 2.241 s\n",
      "Step 183000 (epoch   57.19), loss: 0.003823, time: 2.242 s, error: 0.011878\n",
      "Step 183020 (epoch   57.19), loss: 0.006863, time: 15.138 s\n",
      "Step 183040 (epoch   57.20), loss: 0.004122, time: 2.259 s\n",
      "Step 183060 (epoch   57.21), loss: 0.005390, time: 2.234 s\n",
      "Step 183080 (epoch   57.21), loss: 0.005254, time: 2.245 s\n",
      "Step 183100 (epoch   57.22), loss: 0.004460, time: 2.244 s\n",
      "Step 183120 (epoch   57.23), loss: 0.005996, time: 2.240 s\n",
      "Step 183140 (epoch   57.23), loss: 0.005364, time: 2.234 s\n",
      "Step 183160 (epoch   57.24), loss: 0.003168, time: 2.239 s\n",
      "Step 183180 (epoch   57.24), loss: 0.005006, time: 2.244 s\n",
      "Step 183200 (epoch   57.25), loss: 0.006980, time: 2.253 s, error: 0.011037\n",
      "Step 183220 (epoch   57.26), loss: 0.005879, time: 15.030 s\n",
      "Step 183240 (epoch   57.26), loss: 0.005109, time: 2.241 s\n",
      "Step 183260 (epoch   57.27), loss: 0.004465, time: 2.236 s\n",
      "Step 183280 (epoch   57.27), loss: 0.005089, time: 2.223 s\n",
      "Step 183300 (epoch   57.28), loss: 0.004151, time: 2.258 s\n",
      "Step 183320 (epoch   57.29), loss: 0.006042, time: 2.255 s\n",
      "Step 183340 (epoch   57.29), loss: 0.003539, time: 2.251 s\n",
      "Step 183360 (epoch   57.30), loss: 0.004531, time: 2.239 s\n",
      "Step 183380 (epoch   57.31), loss: 0.005200, time: 2.249 s\n",
      "Step 183400 (epoch   57.31), loss: 0.005291, time: 2.242 s, error: 0.011323\n",
      "Step 183420 (epoch   57.32), loss: 0.006930, time: 15.088 s\n",
      "Step 183440 (epoch   57.33), loss: 0.003647, time: 2.259 s\n",
      "Step 183460 (epoch   57.33), loss: 0.006274, time: 2.244 s\n",
      "Step 183480 (epoch   57.34), loss: 0.006268, time: 2.245 s\n",
      "Step 183500 (epoch   57.34), loss: 0.007516, time: 2.245 s\n",
      "Step 183520 (epoch   57.35), loss: 0.003015, time: 2.230 s\n",
      "Step 183540 (epoch   57.36), loss: 0.005274, time: 2.237 s\n",
      "Step 183560 (epoch   57.36), loss: 0.005086, time: 2.256 s\n",
      "Step 183580 (epoch   57.37), loss: 0.006491, time: 2.248 s\n",
      "Step 183600 (epoch   57.38), loss: 0.003908, time: 2.242 s, error: 0.011253\n",
      "Step 183620 (epoch   57.38), loss: 0.007031, time: 15.076 s\n",
      "Step 183640 (epoch   57.39), loss: 0.006323, time: 2.253 s\n",
      "Step 183660 (epoch   57.39), loss: 0.008439, time: 2.236 s\n",
      "Step 183680 (epoch   57.40), loss: 0.004821, time: 2.247 s\n",
      "Step 183700 (epoch   57.41), loss: 0.007876, time: 2.237 s\n",
      "Step 183720 (epoch   57.41), loss: 0.004442, time: 2.244 s\n",
      "Step 183740 (epoch   57.42), loss: 0.004624, time: 2.223 s\n",
      "Step 183760 (epoch   57.42), loss: 0.008526, time: 2.240 s\n",
      "Step 183780 (epoch   57.43), loss: 0.005581, time: 2.244 s\n",
      "Step 183800 (epoch   57.44), loss: 0.003810, time: 2.231 s, error: 0.011165\n",
      "Step 183820 (epoch   57.44), loss: 0.004048, time: 15.289 s\n",
      "Step 183840 (epoch   57.45), loss: 0.009665, time: 2.231 s\n",
      "Step 183860 (epoch   57.46), loss: 0.004471, time: 2.249 s\n",
      "Step 183880 (epoch   57.46), loss: 0.004650, time: 2.232 s\n",
      "Step 183900 (epoch   57.47), loss: 0.004110, time: 2.238 s\n",
      "Step 183920 (epoch   57.48), loss: 0.004363, time: 2.241 s\n",
      "Step 183940 (epoch   57.48), loss: 0.003988, time: 2.229 s\n",
      "Step 183960 (epoch   57.49), loss: 0.004321, time: 2.248 s\n",
      "Step 183980 (epoch   57.49), loss: 0.003015, time: 2.253 s\n",
      "Step 184000 (epoch   57.50), loss: 0.006173, time: 2.249 s, error: 0.013763\n",
      "\n",
      "Time since beginning  : 32224.398 s\n",
      "\n",
      "Step 184020 (epoch   57.51), loss: 0.007468, time: 15.368 s\n",
      "Step 184040 (epoch   57.51), loss: 0.005435, time: 2.244 s\n",
      "Step 184060 (epoch   57.52), loss: 0.003054, time: 2.233 s\n",
      "Step 184080 (epoch   57.52), loss: 0.004733, time: 2.236 s\n",
      "Step 184100 (epoch   57.53), loss: 0.005233, time: 2.226 s\n",
      "Step 184120 (epoch   57.54), loss: 0.005667, time: 2.244 s\n",
      "Step 184140 (epoch   57.54), loss: 0.004051, time: 2.225 s\n",
      "Step 184160 (epoch   57.55), loss: 0.004213, time: 2.240 s\n",
      "Step 184180 (epoch   57.56), loss: 0.005724, time: 2.241 s\n",
      "Step 184200 (epoch   57.56), loss: 0.005891, time: 2.238 s, error: 0.012083\n",
      "Step 184220 (epoch   57.57), loss: 0.003267, time: 15.040 s\n",
      "Step 184240 (epoch   57.58), loss: 0.003345, time: 2.246 s\n",
      "Step 184260 (epoch   57.58), loss: 0.005663, time: 2.240 s\n",
      "Step 184280 (epoch   57.59), loss: 0.004941, time: 2.248 s\n",
      "Step 184300 (epoch   57.59), loss: 0.004981, time: 2.242 s\n",
      "Step 184320 (epoch   57.60), loss: 0.007966, time: 2.234 s\n",
      "Step 184340 (epoch   57.61), loss: 0.004652, time: 2.241 s\n",
      "Step 184360 (epoch   57.61), loss: 0.003731, time: 2.251 s\n",
      "Step 184380 (epoch   57.62), loss: 0.005324, time: 2.238 s\n",
      "Step 184400 (epoch   57.62), loss: 0.005288, time: 2.235 s, error: 0.011161\n",
      "Step 184420 (epoch   57.63), loss: 0.004853, time: 15.138 s\n",
      "Step 184440 (epoch   57.64), loss: 0.005620, time: 2.233 s\n",
      "Step 184460 (epoch   57.64), loss: 0.005458, time: 2.254 s\n",
      "Step 184480 (epoch   57.65), loss: 0.003360, time: 2.243 s\n",
      "Step 184500 (epoch   57.66), loss: 0.004128, time: 2.245 s\n",
      "Step 184520 (epoch   57.66), loss: 0.006382, time: 2.238 s\n",
      "Step 184540 (epoch   57.67), loss: 0.003468, time: 2.247 s\n",
      "Step 184560 (epoch   57.67), loss: 0.005234, time: 2.234 s\n",
      "Step 184580 (epoch   57.68), loss: 0.007261, time: 2.243 s\n",
      "Step 184600 (epoch   57.69), loss: 0.003346, time: 2.245 s, error: 0.010964\n",
      "Step 184620 (epoch   57.69), loss: 0.005887, time: 15.169 s\n",
      "Step 184640 (epoch   57.70), loss: 0.004113, time: 2.237 s\n",
      "Step 184660 (epoch   57.71), loss: 0.006499, time: 2.243 s\n",
      "Step 184680 (epoch   57.71), loss: 0.006335, time: 2.241 s\n",
      "Step 184700 (epoch   57.72), loss: 0.004373, time: 2.243 s\n",
      "Step 184720 (epoch   57.73), loss: 0.003295, time: 2.251 s\n",
      "Step 184740 (epoch   57.73), loss: 0.006389, time: 2.257 s\n",
      "Step 184760 (epoch   57.74), loss: 0.003456, time: 2.247 s\n",
      "Step 184780 (epoch   57.74), loss: 0.006744, time: 2.232 s\n",
      "Step 184800 (epoch   57.75), loss: 0.005094, time: 2.250 s, error: 0.010999\n",
      "Step 184820 (epoch   57.76), loss: 0.005258, time: 15.067 s\n",
      "Step 184840 (epoch   57.76), loss: 0.006582, time: 2.246 s\n",
      "Step 184860 (epoch   57.77), loss: 0.006955, time: 2.245 s\n",
      "Step 184880 (epoch   57.77), loss: 0.005801, time: 2.234 s\n",
      "Step 184900 (epoch   57.78), loss: 0.004203, time: 2.230 s\n",
      "Step 184920 (epoch   57.79), loss: 0.004158, time: 2.242 s\n",
      "Step 184940 (epoch   57.79), loss: 0.005595, time: 2.255 s\n",
      "Step 184960 (epoch   57.80), loss: 0.005172, time: 2.229 s\n",
      "Step 184980 (epoch   57.81), loss: 0.007430, time: 2.249 s\n",
      "Step 185000 (epoch   57.81), loss: 0.004491, time: 2.258 s, error: 0.011115\n",
      "Step 185020 (epoch   57.82), loss: 0.004823, time: 15.143 s\n",
      "Step 185040 (epoch   57.83), loss: 0.006182, time: 2.246 s\n",
      "Step 185060 (epoch   57.83), loss: 0.004077, time: 2.241 s\n",
      "Step 185080 (epoch   57.84), loss: 0.005748, time: 2.241 s\n",
      "Step 185100 (epoch   57.84), loss: 0.003439, time: 2.241 s\n",
      "Step 185120 (epoch   57.85), loss: 0.004640, time: 2.254 s\n",
      "Step 185140 (epoch   57.86), loss: 0.004092, time: 2.254 s\n",
      "Step 185160 (epoch   57.86), loss: 0.006557, time: 2.243 s\n",
      "Step 185180 (epoch   57.87), loss: 0.006553, time: 2.236 s\n",
      "Step 185200 (epoch   57.88), loss: 0.004451, time: 2.240 s, error: 0.010793\n",
      "Step 185220 (epoch   57.88), loss: 0.004638, time: 15.247 s\n",
      "Step 185240 (epoch   57.89), loss: 0.005186, time: 2.223 s\n",
      "Step 185260 (epoch   57.89), loss: 0.005699, time: 2.243 s\n",
      "Step 185280 (epoch   57.90), loss: 0.004176, time: 2.237 s\n",
      "Step 185300 (epoch   57.91), loss: 0.005794, time: 2.238 s\n",
      "Step 185320 (epoch   57.91), loss: 0.006689, time: 2.235 s\n",
      "Step 185340 (epoch   57.92), loss: 0.004538, time: 2.248 s\n",
      "Step 185360 (epoch   57.92), loss: 0.005427, time: 2.253 s\n",
      "Step 185380 (epoch   57.93), loss: 0.004434, time: 2.249 s\n",
      "Step 185400 (epoch   57.94), loss: 0.004132, time: 2.242 s, error: 0.010922\n",
      "Step 185420 (epoch   57.94), loss: 0.005403, time: 15.317 s\n",
      "Step 185440 (epoch   57.95), loss: 0.004577, time: 2.248 s\n",
      "Step 185460 (epoch   57.96), loss: 0.009483, time: 2.238 s\n",
      "Step 185480 (epoch   57.96), loss: 0.004480, time: 2.244 s\n",
      "Step 185500 (epoch   57.97), loss: 0.004388, time: 2.239 s\n",
      "Step 185520 (epoch   57.98), loss: 0.004364, time: 2.239 s\n",
      "Step 185540 (epoch   57.98), loss: 0.005947, time: 2.236 s\n",
      "Step 185560 (epoch   57.99), loss: 0.003451, time: 2.243 s\n",
      "Step 185580 (epoch   57.99), loss: 0.012541, time: 2.240 s\n",
      "Step 185600 (epoch   58.00), loss: 0.004495, time: 2.240 s, error: 0.011422\n",
      "Step 185620 (epoch   58.01), loss: 0.007699, time: 15.022 s\n",
      "Step 185640 (epoch   58.01), loss: 0.004677, time: 2.232 s\n",
      "Step 185660 (epoch   58.02), loss: 0.003270, time: 2.255 s\n",
      "Step 185680 (epoch   58.02), loss: 0.006028, time: 2.258 s\n",
      "Step 185700 (epoch   58.03), loss: 0.004686, time: 2.238 s\n",
      "Step 185720 (epoch   58.04), loss: 0.005264, time: 2.255 s\n",
      "Step 185740 (epoch   58.04), loss: 0.005766, time: 2.244 s\n",
      "Step 185760 (epoch   58.05), loss: 0.006325, time: 2.239 s\n",
      "Step 185780 (epoch   58.06), loss: 0.003476, time: 2.241 s\n",
      "Step 185800 (epoch   58.06), loss: 0.004198, time: 2.236 s, error: 0.010910\n",
      "Step 185820 (epoch   58.07), loss: 0.004582, time: 15.086 s\n",
      "Step 185840 (epoch   58.08), loss: 0.004046, time: 2.247 s\n",
      "Step 185860 (epoch   58.08), loss: 0.006810, time: 2.242 s\n",
      "Step 185880 (epoch   58.09), loss: 0.005578, time: 2.243 s\n",
      "Step 185900 (epoch   58.09), loss: 0.003998, time: 2.224 s\n",
      "Step 185920 (epoch   58.10), loss: 0.003534, time: 2.249 s\n",
      "Step 185940 (epoch   58.11), loss: 0.005547, time: 2.257 s\n",
      "Step 185960 (epoch   58.11), loss: 0.004374, time: 2.249 s\n",
      "Step 185980 (epoch   58.12), loss: 0.004849, time: 2.220 s\n",
      "Step 186000 (epoch   58.12), loss: 0.005203, time: 2.241 s, error: 0.010929\n",
      "\n",
      "Time since beginning  : 32577.669 s\n",
      "\n",
      "Step 186020 (epoch   58.13), loss: 0.003886, time: 15.253 s\n",
      "Step 186040 (epoch   58.14), loss: 0.003518, time: 2.245 s\n",
      "Step 186060 (epoch   58.14), loss: 0.003756, time: 2.234 s\n",
      "Step 186080 (epoch   58.15), loss: 0.003927, time: 2.240 s\n",
      "Step 186100 (epoch   58.16), loss: 0.005416, time: 2.248 s\n",
      "Step 186120 (epoch   58.16), loss: 0.003260, time: 2.240 s\n",
      "Step 186140 (epoch   58.17), loss: 0.004324, time: 2.223 s\n",
      "Step 186160 (epoch   58.17), loss: 0.003985, time: 2.249 s\n",
      "Step 186180 (epoch   58.18), loss: 0.004102, time: 2.248 s\n",
      "Step 186200 (epoch   58.19), loss: 0.004977, time: 2.255 s, error: 0.011835\n",
      "Step 186220 (epoch   58.19), loss: 0.006651, time: 15.020 s\n",
      "Step 186240 (epoch   58.20), loss: 0.004254, time: 2.247 s\n",
      "Step 186260 (epoch   58.21), loss: 0.003459, time: 2.243 s\n",
      "Step 186280 (epoch   58.21), loss: 0.005614, time: 2.245 s\n",
      "Step 186300 (epoch   58.22), loss: 0.003488, time: 2.232 s\n",
      "Step 186320 (epoch   58.23), loss: 0.004970, time: 2.243 s\n",
      "Step 186340 (epoch   58.23), loss: 0.004184, time: 2.236 s\n",
      "Step 186360 (epoch   58.24), loss: 0.004430, time: 2.244 s\n",
      "Step 186380 (epoch   58.24), loss: 0.004614, time: 2.237 s\n",
      "Step 186400 (epoch   58.25), loss: 0.005556, time: 2.239 s, error: 0.011041\n",
      "Step 186420 (epoch   58.26), loss: 0.002775, time: 15.245 s\n",
      "Step 186440 (epoch   58.26), loss: 0.005882, time: 2.243 s\n",
      "Step 186460 (epoch   58.27), loss: 0.004208, time: 2.230 s\n",
      "Step 186480 (epoch   58.27), loss: 0.006580, time: 2.234 s\n",
      "Step 186500 (epoch   58.28), loss: 0.004768, time: 2.248 s\n",
      "Step 186520 (epoch   58.29), loss: 0.004834, time: 2.243 s\n",
      "Step 186540 (epoch   58.29), loss: 0.003477, time: 2.242 s\n",
      "Step 186560 (epoch   58.30), loss: 0.003433, time: 2.240 s\n",
      "Step 186580 (epoch   58.31), loss: 0.006367, time: 2.238 s\n",
      "Step 186600 (epoch   58.31), loss: 0.003510, time: 2.237 s, error: 0.011151\n",
      "Step 186620 (epoch   58.32), loss: 0.004841, time: 15.284 s\n",
      "Step 186640 (epoch   58.33), loss: 0.005441, time: 2.231 s\n",
      "Step 186660 (epoch   58.33), loss: 0.004761, time: 2.236 s\n",
      "Step 186680 (epoch   58.34), loss: 0.006334, time: 2.233 s\n",
      "Step 186700 (epoch   58.34), loss: 0.004295, time: 2.240 s\n",
      "Step 186720 (epoch   58.35), loss: 0.005493, time: 2.251 s\n",
      "Step 186740 (epoch   58.36), loss: 0.004504, time: 2.242 s\n",
      "Step 186760 (epoch   58.36), loss: 0.003140, time: 2.231 s\n",
      "Step 186780 (epoch   58.37), loss: 0.005136, time: 2.234 s\n",
      "Step 186800 (epoch   58.38), loss: 0.006599, time: 2.240 s, error: 0.011073\n",
      "Step 186820 (epoch   58.38), loss: 0.004741, time: 15.142 s\n",
      "Step 186840 (epoch   58.39), loss: 0.006296, time: 2.238 s\n",
      "Step 186860 (epoch   58.39), loss: 0.004661, time: 2.261 s\n",
      "Step 186880 (epoch   58.40), loss: 0.004752, time: 2.233 s\n",
      "Step 186900 (epoch   58.41), loss: 0.004051, time: 2.243 s\n",
      "Step 186920 (epoch   58.41), loss: 0.004396, time: 2.248 s\n",
      "Step 186940 (epoch   58.42), loss: 0.007654, time: 2.253 s\n",
      "Step 186960 (epoch   58.42), loss: 0.003921, time: 2.235 s\n",
      "Step 186980 (epoch   58.43), loss: 0.005018, time: 2.250 s\n",
      "Step 187000 (epoch   58.44), loss: 0.006769, time: 2.225 s, error: 0.011116\n",
      "Step 187020 (epoch   58.44), loss: 0.004070, time: 15.066 s\n",
      "Step 187040 (epoch   58.45), loss: 0.005025, time: 2.239 s\n",
      "Step 187060 (epoch   58.46), loss: 0.006738, time: 2.239 s\n",
      "Step 187080 (epoch   58.46), loss: 0.004287, time: 2.238 s\n",
      "Step 187100 (epoch   58.47), loss: 0.006689, time: 2.249 s\n",
      "Step 187120 (epoch   58.48), loss: 0.004803, time: 2.259 s\n",
      "Step 187140 (epoch   58.48), loss: 0.010931, time: 2.234 s\n",
      "Step 187160 (epoch   58.49), loss: 0.003816, time: 2.242 s\n",
      "Step 187180 (epoch   58.49), loss: 0.004116, time: 2.240 s\n",
      "Step 187200 (epoch   58.50), loss: 0.008938, time: 2.238 s, error: 0.014065\n",
      "Step 187220 (epoch   58.51), loss: 0.011171, time: 15.017 s\n",
      "Step 187240 (epoch   58.51), loss: 0.005066, time: 2.226 s\n",
      "Step 187260 (epoch   58.52), loss: 0.003586, time: 2.244 s\n",
      "Step 187280 (epoch   58.52), loss: 0.006917, time: 2.231 s\n",
      "Step 187300 (epoch   58.53), loss: 0.005582, time: 2.229 s\n",
      "Step 187320 (epoch   58.54), loss: 0.011309, time: 2.241 s\n",
      "Step 187340 (epoch   58.54), loss: 0.007591, time: 2.231 s\n",
      "Step 187360 (epoch   58.55), loss: 0.004842, time: 2.255 s\n",
      "Step 187380 (epoch   58.56), loss: 0.005647, time: 2.257 s\n",
      "Step 187400 (epoch   58.56), loss: 0.005006, time: 2.232 s, error: 0.012228\n",
      "Step 187420 (epoch   58.57), loss: 0.004822, time: 15.086 s\n",
      "Step 187440 (epoch   58.58), loss: 0.005888, time: 2.249 s\n",
      "Step 187460 (epoch   58.58), loss: 0.003777, time: 2.238 s\n",
      "Step 187480 (epoch   58.59), loss: 0.006975, time: 2.235 s\n",
      "Step 187500 (epoch   58.59), loss: 0.003963, time: 2.240 s\n",
      "Step 187520 (epoch   58.60), loss: 0.005537, time: 2.225 s\n",
      "Step 187540 (epoch   58.61), loss: 0.008499, time: 2.240 s\n",
      "Step 187560 (epoch   58.61), loss: 0.004901, time: 2.228 s\n",
      "Step 187580 (epoch   58.62), loss: 0.003477, time: 2.232 s\n",
      "Step 187600 (epoch   58.62), loss: 0.005194, time: 2.246 s, error: 0.011348\n",
      "Step 187620 (epoch   58.63), loss: 0.004899, time: 15.281 s\n",
      "Step 187640 (epoch   58.64), loss: 0.004912, time: 2.239 s\n",
      "Step 187660 (epoch   58.64), loss: 0.006080, time: 2.236 s\n",
      "Step 187680 (epoch   58.65), loss: 0.003658, time: 2.244 s\n",
      "Step 187700 (epoch   58.66), loss: 0.004568, time: 2.246 s\n",
      "Step 187720 (epoch   58.66), loss: 0.004589, time: 2.243 s\n",
      "Step 187740 (epoch   58.67), loss: 0.005844, time: 2.240 s\n",
      "Step 187760 (epoch   58.67), loss: 0.005731, time: 2.252 s\n",
      "Step 187780 (epoch   58.68), loss: 0.004485, time: 2.254 s\n",
      "Step 187800 (epoch   58.69), loss: 0.006716, time: 2.245 s, error: 0.010962\n",
      "Step 187820 (epoch   58.69), loss: 0.004452, time: 15.290 s\n",
      "Step 187840 (epoch   58.70), loss: 0.005783, time: 2.240 s\n",
      "Step 187860 (epoch   58.71), loss: 0.003694, time: 2.234 s\n",
      "Step 187880 (epoch   58.71), loss: 0.004189, time: 2.246 s\n",
      "Step 187900 (epoch   58.72), loss: 0.004958, time: 2.254 s\n",
      "Step 187920 (epoch   58.73), loss: 0.005868, time: 2.251 s\n",
      "Step 187940 (epoch   58.73), loss: 0.005923, time: 2.251 s\n",
      "Step 187960 (epoch   58.74), loss: 0.005769, time: 2.258 s\n",
      "Step 187980 (epoch   58.74), loss: 0.005454, time: 2.255 s\n",
      "Step 188000 (epoch   58.75), loss: 0.005527, time: 2.235 s, error: 0.010928\n",
      "\n",
      "Time since beginning  : 32931.023 s\n",
      "\n",
      "Step 188020 (epoch   58.76), loss: 0.004876, time: 15.225 s\n",
      "Step 188040 (epoch   58.76), loss: 0.005541, time: 2.255 s\n",
      "Step 188060 (epoch   58.77), loss: 0.007267, time: 2.242 s\n",
      "Step 188080 (epoch   58.77), loss: 0.005566, time: 2.235 s\n",
      "Step 188100 (epoch   58.78), loss: 0.005538, time: 2.251 s\n",
      "Step 188120 (epoch   58.79), loss: 0.004147, time: 2.238 s\n",
      "Step 188140 (epoch   58.79), loss: 0.004269, time: 2.248 s\n",
      "Step 188160 (epoch   58.80), loss: 0.006938, time: 2.253 s\n",
      "Step 188180 (epoch   58.81), loss: 0.006744, time: 2.246 s\n",
      "Step 188200 (epoch   58.81), loss: 0.004258, time: 2.237 s, error: 0.010984\n",
      "Step 188220 (epoch   58.82), loss: 0.004450, time: 15.063 s\n",
      "Step 188240 (epoch   58.83), loss: 0.004539, time: 2.251 s\n",
      "Step 188260 (epoch   58.83), loss: 0.003140, time: 2.240 s\n",
      "Step 188280 (epoch   58.84), loss: 0.003576, time: 2.256 s\n",
      "Step 188300 (epoch   58.84), loss: 0.004266, time: 2.247 s\n",
      "Step 188320 (epoch   58.85), loss: 0.004658, time: 2.252 s\n",
      "Step 188340 (epoch   58.86), loss: 0.003160, time: 2.243 s\n",
      "Step 188360 (epoch   58.86), loss: 0.003621, time: 2.241 s\n",
      "Step 188380 (epoch   58.87), loss: 0.004404, time: 2.252 s\n",
      "Step 188400 (epoch   58.88), loss: 0.004216, time: 2.244 s, error: 0.010753\n",
      "Step 188420 (epoch   58.88), loss: 0.004324, time: 15.067 s\n",
      "Step 188440 (epoch   58.89), loss: 0.005661, time: 2.243 s\n",
      "Step 188460 (epoch   58.89), loss: 0.005056, time: 2.246 s\n",
      "Step 188480 (epoch   58.90), loss: 0.008375, time: 2.242 s\n",
      "Step 188500 (epoch   58.91), loss: 0.010955, time: 2.237 s\n",
      "Step 188520 (epoch   58.91), loss: 0.006471, time: 2.233 s\n",
      "Step 188540 (epoch   58.92), loss: 0.003935, time: 2.252 s\n",
      "Step 188560 (epoch   58.92), loss: 0.003724, time: 2.257 s\n",
      "Step 188580 (epoch   58.93), loss: 0.005173, time: 2.243 s\n",
      "Step 188600 (epoch   58.94), loss: 0.005162, time: 2.240 s, error: 0.010875\n",
      "Step 188620 (epoch   58.94), loss: 0.003418, time: 15.056 s\n",
      "Step 188640 (epoch   58.95), loss: 0.007016, time: 2.242 s\n",
      "Step 188660 (epoch   58.96), loss: 0.004280, time: 2.234 s\n",
      "Step 188680 (epoch   58.96), loss: 0.008609, time: 2.246 s\n",
      "Step 188700 (epoch   58.97), loss: 0.005640, time: 2.224 s\n",
      "Step 188720 (epoch   58.98), loss: 0.004237, time: 2.251 s\n",
      "Step 188740 (epoch   58.98), loss: 0.003978, time: 2.236 s\n",
      "Step 188760 (epoch   58.99), loss: 0.005902, time: 2.242 s\n",
      "Step 188780 (epoch   58.99), loss: 0.002003, time: 2.245 s\n",
      "Step 188800 (epoch   59.00), loss: 0.005665, time: 2.249 s, error: 0.011128\n",
      "Step 188820 (epoch   59.01), loss: 0.004686, time: 15.223 s\n",
      "Step 188840 (epoch   59.01), loss: 0.005994, time: 2.232 s\n",
      "Step 188860 (epoch   59.02), loss: 0.003162, time: 2.246 s\n",
      "Step 188880 (epoch   59.02), loss: 0.005643, time: 2.252 s\n",
      "Step 188900 (epoch   59.03), loss: 0.004200, time: 2.251 s\n",
      "Step 188920 (epoch   59.04), loss: 0.010526, time: 2.235 s\n",
      "Step 188940 (epoch   59.04), loss: 0.004560, time: 2.248 s\n",
      "Step 188960 (epoch   59.05), loss: 0.003026, time: 2.237 s\n",
      "Step 188980 (epoch   59.06), loss: 0.004139, time: 2.257 s\n",
      "Step 189000 (epoch   59.06), loss: 0.003629, time: 2.244 s, error: 0.010785\n",
      "Step 189020 (epoch   59.07), loss: 0.004503, time: 15.290 s\n",
      "Step 189040 (epoch   59.08), loss: 0.009091, time: 2.255 s\n",
      "Step 189060 (epoch   59.08), loss: 0.004794, time: 2.239 s\n",
      "Step 189080 (epoch   59.09), loss: 0.006351, time: 2.247 s\n",
      "Step 189100 (epoch   59.09), loss: 0.005437, time: 2.243 s\n",
      "Step 189120 (epoch   59.10), loss: 0.005233, time: 2.248 s\n",
      "Step 189140 (epoch   59.11), loss: 0.005871, time: 2.234 s\n",
      "Step 189160 (epoch   59.11), loss: 0.003834, time: 2.243 s\n",
      "Step 189180 (epoch   59.12), loss: 0.003300, time: 2.233 s\n",
      "Step 189200 (epoch   59.12), loss: 0.004900, time: 2.247 s, error: 0.010855\n",
      "Step 189220 (epoch   59.13), loss: 0.004016, time: 15.128 s\n",
      "Step 189240 (epoch   59.14), loss: 0.005641, time: 2.237 s\n",
      "Step 189260 (epoch   59.14), loss: 0.003961, time: 2.247 s\n",
      "Step 189280 (epoch   59.15), loss: 0.006808, time: 2.241 s\n",
      "Step 189300 (epoch   59.16), loss: 0.004669, time: 2.249 s\n",
      "Step 189320 (epoch   59.16), loss: 0.010076, time: 2.235 s\n",
      "Step 189340 (epoch   59.17), loss: 0.003925, time: 2.239 s\n",
      "Step 189360 (epoch   59.17), loss: 0.005140, time: 2.236 s\n",
      "Step 189380 (epoch   59.18), loss: 0.004839, time: 2.243 s\n",
      "Step 189400 (epoch   59.19), loss: 0.003948, time: 2.251 s, error: 0.011580\n",
      "Step 189420 (epoch   59.19), loss: 0.004549, time: 15.040 s\n",
      "Step 189440 (epoch   59.20), loss: 0.004244, time: 2.243 s\n",
      "Step 189460 (epoch   59.21), loss: 0.004072, time: 2.230 s\n",
      "Step 189480 (epoch   59.21), loss: 0.003114, time: 2.261 s\n",
      "Step 189500 (epoch   59.22), loss: 0.004376, time: 2.247 s\n",
      "Step 189520 (epoch   59.23), loss: 0.003974, time: 2.238 s\n",
      "Step 189540 (epoch   59.23), loss: 0.004527, time: 2.249 s\n",
      "Step 189560 (epoch   59.24), loss: 0.004553, time: 2.239 s\n",
      "Step 189580 (epoch   59.24), loss: 0.006916, time: 2.251 s\n",
      "Step 189600 (epoch   59.25), loss: 0.004548, time: 2.235 s, error: 0.011006\n",
      "Step 189620 (epoch   59.26), loss: 0.007640, time: 15.066 s\n",
      "Step 189640 (epoch   59.26), loss: 0.004346, time: 2.238 s\n",
      "Step 189660 (epoch   59.27), loss: 0.004685, time: 2.234 s\n",
      "Step 189680 (epoch   59.27), loss: 0.004567, time: 2.248 s\n",
      "Step 189700 (epoch   59.28), loss: 0.005340, time: 2.237 s\n",
      "Step 189720 (epoch   59.29), loss: 0.005395, time: 2.249 s\n",
      "Step 189740 (epoch   59.29), loss: 0.006369, time: 2.249 s\n",
      "Step 189760 (epoch   59.30), loss: 0.006733, time: 2.234 s\n",
      "Step 189780 (epoch   59.31), loss: 0.006292, time: 2.247 s\n",
      "Step 189800 (epoch   59.31), loss: 0.004233, time: 2.251 s, error: 0.010970\n",
      "Step 189820 (epoch   59.32), loss: 0.004070, time: 15.087 s\n",
      "Step 189840 (epoch   59.33), loss: 0.004923, time: 2.240 s\n",
      "Step 189860 (epoch   59.33), loss: 0.005370, time: 2.254 s\n",
      "Step 189880 (epoch   59.34), loss: 0.008278, time: 2.244 s\n",
      "Step 189900 (epoch   59.34), loss: 0.004642, time: 2.237 s\n",
      "Step 189920 (epoch   59.35), loss: 0.003661, time: 2.239 s\n",
      "Step 189940 (epoch   59.36), loss: 0.004692, time: 2.241 s\n",
      "Step 189960 (epoch   59.36), loss: 0.006123, time: 2.241 s\n",
      "Step 189980 (epoch   59.37), loss: 0.004510, time: 2.250 s\n",
      "Step 190000 (epoch   59.38), loss: 0.004025, time: 2.257 s, error: 0.010983\n",
      "\n",
      "Time since beginning  : 33284.122 s\n",
      "\n",
      "Step 190020 (epoch   59.38), loss: 0.004470, time: 15.137 s\n",
      "Step 190040 (epoch   59.39), loss: 0.005958, time: 2.248 s\n",
      "Step 190060 (epoch   59.39), loss: 0.004041, time: 2.227 s\n",
      "Step 190080 (epoch   59.40), loss: 0.006939, time: 2.250 s\n",
      "Step 190100 (epoch   59.41), loss: 0.005784, time: 2.233 s\n",
      "Step 190120 (epoch   59.41), loss: 0.005344, time: 2.245 s\n",
      "Step 190140 (epoch   59.42), loss: 0.007115, time: 2.238 s\n",
      "Step 190160 (epoch   59.42), loss: 0.006224, time: 2.252 s\n",
      "Step 190180 (epoch   59.43), loss: 0.005059, time: 2.232 s\n",
      "Step 190200 (epoch   59.44), loss: 0.005640, time: 2.250 s, error: 0.011047\n",
      "Step 190220 (epoch   59.44), loss: 0.003493, time: 15.237 s\n",
      "Step 190240 (epoch   59.45), loss: 0.011157, time: 2.234 s\n",
      "Step 190260 (epoch   59.46), loss: 0.004644, time: 2.250 s\n",
      "Step 190280 (epoch   59.46), loss: 0.003341, time: 2.240 s\n",
      "Step 190300 (epoch   59.47), loss: 0.003424, time: 2.243 s\n",
      "Step 190320 (epoch   59.48), loss: 0.004261, time: 2.231 s\n",
      "Step 190340 (epoch   59.48), loss: 0.004402, time: 2.249 s\n",
      "Step 190360 (epoch   59.49), loss: 0.003894, time: 2.232 s\n",
      "Step 190380 (epoch   59.49), loss: 0.008173, time: 2.243 s\n",
      "Step 190400 (epoch   59.50), loss: 0.010104, time: 2.227 s, error: 0.012966\n",
      "Step 190420 (epoch   59.51), loss: 0.005108, time: 15.271 s\n",
      "Step 190440 (epoch   59.51), loss: 0.003981, time: 2.248 s\n",
      "Step 190460 (epoch   59.52), loss: 0.004050, time: 2.237 s\n",
      "Step 190480 (epoch   59.52), loss: 0.003374, time: 2.255 s\n",
      "Step 190500 (epoch   59.53), loss: 0.002880, time: 2.229 s\n",
      "Step 190520 (epoch   59.54), loss: 0.005822, time: 2.247 s\n",
      "Step 190540 (epoch   59.54), loss: 0.011524, time: 2.237 s\n",
      "Step 190560 (epoch   59.55), loss: 0.003793, time: 2.234 s\n",
      "Step 190580 (epoch   59.56), loss: 0.003964, time: 2.240 s\n",
      "Step 190600 (epoch   59.56), loss: 0.006606, time: 2.252 s, error: 0.011593\n",
      "Step 190620 (epoch   59.57), loss: 0.003805, time: 15.093 s\n",
      "Step 190640 (epoch   59.58), loss: 0.003342, time: 2.244 s\n",
      "Step 190660 (epoch   59.58), loss: 0.004846, time: 2.261 s\n",
      "Step 190680 (epoch   59.59), loss: 0.004775, time: 2.241 s\n",
      "Step 190700 (epoch   59.59), loss: 0.007752, time: 2.246 s\n",
      "Step 190720 (epoch   59.60), loss: 0.003918, time: 2.242 s\n",
      "Step 190740 (epoch   59.61), loss: 0.005617, time: 2.234 s\n",
      "Step 190760 (epoch   59.61), loss: 0.007402, time: 2.247 s\n",
      "Step 190780 (epoch   59.62), loss: 0.004470, time: 2.240 s\n",
      "Step 190800 (epoch   59.62), loss: 0.004735, time: 2.241 s, error: 0.011468\n",
      "Step 190820 (epoch   59.63), loss: 0.004027, time: 15.077 s\n",
      "Step 190840 (epoch   59.64), loss: 0.004172, time: 2.242 s\n",
      "Step 190860 (epoch   59.64), loss: 0.004219, time: 2.242 s\n",
      "Step 190880 (epoch   59.65), loss: 0.006314, time: 2.239 s\n",
      "Step 190900 (epoch   59.66), loss: 0.004379, time: 2.240 s\n",
      "Step 190920 (epoch   59.66), loss: 0.004280, time: 2.260 s\n",
      "Step 190940 (epoch   59.67), loss: 0.004777, time: 2.254 s\n",
      "Step 190960 (epoch   59.67), loss: 0.004503, time: 2.241 s\n",
      "Step 190980 (epoch   59.68), loss: 0.004945, time: 2.249 s\n",
      "Step 191000 (epoch   59.69), loss: 0.003515, time: 2.229 s, error: 0.010953\n",
      "Step 191020 (epoch   59.69), loss: 0.006095, time: 15.065 s\n",
      "Step 191040 (epoch   59.70), loss: 0.004976, time: 2.247 s\n",
      "Step 191060 (epoch   59.71), loss: 0.005285, time: 2.241 s\n",
      "Step 191080 (epoch   59.71), loss: 0.003440, time: 2.242 s\n",
      "Step 191100 (epoch   59.72), loss: 0.003896, time: 2.238 s\n",
      "Step 191120 (epoch   59.73), loss: 0.003435, time: 2.247 s\n",
      "Step 191140 (epoch   59.73), loss: 0.003565, time: 2.238 s\n",
      "Step 191160 (epoch   59.74), loss: 0.004893, time: 2.245 s\n",
      "Step 191180 (epoch   59.74), loss: 0.004046, time: 2.246 s\n",
      "Step 191200 (epoch   59.75), loss: 0.004579, time: 2.255 s, error: 0.010904\n",
      "Step 191220 (epoch   59.76), loss: 0.007466, time: 15.082 s\n",
      "Step 191240 (epoch   59.76), loss: 0.003776, time: 2.240 s\n",
      "Step 191260 (epoch   59.77), loss: 0.004049, time: 2.247 s\n",
      "Step 191280 (epoch   59.77), loss: 0.005249, time: 2.240 s\n",
      "Step 191300 (epoch   59.78), loss: 0.005087, time: 2.246 s\n",
      "Step 191320 (epoch   59.79), loss: 0.005716, time: 2.240 s\n",
      "Step 191340 (epoch   59.79), loss: 0.005075, time: 2.244 s\n",
      "Step 191360 (epoch   59.80), loss: 0.005316, time: 2.232 s\n",
      "Step 191380 (epoch   59.81), loss: 0.004453, time: 2.249 s\n",
      "Step 191400 (epoch   59.81), loss: 0.003411, time: 2.239 s, error: 0.010854\n",
      "Step 191420 (epoch   59.82), loss: 0.003705, time: 15.286 s\n",
      "Step 191440 (epoch   59.83), loss: 0.005155, time: 2.251 s\n",
      "Step 191460 (epoch   59.83), loss: 0.003390, time: 2.233 s\n",
      "Step 191480 (epoch   59.84), loss: 0.004118, time: 2.249 s\n",
      "Step 191500 (epoch   59.84), loss: 0.005225, time: 2.241 s\n",
      "Step 191520 (epoch   59.85), loss: 0.005426, time: 2.255 s\n",
      "Step 191540 (epoch   59.86), loss: 0.003781, time: 2.259 s\n",
      "Step 191560 (epoch   59.86), loss: 0.004415, time: 2.251 s\n",
      "Step 191580 (epoch   59.87), loss: 0.003925, time: 2.242 s\n",
      "Step 191600 (epoch   59.88), loss: 0.006198, time: 2.239 s, error: 0.010712\n",
      "Step 191620 (epoch   59.88), loss: 0.003603, time: 15.236 s\n",
      "Step 191640 (epoch   59.89), loss: 0.003174, time: 2.244 s\n",
      "Step 191660 (epoch   59.89), loss: 0.011978, time: 2.242 s\n",
      "Step 191680 (epoch   59.90), loss: 0.004223, time: 2.248 s\n",
      "Step 191700 (epoch   59.91), loss: 0.006710, time: 2.235 s\n",
      "Step 191720 (epoch   59.91), loss: 0.006442, time: 2.239 s\n",
      "Step 191740 (epoch   59.92), loss: 0.004416, time: 2.255 s\n",
      "Step 191760 (epoch   59.92), loss: 0.004243, time: 2.243 s\n",
      "Step 191780 (epoch   59.93), loss: 0.005391, time: 2.245 s\n",
      "Step 191800 (epoch   59.94), loss: 0.007175, time: 2.241 s, error: 0.010849\n",
      "Step 191820 (epoch   59.94), loss: 0.004653, time: 15.109 s\n",
      "Step 191840 (epoch   59.95), loss: 0.009067, time: 2.261 s\n",
      "Step 191860 (epoch   59.96), loss: 0.004515, time: 2.244 s\n",
      "Step 191880 (epoch   59.96), loss: 0.004407, time: 2.235 s\n",
      "Step 191900 (epoch   59.97), loss: 0.003774, time: 2.230 s\n",
      "Step 191920 (epoch   59.98), loss: 0.007004, time: 2.246 s\n",
      "Step 191940 (epoch   59.98), loss: 0.010387, time: 2.243 s\n",
      "Step 191960 (epoch   59.99), loss: 0.004878, time: 2.229 s\n",
      "Step 191980 (epoch   59.99), loss: 0.003296, time: 2.231 s\n",
      "Step 192000 (epoch   60.00), loss: 0.004848, time: 2.242 s, error: 0.010959\n",
      "\n",
      "Time since beginning  : 33637.554 s\n",
      "\n",
      "Step 192020 (epoch   60.01), loss: 0.004802, time: 15.112 s\n",
      "Step 192040 (epoch   60.01), loss: 0.003201, time: 2.234 s\n",
      "Step 192060 (epoch   60.02), loss: 0.005079, time: 2.243 s\n",
      "Step 192080 (epoch   60.02), loss: 0.007640, time: 2.221 s\n",
      "Step 192100 (epoch   60.03), loss: 0.008216, time: 2.259 s\n",
      "Step 192120 (epoch   60.04), loss: 0.004154, time: 2.245 s\n",
      "Step 192140 (epoch   60.04), loss: 0.005730, time: 2.246 s\n",
      "Step 192160 (epoch   60.05), loss: 0.004085, time: 2.245 s\n",
      "Step 192180 (epoch   60.06), loss: 0.005534, time: 2.238 s\n",
      "Step 192200 (epoch   60.06), loss: 0.006184, time: 2.238 s, error: 0.010762\n",
      "Step 192220 (epoch   60.07), loss: 0.005924, time: 15.071 s\n",
      "Step 192240 (epoch   60.08), loss: 0.005500, time: 2.235 s\n",
      "Step 192260 (epoch   60.08), loss: 0.005057, time: 2.239 s\n",
      "Step 192280 (epoch   60.09), loss: 0.005057, time: 2.233 s\n",
      "Step 192300 (epoch   60.09), loss: 0.004601, time: 2.246 s\n",
      "Step 192320 (epoch   60.10), loss: 0.004276, time: 2.230 s\n",
      "Step 192340 (epoch   60.11), loss: 0.004043, time: 2.240 s\n",
      "Step 192360 (epoch   60.11), loss: 0.003705, time: 2.257 s\n",
      "Step 192380 (epoch   60.12), loss: 0.003360, time: 2.258 s\n",
      "Step 192400 (epoch   60.12), loss: 0.004714, time: 2.232 s, error: 0.010917\n",
      "Step 192420 (epoch   60.13), loss: 0.004428, time: 15.035 s\n",
      "Step 192440 (epoch   60.14), loss: 0.004955, time: 2.250 s\n",
      "Step 192460 (epoch   60.14), loss: 0.004164, time: 2.246 s\n",
      "Step 192480 (epoch   60.15), loss: 0.007639, time: 2.252 s\n",
      "Step 192500 (epoch   60.16), loss: 0.004839, time: 2.235 s\n",
      "Step 192520 (epoch   60.16), loss: 0.003969, time: 2.248 s\n",
      "Step 192540 (epoch   60.17), loss: 0.004927, time: 2.243 s\n",
      "Step 192560 (epoch   60.17), loss: 0.003851, time: 2.240 s\n",
      "Step 192580 (epoch   60.18), loss: 0.004878, time: 2.238 s\n",
      "Step 192600 (epoch   60.19), loss: 0.003814, time: 2.241 s, error: 0.011283\n",
      "Step 192620 (epoch   60.19), loss: 0.006150, time: 15.272 s\n",
      "Step 192640 (epoch   60.20), loss: 0.002550, time: 2.247 s\n",
      "Step 192660 (epoch   60.21), loss: 0.005139, time: 2.241 s\n",
      "Step 192680 (epoch   60.21), loss: 0.005727, time: 2.240 s\n",
      "Step 192700 (epoch   60.22), loss: 0.010466, time: 2.249 s\n",
      "Step 192720 (epoch   60.23), loss: 0.005643, time: 2.249 s\n",
      "Step 192740 (epoch   60.23), loss: 0.005012, time: 2.238 s\n",
      "Step 192760 (epoch   60.24), loss: 0.005387, time: 2.237 s\n",
      "Step 192780 (epoch   60.24), loss: 0.004117, time: 2.231 s\n",
      "Step 192800 (epoch   60.25), loss: 0.007291, time: 2.241 s, error: 0.010821\n",
      "Step 192820 (epoch   60.26), loss: 0.004325, time: 15.266 s\n",
      "Step 192840 (epoch   60.26), loss: 0.004792, time: 2.241 s\n",
      "Step 192860 (epoch   60.27), loss: 0.003208, time: 2.243 s\n",
      "Step 192880 (epoch   60.27), loss: 0.004941, time: 2.238 s\n",
      "Step 192900 (epoch   60.28), loss: 0.004072, time: 2.237 s\n",
      "Step 192920 (epoch   60.29), loss: 0.005139, time: 2.237 s\n",
      "Step 192940 (epoch   60.29), loss: 0.005831, time: 2.238 s\n",
      "Step 192960 (epoch   60.30), loss: 0.004031, time: 2.239 s\n",
      "Step 192980 (epoch   60.31), loss: 0.004959, time: 2.250 s\n",
      "Step 193000 (epoch   60.31), loss: 0.005555, time: 2.246 s, error: 0.010746\n",
      "Step 193020 (epoch   60.32), loss: 0.003559, time: 15.087 s\n",
      "Step 193040 (epoch   60.33), loss: 0.008066, time: 2.249 s\n",
      "Step 193060 (epoch   60.33), loss: 0.007155, time: 2.232 s\n",
      "Step 193080 (epoch   60.34), loss: 0.006056, time: 2.242 s\n",
      "Step 193100 (epoch   60.34), loss: 0.005331, time: 2.240 s\n",
      "Step 193120 (epoch   60.35), loss: 0.005593, time: 2.244 s\n",
      "Step 193140 (epoch   60.36), loss: 0.004722, time: 2.242 s\n",
      "Step 193160 (epoch   60.36), loss: 0.007431, time: 2.242 s\n",
      "Step 193180 (epoch   60.37), loss: 0.005994, time: 2.237 s\n",
      "Step 193200 (epoch   60.38), loss: 0.005698, time: 2.241 s, error: 0.010896\n",
      "Step 193220 (epoch   60.38), loss: 0.004943, time: 15.053 s\n",
      "Step 193240 (epoch   60.39), loss: 0.009164, time: 2.227 s\n",
      "Step 193260 (epoch   60.39), loss: 0.004130, time: 2.235 s\n",
      "Step 193280 (epoch   60.40), loss: 0.005289, time: 2.257 s\n",
      "Step 193300 (epoch   60.41), loss: 0.004389, time: 2.245 s\n",
      "Step 193320 (epoch   60.41), loss: 0.005383, time: 2.252 s\n",
      "Step 193340 (epoch   60.42), loss: 0.004687, time: 2.235 s\n",
      "Step 193360 (epoch   60.42), loss: 0.003876, time: 2.247 s\n",
      "Step 193380 (epoch   60.43), loss: 0.003938, time: 2.229 s\n",
      "Step 193400 (epoch   60.44), loss: 0.005347, time: 2.249 s, error: 0.010990\n",
      "Step 193420 (epoch   60.44), loss: 0.008259, time: 15.026 s\n",
      "Step 193440 (epoch   60.45), loss: 0.003218, time: 2.222 s\n",
      "Step 193460 (epoch   60.46), loss: 0.005880, time: 2.242 s\n",
      "Step 193480 (epoch   60.46), loss: 0.003749, time: 2.222 s\n",
      "Step 193500 (epoch   60.47), loss: 0.003001, time: 2.243 s\n",
      "Step 193520 (epoch   60.48), loss: 0.004697, time: 2.254 s\n",
      "Step 193540 (epoch   60.48), loss: 0.006634, time: 2.249 s\n",
      "Step 193560 (epoch   60.49), loss: 0.004261, time: 2.250 s\n",
      "Step 193580 (epoch   60.49), loss: 0.004620, time: 2.241 s\n",
      "Step 193600 (epoch   60.50), loss: 0.005439, time: 2.251 s, error: 0.011723\n",
      "Step 193620 (epoch   60.51), loss: 0.004303, time: 15.213 s\n",
      "Step 193640 (epoch   60.51), loss: 0.004520, time: 2.235 s\n",
      "Step 193660 (epoch   60.52), loss: 0.006881, time: 2.242 s\n",
      "Step 193680 (epoch   60.52), loss: 0.004791, time: 2.229 s\n",
      "Step 193700 (epoch   60.53), loss: 0.005271, time: 2.255 s\n",
      "Step 193720 (epoch   60.54), loss: 0.007913, time: 2.231 s\n",
      "Step 193740 (epoch   60.54), loss: 0.004577, time: 2.251 s\n",
      "Step 193760 (epoch   60.55), loss: 0.004931, time: 2.236 s\n",
      "Step 193780 (epoch   60.56), loss: 0.004998, time: 2.253 s\n",
      "Step 193800 (epoch   60.56), loss: 0.003489, time: 2.257 s, error: 0.011121\n",
      "Step 193820 (epoch   60.57), loss: 0.003932, time: 15.106 s\n",
      "Step 193840 (epoch   60.58), loss: 0.003670, time: 2.239 s\n",
      "Step 193860 (epoch   60.58), loss: 0.006620, time: 2.241 s\n",
      "Step 193880 (epoch   60.59), loss: 0.005787, time: 2.241 s\n",
      "Step 193900 (epoch   60.59), loss: 0.004380, time: 2.247 s\n",
      "Step 193920 (epoch   60.60), loss: 0.004127, time: 2.227 s\n",
      "Step 193940 (epoch   60.61), loss: 0.004229, time: 2.238 s\n",
      "Step 193960 (epoch   60.61), loss: 0.004262, time: 2.250 s\n",
      "Step 193980 (epoch   60.62), loss: 0.004882, time: 2.234 s\n",
      "Step 194000 (epoch   60.62), loss: 0.007238, time: 2.237 s, error: 0.011269\n",
      "\n",
      "Time since beginning  : 33990.721 s\n",
      "\n",
      "Step 194020 (epoch   60.63), loss: 0.004632, time: 15.300 s\n",
      "Step 194040 (epoch   60.64), loss: 0.005294, time: 2.241 s\n",
      "Step 194060 (epoch   60.64), loss: 0.005130, time: 2.228 s\n",
      "Step 194080 (epoch   60.65), loss: 0.004292, time: 2.243 s\n",
      "Step 194100 (epoch   60.66), loss: 0.006958, time: 2.239 s\n",
      "Step 194120 (epoch   60.66), loss: 0.008006, time: 2.235 s\n",
      "Step 194140 (epoch   60.67), loss: 0.006967, time: 2.245 s\n",
      "Step 194160 (epoch   60.67), loss: 0.006674, time: 2.246 s\n",
      "Step 194180 (epoch   60.68), loss: 0.005306, time: 2.242 s\n",
      "Step 194200 (epoch   60.69), loss: 0.005907, time: 2.231 s, error: 0.010910\n",
      "Step 194220 (epoch   60.69), loss: 0.005007, time: 15.267 s\n",
      "Step 194240 (epoch   60.70), loss: 0.005344, time: 2.245 s\n",
      "Step 194260 (epoch   60.71), loss: 0.003841, time: 2.240 s\n",
      "Step 194280 (epoch   60.71), loss: 0.018071, time: 2.253 s\n",
      "Step 194300 (epoch   60.72), loss: 0.005673, time: 2.235 s\n",
      "Step 194320 (epoch   60.73), loss: 0.004110, time: 2.246 s\n",
      "Step 194340 (epoch   60.73), loss: 0.004840, time: 2.231 s\n",
      "Step 194360 (epoch   60.74), loss: 0.006999, time: 2.243 s\n",
      "Step 194380 (epoch   60.74), loss: 0.004421, time: 2.242 s\n",
      "Step 194400 (epoch   60.75), loss: 0.008191, time: 2.246 s, error: 0.010843\n",
      "Step 194420 (epoch   60.76), loss: 0.004891, time: 15.096 s\n",
      "Step 194440 (epoch   60.76), loss: 0.004023, time: 2.249 s\n",
      "Step 194460 (epoch   60.77), loss: 0.004578, time: 2.258 s\n",
      "Step 194480 (epoch   60.77), loss: 0.005220, time: 2.261 s\n",
      "Step 194500 (epoch   60.78), loss: 0.006219, time: 2.250 s\n",
      "Step 194520 (epoch   60.79), loss: 0.006227, time: 2.233 s\n",
      "Step 194540 (epoch   60.79), loss: 0.005924, time: 2.230 s\n",
      "Step 194560 (epoch   60.80), loss: 0.004291, time: 2.241 s\n",
      "Step 194580 (epoch   60.81), loss: 0.004887, time: 2.235 s\n",
      "Step 194600 (epoch   60.81), loss: 0.004589, time: 2.245 s, error: 0.010741\n",
      "Step 194620 (epoch   60.82), loss: 0.004917, time: 15.105 s\n",
      "Step 194640 (epoch   60.83), loss: 0.005641, time: 2.224 s\n",
      "Step 194660 (epoch   60.83), loss: 0.007997, time: 2.245 s\n",
      "Step 194680 (epoch   60.84), loss: 0.004097, time: 2.233 s\n",
      "Step 194700 (epoch   60.84), loss: 0.005375, time: 2.250 s\n",
      "Step 194720 (epoch   60.85), loss: 0.004441, time: 2.251 s\n",
      "Step 194740 (epoch   60.86), loss: 0.002889, time: 2.257 s\n",
      "Step 194760 (epoch   60.86), loss: 0.003352, time: 2.249 s\n",
      "Step 194780 (epoch   60.87), loss: 0.004913, time: 2.235 s\n",
      "Step 194800 (epoch   60.88), loss: 0.002887, time: 2.248 s, error: 0.010629\n",
      "Step 194820 (epoch   60.88), loss: 0.004590, time: 15.051 s\n",
      "Step 194840 (epoch   60.89), loss: 0.004496, time: 2.244 s\n",
      "Step 194860 (epoch   60.89), loss: 0.003706, time: 2.242 s\n",
      "Step 194880 (epoch   60.90), loss: 0.006813, time: 2.245 s\n",
      "Step 194900 (epoch   60.91), loss: 0.008817, time: 2.223 s\n",
      "Step 194920 (epoch   60.91), loss: 0.005624, time: 2.250 s\n",
      "Step 194940 (epoch   60.92), loss: 0.007049, time: 2.251 s\n",
      "Step 194960 (epoch   60.92), loss: 0.003035, time: 2.244 s\n",
      "Step 194980 (epoch   60.93), loss: 0.004125, time: 2.254 s\n",
      "Step 195000 (epoch   60.94), loss: 0.004289, time: 2.243 s, error: 0.010827\n",
      "Step 195020 (epoch   60.94), loss: 0.004100, time: 15.086 s\n",
      "Step 195040 (epoch   60.95), loss: 0.003135, time: 2.230 s\n",
      "Step 195060 (epoch   60.96), loss: 0.004270, time: 2.234 s\n",
      "Step 195080 (epoch   60.96), loss: 0.003816, time: 2.236 s\n",
      "Step 195100 (epoch   60.97), loss: 0.005177, time: 2.240 s\n",
      "Step 195120 (epoch   60.98), loss: 0.005054, time: 2.242 s\n",
      "Step 195140 (epoch   60.98), loss: 0.004760, time: 2.240 s\n",
      "Step 195160 (epoch   60.99), loss: 0.004256, time: 2.251 s\n",
      "Step 195180 (epoch   60.99), loss: 0.004090, time: 2.256 s\n",
      "Step 195200 (epoch   61.00), loss: 0.005430, time: 2.248 s, error: 0.011138\n",
      "Step 195220 (epoch   61.01), loss: 0.003611, time: 15.302 s\n",
      "Step 195240 (epoch   61.01), loss: 0.006683, time: 2.237 s\n",
      "Step 195260 (epoch   61.02), loss: 0.006658, time: 2.241 s\n",
      "Step 195280 (epoch   61.02), loss: 0.003836, time: 2.224 s\n",
      "Step 195300 (epoch   61.03), loss: 0.004635, time: 2.237 s\n",
      "Step 195320 (epoch   61.04), loss: 0.004443, time: 2.249 s\n",
      "Step 195340 (epoch   61.04), loss: 0.004222, time: 2.244 s\n",
      "Step 195360 (epoch   61.05), loss: 0.007918, time: 2.244 s\n",
      "Step 195380 (epoch   61.06), loss: 0.003577, time: 2.234 s\n",
      "Step 195400 (epoch   61.06), loss: 0.003907, time: 2.255 s, error: 0.010919\n",
      "Step 195420 (epoch   61.07), loss: 0.006421, time: 15.279 s\n",
      "Step 195440 (epoch   61.08), loss: 0.003800, time: 2.242 s\n",
      "Step 195460 (epoch   61.08), loss: 0.004118, time: 2.241 s\n",
      "Step 195480 (epoch   61.09), loss: 0.004185, time: 2.239 s\n",
      "Step 195500 (epoch   61.09), loss: 0.004425, time: 2.245 s\n",
      "Step 195520 (epoch   61.10), loss: 0.003799, time: 2.236 s\n",
      "Step 195540 (epoch   61.11), loss: 0.004425, time: 2.254 s\n",
      "Step 195560 (epoch   61.11), loss: 0.006427, time: 2.253 s\n",
      "Step 195580 (epoch   61.12), loss: 0.009032, time: 2.257 s\n",
      "Step 195600 (epoch   61.12), loss: 0.004000, time: 2.248 s, error: 0.010923\n",
      "Step 195620 (epoch   61.13), loss: 0.003204, time: 15.088 s\n",
      "Step 195640 (epoch   61.14), loss: 0.008070, time: 2.251 s\n",
      "Step 195660 (epoch   61.14), loss: 0.002745, time: 2.253 s\n",
      "Step 195680 (epoch   61.15), loss: 0.005439, time: 2.248 s\n",
      "Step 195700 (epoch   61.16), loss: 0.004694, time: 2.234 s\n",
      "Step 195720 (epoch   61.16), loss: 0.004649, time: 2.238 s\n",
      "Step 195740 (epoch   61.17), loss: 0.004077, time: 2.238 s\n",
      "Step 195760 (epoch   61.17), loss: 0.003286, time: 2.244 s\n",
      "Step 195780 (epoch   61.18), loss: 0.003123, time: 2.230 s\n",
      "Step 195800 (epoch   61.19), loss: 0.004236, time: 2.248 s, error: 0.011111\n",
      "Step 195820 (epoch   61.19), loss: 0.003514, time: 15.066 s\n",
      "Step 195840 (epoch   61.20), loss: 0.004336, time: 2.246 s\n",
      "Step 195860 (epoch   61.21), loss: 0.003011, time: 2.248 s\n",
      "Step 195880 (epoch   61.21), loss: 0.008312, time: 2.254 s\n",
      "Step 195900 (epoch   61.22), loss: 0.003606, time: 2.257 s\n",
      "Step 195920 (epoch   61.23), loss: 0.007544, time: 2.248 s\n",
      "Step 195940 (epoch   61.23), loss: 0.003271, time: 2.252 s\n",
      "Step 195960 (epoch   61.24), loss: 0.005250, time: 2.235 s\n",
      "Step 195980 (epoch   61.24), loss: 0.007042, time: 2.250 s\n",
      "Step 196000 (epoch   61.25), loss: 0.004597, time: 2.225 s, error: 0.010745\n",
      "\n",
      "Time since beginning  : 34344.121 s\n",
      "\n",
      "Step 196020 (epoch   61.26), loss: 0.005313, time: 15.207 s\n",
      "Step 196040 (epoch   61.26), loss: 0.003726, time: 2.244 s\n",
      "Step 196060 (epoch   61.27), loss: 0.011526, time: 2.252 s\n",
      "Step 196080 (epoch   61.27), loss: 0.008640, time: 2.239 s\n",
      "Step 196100 (epoch   61.28), loss: 0.005359, time: 2.242 s\n",
      "Step 196120 (epoch   61.29), loss: 0.003802, time: 2.239 s\n",
      "Step 196140 (epoch   61.29), loss: 0.007624, time: 2.252 s\n",
      "Step 196160 (epoch   61.30), loss: 0.010098, time: 2.247 s\n",
      "Step 196180 (epoch   61.31), loss: 0.005616, time: 2.246 s\n",
      "Step 196200 (epoch   61.31), loss: 0.007534, time: 2.233 s, error: 0.010648\n",
      "Step 196220 (epoch   61.32), loss: 0.005714, time: 15.174 s\n",
      "Step 196240 (epoch   61.33), loss: 0.006084, time: 2.237 s\n",
      "Step 196260 (epoch   61.33), loss: 0.004520, time: 2.256 s\n",
      "Step 196280 (epoch   61.34), loss: 0.003715, time: 2.245 s\n",
      "Step 196300 (epoch   61.34), loss: 0.003424, time: 2.248 s\n",
      "Step 196320 (epoch   61.35), loss: 0.003921, time: 2.239 s\n",
      "Step 196340 (epoch   61.36), loss: 0.003437, time: 2.254 s\n",
      "Step 196360 (epoch   61.36), loss: 0.003436, time: 2.237 s\n",
      "Step 196380 (epoch   61.37), loss: 0.006499, time: 2.244 s\n",
      "Step 196400 (epoch   61.38), loss: 0.003716, time: 2.235 s, error: 0.010865\n",
      "Step 196420 (epoch   61.38), loss: 0.003714, time: 15.250 s\n",
      "Step 196440 (epoch   61.39), loss: 0.005247, time: 2.241 s\n",
      "Step 196460 (epoch   61.39), loss: 0.006434, time: 2.248 s\n",
      "Step 196480 (epoch   61.40), loss: 0.003102, time: 2.245 s\n",
      "Step 196500 (epoch   61.41), loss: 0.004543, time: 2.231 s\n",
      "Step 196520 (epoch   61.41), loss: 0.004334, time: 2.252 s\n",
      "Step 196540 (epoch   61.42), loss: 0.004293, time: 2.231 s\n",
      "Step 196560 (epoch   61.42), loss: 0.005904, time: 2.244 s\n",
      "Step 196580 (epoch   61.43), loss: 0.004881, time: 2.245 s\n",
      "Step 196600 (epoch   61.44), loss: 0.004795, time: 2.238 s, error: 0.010923\n",
      "Step 196620 (epoch   61.44), loss: 0.005669, time: 15.244 s\n",
      "Step 196640 (epoch   61.45), loss: 0.005847, time: 2.233 s\n",
      "Step 196660 (epoch   61.46), loss: 0.003877, time: 2.245 s\n",
      "Step 196680 (epoch   61.46), loss: 0.006442, time: 2.238 s\n",
      "Step 196700 (epoch   61.47), loss: 0.013206, time: 2.241 s\n",
      "Step 196720 (epoch   61.48), loss: 0.004124, time: 2.228 s\n",
      "Step 196740 (epoch   61.48), loss: 0.004035, time: 2.236 s\n",
      "Step 196760 (epoch   61.49), loss: 0.003815, time: 2.246 s\n",
      "Step 196780 (epoch   61.49), loss: 0.004655, time: 2.236 s\n",
      "Step 196800 (epoch   61.50), loss: 0.005939, time: 2.241 s, error: 0.010994\n",
      "Step 196820 (epoch   61.51), loss: 0.004325, time: 15.059 s\n",
      "Step 196840 (epoch   61.51), loss: 0.003776, time: 2.259 s\n",
      "Step 196860 (epoch   61.52), loss: 0.013977, time: 2.249 s\n",
      "Step 196880 (epoch   61.52), loss: 0.006651, time: 2.241 s\n",
      "Step 196900 (epoch   61.53), loss: 0.003177, time: 2.242 s\n",
      "Step 196920 (epoch   61.54), loss: 0.008283, time: 2.234 s\n",
      "Step 196940 (epoch   61.54), loss: 0.003625, time: 2.251 s\n",
      "Step 196960 (epoch   61.55), loss: 0.004476, time: 2.231 s\n",
      "Step 196980 (epoch   61.56), loss: 0.004821, time: 2.252 s\n",
      "Step 197000 (epoch   61.56), loss: 0.003563, time: 2.243 s, error: 0.011070\n",
      "Step 197020 (epoch   61.57), loss: 0.002287, time: 15.017 s\n",
      "Step 197040 (epoch   61.58), loss: 0.003949, time: 2.244 s\n",
      "Step 197060 (epoch   61.58), loss: 0.005132, time: 2.234 s\n",
      "Step 197080 (epoch   61.59), loss: 0.005180, time: 2.255 s\n",
      "Step 197100 (epoch   61.59), loss: 0.005227, time: 2.255 s\n",
      "Step 197120 (epoch   61.60), loss: 0.008537, time: 2.246 s\n",
      "Step 197140 (epoch   61.61), loss: 0.003687, time: 2.234 s\n",
      "Step 197160 (epoch   61.61), loss: 0.002967, time: 2.234 s\n",
      "Step 197180 (epoch   61.62), loss: 0.004462, time: 2.237 s\n",
      "Step 197200 (epoch   61.62), loss: 0.004751, time: 2.245 s, error: 0.011077\n",
      "Step 197220 (epoch   61.63), loss: 0.003925, time: 15.058 s\n",
      "Step 197240 (epoch   61.64), loss: 0.005022, time: 2.247 s\n",
      "Step 197260 (epoch   61.64), loss: 0.005861, time: 2.255 s\n",
      "Step 197280 (epoch   61.65), loss: 0.007599, time: 2.254 s\n",
      "Step 197300 (epoch   61.66), loss: 0.003861, time: 2.255 s\n",
      "Step 197320 (epoch   61.66), loss: 0.005458, time: 2.247 s\n",
      "Step 197340 (epoch   61.67), loss: 0.004699, time: 2.248 s\n",
      "Step 197360 (epoch   61.67), loss: 0.004081, time: 2.253 s\n",
      "Step 197380 (epoch   61.68), loss: 0.003971, time: 2.241 s\n",
      "Step 197400 (epoch   61.69), loss: 0.003242, time: 2.242 s, error: 0.010826\n",
      "Step 197420 (epoch   61.69), loss: 0.004427, time: 15.094 s\n",
      "Step 197440 (epoch   61.70), loss: 0.003045, time: 2.253 s\n",
      "Step 197460 (epoch   61.71), loss: 0.003856, time: 2.236 s\n",
      "Step 197480 (epoch   61.71), loss: 0.005152, time: 2.245 s\n",
      "Step 197500 (epoch   61.72), loss: 0.005244, time: 2.237 s\n",
      "Step 197520 (epoch   61.73), loss: 0.003208, time: 2.232 s\n",
      "Step 197540 (epoch   61.73), loss: 0.004909, time: 2.243 s\n",
      "Step 197560 (epoch   61.74), loss: 0.005111, time: 2.232 s\n",
      "Step 197580 (epoch   61.74), loss: 0.006472, time: 2.226 s\n",
      "Step 197600 (epoch   61.75), loss: 0.003579, time: 2.250 s, error: 0.010811\n",
      "Step 197620 (epoch   61.76), loss: 0.004802, time: 15.181 s\n",
      "Step 197640 (epoch   61.76), loss: 0.003241, time: 2.246 s\n",
      "Step 197660 (epoch   61.77), loss: 0.004037, time: 2.228 s\n",
      "Step 197680 (epoch   61.77), loss: 0.004133, time: 2.240 s\n",
      "Step 197700 (epoch   61.78), loss: 0.007340, time: 2.241 s\n",
      "Step 197720 (epoch   61.79), loss: 0.004531, time: 2.240 s\n",
      "Step 197740 (epoch   61.79), loss: 0.008723, time: 2.239 s\n",
      "Step 197760 (epoch   61.80), loss: 0.005325, time: 2.247 s\n",
      "Step 197780 (epoch   61.81), loss: 0.004525, time: 2.250 s\n",
      "Step 197800 (epoch   61.81), loss: 0.006360, time: 2.240 s, error: 0.010697\n",
      "Step 197820 (epoch   61.82), loss: 0.004126, time: 15.236 s\n",
      "Step 197840 (epoch   61.83), loss: 0.007054, time: 2.237 s\n",
      "Step 197860 (epoch   61.83), loss: 0.002901, time: 2.245 s\n",
      "Step 197880 (epoch   61.84), loss: 0.003256, time: 2.234 s\n",
      "Step 197900 (epoch   61.84), loss: 0.004206, time: 2.242 s\n",
      "Step 197920 (epoch   61.85), loss: 0.004915, time: 2.240 s\n",
      "Step 197940 (epoch   61.86), loss: 0.005999, time: 2.250 s\n",
      "Step 197960 (epoch   61.86), loss: 0.004321, time: 2.245 s\n",
      "Step 197980 (epoch   61.87), loss: 0.005001, time: 2.246 s\n",
      "Step 198000 (epoch   61.88), loss: 0.003778, time: 2.227 s, error: 0.010590\n",
      "\n",
      "Time since beginning  : 34697.481 s\n",
      "\n",
      "Step 198020 (epoch   61.88), loss: 0.005309, time: 15.218 s\n",
      "Step 198040 (epoch   61.89), loss: 0.004743, time: 2.244 s\n",
      "Step 198060 (epoch   61.89), loss: 0.003521, time: 2.228 s\n",
      "Step 198080 (epoch   61.90), loss: 0.005441, time: 2.228 s\n",
      "Step 198100 (epoch   61.91), loss: 0.006601, time: 2.228 s\n",
      "Step 198120 (epoch   61.91), loss: 0.003335, time: 2.243 s\n",
      "Step 198140 (epoch   61.92), loss: 0.006189, time: 2.256 s\n",
      "Step 198160 (epoch   61.92), loss: 0.005210, time: 2.249 s\n",
      "Step 198180 (epoch   61.93), loss: 0.003366, time: 2.243 s\n",
      "Step 198200 (epoch   61.94), loss: 0.003217, time: 2.233 s, error: 0.010809\n",
      "Step 198220 (epoch   61.94), loss: 0.003394, time: 15.074 s\n",
      "Step 198240 (epoch   61.95), loss: 0.004700, time: 2.246 s\n",
      "Step 198260 (epoch   61.96), loss: 0.003740, time: 2.259 s\n",
      "Step 198280 (epoch   61.96), loss: 0.004086, time: 2.248 s\n",
      "Step 198300 (epoch   61.97), loss: 0.004008, time: 2.246 s\n",
      "Step 198320 (epoch   61.98), loss: 0.005337, time: 2.231 s\n",
      "Step 198340 (epoch   61.98), loss: 0.004972, time: 2.246 s\n",
      "Step 198360 (epoch   61.99), loss: 0.006882, time: 2.237 s\n",
      "Step 198380 (epoch   61.99), loss: 0.007113, time: 2.250 s\n",
      "Step 198400 (epoch   62.00), loss: 0.003811, time: 2.234 s, error: 0.011594\n",
      "Step 198420 (epoch   62.01), loss: 0.004124, time: 15.175 s\n",
      "Step 198440 (epoch   62.01), loss: 0.005780, time: 2.251 s\n",
      "Step 198460 (epoch   62.02), loss: 0.003995, time: 2.255 s\n",
      "Step 198480 (epoch   62.02), loss: 0.005549, time: 2.244 s\n",
      "Step 198500 (epoch   62.03), loss: 0.007919, time: 2.244 s\n",
      "Step 198520 (epoch   62.04), loss: 0.003581, time: 2.254 s\n",
      "Step 198540 (epoch   62.04), loss: 0.005023, time: 2.258 s\n",
      "Step 198560 (epoch   62.05), loss: 0.005384, time: 2.240 s\n",
      "Step 198580 (epoch   62.06), loss: 0.003938, time: 2.235 s\n",
      "Step 198600 (epoch   62.06), loss: 0.004635, time: 2.232 s, error: 0.011113\n",
      "Step 198620 (epoch   62.07), loss: 0.004701, time: 15.077 s\n",
      "Step 198640 (epoch   62.08), loss: 0.005151, time: 2.227 s\n",
      "Step 198660 (epoch   62.08), loss: 0.004200, time: 2.233 s\n",
      "Step 198680 (epoch   62.09), loss: 0.006425, time: 2.232 s\n",
      "Step 198700 (epoch   62.09), loss: 0.004001, time: 2.246 s\n",
      "Step 198720 (epoch   62.10), loss: 0.006067, time: 2.240 s\n",
      "Step 198740 (epoch   62.11), loss: 0.004379, time: 2.253 s\n",
      "Step 198760 (epoch   62.11), loss: 0.003443, time: 2.249 s\n",
      "Step 198780 (epoch   62.12), loss: 0.006438, time: 2.252 s\n",
      "Step 198800 (epoch   62.12), loss: 0.003348, time: 2.249 s, error: 0.010899\n",
      "Step 198820 (epoch   62.13), loss: 0.004577, time: 15.039 s\n",
      "Step 198840 (epoch   62.14), loss: 0.006067, time: 2.240 s\n",
      "Step 198860 (epoch   62.14), loss: 0.006827, time: 2.237 s\n",
      "Step 198880 (epoch   62.15), loss: 0.005540, time: 2.230 s\n",
      "Step 198900 (epoch   62.16), loss: 0.008892, time: 2.248 s\n",
      "Step 198920 (epoch   62.16), loss: 0.004331, time: 2.240 s\n",
      "Step 198940 (epoch   62.17), loss: 0.004537, time: 2.252 s\n",
      "Step 198960 (epoch   62.17), loss: 0.002950, time: 2.239 s\n",
      "Step 198980 (epoch   62.18), loss: 0.003844, time: 2.252 s\n",
      "Step 199000 (epoch   62.19), loss: 0.005381, time: 2.237 s, error: 0.011112\n",
      "Step 199020 (epoch   62.19), loss: 0.004606, time: 15.286 s\n",
      "Step 199040 (epoch   62.20), loss: 0.004174, time: 2.247 s\n",
      "Step 199060 (epoch   62.21), loss: 0.005283, time: 2.239 s\n",
      "Step 199080 (epoch   62.21), loss: 0.007931, time: 2.248 s\n",
      "Step 199100 (epoch   62.22), loss: 0.005071, time: 2.246 s\n",
      "Step 199120 (epoch   62.23), loss: 0.006120, time: 2.240 s\n",
      "Step 199140 (epoch   62.23), loss: 0.002636, time: 2.224 s\n",
      "Step 199160 (epoch   62.24), loss: 0.003548, time: 2.246 s\n",
      "Step 199180 (epoch   62.24), loss: 0.004449, time: 2.241 s\n",
      "Step 199200 (epoch   62.25), loss: 0.006550, time: 2.249 s, error: 0.010814\n",
      "Step 199220 (epoch   62.26), loss: 0.005392, time: 15.231 s\n",
      "Step 199240 (epoch   62.26), loss: 0.003841, time: 2.237 s\n",
      "Step 199260 (epoch   62.27), loss: 0.004614, time: 2.248 s\n",
      "Step 199280 (epoch   62.27), loss: 0.004034, time: 2.254 s\n",
      "Step 199300 (epoch   62.28), loss: 0.003383, time: 2.233 s\n",
      "Step 199320 (epoch   62.29), loss: 0.003632, time: 2.242 s\n",
      "Step 199340 (epoch   62.29), loss: 0.004130, time: 2.231 s\n",
      "Step 199360 (epoch   62.30), loss: 0.004347, time: 2.251 s\n",
      "Step 199380 (epoch   62.31), loss: 0.008254, time: 2.241 s\n",
      "Step 199400 (epoch   62.31), loss: 0.004256, time: 2.240 s, error: 0.010586\n",
      "Step 199420 (epoch   62.32), loss: 0.005111, time: 15.003 s\n",
      "Step 199440 (epoch   62.33), loss: 0.003622, time: 2.248 s\n",
      "Step 199460 (epoch   62.33), loss: 0.002671, time: 2.254 s\n",
      "Step 199480 (epoch   62.34), loss: 0.003072, time: 2.230 s\n",
      "Step 199500 (epoch   62.34), loss: 0.003717, time: 2.244 s\n",
      "Step 199520 (epoch   62.35), loss: 0.003706, time: 2.237 s\n",
      "Step 199540 (epoch   62.36), loss: 0.007481, time: 2.238 s\n",
      "Step 199560 (epoch   62.36), loss: 0.004308, time: 2.226 s\n",
      "Step 199580 (epoch   62.37), loss: 0.005438, time: 2.238 s\n",
      "Step 199600 (epoch   62.38), loss: 0.003988, time: 2.247 s, error: 0.010889\n",
      "Step 199620 (epoch   62.38), loss: 0.003127, time: 15.062 s\n",
      "Step 199640 (epoch   62.39), loss: 0.004518, time: 2.252 s\n",
      "Step 199660 (epoch   62.39), loss: 0.004262, time: 2.243 s\n",
      "Step 199680 (epoch   62.40), loss: 0.004235, time: 2.230 s\n",
      "Step 199700 (epoch   62.41), loss: 0.004078, time: 2.245 s\n",
      "Step 199720 (epoch   62.41), loss: 0.003250, time: 2.251 s\n",
      "Step 199740 (epoch   62.42), loss: 0.004262, time: 2.232 s\n",
      "Step 199760 (epoch   62.42), loss: 0.003969, time: 2.237 s\n",
      "Step 199780 (epoch   62.43), loss: 0.004955, time: 2.241 s\n",
      "Step 199800 (epoch   62.44), loss: 0.005721, time: 2.234 s, error: 0.010820\n",
      "Step 199820 (epoch   62.44), loss: 0.005449, time: 15.060 s\n",
      "Step 199840 (epoch   62.45), loss: 0.005984, time: 2.234 s\n",
      "Step 199860 (epoch   62.46), loss: 0.003778, time: 2.238 s\n",
      "Step 199880 (epoch   62.46), loss: 0.003239, time: 2.244 s\n",
      "Step 199900 (epoch   62.47), loss: 0.004174, time: 2.242 s\n",
      "Step 199920 (epoch   62.48), loss: 0.006294, time: 2.250 s\n",
      "Step 199940 (epoch   62.48), loss: 0.005415, time: 2.247 s\n",
      "Step 199960 (epoch   62.49), loss: 0.005417, time: 2.256 s\n",
      "Step 199980 (epoch   62.49), loss: 0.004234, time: 2.260 s\n",
      "Step 200000 (epoch   62.50), loss: 0.008730, time: 2.236 s, error: 0.011147\n",
      "\n",
      "Time since beginning  : 35050.424 s\n",
      "\n",
      "Step 200020 (epoch   62.51), loss: 0.006538, time: 15.126 s\n",
      "Step 200040 (epoch   62.51), loss: 0.004766, time: 2.246 s\n",
      "Step 200060 (epoch   62.52), loss: 0.004789, time: 2.236 s\n",
      "Step 200080 (epoch   62.52), loss: 0.007334, time: 2.231 s\n",
      "Step 200100 (epoch   62.53), loss: 0.003943, time: 2.252 s\n",
      "Step 200120 (epoch   62.54), loss: 0.009681, time: 2.250 s\n",
      "Step 200140 (epoch   62.54), loss: 0.006476, time: 2.249 s\n",
      "Step 200160 (epoch   62.55), loss: 0.004302, time: 2.244 s\n",
      "Step 200180 (epoch   62.56), loss: 0.003810, time: 2.253 s\n",
      "Step 200200 (epoch   62.56), loss: 0.004038, time: 2.245 s, error: 0.011297\n",
      "Step 200220 (epoch   62.57), loss: 0.004535, time: 15.269 s\n",
      "Step 200240 (epoch   62.58), loss: 0.005758, time: 2.243 s\n",
      "Step 200260 (epoch   62.58), loss: 0.004844, time: 2.235 s\n",
      "Step 200280 (epoch   62.59), loss: 0.006114, time: 2.250 s\n",
      "Step 200300 (epoch   62.59), loss: 0.003117, time: 2.255 s\n",
      "Step 200320 (epoch   62.60), loss: 0.004478, time: 2.254 s\n",
      "Step 200340 (epoch   62.61), loss: 0.003655, time: 2.253 s\n",
      "Step 200360 (epoch   62.61), loss: 0.003516, time: 2.245 s\n",
      "Step 200380 (epoch   62.62), loss: 0.006012, time: 2.249 s\n",
      "Step 200400 (epoch   62.62), loss: 0.004123, time: 2.249 s, error: 0.010903\n",
      "Step 200420 (epoch   62.63), loss: 0.003534, time: 15.373 s\n",
      "Step 200440 (epoch   62.64), loss: 0.004377, time: 2.240 s\n",
      "Step 200460 (epoch   62.64), loss: 0.004987, time: 2.234 s\n",
      "Step 200480 (epoch   62.65), loss: 0.003478, time: 2.241 s\n",
      "Step 200500 (epoch   62.66), loss: 0.007974, time: 2.240 s\n",
      "Step 200520 (epoch   62.66), loss: 0.005306, time: 2.236 s\n",
      "Step 200540 (epoch   62.67), loss: 0.004274, time: 2.235 s\n",
      "Step 200560 (epoch   62.67), loss: 0.007777, time: 2.243 s\n",
      "Step 200580 (epoch   62.68), loss: 0.007930, time: 2.229 s\n",
      "Step 200600 (epoch   62.69), loss: 0.003656, time: 2.248 s, error: 0.010690\n",
      "Step 200620 (epoch   62.69), loss: 0.004565, time: 15.121 s\n",
      "Step 200640 (epoch   62.70), loss: 0.023920, time: 2.260 s\n",
      "Step 200660 (epoch   62.71), loss: 0.002777, time: 2.251 s\n",
      "Step 200680 (epoch   62.71), loss: 0.009865, time: 2.222 s\n",
      "Step 200700 (epoch   62.72), loss: 0.003737, time: 2.241 s\n",
      "Step 200720 (epoch   62.73), loss: 0.005452, time: 2.240 s\n",
      "Step 200740 (epoch   62.73), loss: 0.005284, time: 2.241 s\n",
      "Step 200760 (epoch   62.74), loss: 0.004188, time: 2.238 s\n",
      "Step 200780 (epoch   62.74), loss: 0.003478, time: 2.238 s\n",
      "Step 200800 (epoch   62.75), loss: 0.003611, time: 2.245 s, error: 0.010806\n",
      "Step 200820 (epoch   62.76), loss: 0.005004, time: 15.079 s\n",
      "Step 200840 (epoch   62.76), loss: 0.003879, time: 2.233 s\n",
      "Step 200860 (epoch   62.77), loss: 0.004385, time: 2.238 s\n",
      "Step 200880 (epoch   62.77), loss: 0.008059, time: 2.245 s\n",
      "Step 200900 (epoch   62.78), loss: 0.003866, time: 2.249 s\n",
      "Step 200920 (epoch   62.79), loss: 0.003854, time: 2.243 s\n",
      "Step 200940 (epoch   62.79), loss: 0.006097, time: 2.235 s\n",
      "Step 200960 (epoch   62.80), loss: 0.004644, time: 2.240 s\n",
      "Step 200980 (epoch   62.81), loss: 0.002820, time: 2.241 s\n",
      "Step 201000 (epoch   62.81), loss: 0.004722, time: 2.228 s, error: 0.010672\n",
      "Step 201020 (epoch   62.82), loss: 0.005214, time: 15.097 s\n",
      "Step 201040 (epoch   62.83), loss: 0.005336, time: 2.239 s\n",
      "Step 201060 (epoch   62.83), loss: 0.004818, time: 2.234 s\n",
      "Step 201080 (epoch   62.84), loss: 0.005615, time: 2.234 s\n",
      "Step 201100 (epoch   62.84), loss: 0.004739, time: 2.247 s\n",
      "Step 201120 (epoch   62.85), loss: 0.004180, time: 2.245 s\n",
      "Step 201140 (epoch   62.86), loss: 0.005205, time: 2.243 s\n",
      "Step 201160 (epoch   62.86), loss: 0.005324, time: 2.261 s\n",
      "Step 201180 (epoch   62.87), loss: 0.003609, time: 2.236 s\n",
      "Step 201200 (epoch   62.88), loss: 0.003040, time: 2.251 s, error: 0.010525\n",
      "Step 201220 (epoch   62.88), loss: 0.003795, time: 15.071 s\n",
      "Step 201240 (epoch   62.89), loss: 0.007167, time: 2.245 s\n",
      "Step 201260 (epoch   62.89), loss: 0.006897, time: 2.238 s\n",
      "Step 201280 (epoch   62.90), loss: 0.003940, time: 2.251 s\n",
      "Step 201300 (epoch   62.91), loss: 0.006601, time: 2.249 s\n",
      "Step 201320 (epoch   62.91), loss: 0.004003, time: 2.253 s\n",
      "Step 201340 (epoch   62.92), loss: 0.007057, time: 2.252 s\n",
      "Step 201360 (epoch   62.92), loss: 0.006451, time: 2.237 s\n",
      "Step 201380 (epoch   62.93), loss: 0.004619, time: 2.244 s\n",
      "Step 201400 (epoch   62.94), loss: 0.004363, time: 2.247 s, error: 0.010811\n",
      "Step 201420 (epoch   62.94), loss: 0.003937, time: 15.281 s\n",
      "Step 201440 (epoch   62.95), loss: 0.009083, time: 2.241 s\n",
      "Step 201460 (epoch   62.96), loss: 0.004632, time: 2.243 s\n",
      "Step 201480 (epoch   62.96), loss: 0.003817, time: 2.218 s\n",
      "Step 201500 (epoch   62.97), loss: 0.003164, time: 2.234 s\n",
      "Step 201520 (epoch   62.98), loss: 0.004680, time: 2.230 s\n",
      "Step 201540 (epoch   62.98), loss: 0.006265, time: 2.244 s\n",
      "Step 201560 (epoch   62.99), loss: 0.004206, time: 2.224 s\n",
      "Step 201580 (epoch   62.99), loss: 0.003028, time: 2.247 s\n",
      "Step 201600 (epoch   63.00), loss: 0.004154, time: 2.240 s, error: 0.011781\n",
      "Step 201620 (epoch   63.01), loss: 0.005127, time: 15.301 s\n",
      "Step 201640 (epoch   63.01), loss: 0.003828, time: 2.252 s\n",
      "Step 201660 (epoch   63.02), loss: 0.003530, time: 2.244 s\n",
      "Step 201680 (epoch   63.02), loss: 0.004483, time: 2.249 s\n",
      "Step 201700 (epoch   63.03), loss: 0.005552, time: 2.238 s\n",
      "Step 201720 (epoch   63.04), loss: 0.004174, time: 2.252 s\n",
      "Step 201740 (epoch   63.04), loss: 0.002977, time: 2.254 s\n",
      "Step 201760 (epoch   63.05), loss: 0.005267, time: 2.254 s\n",
      "Step 201780 (epoch   63.06), loss: 0.009403, time: 2.253 s\n",
      "Step 201800 (epoch   63.06), loss: 0.013317, time: 2.248 s, error: 0.010919\n",
      "Step 201820 (epoch   63.07), loss: 0.005641, time: 15.198 s\n",
      "Step 201840 (epoch   63.08), loss: 0.003208, time: 2.252 s\n",
      "Step 201860 (epoch   63.08), loss: 0.004515, time: 2.251 s\n",
      "Step 201880 (epoch   63.09), loss: 0.004510, time: 2.250 s\n",
      "Step 201900 (epoch   63.09), loss: 0.004232, time: 2.238 s\n",
      "Step 201920 (epoch   63.10), loss: 0.003114, time: 2.249 s\n",
      "Step 201940 (epoch   63.11), loss: 0.002636, time: 2.238 s\n",
      "Step 201960 (epoch   63.11), loss: 0.004333, time: 2.244 s\n",
      "Step 201980 (epoch   63.12), loss: 0.003302, time: 2.249 s\n",
      "Step 202000 (epoch   63.12), loss: 0.006036, time: 2.232 s, error: 0.010810\n",
      "\n",
      "Time since beginning  : 35404.298 s\n",
      "\n",
      "Step 202020 (epoch   63.13), loss: 0.004453, time: 15.180 s\n",
      "Step 202040 (epoch   63.14), loss: 0.003386, time: 2.248 s\n",
      "Step 202060 (epoch   63.14), loss: 0.005616, time: 2.241 s\n",
      "Step 202080 (epoch   63.15), loss: 0.004044, time: 2.258 s\n",
      "Step 202100 (epoch   63.16), loss: 0.006437, time: 2.240 s\n",
      "Step 202120 (epoch   63.16), loss: 0.005254, time: 2.238 s\n",
      "Step 202140 (epoch   63.17), loss: 0.010642, time: 2.227 s\n",
      "Step 202160 (epoch   63.17), loss: 0.004154, time: 2.252 s\n",
      "Step 202180 (epoch   63.18), loss: 0.004989, time: 2.246 s\n",
      "Step 202200 (epoch   63.19), loss: 0.006533, time: 2.247 s, error: 0.010947\n",
      "Step 202220 (epoch   63.19), loss: 0.004491, time: 15.057 s\n",
      "Step 202240 (epoch   63.20), loss: 0.005569, time: 2.232 s\n",
      "Step 202260 (epoch   63.21), loss: 0.005725, time: 2.244 s\n",
      "Step 202280 (epoch   63.21), loss: 0.003593, time: 2.233 s\n",
      "Step 202300 (epoch   63.22), loss: 0.006741, time: 2.246 s\n",
      "Step 202320 (epoch   63.23), loss: 0.004591, time: 2.239 s\n",
      "Step 202340 (epoch   63.23), loss: 0.006971, time: 2.259 s\n",
      "Step 202360 (epoch   63.24), loss: 0.005636, time: 2.249 s\n",
      "Step 202380 (epoch   63.24), loss: 0.005365, time: 2.241 s\n",
      "Step 202400 (epoch   63.25), loss: 0.006263, time: 2.243 s, error: 0.010931\n",
      "Step 202420 (epoch   63.26), loss: 0.006974, time: 15.153 s\n",
      "Step 202440 (epoch   63.26), loss: 0.020592, time: 2.246 s\n",
      "Step 202460 (epoch   63.27), loss: 0.008139, time: 2.233 s\n",
      "Step 202480 (epoch   63.27), loss: 0.005369, time: 2.242 s\n",
      "Step 202500 (epoch   63.28), loss: 0.006931, time: 2.244 s\n",
      "Step 202520 (epoch   63.29), loss: 0.003276, time: 2.234 s\n",
      "Step 202540 (epoch   63.29), loss: 0.003946, time: 2.246 s\n",
      "Step 202560 (epoch   63.30), loss: 0.003159, time: 2.236 s\n",
      "Step 202580 (epoch   63.31), loss: 0.008409, time: 2.253 s\n",
      "Step 202600 (epoch   63.31), loss: 0.004396, time: 2.260 s, error: 0.010547\n",
      "Step 202620 (epoch   63.32), loss: 0.004480, time: 15.177 s\n",
      "Step 202640 (epoch   63.33), loss: 0.003556, time: 2.249 s\n",
      "Step 202660 (epoch   63.33), loss: 0.007563, time: 2.249 s\n",
      "Step 202680 (epoch   63.34), loss: 0.003695, time: 2.249 s\n",
      "Step 202700 (epoch   63.34), loss: 0.003970, time: 2.226 s\n",
      "Step 202720 (epoch   63.35), loss: 0.004078, time: 2.244 s\n",
      "Step 202740 (epoch   63.36), loss: 0.004291, time: 2.251 s\n",
      "Step 202760 (epoch   63.36), loss: 0.005121, time: 2.246 s\n",
      "Step 202780 (epoch   63.37), loss: 0.004501, time: 2.227 s\n",
      "Step 202800 (epoch   63.38), loss: 0.005380, time: 2.245 s, error: 0.010919\n",
      "Step 202820 (epoch   63.38), loss: 0.003623, time: 15.284 s\n",
      "Step 202840 (epoch   63.39), loss: 0.006030, time: 2.231 s\n",
      "Step 202860 (epoch   63.39), loss: 0.004738, time: 2.249 s\n",
      "Step 202880 (epoch   63.40), loss: 0.006212, time: 2.237 s\n",
      "Step 202900 (epoch   63.41), loss: 0.004034, time: 2.238 s\n",
      "Step 202920 (epoch   63.41), loss: 0.005990, time: 2.237 s\n",
      "Step 202940 (epoch   63.42), loss: 0.004741, time: 2.239 s\n",
      "Step 202960 (epoch   63.42), loss: 0.005867, time: 2.238 s\n",
      "Step 202980 (epoch   63.43), loss: 0.008163, time: 2.247 s\n",
      "Step 203000 (epoch   63.44), loss: 0.002563, time: 2.244 s, error: 0.010768\n",
      "Step 203020 (epoch   63.44), loss: 0.004329, time: 15.269 s\n",
      "Step 203040 (epoch   63.45), loss: 0.005072, time: 2.251 s\n",
      "Step 203060 (epoch   63.46), loss: 0.003588, time: 2.234 s\n",
      "Step 203080 (epoch   63.46), loss: 0.005113, time: 2.248 s\n",
      "Step 203100 (epoch   63.47), loss: 0.004172, time: 2.230 s\n",
      "Step 203120 (epoch   63.48), loss: 0.005728, time: 2.243 s\n",
      "Step 203140 (epoch   63.48), loss: 0.004320, time: 2.236 s\n",
      "Step 203160 (epoch   63.49), loss: 0.004313, time: 2.251 s\n",
      "Step 203180 (epoch   63.49), loss: 0.005692, time: 2.238 s\n",
      "Step 203200 (epoch   63.50), loss: 0.004650, time: 2.244 s, error: 0.011520\n",
      "Step 203220 (epoch   63.51), loss: 0.004329, time: 15.123 s\n",
      "Step 203240 (epoch   63.51), loss: 0.003723, time: 2.235 s\n",
      "Step 203260 (epoch   63.52), loss: 0.004051, time: 2.256 s\n",
      "Step 203280 (epoch   63.52), loss: 0.005994, time: 2.249 s\n",
      "Step 203300 (epoch   63.53), loss: 0.004151, time: 2.227 s\n",
      "Step 203320 (epoch   63.54), loss: 0.004635, time: 2.253 s\n",
      "Step 203340 (epoch   63.54), loss: 0.004176, time: 2.240 s\n",
      "Step 203360 (epoch   63.55), loss: 0.006675, time: 2.242 s\n",
      "Step 203380 (epoch   63.56), loss: 0.005478, time: 2.245 s\n",
      "Step 203400 (epoch   63.56), loss: 0.006014, time: 2.251 s, error: 0.011491\n",
      "Step 203420 (epoch   63.57), loss: 0.004991, time: 15.088 s\n",
      "Step 203440 (epoch   63.58), loss: 0.004223, time: 2.238 s\n",
      "Step 203460 (epoch   63.58), loss: 0.003422, time: 2.236 s\n",
      "Step 203480 (epoch   63.59), loss: 0.005589, time: 2.244 s\n",
      "Step 203500 (epoch   63.59), loss: 0.003750, time: 2.252 s\n",
      "Step 203520 (epoch   63.60), loss: 0.004535, time: 2.253 s\n",
      "Step 203540 (epoch   63.61), loss: 0.004740, time: 2.259 s\n",
      "Step 203560 (epoch   63.61), loss: 0.005047, time: 2.244 s\n",
      "Step 203580 (epoch   63.62), loss: 0.005413, time: 2.236 s\n",
      "Step 203600 (epoch   63.62), loss: 0.005018, time: 2.232 s, error: 0.010844\n",
      "Step 203620 (epoch   63.63), loss: 0.006374, time: 15.059 s\n",
      "Step 203640 (epoch   63.64), loss: 0.006732, time: 2.247 s\n",
      "Step 203660 (epoch   63.64), loss: 0.004143, time: 2.237 s\n",
      "Step 203680 (epoch   63.65), loss: 0.003920, time: 2.251 s\n",
      "Step 203700 (epoch   63.66), loss: 0.012003, time: 2.235 s\n",
      "Step 203720 (epoch   63.66), loss: 0.004202, time: 2.241 s\n",
      "Step 203740 (epoch   63.67), loss: 0.005459, time: 2.249 s\n",
      "Step 203760 (epoch   63.67), loss: 0.005330, time: 2.241 s\n",
      "Step 203780 (epoch   63.68), loss: 0.004087, time: 2.250 s\n",
      "Step 203800 (epoch   63.69), loss: 0.004820, time: 2.258 s, error: 0.010657\n",
      "Step 203820 (epoch   63.69), loss: 0.005678, time: 15.063 s\n",
      "Step 203840 (epoch   63.70), loss: 0.006986, time: 2.226 s\n",
      "Step 203860 (epoch   63.71), loss: 0.004232, time: 2.243 s\n",
      "Step 203880 (epoch   63.71), loss: 0.005939, time: 2.243 s\n",
      "Step 203900 (epoch   63.72), loss: 0.005571, time: 2.237 s\n",
      "Step 203920 (epoch   63.73), loss: 0.003848, time: 2.234 s\n",
      "Step 203940 (epoch   63.73), loss: 0.005570, time: 2.248 s\n",
      "Step 203960 (epoch   63.74), loss: 0.004210, time: 2.249 s\n",
      "Step 203980 (epoch   63.74), loss: 0.003673, time: 2.226 s\n",
      "Step 204000 (epoch   63.75), loss: 0.004198, time: 2.242 s, error: 0.010851\n",
      "\n",
      "Time since beginning  : 35757.786 s\n",
      "\n",
      "Step 204020 (epoch   63.76), loss: 0.008066, time: 15.366 s\n",
      "Step 204040 (epoch   63.76), loss: 0.005528, time: 2.238 s\n",
      "Step 204060 (epoch   63.77), loss: 0.003870, time: 2.242 s\n",
      "Step 204080 (epoch   63.77), loss: 0.004070, time: 2.238 s\n",
      "Step 204100 (epoch   63.78), loss: 0.005320, time: 2.232 s\n",
      "Step 204120 (epoch   63.79), loss: 0.005372, time: 2.242 s\n",
      "Step 204140 (epoch   63.79), loss: 0.006505, time: 2.253 s\n",
      "Step 204160 (epoch   63.80), loss: 0.004282, time: 2.240 s\n",
      "Step 204180 (epoch   63.81), loss: 0.003236, time: 2.240 s\n",
      "Step 204200 (epoch   63.81), loss: 0.005376, time: 2.238 s, error: 0.010652\n",
      "Step 204220 (epoch   63.82), loss: 0.003352, time: 15.257 s\n",
      "Step 204240 (epoch   63.83), loss: 0.003598, time: 2.253 s\n",
      "Step 204260 (epoch   63.83), loss: 0.004308, time: 2.245 s\n",
      "Step 204280 (epoch   63.84), loss: 0.005310, time: 2.238 s\n",
      "Step 204300 (epoch   63.84), loss: 0.003651, time: 2.238 s\n",
      "Step 204320 (epoch   63.85), loss: 0.010021, time: 2.250 s\n",
      "Step 204340 (epoch   63.86), loss: 0.002799, time: 2.233 s\n",
      "Step 204360 (epoch   63.86), loss: 0.002914, time: 2.245 s\n",
      "Step 204380 (epoch   63.87), loss: 0.006033, time: 2.245 s\n",
      "Step 204400 (epoch   63.88), loss: 0.004147, time: 2.243 s, error: 0.010493\n",
      "Step 204420 (epoch   63.88), loss: 0.004273, time: 15.029 s\n",
      "Step 204440 (epoch   63.89), loss: 0.002995, time: 2.254 s\n",
      "Step 204460 (epoch   63.89), loss: 0.003799, time: 2.247 s\n",
      "Step 204480 (epoch   63.90), loss: 0.004569, time: 2.231 s\n",
      "Step 204500 (epoch   63.91), loss: 0.004936, time: 2.251 s\n",
      "Step 204520 (epoch   63.91), loss: 0.003943, time: 2.246 s\n",
      "Step 204540 (epoch   63.92), loss: 0.005462, time: 2.235 s\n",
      "Step 204560 (epoch   63.92), loss: 0.003651, time: 2.222 s\n",
      "Step 204580 (epoch   63.93), loss: 0.003857, time: 2.246 s\n",
      "Step 204600 (epoch   63.94), loss: 0.003429, time: 2.229 s, error: 0.010823\n",
      "Step 204620 (epoch   63.94), loss: 0.006249, time: 15.035 s\n",
      "Step 204640 (epoch   63.95), loss: 0.004130, time: 2.252 s\n",
      "Step 204660 (epoch   63.96), loss: 0.004502, time: 2.246 s\n",
      "Step 204680 (epoch   63.96), loss: 0.005549, time: 2.252 s\n",
      "Step 204700 (epoch   63.97), loss: 0.003189, time: 2.245 s\n",
      "Step 204720 (epoch   63.98), loss: 0.004577, time: 2.247 s\n",
      "Step 204740 (epoch   63.98), loss: 0.009096, time: 2.228 s\n",
      "Step 204760 (epoch   63.99), loss: 0.004099, time: 2.247 s\n",
      "Step 204780 (epoch   63.99), loss: 0.005681, time: 2.237 s\n",
      "Step 204800 (epoch   64.00), loss: 0.009226, time: 2.251 s, error: 0.011355\n",
      "Step 204820 (epoch   64.01), loss: 0.003731, time: 15.035 s\n",
      "Step 204840 (epoch   64.01), loss: 0.003581, time: 2.239 s\n",
      "Step 204860 (epoch   64.02), loss: 0.004049, time: 2.255 s\n",
      "Step 204880 (epoch   64.03), loss: 0.006146, time: 2.255 s\n",
      "Step 204900 (epoch   64.03), loss: 0.004069, time: 2.253 s\n",
      "Step 204920 (epoch   64.04), loss: 0.003177, time: 2.252 s\n",
      "Step 204940 (epoch   64.04), loss: 0.004490, time: 2.252 s\n",
      "Step 204960 (epoch   64.05), loss: 0.004066, time: 2.243 s\n",
      "Step 204980 (epoch   64.06), loss: 0.005153, time: 2.247 s\n",
      "Step 205000 (epoch   64.06), loss: 0.003132, time: 2.240 s, error: 0.010633\n",
      "Step 205020 (epoch   64.07), loss: 0.004029, time: 15.085 s\n",
      "Step 205040 (epoch   64.08), loss: 0.004100, time: 2.244 s\n",
      "Step 205060 (epoch   64.08), loss: 0.004402, time: 2.247 s\n",
      "Step 205080 (epoch   64.09), loss: 0.004325, time: 2.239 s\n",
      "Step 205100 (epoch   64.09), loss: 0.005296, time: 2.237 s\n",
      "Step 205120 (epoch   64.10), loss: 0.006505, time: 2.243 s\n",
      "Step 205140 (epoch   64.11), loss: 0.005852, time: 2.239 s\n",
      "Step 205160 (epoch   64.11), loss: 0.002636, time: 2.234 s\n",
      "Step 205180 (epoch   64.12), loss: 0.009470, time: 2.235 s\n",
      "Step 205200 (epoch   64.12), loss: 0.003847, time: 2.232 s, error: 0.010678\n",
      "Step 205220 (epoch   64.13), loss: 0.003266, time: 15.214 s\n",
      "Step 205240 (epoch   64.14), loss: 0.004550, time: 2.238 s\n",
      "Step 205260 (epoch   64.14), loss: 0.004786, time: 2.242 s\n",
      "Step 205280 (epoch   64.15), loss: 0.004570, time: 2.250 s\n",
      "Step 205300 (epoch   64.16), loss: 0.004071, time: 2.240 s\n",
      "Step 205320 (epoch   64.16), loss: 0.004774, time: 2.252 s\n",
      "Step 205340 (epoch   64.17), loss: 0.004866, time: 2.238 s\n",
      "Step 205360 (epoch   64.17), loss: 0.005606, time: 2.255 s\n",
      "Step 205380 (epoch   64.18), loss: 0.004865, time: 2.251 s\n",
      "Step 205400 (epoch   64.19), loss: 0.003077, time: 2.253 s, error: 0.010815\n",
      "Step 205420 (epoch   64.19), loss: 0.004304, time: 15.254 s\n",
      "Step 205440 (epoch   64.20), loss: 0.006831, time: 2.237 s\n",
      "Step 205460 (epoch   64.21), loss: 0.005269, time: 2.241 s\n",
      "Step 205480 (epoch   64.21), loss: 0.005048, time: 2.247 s\n",
      "Step 205500 (epoch   64.22), loss: 0.002738, time: 2.238 s\n",
      "Step 205520 (epoch   64.22), loss: 0.004560, time: 2.233 s\n",
      "Step 205540 (epoch   64.23), loss: 0.006609, time: 2.241 s\n",
      "Step 205560 (epoch   64.24), loss: 0.004930, time: 2.240 s\n",
      "Step 205580 (epoch   64.24), loss: 0.006264, time: 2.254 s\n",
      "Step 205600 (epoch   64.25), loss: 0.005260, time: 2.234 s, error: 0.010859\n",
      "Step 205620 (epoch   64.26), loss: 0.004610, time: 15.178 s\n",
      "Step 205640 (epoch   64.26), loss: 0.004442, time: 2.250 s\n",
      "Step 205660 (epoch   64.27), loss: 0.006337, time: 2.246 s\n",
      "Step 205680 (epoch   64.28), loss: 0.005257, time: 2.240 s\n",
      "Step 205700 (epoch   64.28), loss: 0.004398, time: 2.243 s\n",
      "Step 205720 (epoch   64.29), loss: 0.004085, time: 2.236 s\n",
      "Step 205740 (epoch   64.29), loss: 0.005367, time: 2.248 s\n",
      "Step 205760 (epoch   64.30), loss: 0.001859, time: 2.230 s\n",
      "Step 205780 (epoch   64.31), loss: 0.004749, time: 2.231 s\n",
      "Step 205800 (epoch   64.31), loss: 0.008574, time: 2.235 s, error: 0.010589\n",
      "Step 205820 (epoch   64.32), loss: 0.005113, time: 15.080 s\n",
      "Step 205840 (epoch   64.33), loss: 0.004712, time: 2.231 s\n",
      "Step 205860 (epoch   64.33), loss: 0.005458, time: 2.243 s\n",
      "Step 205880 (epoch   64.34), loss: 0.005860, time: 2.253 s\n",
      "Step 205900 (epoch   64.34), loss: 0.003806, time: 2.256 s\n",
      "Step 205920 (epoch   64.35), loss: 0.003409, time: 2.237 s\n",
      "Step 205940 (epoch   64.36), loss: 0.009078, time: 2.233 s\n",
      "Step 205960 (epoch   64.36), loss: 0.004668, time: 2.235 s\n",
      "Step 205980 (epoch   64.37), loss: 0.006125, time: 2.251 s\n",
      "Step 206000 (epoch   64.38), loss: 0.003801, time: 2.242 s, error: 0.010878\n",
      "\n",
      "Time since beginning  : 36110.890 s\n",
      "\n",
      "Step 206020 (epoch   64.38), loss: 0.008499, time: 15.111 s\n",
      "Step 206040 (epoch   64.39), loss: 0.004666, time: 2.232 s\n",
      "Step 206060 (epoch   64.39), loss: 0.003988, time: 2.240 s\n",
      "Step 206080 (epoch   64.40), loss: 0.004787, time: 2.245 s\n",
      "Step 206100 (epoch   64.41), loss: 0.005490, time: 2.241 s\n",
      "Step 206120 (epoch   64.41), loss: 0.006658, time: 2.228 s\n",
      "Step 206140 (epoch   64.42), loss: 0.005192, time: 2.244 s\n",
      "Step 206160 (epoch   64.42), loss: 0.004317, time: 2.250 s\n",
      "Step 206180 (epoch   64.43), loss: 0.005281, time: 2.242 s\n",
      "Step 206200 (epoch   64.44), loss: 0.003344, time: 2.239 s, error: 0.010776\n",
      "Step 206220 (epoch   64.44), loss: 0.004286, time: 15.063 s\n",
      "Step 206240 (epoch   64.45), loss: 0.007472, time: 2.233 s\n",
      "Step 206260 (epoch   64.46), loss: 0.006808, time: 2.232 s\n",
      "Step 206280 (epoch   64.46), loss: 0.002667, time: 2.239 s\n",
      "Step 206300 (epoch   64.47), loss: 0.004086, time: 2.230 s\n",
      "Step 206320 (epoch   64.47), loss: 0.004485, time: 2.247 s\n",
      "Step 206340 (epoch   64.48), loss: 0.005417, time: 2.225 s\n",
      "Step 206360 (epoch   64.49), loss: 0.005184, time: 2.232 s\n",
      "Step 206380 (epoch   64.49), loss: 0.004169, time: 2.226 s\n",
      "Step 206400 (epoch   64.50), loss: 0.004636, time: 2.252 s, error: 0.011733\n",
      "Step 206420 (epoch   64.51), loss: 0.004922, time: 15.197 s\n",
      "Step 206440 (epoch   64.51), loss: 0.004643, time: 2.240 s\n",
      "Step 206460 (epoch   64.52), loss: 0.003828, time: 2.248 s\n",
      "Step 206480 (epoch   64.53), loss: 0.003433, time: 2.242 s\n",
      "Step 206500 (epoch   64.53), loss: 0.004455, time: 2.242 s\n",
      "Step 206520 (epoch   64.54), loss: 0.002786, time: 2.233 s\n",
      "Step 206540 (epoch   64.54), loss: 0.003938, time: 2.246 s\n",
      "Step 206560 (epoch   64.55), loss: 0.003616, time: 2.242 s\n",
      "Step 206580 (epoch   64.56), loss: 0.003022, time: 2.242 s\n",
      "Step 206600 (epoch   64.56), loss: 0.004199, time: 2.234 s, error: 0.011719\n",
      "Step 206620 (epoch   64.57), loss: 0.004208, time: 15.267 s\n",
      "Step 206640 (epoch   64.58), loss: 0.005391, time: 2.237 s\n",
      "Step 206660 (epoch   64.58), loss: 0.003387, time: 2.229 s\n",
      "Step 206680 (epoch   64.59), loss: 0.003981, time: 2.241 s\n",
      "Step 206700 (epoch   64.59), loss: 0.004121, time: 2.236 s\n",
      "Step 206720 (epoch   64.60), loss: 0.002751, time: 2.242 s\n",
      "Step 206740 (epoch   64.61), loss: 0.007047, time: 2.239 s\n",
      "Step 206760 (epoch   64.61), loss: 0.005652, time: 2.240 s\n",
      "Step 206780 (epoch   64.62), loss: 0.005256, time: 2.239 s\n",
      "Step 206800 (epoch   64.62), loss: 0.005797, time: 2.240 s, error: 0.010893\n",
      "Step 206820 (epoch   64.63), loss: 0.004202, time: 15.124 s\n",
      "Step 206840 (epoch   64.64), loss: 0.003228, time: 2.231 s\n",
      "Step 206860 (epoch   64.64), loss: 0.013016, time: 2.233 s\n",
      "Step 206880 (epoch   64.65), loss: 0.004242, time: 2.237 s\n",
      "Step 206900 (epoch   64.66), loss: 0.004587, time: 2.235 s\n",
      "Step 206920 (epoch   64.66), loss: 0.004180, time: 2.233 s\n",
      "Step 206940 (epoch   64.67), loss: 0.003576, time: 2.249 s\n",
      "Step 206960 (epoch   64.67), loss: 0.004555, time: 2.232 s\n",
      "Step 206980 (epoch   64.68), loss: 0.004310, time: 2.251 s\n",
      "Step 207000 (epoch   64.69), loss: 0.004700, time: 2.236 s, error: 0.010675\n",
      "Step 207020 (epoch   64.69), loss: 0.004265, time: 15.077 s\n",
      "Step 207040 (epoch   64.70), loss: 0.005939, time: 2.252 s\n",
      "Step 207060 (epoch   64.71), loss: 0.004386, time: 2.247 s\n",
      "Step 207080 (epoch   64.71), loss: 0.004721, time: 2.253 s\n",
      "Step 207100 (epoch   64.72), loss: 0.004233, time: 2.236 s\n",
      "Step 207120 (epoch   64.72), loss: 0.003666, time: 2.254 s\n",
      "Step 207140 (epoch   64.73), loss: 0.004666, time: 2.243 s\n",
      "Step 207160 (epoch   64.74), loss: 0.003242, time: 2.241 s\n",
      "Step 207180 (epoch   64.74), loss: 0.004995, time: 2.248 s\n",
      "Step 207200 (epoch   64.75), loss: 0.006927, time: 2.251 s, error: 0.010796\n",
      "Step 207220 (epoch   64.76), loss: 0.004329, time: 15.036 s\n",
      "Step 207240 (epoch   64.76), loss: 0.004799, time: 2.237 s\n",
      "Step 207260 (epoch   64.77), loss: 0.005204, time: 2.229 s\n",
      "Step 207280 (epoch   64.78), loss: 0.006876, time: 2.223 s\n",
      "Step 207300 (epoch   64.78), loss: 0.006410, time: 2.234 s\n",
      "Step 207320 (epoch   64.79), loss: 0.003570, time: 2.246 s\n",
      "Step 207340 (epoch   64.79), loss: 0.006311, time: 2.255 s\n",
      "Step 207360 (epoch   64.80), loss: 0.003020, time: 2.237 s\n",
      "Step 207380 (epoch   64.81), loss: 0.003434, time: 2.246 s\n",
      "Step 207400 (epoch   64.81), loss: 0.006018, time: 2.228 s, error: 0.010596\n",
      "Step 207420 (epoch   64.82), loss: 0.004352, time: 15.167 s\n",
      "Step 207440 (epoch   64.83), loss: 0.004858, time: 2.249 s\n",
      "Step 207460 (epoch   64.83), loss: 0.002950, time: 2.237 s\n",
      "Step 207480 (epoch   64.84), loss: 0.003748, time: 2.241 s\n",
      "Step 207500 (epoch   64.84), loss: 0.006580, time: 2.233 s\n",
      "Step 207520 (epoch   64.85), loss: 0.008621, time: 2.241 s\n",
      "Step 207540 (epoch   64.86), loss: 0.004632, time: 2.237 s\n",
      "Step 207560 (epoch   64.86), loss: 0.004462, time: 2.245 s\n",
      "Step 207580 (epoch   64.87), loss: 0.002606, time: 2.242 s\n",
      "Step 207600 (epoch   64.88), loss: 0.003645, time: 2.257 s, error: 0.010491\n",
      "Step 207620 (epoch   64.88), loss: 0.003181, time: 15.038 s\n",
      "Step 207640 (epoch   64.89), loss: 0.005280, time: 2.235 s\n",
      "Step 207660 (epoch   64.89), loss: 0.005628, time: 2.242 s\n",
      "Step 207680 (epoch   64.90), loss: 0.004162, time: 2.240 s\n",
      "Step 207700 (epoch   64.91), loss: 0.007806, time: 2.228 s\n",
      "Step 207720 (epoch   64.91), loss: 0.003557, time: 2.232 s\n",
      "Step 207740 (epoch   64.92), loss: 0.003411, time: 2.244 s\n",
      "Step 207760 (epoch   64.92), loss: 0.005388, time: 2.236 s\n",
      "Step 207780 (epoch   64.93), loss: 0.011630, time: 2.241 s\n",
      "Step 207800 (epoch   64.94), loss: 0.004655, time: 2.246 s, error: 0.010839\n",
      "Step 207820 (epoch   64.94), loss: 0.003700, time: 15.263 s\n",
      "Step 207840 (epoch   64.95), loss: 0.004193, time: 2.243 s\n",
      "Step 207860 (epoch   64.96), loss: 0.006261, time: 2.239 s\n",
      "Step 207880 (epoch   64.96), loss: 0.007119, time: 2.241 s\n",
      "Step 207900 (epoch   64.97), loss: 0.004640, time: 2.242 s\n",
      "Step 207920 (epoch   64.97), loss: 0.004906, time: 2.236 s\n",
      "Step 207940 (epoch   64.98), loss: 0.004066, time: 2.242 s\n",
      "Step 207960 (epoch   64.99), loss: 0.004525, time: 2.235 s\n",
      "Step 207980 (epoch   64.99), loss: 0.003570, time: 2.248 s\n",
      "Step 208000 (epoch   65.00), loss: 0.004613, time: 2.246 s, error: 0.010865\n",
      "\n",
      "Time since beginning  : 36464.053 s\n",
      "\n",
      "Step 208020 (epoch   65.01), loss: 0.005229, time: 15.356 s\n",
      "Step 208040 (epoch   65.01), loss: 0.005264, time: 2.253 s\n",
      "Step 208060 (epoch   65.02), loss: 0.003460, time: 2.243 s\n",
      "Step 208080 (epoch   65.03), loss: 0.008116, time: 2.239 s\n",
      "Step 208100 (epoch   65.03), loss: 0.003203, time: 2.253 s\n",
      "Step 208120 (epoch   65.04), loss: 0.004961, time: 2.256 s\n",
      "Step 208140 (epoch   65.04), loss: 0.005175, time: 2.246 s\n",
      "Step 208160 (epoch   65.05), loss: 0.004764, time: 2.239 s\n",
      "Step 208180 (epoch   65.06), loss: 0.003363, time: 2.239 s\n",
      "Step 208200 (epoch   65.06), loss: 0.005207, time: 2.248 s, error: 0.010690\n",
      "Step 208220 (epoch   65.07), loss: 0.004181, time: 15.016 s\n",
      "Step 208240 (epoch   65.08), loss: 0.005347, time: 2.242 s\n",
      "Step 208260 (epoch   65.08), loss: 0.004703, time: 2.260 s\n",
      "Step 208280 (epoch   65.09), loss: 0.002070, time: 2.235 s\n",
      "Step 208300 (epoch   65.09), loss: 0.004903, time: 2.239 s\n",
      "Step 208320 (epoch   65.10), loss: 0.003695, time: 2.231 s\n",
      "Step 208340 (epoch   65.11), loss: 0.003788, time: 2.238 s\n",
      "Step 208360 (epoch   65.11), loss: 0.005554, time: 2.225 s\n",
      "Step 208380 (epoch   65.12), loss: 0.007311, time: 2.238 s\n",
      "Step 208400 (epoch   65.12), loss: 0.005141, time: 2.241 s, error: 0.010654\n",
      "Step 208420 (epoch   65.13), loss: 0.005099, time: 15.029 s\n",
      "Step 208440 (epoch   65.14), loss: 0.003758, time: 2.233 s\n",
      "Step 208460 (epoch   65.14), loss: 0.010373, time: 2.236 s\n",
      "Step 208480 (epoch   65.15), loss: 0.005216, time: 2.242 s\n",
      "Step 208500 (epoch   65.16), loss: 0.008746, time: 2.245 s\n",
      "Step 208520 (epoch   65.16), loss: 0.004414, time: 2.260 s\n",
      "Step 208540 (epoch   65.17), loss: 0.003497, time: 2.233 s\n",
      "Step 208560 (epoch   65.17), loss: 0.003920, time: 2.249 s\n",
      "Step 208580 (epoch   65.18), loss: 0.006459, time: 2.236 s\n",
      "Step 208600 (epoch   65.19), loss: 0.004219, time: 2.229 s, error: 0.010699\n",
      "Step 208620 (epoch   65.19), loss: 0.004529, time: 15.032 s\n",
      "Step 208640 (epoch   65.20), loss: 0.003701, time: 2.230 s\n",
      "Step 208660 (epoch   65.21), loss: 0.005448, time: 2.247 s\n",
      "Step 208680 (epoch   65.21), loss: 0.005552, time: 2.244 s\n",
      "Step 208700 (epoch   65.22), loss: 0.003589, time: 2.235 s\n",
      "Step 208720 (epoch   65.22), loss: 0.004577, time: 2.237 s\n",
      "Step 208740 (epoch   65.23), loss: 0.001937, time: 2.229 s\n",
      "Step 208760 (epoch   65.24), loss: 0.005003, time: 2.233 s\n",
      "Step 208780 (epoch   65.24), loss: 0.004667, time: 2.260 s\n",
      "Step 208800 (epoch   65.25), loss: 0.007632, time: 2.227 s, error: 0.010587\n",
      "Step 208820 (epoch   65.26), loss: 0.002854, time: 15.084 s\n",
      "Step 208840 (epoch   65.26), loss: 0.004815, time: 2.241 s\n",
      "Step 208860 (epoch   65.27), loss: 0.003375, time: 2.254 s\n",
      "Step 208880 (epoch   65.28), loss: 0.003530, time: 2.250 s\n",
      "Step 208900 (epoch   65.28), loss: 0.003965, time: 2.228 s\n",
      "Step 208920 (epoch   65.29), loss: 0.004325, time: 2.232 s\n",
      "Step 208940 (epoch   65.29), loss: 0.004494, time: 2.246 s\n",
      "Step 208960 (epoch   65.30), loss: 0.007229, time: 2.241 s\n",
      "Step 208980 (epoch   65.31), loss: 0.005502, time: 2.237 s\n",
      "Step 209000 (epoch   65.31), loss: 0.005514, time: 2.238 s, error: 0.010649\n",
      "Step 209020 (epoch   65.32), loss: 0.006276, time: 15.356 s\n",
      "Step 209040 (epoch   65.33), loss: 0.004587, time: 2.252 s\n",
      "Step 209060 (epoch   65.33), loss: 0.002481, time: 2.241 s\n",
      "Step 209080 (epoch   65.34), loss: 0.004831, time: 2.244 s\n",
      "Step 209100 (epoch   65.34), loss: 0.005797, time: 2.233 s\n",
      "Step 209120 (epoch   65.35), loss: 0.003712, time: 2.228 s\n",
      "Step 209140 (epoch   65.36), loss: 0.007102, time: 2.240 s\n",
      "Step 209160 (epoch   65.36), loss: 0.004351, time: 2.242 s\n",
      "Step 209180 (epoch   65.37), loss: 0.004730, time: 2.242 s\n",
      "Step 209200 (epoch   65.38), loss: 0.005666, time: 2.246 s, error: 0.010805\n",
      "Step 209220 (epoch   65.38), loss: 0.003454, time: 15.245 s\n",
      "Step 209240 (epoch   65.39), loss: 0.004378, time: 2.242 s\n",
      "Step 209260 (epoch   65.39), loss: 0.007203, time: 2.247 s\n",
      "Step 209280 (epoch   65.40), loss: 0.007208, time: 2.239 s\n",
      "Step 209300 (epoch   65.41), loss: 0.006253, time: 2.244 s\n",
      "Step 209320 (epoch   65.41), loss: 0.003695, time: 2.236 s\n",
      "Step 209340 (epoch   65.42), loss: 0.003164, time: 2.241 s\n",
      "Step 209360 (epoch   65.42), loss: 0.004523, time: 2.234 s\n",
      "Step 209380 (epoch   65.43), loss: 0.003276, time: 2.244 s\n",
      "Step 209400 (epoch   65.44), loss: 0.003947, time: 2.230 s, error: 0.010765\n",
      "Step 209420 (epoch   65.44), loss: 0.004315, time: 15.078 s\n",
      "Step 209440 (epoch   65.45), loss: 0.006107, time: 2.262 s\n",
      "Step 209460 (epoch   65.46), loss: 0.003840, time: 2.224 s\n",
      "Step 209480 (epoch   65.46), loss: 0.004434, time: 2.235 s\n",
      "Step 209500 (epoch   65.47), loss: 0.004872, time: 2.229 s\n",
      "Step 209520 (epoch   65.47), loss: 0.003844, time: 2.245 s\n",
      "Step 209540 (epoch   65.48), loss: 0.003540, time: 2.239 s\n",
      "Step 209560 (epoch   65.49), loss: 0.003793, time: 2.242 s\n",
      "Step 209580 (epoch   65.49), loss: 0.006748, time: 2.235 s\n",
      "Step 209600 (epoch   65.50), loss: 0.004414, time: 2.232 s, error: 0.011723\n",
      "Step 209620 (epoch   65.51), loss: 0.005102, time: 15.034 s\n",
      "Step 209640 (epoch   65.51), loss: 0.005087, time: 2.246 s\n",
      "Step 209660 (epoch   65.52), loss: 0.003221, time: 2.243 s\n",
      "Step 209680 (epoch   65.53), loss: 0.003762, time: 2.255 s\n",
      "Step 209700 (epoch   65.53), loss: 0.003846, time: 2.255 s\n",
      "Step 209720 (epoch   65.54), loss: 0.003860, time: 2.232 s\n",
      "Step 209740 (epoch   65.54), loss: 0.002838, time: 2.254 s\n",
      "Step 209760 (epoch   65.55), loss: 0.005168, time: 2.238 s\n",
      "Step 209780 (epoch   65.56), loss: 0.005949, time: 2.245 s\n",
      "Step 209800 (epoch   65.56), loss: 0.007803, time: 2.238 s, error: 0.011649\n",
      "Step 209820 (epoch   65.57), loss: 0.004745, time: 15.043 s\n",
      "Step 209840 (epoch   65.58), loss: 0.004573, time: 2.234 s\n",
      "Step 209860 (epoch   65.58), loss: 0.003309, time: 2.239 s\n",
      "Step 209880 (epoch   65.59), loss: 0.004060, time: 2.243 s\n",
      "Step 209900 (epoch   65.59), loss: 0.005300, time: 2.221 s\n",
      "Step 209920 (epoch   65.60), loss: 0.004614, time: 2.243 s\n",
      "Step 209940 (epoch   65.61), loss: 0.003972, time: 2.235 s\n",
      "Step 209960 (epoch   65.61), loss: 0.005077, time: 2.262 s\n",
      "Step 209980 (epoch   65.62), loss: 0.006817, time: 2.235 s\n",
      "Step 210000 (epoch   65.62), loss: 0.004415, time: 2.247 s, error: 0.010972\n",
      "\n",
      "Time since beginning  : 36816.916 s\n",
      "\n",
      "Step 210020 (epoch   65.63), loss: 0.004891, time: 15.303 s\n",
      "Step 210040 (epoch   65.64), loss: 0.003745, time: 2.246 s\n",
      "Step 210060 (epoch   65.64), loss: 0.004195, time: 2.242 s\n",
      "Step 210080 (epoch   65.65), loss: 0.003656, time: 2.242 s\n",
      "Step 210100 (epoch   65.66), loss: 0.003730, time: 2.254 s\n",
      "Step 210120 (epoch   65.66), loss: 0.002595, time: 2.255 s\n",
      "Step 210140 (epoch   65.67), loss: 0.004156, time: 2.245 s\n",
      "Step 210160 (epoch   65.67), loss: 0.006857, time: 2.243 s\n",
      "Step 210180 (epoch   65.68), loss: 0.004227, time: 2.252 s\n",
      "Step 210200 (epoch   65.69), loss: 0.004620, time: 2.235 s, error: 0.010591\n",
      "Step 210220 (epoch   65.69), loss: 0.004708, time: 15.209 s\n",
      "Step 210240 (epoch   65.70), loss: 0.004497, time: 2.238 s\n",
      "Step 210260 (epoch   65.71), loss: 0.004790, time: 2.227 s\n",
      "Step 210280 (epoch   65.71), loss: 0.003514, time: 2.252 s\n",
      "Step 210300 (epoch   65.72), loss: 0.004250, time: 2.240 s\n",
      "Step 210320 (epoch   65.72), loss: 0.003984, time: 2.240 s\n",
      "Step 210340 (epoch   65.73), loss: 0.003676, time: 2.239 s\n",
      "Step 210360 (epoch   65.74), loss: 0.003833, time: 2.255 s\n",
      "Step 210380 (epoch   65.74), loss: 0.003241, time: 2.235 s\n",
      "Step 210400 (epoch   65.75), loss: 0.003439, time: 2.242 s, error: 0.010757\n",
      "Step 210420 (epoch   65.76), loss: 0.003399, time: 15.276 s\n",
      "Step 210440 (epoch   65.76), loss: 0.003939, time: 2.246 s\n",
      "Step 210460 (epoch   65.77), loss: 0.004145, time: 2.248 s\n",
      "Step 210480 (epoch   65.78), loss: 0.005514, time: 2.251 s\n",
      "Step 210500 (epoch   65.78), loss: 0.003484, time: 2.247 s\n",
      "Step 210520 (epoch   65.79), loss: 0.003542, time: 2.236 s\n",
      "Step 210540 (epoch   65.79), loss: 0.007444, time: 2.237 s\n",
      "Step 210560 (epoch   65.80), loss: 0.004212, time: 2.237 s\n",
      "Step 210580 (epoch   65.81), loss: 0.003928, time: 2.226 s\n",
      "Step 210600 (epoch   65.81), loss: 0.002318, time: 2.245 s, error: 0.010567\n",
      "Step 210620 (epoch   65.82), loss: 0.004581, time: 15.127 s\n",
      "Step 210640 (epoch   65.83), loss: 0.007811, time: 2.230 s\n",
      "Step 210660 (epoch   65.83), loss: 0.003034, time: 2.239 s\n",
      "Step 210680 (epoch   65.84), loss: 0.005050, time: 2.237 s\n",
      "Step 210700 (epoch   65.84), loss: 0.005773, time: 2.244 s\n",
      "Step 210720 (epoch   65.85), loss: 0.004461, time: 2.239 s\n",
      "Step 210740 (epoch   65.86), loss: 0.005934, time: 2.239 s\n",
      "Step 210760 (epoch   65.86), loss: 0.004028, time: 2.250 s\n",
      "Step 210780 (epoch   65.87), loss: 0.003893, time: 2.238 s\n",
      "Step 210800 (epoch   65.88), loss: 0.004132, time: 2.242 s, error: 0.010550\n",
      "Step 210820 (epoch   65.88), loss: 0.004274, time: 15.056 s\n",
      "Step 210840 (epoch   65.89), loss: 0.006620, time: 2.229 s\n",
      "Step 210860 (epoch   65.89), loss: 0.005006, time: 2.249 s\n",
      "Step 210880 (epoch   65.90), loss: 0.003972, time: 2.255 s\n",
      "Step 210900 (epoch   65.91), loss: 0.005178, time: 2.238 s\n",
      "Step 210920 (epoch   65.91), loss: 0.003856, time: 2.249 s\n",
      "Step 210940 (epoch   65.92), loss: 0.003968, time: 2.253 s\n",
      "Step 210960 (epoch   65.92), loss: 0.003323, time: 2.241 s\n",
      "Step 210980 (epoch   65.93), loss: 0.003673, time: 2.252 s\n",
      "Step 211000 (epoch   65.94), loss: 0.004127, time: 2.234 s, error: 0.010856\n",
      "Step 211020 (epoch   65.94), loss: 0.005352, time: 15.088 s\n",
      "Step 211040 (epoch   65.95), loss: 0.005155, time: 2.251 s\n",
      "Step 211060 (epoch   65.96), loss: 0.005202, time: 2.250 s\n",
      "Step 211080 (epoch   65.96), loss: 0.007194, time: 2.250 s\n",
      "Step 211100 (epoch   65.97), loss: 0.003825, time: 2.237 s\n",
      "Step 211120 (epoch   65.97), loss: 0.003974, time: 2.231 s\n",
      "Step 211140 (epoch   65.98), loss: 0.004944, time: 2.260 s\n",
      "Step 211160 (epoch   65.99), loss: 0.004353, time: 2.227 s\n",
      "Step 211180 (epoch   65.99), loss: 0.005145, time: 2.237 s\n",
      "Step 211200 (epoch   66.00), loss: 0.004764, time: 2.237 s, error: 0.010726\n",
      "Step 211220 (epoch   66.01), loss: 0.004700, time: 15.062 s\n",
      "Step 211240 (epoch   66.01), loss: 0.004222, time: 2.249 s\n",
      "Step 211260 (epoch   66.02), loss: 0.003521, time: 2.234 s\n",
      "Step 211280 (epoch   66.03), loss: 0.004400, time: 2.241 s\n",
      "Step 211300 (epoch   66.03), loss: 0.002974, time: 2.237 s\n",
      "Step 211320 (epoch   66.04), loss: 0.005753, time: 2.240 s\n",
      "Step 211340 (epoch   66.04), loss: 0.005295, time: 2.235 s\n",
      "Step 211360 (epoch   66.05), loss: 0.006908, time: 2.249 s\n",
      "Step 211380 (epoch   66.06), loss: 0.003829, time: 2.230 s\n",
      "Step 211400 (epoch   66.06), loss: 0.008169, time: 2.261 s, error: 0.010817\n",
      "Step 211420 (epoch   66.07), loss: 0.003445, time: 15.079 s\n",
      "Step 211440 (epoch   66.08), loss: 0.005354, time: 2.237 s\n",
      "Step 211460 (epoch   66.08), loss: 0.008077, time: 2.239 s\n",
      "Step 211480 (epoch   66.09), loss: 0.003496, time: 2.252 s\n",
      "Step 211500 (epoch   66.09), loss: 0.003584, time: 2.246 s\n",
      "Step 211520 (epoch   66.10), loss: 0.003030, time: 2.246 s\n",
      "Step 211540 (epoch   66.11), loss: 0.003838, time: 2.236 s\n",
      "Step 211560 (epoch   66.11), loss: 0.004255, time: 2.241 s\n",
      "Step 211580 (epoch   66.12), loss: 0.004093, time: 2.242 s\n",
      "Step 211600 (epoch   66.12), loss: 0.005953, time: 2.248 s, error: 0.010803\n",
      "Step 211620 (epoch   66.13), loss: 0.003823, time: 15.257 s\n",
      "Step 211640 (epoch   66.14), loss: 0.004495, time: 2.245 s\n",
      "Step 211660 (epoch   66.14), loss: 0.004783, time: 2.249 s\n",
      "Step 211680 (epoch   66.15), loss: 0.005737, time: 2.244 s\n",
      "Step 211700 (epoch   66.16), loss: 0.003752, time: 2.251 s\n",
      "Step 211720 (epoch   66.16), loss: 0.004988, time: 2.238 s\n",
      "Step 211740 (epoch   66.17), loss: 0.005811, time: 2.242 s\n",
      "Step 211760 (epoch   66.17), loss: 0.004035, time: 2.225 s\n",
      "Step 211780 (epoch   66.18), loss: 0.004738, time: 2.248 s\n",
      "Step 211800 (epoch   66.19), loss: 0.005094, time: 2.244 s, error: 0.010624\n",
      "Step 211820 (epoch   66.19), loss: 0.004800, time: 15.236 s\n",
      "Step 211840 (epoch   66.20), loss: 0.002925, time: 2.244 s\n",
      "Step 211860 (epoch   66.21), loss: 0.005196, time: 2.241 s\n",
      "Step 211880 (epoch   66.21), loss: 0.005618, time: 2.240 s\n",
      "Step 211900 (epoch   66.22), loss: 0.004289, time: 2.247 s\n",
      "Step 211920 (epoch   66.22), loss: 0.008478, time: 2.233 s\n",
      "Step 211940 (epoch   66.23), loss: 0.005049, time: 2.239 s\n",
      "Step 211960 (epoch   66.24), loss: 0.005388, time: 2.238 s\n",
      "Step 211980 (epoch   66.24), loss: 0.004544, time: 2.244 s\n",
      "Step 212000 (epoch   66.25), loss: 0.005629, time: 2.232 s, error: 0.010581\n",
      "\n",
      "Time since beginning  : 37170.247 s\n",
      "\n",
      "Step 212020 (epoch   66.26), loss: 0.005272, time: 15.104 s\n",
      "Step 212040 (epoch   66.26), loss: 0.004558, time: 2.246 s\n",
      "Step 212060 (epoch   66.27), loss: 0.005634, time: 2.254 s\n",
      "Step 212080 (epoch   66.28), loss: 0.005982, time: 2.253 s\n",
      "Step 212100 (epoch   66.28), loss: 0.003625, time: 2.242 s\n",
      "Step 212120 (epoch   66.29), loss: 0.004217, time: 2.240 s\n",
      "Step 212140 (epoch   66.29), loss: 0.003905, time: 2.239 s\n",
      "Step 212160 (epoch   66.30), loss: 0.003609, time: 2.243 s\n",
      "Step 212180 (epoch   66.31), loss: 0.003581, time: 2.231 s\n",
      "Step 212200 (epoch   66.31), loss: 0.003575, time: 2.246 s, error: 0.010622\n",
      "Step 212220 (epoch   66.32), loss: 0.004474, time: 15.048 s\n",
      "Step 212240 (epoch   66.33), loss: 0.002911, time: 2.227 s\n",
      "Step 212260 (epoch   66.33), loss: 0.003508, time: 2.246 s\n",
      "Step 212280 (epoch   66.34), loss: 0.006494, time: 2.236 s\n",
      "Step 212300 (epoch   66.34), loss: 0.003297, time: 2.243 s\n",
      "Step 212320 (epoch   66.35), loss: 0.008651, time: 2.248 s\n",
      "Step 212340 (epoch   66.36), loss: 0.005301, time: 2.242 s\n",
      "Step 212360 (epoch   66.36), loss: 0.004765, time: 2.240 s\n",
      "Step 212380 (epoch   66.37), loss: 0.005366, time: 2.236 s\n",
      "Step 212400 (epoch   66.38), loss: 0.004423, time: 2.234 s, error: 0.010781\n",
      "Step 212420 (epoch   66.38), loss: 0.006113, time: 15.045 s\n",
      "Step 212440 (epoch   66.39), loss: 0.004051, time: 2.242 s\n",
      "Step 212460 (epoch   66.39), loss: 0.003448, time: 2.231 s\n",
      "Step 212480 (epoch   66.40), loss: 0.004189, time: 2.241 s\n",
      "Step 212500 (epoch   66.41), loss: 0.003357, time: 2.228 s\n",
      "Step 212520 (epoch   66.41), loss: 0.005560, time: 2.246 s\n",
      "Step 212540 (epoch   66.42), loss: 0.003861, time: 2.248 s\n",
      "Step 212560 (epoch   66.42), loss: 0.002646, time: 2.245 s\n",
      "Step 212580 (epoch   66.43), loss: 0.005228, time: 2.261 s\n",
      "Step 212600 (epoch   66.44), loss: 0.005349, time: 2.242 s, error: 0.010850\n",
      "Step 212620 (epoch   66.44), loss: 0.013794, time: 15.024 s\n",
      "Step 212640 (epoch   66.45), loss: 0.008508, time: 2.228 s\n",
      "Step 212660 (epoch   66.46), loss: 0.003690, time: 2.239 s\n",
      "Step 212680 (epoch   66.46), loss: 0.005050, time: 2.229 s\n",
      "Step 212700 (epoch   66.47), loss: 0.004388, time: 2.243 s\n",
      "Step 212720 (epoch   66.47), loss: 0.004236, time: 2.248 s\n",
      "Step 212740 (epoch   66.48), loss: 0.002687, time: 2.241 s\n",
      "Step 212760 (epoch   66.49), loss: 0.004651, time: 2.242 s\n",
      "Step 212780 (epoch   66.49), loss: 0.005285, time: 2.231 s\n",
      "Step 212800 (epoch   66.50), loss: 0.004675, time: 2.247 s, error: 0.011357\n",
      "Step 212820 (epoch   66.51), loss: 0.004213, time: 15.236 s\n",
      "Step 212840 (epoch   66.51), loss: 0.003276, time: 2.245 s\n",
      "Step 212860 (epoch   66.52), loss: 0.003763, time: 2.253 s\n",
      "Step 212880 (epoch   66.53), loss: 0.005832, time: 2.238 s\n",
      "Step 212900 (epoch   66.53), loss: 0.004959, time: 2.240 s\n",
      "Step 212920 (epoch   66.54), loss: 0.003619, time: 2.251 s\n",
      "Step 212940 (epoch   66.54), loss: 0.004869, time: 2.247 s\n",
      "Step 212960 (epoch   66.55), loss: 0.003791, time: 2.237 s\n",
      "Step 212980 (epoch   66.56), loss: 0.003930, time: 2.236 s\n",
      "Step 213000 (epoch   66.56), loss: 0.005318, time: 2.225 s, error: 0.011571\n",
      "Step 213020 (epoch   66.57), loss: 0.008651, time: 15.272 s\n",
      "Step 213040 (epoch   66.58), loss: 0.010333, time: 2.235 s\n",
      "Step 213060 (epoch   66.58), loss: 0.004499, time: 2.234 s\n",
      "Step 213080 (epoch   66.59), loss: 0.004277, time: 2.242 s\n",
      "Step 213100 (epoch   66.59), loss: 0.006078, time: 2.232 s\n",
      "Step 213120 (epoch   66.60), loss: 0.005872, time: 2.229 s\n",
      "Step 213140 (epoch   66.61), loss: 0.005236, time: 2.228 s\n",
      "Step 213160 (epoch   66.61), loss: 0.006049, time: 2.240 s\n",
      "Step 213180 (epoch   66.62), loss: 0.002979, time: 2.247 s\n",
      "Step 213200 (epoch   66.62), loss: 0.004570, time: 2.243 s, error: 0.010882\n",
      "Step 213220 (epoch   66.63), loss: 0.004337, time: 15.080 s\n",
      "Step 213240 (epoch   66.64), loss: 0.007084, time: 2.260 s\n",
      "Step 213260 (epoch   66.64), loss: 0.003615, time: 2.239 s\n",
      "Step 213280 (epoch   66.65), loss: 0.003440, time: 2.251 s\n",
      "Step 213300 (epoch   66.66), loss: 0.005068, time: 2.236 s\n",
      "Step 213320 (epoch   66.66), loss: 0.003854, time: 2.236 s\n",
      "Step 213340 (epoch   66.67), loss: 0.005926, time: 2.235 s\n",
      "Step 213360 (epoch   66.67), loss: 0.004440, time: 2.245 s\n",
      "Step 213380 (epoch   66.68), loss: 0.004170, time: 2.243 s\n",
      "Step 213400 (epoch   66.69), loss: 0.003984, time: 2.239 s, error: 0.010548\n",
      "Step 213420 (epoch   66.69), loss: 0.005466, time: 15.039 s\n",
      "Step 213440 (epoch   66.70), loss: 0.003673, time: 2.257 s\n",
      "Step 213460 (epoch   66.71), loss: 0.005204, time: 2.248 s\n",
      "Step 213480 (epoch   66.71), loss: 0.004672, time: 2.252 s\n",
      "Step 213500 (epoch   66.72), loss: 0.004528, time: 2.248 s\n",
      "Step 213520 (epoch   66.72), loss: 0.005170, time: 2.236 s\n",
      "Step 213540 (epoch   66.73), loss: 0.004459, time: 2.247 s\n",
      "Step 213560 (epoch   66.74), loss: 0.003806, time: 2.248 s\n",
      "Step 213580 (epoch   66.74), loss: 0.003143, time: 2.238 s\n",
      "Step 213600 (epoch   66.75), loss: 0.002897, time: 2.248 s, error: 0.010796\n",
      "Step 213620 (epoch   66.76), loss: 0.006418, time: 15.081 s\n",
      "Step 213640 (epoch   66.76), loss: 0.008279, time: 2.232 s\n",
      "Step 213660 (epoch   66.77), loss: 0.004114, time: 2.248 s\n",
      "Step 213680 (epoch   66.78), loss: 0.004660, time: 2.235 s\n",
      "Step 213700 (epoch   66.78), loss: 0.004222, time: 2.249 s\n",
      "Step 213720 (epoch   66.79), loss: 0.002386, time: 2.230 s\n",
      "Step 213740 (epoch   66.79), loss: 0.005511, time: 2.246 s\n",
      "Step 213760 (epoch   66.80), loss: 0.007232, time: 2.248 s\n",
      "Step 213780 (epoch   66.81), loss: 0.003112, time: 2.237 s\n",
      "Step 213800 (epoch   66.81), loss: 0.004344, time: 2.231 s, error: 0.010549\n",
      "Step 213820 (epoch   66.82), loss: 0.004995, time: 15.072 s\n",
      "Step 213840 (epoch   66.83), loss: 0.007341, time: 2.231 s\n",
      "Step 213860 (epoch   66.83), loss: 0.006329, time: 2.236 s\n",
      "Step 213880 (epoch   66.84), loss: 0.003694, time: 2.227 s\n",
      "Step 213900 (epoch   66.84), loss: 0.007648, time: 2.242 s\n",
      "Step 213920 (epoch   66.85), loss: 0.006036, time: 2.232 s\n",
      "Step 213940 (epoch   66.86), loss: 0.003240, time: 2.236 s\n",
      "Step 213960 (epoch   66.86), loss: 0.003656, time: 2.223 s\n",
      "Step 213980 (epoch   66.87), loss: 0.002710, time: 2.242 s\n",
      "Step 214000 (epoch   66.88), loss: 0.003458, time: 2.242 s, error: 0.010751\n",
      "\n",
      "Time since beginning  : 37523.167 s\n",
      "\n",
      "Step 214020 (epoch   66.88), loss: 0.005056, time: 15.383 s\n",
      "Step 214040 (epoch   66.89), loss: 0.003634, time: 2.237 s\n",
      "Step 214060 (epoch   66.89), loss: 0.005921, time: 2.241 s\n",
      "Step 214080 (epoch   66.90), loss: 0.004178, time: 2.249 s\n",
      "Step 214100 (epoch   66.91), loss: 0.005264, time: 2.240 s\n",
      "Step 214120 (epoch   66.91), loss: 0.004994, time: 2.254 s\n",
      "Step 214140 (epoch   66.92), loss: 0.005449, time: 2.236 s\n",
      "Step 214160 (epoch   66.92), loss: 0.004501, time: 2.245 s\n",
      "Step 214180 (epoch   66.93), loss: 0.004429, time: 2.239 s\n",
      "Step 214200 (epoch   66.94), loss: 0.003080, time: 2.239 s, error: 0.010696\n",
      "Step 214220 (epoch   66.94), loss: 0.004150, time: 15.244 s\n",
      "Step 214240 (epoch   66.95), loss: 0.003619, time: 2.230 s\n",
      "Step 214260 (epoch   66.96), loss: 0.002934, time: 2.246 s\n",
      "Step 214280 (epoch   66.96), loss: 0.005954, time: 2.231 s\n",
      "Step 214300 (epoch   66.97), loss: 0.004610, time: 2.243 s\n",
      "Step 214320 (epoch   66.97), loss: 0.007734, time: 2.236 s\n",
      "Step 214340 (epoch   66.98), loss: 0.003478, time: 2.243 s\n",
      "Step 214360 (epoch   66.99), loss: 0.007610, time: 2.233 s\n",
      "Step 214380 (epoch   66.99), loss: 0.004111, time: 2.246 s\n",
      "Step 214400 (epoch   67.00), loss: 0.004140, time: 2.242 s, error: 0.010618\n",
      "Step 214420 (epoch   67.01), loss: 0.004772, time: 15.088 s\n",
      "Step 214440 (epoch   67.01), loss: 0.003417, time: 2.257 s\n",
      "Step 214460 (epoch   67.02), loss: 0.003746, time: 2.240 s\n",
      "Step 214480 (epoch   67.03), loss: 0.004739, time: 2.243 s\n",
      "Step 214500 (epoch   67.03), loss: 0.004531, time: 2.240 s\n",
      "Step 214520 (epoch   67.04), loss: 0.004445, time: 2.250 s\n",
      "Step 214540 (epoch   67.04), loss: 0.007684, time: 2.243 s\n",
      "Step 214560 (epoch   67.05), loss: 0.003234, time: 2.255 s\n",
      "Step 214580 (epoch   67.06), loss: 0.003855, time: 2.236 s\n",
      "Step 214600 (epoch   67.06), loss: 0.004459, time: 2.248 s, error: 0.010784\n",
      "Step 214620 (epoch   67.07), loss: 0.003526, time: 15.112 s\n",
      "Step 214640 (epoch   67.08), loss: 0.003688, time: 2.252 s\n",
      "Step 214660 (epoch   67.08), loss: 0.005001, time: 2.241 s\n",
      "Step 214680 (epoch   67.09), loss: 0.006211, time: 2.252 s\n",
      "Step 214700 (epoch   67.09), loss: 0.005103, time: 2.247 s\n",
      "Step 214720 (epoch   67.10), loss: 0.007938, time: 2.249 s\n",
      "Step 214740 (epoch   67.11), loss: 0.013135, time: 2.235 s\n",
      "Step 214760 (epoch   67.11), loss: 0.004205, time: 2.244 s\n",
      "Step 214780 (epoch   67.12), loss: 0.003962, time: 2.236 s\n",
      "Step 214800 (epoch   67.12), loss: 0.007052, time: 2.236 s, error: 0.010882\n",
      "Step 214820 (epoch   67.13), loss: 0.005693, time: 15.071 s\n",
      "Step 214840 (epoch   67.14), loss: 0.005888, time: 2.241 s\n",
      "Step 214860 (epoch   67.14), loss: 0.004161, time: 2.234 s\n",
      "Step 214880 (epoch   67.15), loss: 0.003759, time: 2.235 s\n",
      "Step 214900 (epoch   67.16), loss: 0.005328, time: 2.246 s\n",
      "Step 214920 (epoch   67.16), loss: 0.004306, time: 2.242 s\n",
      "Step 214940 (epoch   67.17), loss: 0.004489, time: 2.252 s\n",
      "Step 214960 (epoch   67.17), loss: 0.007121, time: 2.259 s\n",
      "Step 214980 (epoch   67.18), loss: 0.006275, time: 2.248 s\n",
      "Step 215000 (epoch   67.19), loss: 0.003560, time: 2.242 s, error: 0.010575\n",
      "Step 215020 (epoch   67.19), loss: 0.004257, time: 15.072 s\n",
      "Step 215040 (epoch   67.20), loss: 0.004912, time: 2.244 s\n",
      "Step 215060 (epoch   67.21), loss: 0.003809, time: 2.227 s\n",
      "Step 215080 (epoch   67.21), loss: 0.003669, time: 2.231 s\n",
      "Step 215100 (epoch   67.22), loss: 0.005338, time: 2.254 s\n",
      "Step 215120 (epoch   67.22), loss: 0.002753, time: 2.238 s\n",
      "Step 215140 (epoch   67.23), loss: 0.007069, time: 2.243 s\n",
      "Step 215160 (epoch   67.24), loss: 0.003784, time: 2.227 s\n",
      "Step 215180 (epoch   67.24), loss: 0.003260, time: 2.249 s\n",
      "Step 215200 (epoch   67.25), loss: 0.003588, time: 2.257 s, error: 0.010861\n",
      "Step 215220 (epoch   67.26), loss: 0.003857, time: 15.107 s\n",
      "Step 215240 (epoch   67.26), loss: 0.005968, time: 2.236 s\n",
      "Step 215260 (epoch   67.27), loss: 0.005737, time: 2.222 s\n",
      "Step 215280 (epoch   67.28), loss: 0.006393, time: 2.248 s\n",
      "Step 215300 (epoch   67.28), loss: 0.003812, time: 2.240 s\n",
      "Step 215320 (epoch   67.29), loss: 0.004827, time: 2.245 s\n",
      "Step 215340 (epoch   67.29), loss: 0.006300, time: 2.242 s\n",
      "Step 215360 (epoch   67.30), loss: 0.004324, time: 2.232 s\n",
      "Step 215380 (epoch   67.31), loss: 0.003632, time: 2.238 s\n",
      "Step 215400 (epoch   67.31), loss: 0.004203, time: 2.240 s, error: 0.010529\n",
      "Step 215420 (epoch   67.32), loss: 0.010345, time: 15.295 s\n",
      "Step 215440 (epoch   67.33), loss: 0.003587, time: 2.247 s\n",
      "Step 215460 (epoch   67.33), loss: 0.003366, time: 2.236 s\n",
      "Step 215480 (epoch   67.34), loss: 0.002754, time: 2.235 s\n",
      "Step 215500 (epoch   67.34), loss: 0.004632, time: 2.244 s\n",
      "Step 215520 (epoch   67.35), loss: 0.003656, time: 2.247 s\n",
      "Step 215540 (epoch   67.36), loss: 0.003120, time: 2.234 s\n",
      "Step 215560 (epoch   67.36), loss: 0.004065, time: 2.231 s\n",
      "Step 215580 (epoch   67.37), loss: 0.003408, time: 2.231 s\n",
      "Step 215600 (epoch   67.38), loss: 0.004517, time: 2.246 s, error: 0.010705\n",
      "Step 215620 (epoch   67.38), loss: 0.004965, time: 15.193 s\n",
      "Step 215640 (epoch   67.39), loss: 0.009934, time: 2.245 s\n",
      "Step 215660 (epoch   67.39), loss: 0.005189, time: 2.234 s\n",
      "Step 215680 (epoch   67.40), loss: 0.006003, time: 2.239 s\n",
      "Step 215700 (epoch   67.41), loss: 0.005470, time: 2.239 s\n",
      "Step 215720 (epoch   67.41), loss: 0.004974, time: 2.237 s\n",
      "Step 215740 (epoch   67.42), loss: 0.004487, time: 2.240 s\n",
      "Step 215760 (epoch   67.42), loss: 0.003114, time: 2.229 s\n",
      "Step 215780 (epoch   67.43), loss: 0.004763, time: 2.236 s\n",
      "Step 215800 (epoch   67.44), loss: 0.004464, time: 2.251 s, error: 0.011037\n",
      "Step 215820 (epoch   67.44), loss: 0.010589, time: 15.090 s\n",
      "Step 215840 (epoch   67.45), loss: 0.006947, time: 2.237 s\n",
      "Step 215860 (epoch   67.46), loss: 0.003590, time: 2.256 s\n",
      "Step 215880 (epoch   67.46), loss: 0.002262, time: 2.255 s\n",
      "Step 215900 (epoch   67.47), loss: 0.004662, time: 2.238 s\n",
      "Step 215920 (epoch   67.47), loss: 0.003917, time: 2.236 s\n",
      "Step 215940 (epoch   67.48), loss: 0.003906, time: 2.239 s\n",
      "Step 215960 (epoch   67.49), loss: 0.004599, time: 2.243 s\n",
      "Step 215980 (epoch   67.49), loss: 0.004864, time: 2.233 s\n",
      "Step 216000 (epoch   67.50), loss: 0.003829, time: 2.249 s, error: 0.010931\n",
      "\n",
      "Time since beginning  : 37876.276 s\n",
      "\n",
      "Step 216020 (epoch   67.51), loss: 0.004963, time: 15.092 s\n",
      "Step 216040 (epoch   67.51), loss: 0.002865, time: 2.235 s\n",
      "Step 216060 (epoch   67.52), loss: 0.007398, time: 2.250 s\n",
      "Step 216080 (epoch   67.53), loss: 0.006702, time: 2.254 s\n",
      "Step 216100 (epoch   67.53), loss: 0.004309, time: 2.233 s\n",
      "Step 216120 (epoch   67.54), loss: 0.002659, time: 2.255 s\n",
      "Step 216140 (epoch   67.54), loss: 0.005609, time: 2.244 s\n",
      "Step 216160 (epoch   67.55), loss: 0.005320, time: 2.233 s\n",
      "Step 216180 (epoch   67.56), loss: 0.003451, time: 2.236 s\n",
      "Step 216200 (epoch   67.56), loss: 0.004652, time: 2.237 s, error: 0.011162\n",
      "Step 216220 (epoch   67.57), loss: 0.004635, time: 15.023 s\n",
      "Step 216240 (epoch   67.58), loss: 0.005290, time: 2.246 s\n",
      "Step 216260 (epoch   67.58), loss: 0.006836, time: 2.237 s\n",
      "Step 216280 (epoch   67.59), loss: 0.003212, time: 2.245 s\n",
      "Step 216300 (epoch   67.59), loss: 0.002514, time: 2.244 s\n",
      "Step 216320 (epoch   67.60), loss: 0.004380, time: 2.251 s\n",
      "Step 216340 (epoch   67.61), loss: 0.006288, time: 2.233 s\n",
      "Step 216360 (epoch   67.61), loss: 0.004331, time: 2.231 s\n",
      "Step 216380 (epoch   67.62), loss: 0.007306, time: 2.253 s\n",
      "Step 216400 (epoch   67.62), loss: 0.005123, time: 2.244 s, error: 0.010868\n",
      "Step 216420 (epoch   67.63), loss: 0.003345, time: 15.041 s\n",
      "Step 216440 (epoch   67.64), loss: 0.004454, time: 2.230 s\n",
      "Step 216460 (epoch   67.64), loss: 0.006582, time: 2.235 s\n",
      "Step 216480 (epoch   67.65), loss: 0.004386, time: 2.236 s\n",
      "Step 216500 (epoch   67.66), loss: 0.005863, time: 2.239 s\n",
      "Step 216520 (epoch   67.66), loss: 0.003218, time: 2.237 s\n",
      "Step 216540 (epoch   67.67), loss: 0.018883, time: 2.248 s\n",
      "Step 216560 (epoch   67.67), loss: 0.004655, time: 2.240 s\n",
      "Step 216580 (epoch   67.68), loss: 0.004176, time: 2.245 s\n",
      "Step 216600 (epoch   67.69), loss: 0.004906, time: 2.229 s, error: 0.010543\n",
      "Step 216620 (epoch   67.69), loss: 0.006044, time: 15.328 s\n",
      "Step 216640 (epoch   67.70), loss: 0.004972, time: 2.236 s\n",
      "Step 216660 (epoch   67.71), loss: 0.003729, time: 2.240 s\n",
      "Step 216680 (epoch   67.71), loss: 0.005438, time: 2.243 s\n",
      "Step 216700 (epoch   67.72), loss: 0.005066, time: 2.237 s\n",
      "Step 216720 (epoch   67.72), loss: 0.003705, time: 2.245 s\n",
      "Step 216740 (epoch   67.73), loss: 0.006457, time: 2.236 s\n",
      "Step 216760 (epoch   67.74), loss: 0.005083, time: 2.229 s\n",
      "Step 216780 (epoch   67.74), loss: 0.005667, time: 2.233 s\n",
      "Step 216800 (epoch   67.75), loss: 0.005425, time: 2.254 s, error: 0.010809\n",
      "Step 216820 (epoch   67.76), loss: 0.004618, time: 15.254 s\n",
      "Step 216840 (epoch   67.76), loss: 0.003242, time: 2.237 s\n",
      "Step 216860 (epoch   67.77), loss: 0.002930, time: 2.239 s\n",
      "Step 216880 (epoch   67.78), loss: 0.004777, time: 2.236 s\n",
      "Step 216900 (epoch   67.78), loss: 0.005385, time: 2.246 s\n",
      "Step 216920 (epoch   67.79), loss: 0.003508, time: 2.232 s\n",
      "Step 216940 (epoch   67.79), loss: 0.006528, time: 2.226 s\n",
      "Step 216960 (epoch   67.80), loss: 0.004907, time: 2.239 s\n",
      "Step 216980 (epoch   67.81), loss: 0.004169, time: 2.235 s\n",
      "Step 217000 (epoch   67.81), loss: 0.004522, time: 2.234 s, error: 0.010555\n",
      "Step 217020 (epoch   67.82), loss: 0.006112, time: 15.055 s\n",
      "Step 217040 (epoch   67.83), loss: 0.006962, time: 2.258 s\n",
      "Step 217060 (epoch   67.83), loss: 0.002634, time: 2.259 s\n",
      "Step 217080 (epoch   67.84), loss: 0.003418, time: 2.254 s\n",
      "Step 217100 (epoch   67.84), loss: 0.004192, time: 2.242 s\n",
      "Step 217120 (epoch   67.85), loss: 0.005646, time: 2.232 s\n",
      "Step 217140 (epoch   67.86), loss: 0.002804, time: 2.238 s\n",
      "Step 217160 (epoch   67.86), loss: 0.004513, time: 2.242 s\n",
      "Step 217180 (epoch   67.87), loss: 0.003925, time: 2.230 s\n",
      "Step 217200 (epoch   67.88), loss: 0.005269, time: 2.247 s, error: 0.010756\n",
      "Step 217220 (epoch   67.88), loss: 0.004156, time: 15.073 s\n",
      "Step 217240 (epoch   67.89), loss: 0.004053, time: 2.236 s\n",
      "Step 217260 (epoch   67.89), loss: 0.006244, time: 2.250 s\n",
      "Step 217280 (epoch   67.90), loss: 0.003825, time: 2.234 s\n",
      "Step 217300 (epoch   67.91), loss: 0.004449, time: 2.256 s\n",
      "Step 217320 (epoch   67.91), loss: 0.005712, time: 2.236 s\n",
      "Step 217340 (epoch   67.92), loss: 0.003365, time: 2.233 s\n",
      "Step 217360 (epoch   67.92), loss: 0.003420, time: 2.232 s\n",
      "Step 217380 (epoch   67.93), loss: 0.004314, time: 2.244 s\n",
      "Step 217400 (epoch   67.94), loss: 0.003469, time: 2.245 s, error: 0.010627\n",
      "Step 217420 (epoch   67.94), loss: 0.005621, time: 15.064 s\n",
      "Step 217440 (epoch   67.95), loss: 0.005833, time: 2.237 s\n",
      "Step 217460 (epoch   67.96), loss: 0.003887, time: 2.245 s\n",
      "Step 217480 (epoch   67.96), loss: 0.003390, time: 2.252 s\n",
      "Step 217500 (epoch   67.97), loss: 0.007332, time: 2.243 s\n",
      "Step 217520 (epoch   67.97), loss: 0.002968, time: 2.249 s\n",
      "Step 217540 (epoch   67.98), loss: 0.005339, time: 2.221 s\n",
      "Step 217560 (epoch   67.99), loss: 0.002802, time: 2.249 s\n",
      "Step 217580 (epoch   67.99), loss: 0.005665, time: 2.246 s\n",
      "Step 217600 (epoch   68.00), loss: 0.008317, time: 2.246 s, error: 0.010546\n",
      "Step 217620 (epoch   68.01), loss: 0.008936, time: 15.058 s\n",
      "Step 217640 (epoch   68.01), loss: 0.003728, time: 2.242 s\n",
      "Step 217660 (epoch   68.02), loss: 0.003821, time: 2.241 s\n",
      "Step 217680 (epoch   68.03), loss: 0.005464, time: 2.229 s\n",
      "Step 217700 (epoch   68.03), loss: 0.004172, time: 2.248 s\n",
      "Step 217720 (epoch   68.04), loss: 0.003358, time: 2.224 s\n",
      "Step 217740 (epoch   68.04), loss: 0.006080, time: 2.234 s\n",
      "Step 217760 (epoch   68.05), loss: 0.003183, time: 2.239 s\n",
      "Step 217780 (epoch   68.06), loss: 0.005046, time: 2.239 s\n",
      "Step 217800 (epoch   68.06), loss: 0.007065, time: 2.240 s, error: 0.010662\n",
      "Step 217820 (epoch   68.07), loss: 0.006153, time: 15.265 s\n",
      "Step 217840 (epoch   68.08), loss: 0.004269, time: 2.245 s\n",
      "Step 217860 (epoch   68.08), loss: 0.004106, time: 2.234 s\n",
      "Step 217880 (epoch   68.09), loss: 0.003935, time: 2.244 s\n",
      "Step 217900 (epoch   68.09), loss: 0.004338, time: 2.230 s\n",
      "Step 217920 (epoch   68.10), loss: 0.003412, time: 2.245 s\n",
      "Step 217940 (epoch   68.11), loss: 0.006154, time: 2.243 s\n",
      "Step 217960 (epoch   68.11), loss: 0.005042, time: 2.238 s\n",
      "Step 217980 (epoch   68.12), loss: 0.002338, time: 2.236 s\n",
      "Step 218000 (epoch   68.12), loss: 0.004854, time: 2.244 s, error: 0.010602\n",
      "\n",
      "Time since beginning  : 38229.390 s\n",
      "\n",
      "Step 218020 (epoch   68.13), loss: 0.004313, time: 15.337 s\n",
      "Step 218040 (epoch   68.14), loss: 0.003183, time: 2.235 s\n",
      "Step 218060 (epoch   68.14), loss: 0.006943, time: 2.255 s\n",
      "Step 218080 (epoch   68.15), loss: 0.005676, time: 2.248 s\n",
      "Step 218100 (epoch   68.16), loss: 0.005073, time: 2.250 s\n",
      "Step 218120 (epoch   68.16), loss: 0.004764, time: 2.238 s\n",
      "Step 218140 (epoch   68.17), loss: 0.005206, time: 2.243 s\n",
      "Step 218160 (epoch   68.17), loss: 0.003423, time: 2.235 s\n",
      "Step 218180 (epoch   68.18), loss: 0.007875, time: 2.242 s\n",
      "Step 218200 (epoch   68.19), loss: 0.007018, time: 2.239 s, error: 0.010575\n",
      "Step 218220 (epoch   68.19), loss: 0.003884, time: 15.033 s\n",
      "Step 218240 (epoch   68.20), loss: 0.003253, time: 2.258 s\n",
      "Step 218260 (epoch   68.21), loss: 0.004612, time: 2.226 s\n",
      "Step 218280 (epoch   68.21), loss: 0.004860, time: 2.239 s\n",
      "Step 218300 (epoch   68.22), loss: 0.003125, time: 2.249 s\n",
      "Step 218320 (epoch   68.22), loss: 0.004320, time: 2.238 s\n",
      "Step 218340 (epoch   68.23), loss: 0.002763, time: 2.237 s\n",
      "Step 218360 (epoch   68.24), loss: 0.003253, time: 2.241 s\n",
      "Step 218380 (epoch   68.24), loss: 0.003805, time: 2.243 s\n",
      "Step 218400 (epoch   68.25), loss: 0.003309, time: 2.230 s, error: 0.011178\n",
      "Step 218420 (epoch   68.26), loss: 0.005863, time: 15.135 s\n",
      "Step 218440 (epoch   68.26), loss: 0.005620, time: 2.248 s\n",
      "Step 218460 (epoch   68.27), loss: 0.004494, time: 2.250 s\n",
      "Step 218480 (epoch   68.28), loss: 0.003254, time: 2.254 s\n",
      "Step 218500 (epoch   68.28), loss: 0.003647, time: 2.250 s\n",
      "Step 218520 (epoch   68.29), loss: 0.004561, time: 2.233 s\n",
      "Step 218540 (epoch   68.29), loss: 0.005408, time: 2.230 s\n",
      "Step 218560 (epoch   68.30), loss: 0.004639, time: 2.245 s\n",
      "Step 218580 (epoch   68.31), loss: 0.005071, time: 2.240 s\n",
      "Step 218600 (epoch   68.31), loss: 0.005200, time: 2.237 s, error: 0.010485\n",
      "Step 218620 (epoch   68.32), loss: 0.003503, time: 15.034 s\n",
      "Step 218640 (epoch   68.33), loss: 0.004321, time: 2.249 s\n",
      "Step 218660 (epoch   68.33), loss: 0.006992, time: 2.226 s\n",
      "Step 218680 (epoch   68.34), loss: 0.002912, time: 2.240 s\n",
      "Step 218700 (epoch   68.34), loss: 0.002492, time: 2.238 s\n",
      "Step 218720 (epoch   68.35), loss: 0.003959, time: 2.239 s\n",
      "Step 218740 (epoch   68.36), loss: 0.004361, time: 2.243 s\n",
      "Step 218760 (epoch   68.36), loss: 0.006235, time: 2.257 s\n",
      "Step 218780 (epoch   68.37), loss: 0.003383, time: 2.228 s\n",
      "Step 218800 (epoch   68.38), loss: 0.004768, time: 2.245 s, error: 0.010643\n",
      "Step 218820 (epoch   68.38), loss: 0.004496, time: 15.046 s\n",
      "Step 218840 (epoch   68.39), loss: 0.003350, time: 2.234 s\n",
      "Step 218860 (epoch   68.39), loss: 0.009702, time: 2.237 s\n",
      "Step 218880 (epoch   68.40), loss: 0.008396, time: 2.231 s\n",
      "Step 218900 (epoch   68.41), loss: 0.006067, time: 2.250 s\n",
      "Step 218920 (epoch   68.41), loss: 0.004446, time: 2.254 s\n",
      "Step 218940 (epoch   68.42), loss: 0.006265, time: 2.247 s\n",
      "Step 218960 (epoch   68.42), loss: 0.004729, time: 2.239 s\n",
      "Step 218980 (epoch   68.43), loss: 0.004257, time: 2.244 s\n",
      "Step 219000 (epoch   68.44), loss: 0.003978, time: 2.243 s, error: 0.011221\n",
      "Step 219020 (epoch   68.44), loss: 0.005920, time: 15.232 s\n",
      "Step 219040 (epoch   68.45), loss: 0.003706, time: 2.237 s\n",
      "Step 219060 (epoch   68.46), loss: 0.002992, time: 2.240 s\n",
      "Step 219080 (epoch   68.46), loss: 0.004505, time: 2.221 s\n",
      "Step 219100 (epoch   68.47), loss: 0.004980, time: 2.235 s\n",
      "Step 219120 (epoch   68.47), loss: 0.008963, time: 2.237 s\n",
      "Step 219140 (epoch   68.48), loss: 0.004615, time: 2.223 s\n",
      "Step 219160 (epoch   68.49), loss: 0.002965, time: 2.247 s\n",
      "Step 219180 (epoch   68.49), loss: 0.003306, time: 2.235 s\n",
      "Step 219200 (epoch   68.50), loss: 0.003797, time: 2.253 s, error: 0.010558\n",
      "Step 219220 (epoch   68.51), loss: 0.005635, time: 15.254 s\n",
      "Step 219240 (epoch   68.51), loss: 0.003823, time: 2.250 s\n",
      "Step 219260 (epoch   68.52), loss: 0.013073, time: 2.232 s\n",
      "Step 219280 (epoch   68.53), loss: 0.004963, time: 2.240 s\n",
      "Step 219300 (epoch   68.53), loss: 0.003731, time: 2.230 s\n",
      "Step 219320 (epoch   68.54), loss: 0.007889, time: 2.236 s\n",
      "Step 219340 (epoch   68.54), loss: 0.005519, time: 2.242 s\n",
      "Step 219360 (epoch   68.55), loss: 0.005481, time: 2.243 s\n",
      "Step 219380 (epoch   68.56), loss: 0.003911, time: 2.234 s\n",
      "Step 219400 (epoch   68.56), loss: 0.005647, time: 2.233 s, error: 0.010785\n",
      "Step 219420 (epoch   68.57), loss: 0.004647, time: 15.164 s\n",
      "Step 219440 (epoch   68.58), loss: 0.005582, time: 2.232 s\n",
      "Step 219460 (epoch   68.58), loss: 0.003765, time: 2.231 s\n",
      "Step 219480 (epoch   68.59), loss: 0.006425, time: 2.247 s\n",
      "Step 219500 (epoch   68.59), loss: 0.003411, time: 2.228 s\n",
      "Step 219520 (epoch   68.60), loss: 0.005401, time: 2.242 s\n",
      "Step 219540 (epoch   68.61), loss: 0.004602, time: 2.241 s\n",
      "Step 219560 (epoch   68.61), loss: 0.007879, time: 2.238 s\n",
      "Step 219580 (epoch   68.62), loss: 0.004622, time: 2.233 s\n",
      "Step 219600 (epoch   68.62), loss: 0.007161, time: 2.238 s, error: 0.010885\n",
      "Step 219620 (epoch   68.63), loss: 0.003135, time: 15.049 s\n",
      "Step 219640 (epoch   68.64), loss: 0.004703, time: 2.244 s\n",
      "Step 219660 (epoch   68.64), loss: 0.003455, time: 2.238 s\n",
      "Step 219680 (epoch   68.65), loss: 0.004232, time: 2.256 s\n",
      "Step 219700 (epoch   68.66), loss: 0.007753, time: 2.235 s\n",
      "Step 219720 (epoch   68.66), loss: 0.003058, time: 2.252 s\n",
      "Step 219740 (epoch   68.67), loss: 0.003574, time: 2.225 s\n",
      "Step 219760 (epoch   68.67), loss: 0.003665, time: 2.236 s\n",
      "Step 219780 (epoch   68.68), loss: 0.004450, time: 2.239 s\n",
      "Step 219800 (epoch   68.69), loss: 0.003742, time: 2.253 s, error: 0.010536\n",
      "Step 219820 (epoch   68.69), loss: 0.005903, time: 15.238 s\n",
      "Step 219840 (epoch   68.70), loss: 0.009049, time: 2.248 s\n",
      "Step 219860 (epoch   68.71), loss: 0.003426, time: 2.238 s\n",
      "Step 219880 (epoch   68.71), loss: 0.003334, time: 2.241 s\n",
      "Step 219900 (epoch   68.72), loss: 0.005853, time: 2.240 s\n",
      "Step 219920 (epoch   68.72), loss: 0.005010, time: 2.248 s\n",
      "Step 219940 (epoch   68.73), loss: 0.005084, time: 2.252 s\n",
      "Step 219960 (epoch   68.74), loss: 0.003781, time: 2.235 s\n",
      "Step 219980 (epoch   68.74), loss: 0.005028, time: 2.238 s\n",
      "Step 220000 (epoch   68.75), loss: 0.003150, time: 2.238 s, error: 0.010695\n",
      "\n",
      "Time since beginning  : 38582.340 s\n",
      "\n",
      "Step 220020 (epoch   68.76), loss: 0.004498, time: 15.132 s\n",
      "Step 220040 (epoch   68.76), loss: 0.003669, time: 2.233 s\n",
      "Step 220060 (epoch   68.77), loss: 0.004681, time: 2.238 s\n",
      "Step 220080 (epoch   68.78), loss: 0.003248, time: 2.241 s\n",
      "Step 220100 (epoch   68.78), loss: 0.004574, time: 2.238 s\n",
      "Step 220120 (epoch   68.79), loss: 0.004960, time: 2.246 s\n",
      "Step 220140 (epoch   68.79), loss: 0.006390, time: 2.234 s\n",
      "Step 220160 (epoch   68.80), loss: 0.003659, time: 2.248 s\n",
      "Step 220180 (epoch   68.81), loss: 0.006057, time: 2.252 s\n",
      "Step 220200 (epoch   68.81), loss: 0.003561, time: 2.249 s, error: 0.010551\n",
      "Step 220220 (epoch   68.82), loss: 0.004194, time: 15.031 s\n",
      "Step 220240 (epoch   68.83), loss: 0.006457, time: 2.241 s\n",
      "Step 220260 (epoch   68.83), loss: 0.002720, time: 2.251 s\n",
      "Step 220280 (epoch   68.84), loss: 0.004936, time: 2.242 s\n",
      "Step 220300 (epoch   68.84), loss: 0.004815, time: 2.243 s\n",
      "Step 220320 (epoch   68.85), loss: 0.002903, time: 2.242 s\n",
      "Step 220340 (epoch   68.86), loss: 0.003562, time: 2.238 s\n",
      "Step 220360 (epoch   68.86), loss: 0.005787, time: 2.244 s\n",
      "Step 220380 (epoch   68.87), loss: 0.003812, time: 2.241 s\n",
      "Step 220400 (epoch   68.88), loss: 0.004174, time: 2.242 s, error: 0.010707\n",
      "Step 220420 (epoch   68.88), loss: 0.004594, time: 15.219 s\n",
      "Step 220440 (epoch   68.89), loss: 0.004659, time: 2.252 s\n",
      "Step 220460 (epoch   68.89), loss: 0.009768, time: 2.239 s\n",
      "Step 220480 (epoch   68.90), loss: 0.003298, time: 2.252 s\n",
      "Step 220500 (epoch   68.91), loss: 0.006447, time: 2.256 s\n",
      "Step 220520 (epoch   68.91), loss: 0.004199, time: 2.239 s\n",
      "Step 220540 (epoch   68.92), loss: 0.005961, time: 2.244 s\n",
      "Step 220560 (epoch   68.92), loss: 0.004354, time: 2.239 s\n",
      "Step 220580 (epoch   68.93), loss: 0.003710, time: 2.250 s\n",
      "Step 220600 (epoch   68.94), loss: 0.003676, time: 2.240 s, error: 0.010666\n",
      "Step 220620 (epoch   68.94), loss: 0.003917, time: 15.235 s\n",
      "Step 220640 (epoch   68.95), loss: 0.007252, time: 2.241 s\n",
      "Step 220660 (epoch   68.96), loss: 0.003857, time: 2.240 s\n",
      "Step 220680 (epoch   68.96), loss: 0.004475, time: 2.242 s\n",
      "Step 220700 (epoch   68.97), loss: 0.004130, time: 2.230 s\n",
      "Step 220720 (epoch   68.97), loss: 0.006887, time: 2.229 s\n",
      "Step 220740 (epoch   68.98), loss: 0.004780, time: 2.229 s\n",
      "Step 220760 (epoch   68.99), loss: 0.003715, time: 2.252 s\n",
      "Step 220780 (epoch   68.99), loss: 0.002875, time: 2.233 s\n",
      "Step 220800 (epoch   69.00), loss: 0.005126, time: 2.235 s, error: 0.010526\n",
      "Step 220820 (epoch   69.01), loss: 0.003292, time: 15.200 s\n",
      "Step 220840 (epoch   69.01), loss: 0.003738, time: 2.249 s\n",
      "Step 220860 (epoch   69.02), loss: 0.006424, time: 2.261 s\n",
      "Step 220880 (epoch   69.03), loss: 0.004729, time: 2.244 s\n",
      "Step 220900 (epoch   69.03), loss: 0.004131, time: 2.253 s\n",
      "Step 220920 (epoch   69.04), loss: 0.004071, time: 2.254 s\n",
      "Step 220940 (epoch   69.04), loss: 0.006069, time: 2.247 s\n",
      "Step 220960 (epoch   69.05), loss: 0.003314, time: 2.254 s\n",
      "Step 220980 (epoch   69.06), loss: 0.007381, time: 2.253 s\n",
      "Step 221000 (epoch   69.06), loss: 0.005619, time: 2.233 s, error: 0.010473\n",
      "Step 221020 (epoch   69.07), loss: 0.004710, time: 15.061 s\n",
      "Step 221040 (epoch   69.08), loss: 0.003204, time: 2.244 s\n",
      "Step 221060 (epoch   69.08), loss: 0.003153, time: 2.228 s\n",
      "Step 221080 (epoch   69.09), loss: 0.004230, time: 2.248 s\n",
      "Step 221100 (epoch   69.09), loss: 0.004113, time: 2.258 s\n",
      "Step 221120 (epoch   69.10), loss: 0.004618, time: 2.258 s\n",
      "Step 221140 (epoch   69.11), loss: 0.003034, time: 2.243 s\n",
      "Step 221160 (epoch   69.11), loss: 0.004173, time: 2.238 s\n",
      "Step 221180 (epoch   69.12), loss: 0.007141, time: 2.256 s\n",
      "Step 221200 (epoch   69.12), loss: 0.003524, time: 2.252 s, error: 0.010380\n",
      "Step 221220 (epoch   69.13), loss: 0.006159, time: 15.082 s\n",
      "Step 221240 (epoch   69.14), loss: 0.004838, time: 2.249 s\n",
      "Step 221260 (epoch   69.14), loss: 0.003764, time: 2.241 s\n",
      "Step 221280 (epoch   69.15), loss: 0.004417, time: 2.250 s\n",
      "Step 221300 (epoch   69.16), loss: 0.003729, time: 2.238 s\n",
      "Step 221320 (epoch   69.16), loss: 0.005326, time: 2.248 s\n",
      "Step 221340 (epoch   69.17), loss: 0.003768, time: 2.236 s\n",
      "Step 221360 (epoch   69.17), loss: 0.005594, time: 2.249 s\n",
      "Step 221380 (epoch   69.18), loss: 0.006175, time: 2.250 s\n",
      "Step 221400 (epoch   69.19), loss: 0.006427, time: 2.243 s, error: 0.010626\n",
      "Step 221420 (epoch   69.19), loss: 0.006003, time: 15.087 s\n",
      "Step 221440 (epoch   69.20), loss: 0.003308, time: 2.231 s\n",
      "Step 221460 (epoch   69.21), loss: 0.006780, time: 2.242 s\n",
      "Step 221480 (epoch   69.21), loss: 0.004981, time: 2.239 s\n",
      "Step 221500 (epoch   69.22), loss: 0.005708, time: 2.237 s\n",
      "Step 221520 (epoch   69.22), loss: 0.004462, time: 2.235 s\n",
      "Step 221540 (epoch   69.23), loss: 0.002782, time: 2.233 s\n",
      "Step 221560 (epoch   69.24), loss: 0.003277, time: 2.232 s\n",
      "Step 221580 (epoch   69.24), loss: 0.004144, time: 2.223 s\n",
      "Step 221600 (epoch   69.25), loss: 0.004466, time: 2.251 s, error: 0.011311\n",
      "Step 221620 (epoch   69.26), loss: 0.004553, time: 15.246 s\n",
      "Step 221640 (epoch   69.26), loss: 0.002873, time: 2.251 s\n",
      "Step 221660 (epoch   69.27), loss: 0.006427, time: 2.247 s\n",
      "Step 221680 (epoch   69.28), loss: 0.003787, time: 2.248 s\n",
      "Step 221700 (epoch   69.28), loss: 0.005395, time: 2.234 s\n",
      "Step 221720 (epoch   69.29), loss: 0.003144, time: 2.246 s\n",
      "Step 221740 (epoch   69.29), loss: 0.004556, time: 2.235 s\n",
      "Step 221760 (epoch   69.30), loss: 0.003686, time: 2.233 s\n",
      "Step 221780 (epoch   69.31), loss: 0.004473, time: 2.232 s\n",
      "Step 221800 (epoch   69.31), loss: 0.004210, time: 2.236 s, error: 0.010470\n",
      "Step 221820 (epoch   69.32), loss: 0.004291, time: 15.234 s\n",
      "Step 221840 (epoch   69.33), loss: 0.003638, time: 2.238 s\n",
      "Step 221860 (epoch   69.33), loss: 0.003768, time: 2.247 s\n",
      "Step 221880 (epoch   69.34), loss: 0.004251, time: 2.243 s\n",
      "Step 221900 (epoch   69.34), loss: 0.004112, time: 2.257 s\n",
      "Step 221920 (epoch   69.35), loss: 0.004546, time: 2.242 s\n",
      "Step 221940 (epoch   69.36), loss: 0.003960, time: 2.242 s\n",
      "Step 221960 (epoch   69.36), loss: 0.007695, time: 2.250 s\n",
      "Step 221980 (epoch   69.37), loss: 0.008664, time: 2.249 s\n",
      "Step 222000 (epoch   69.38), loss: 0.003747, time: 2.234 s, error: 0.010591\n",
      "\n",
      "Time since beginning  : 38935.878 s\n",
      "\n",
      "Step 222020 (epoch   69.38), loss: 0.003578, time: 15.284 s\n",
      "Step 222040 (epoch   69.39), loss: 0.004701, time: 2.253 s\n",
      "Step 222060 (epoch   69.39), loss: 0.011702, time: 2.239 s\n",
      "Step 222080 (epoch   69.40), loss: 0.004489, time: 2.238 s\n",
      "Step 222100 (epoch   69.41), loss: 0.005059, time: 2.236 s\n",
      "Step 222120 (epoch   69.41), loss: 0.006354, time: 2.231 s\n",
      "Step 222140 (epoch   69.42), loss: 0.005612, time: 2.234 s\n",
      "Step 222160 (epoch   69.42), loss: 0.004762, time: 2.221 s\n",
      "Step 222180 (epoch   69.43), loss: 0.006357, time: 2.233 s\n",
      "Step 222200 (epoch   69.44), loss: 0.002987, time: 2.217 s, error: 0.011098\n",
      "Step 222220 (epoch   69.44), loss: 0.006471, time: 15.034 s\n",
      "Step 222240 (epoch   69.45), loss: 0.006312, time: 2.241 s\n",
      "Step 222260 (epoch   69.46), loss: 0.003674, time: 2.241 s\n",
      "Step 222280 (epoch   69.46), loss: 0.003744, time: 2.247 s\n",
      "Step 222300 (epoch   69.47), loss: 0.005541, time: 2.250 s\n",
      "Step 222320 (epoch   69.47), loss: 0.007658, time: 2.227 s\n",
      "Step 222340 (epoch   69.48), loss: 0.004015, time: 2.233 s\n",
      "Step 222360 (epoch   69.49), loss: 0.005193, time: 2.233 s\n",
      "Step 222380 (epoch   69.49), loss: 0.003300, time: 2.230 s\n",
      "Step 222400 (epoch   69.50), loss: 0.005387, time: 2.252 s, error: 0.010342\n",
      "Step 222420 (epoch   69.51), loss: 0.003821, time: 15.025 s\n",
      "Step 222440 (epoch   69.51), loss: 0.004959, time: 2.228 s\n",
      "Step 222460 (epoch   69.52), loss: 0.005752, time: 2.250 s\n",
      "Step 222480 (epoch   69.53), loss: 0.004352, time: 2.232 s\n",
      "Step 222500 (epoch   69.53), loss: 0.004037, time: 2.254 s\n",
      "Step 222520 (epoch   69.54), loss: 0.003770, time: 2.249 s\n",
      "Step 222540 (epoch   69.54), loss: 0.004481, time: 2.246 s\n",
      "Step 222560 (epoch   69.55), loss: 0.004225, time: 2.243 s\n",
      "Step 222580 (epoch   69.56), loss: 0.003269, time: 2.239 s\n",
      "Step 222600 (epoch   69.56), loss: 0.006207, time: 2.245 s, error: 0.010636\n",
      "Step 222620 (epoch   69.57), loss: 0.005667, time: 15.092 s\n",
      "Step 222640 (epoch   69.58), loss: 0.004143, time: 2.237 s\n",
      "Step 222660 (epoch   69.58), loss: 0.004997, time: 2.240 s\n",
      "Step 222680 (epoch   69.59), loss: 0.004388, time: 2.239 s\n",
      "Step 222700 (epoch   69.59), loss: 0.004092, time: 2.239 s\n",
      "Step 222720 (epoch   69.60), loss: 0.003776, time: 2.247 s\n",
      "Step 222740 (epoch   69.61), loss: 0.003939, time: 2.244 s\n",
      "Step 222760 (epoch   69.61), loss: 0.003427, time: 2.233 s\n",
      "Step 222780 (epoch   69.62), loss: 0.005734, time: 2.252 s\n",
      "Step 222800 (epoch   69.62), loss: 0.004140, time: 2.254 s, error: 0.010975\n",
      "Step 222820 (epoch   69.63), loss: 0.005166, time: 15.105 s\n",
      "Step 222840 (epoch   69.64), loss: 0.002987, time: 2.255 s\n",
      "Step 222860 (epoch   69.64), loss: 0.002793, time: 2.240 s\n",
      "Step 222880 (epoch   69.65), loss: 0.004021, time: 2.239 s\n",
      "Step 222900 (epoch   69.66), loss: 0.003485, time: 2.228 s\n",
      "Step 222920 (epoch   69.66), loss: 0.004368, time: 2.241 s\n",
      "Step 222940 (epoch   69.67), loss: 0.004782, time: 2.221 s\n",
      "Step 222960 (epoch   69.67), loss: 0.007794, time: 2.231 s\n",
      "Step 222980 (epoch   69.68), loss: 0.004194, time: 2.223 s\n",
      "Step 223000 (epoch   69.69), loss: 0.003910, time: 2.238 s, error: 0.010506\n",
      "Step 223020 (epoch   69.69), loss: 0.004909, time: 15.109 s\n",
      "Step 223040 (epoch   69.70), loss: 0.004793, time: 2.248 s\n",
      "Step 223060 (epoch   69.71), loss: 0.005163, time: 2.243 s\n",
      "Step 223080 (epoch   69.71), loss: 0.003205, time: 2.240 s\n",
      "Step 223100 (epoch   69.72), loss: 0.002649, time: 2.245 s\n",
      "Step 223120 (epoch   69.72), loss: 0.006454, time: 2.240 s\n",
      "Step 223140 (epoch   69.73), loss: 0.003468, time: 2.249 s\n",
      "Step 223160 (epoch   69.74), loss: 0.003857, time: 2.242 s\n",
      "Step 223180 (epoch   69.74), loss: 0.004083, time: 2.245 s\n",
      "Step 223200 (epoch   69.75), loss: 0.003146, time: 2.253 s, error: 0.010633\n",
      "Step 223220 (epoch   69.76), loss: 0.004045, time: 15.059 s\n",
      "Step 223240 (epoch   69.76), loss: 0.004102, time: 2.239 s\n",
      "Step 223260 (epoch   69.77), loss: 0.004126, time: 2.253 s\n",
      "Step 223280 (epoch   69.78), loss: 0.003043, time: 2.244 s\n",
      "Step 223300 (epoch   69.78), loss: 0.003320, time: 2.244 s\n",
      "Step 223320 (epoch   69.79), loss: 0.005129, time: 2.240 s\n",
      "Step 223340 (epoch   69.79), loss: 0.003797, time: 2.230 s\n",
      "Step 223360 (epoch   69.80), loss: 0.005910, time: 2.236 s\n",
      "Step 223380 (epoch   69.81), loss: 0.004267, time: 2.245 s\n",
      "Step 223400 (epoch   69.81), loss: 0.004925, time: 2.256 s, error: 0.010540\n",
      "Step 223420 (epoch   69.82), loss: 0.003458, time: 15.158 s\n",
      "Step 223440 (epoch   69.83), loss: 0.004048, time: 2.250 s\n",
      "Step 223460 (epoch   69.83), loss: 0.003675, time: 2.238 s\n",
      "Step 223480 (epoch   69.84), loss: 0.003977, time: 2.249 s\n",
      "Step 223500 (epoch   69.84), loss: 0.003662, time: 2.235 s\n",
      "Step 223520 (epoch   69.85), loss: 0.003785, time: 2.240 s\n",
      "Step 223540 (epoch   69.86), loss: 0.006455, time: 2.240 s\n",
      "Step 223560 (epoch   69.86), loss: 0.005767, time: 2.244 s\n",
      "Step 223580 (epoch   69.87), loss: 0.004136, time: 2.243 s\n",
      "Step 223600 (epoch   69.88), loss: 0.002590, time: 2.244 s, error: 0.010590\n",
      "Step 223620 (epoch   69.88), loss: 0.003818, time: 15.265 s\n",
      "Step 223640 (epoch   69.89), loss: 0.007121, time: 2.227 s\n",
      "Step 223660 (epoch   69.89), loss: 0.004074, time: 2.251 s\n",
      "Step 223680 (epoch   69.90), loss: 0.004478, time: 2.229 s\n",
      "Step 223700 (epoch   69.91), loss: 0.006810, time: 2.244 s\n",
      "Step 223720 (epoch   69.91), loss: 0.004941, time: 2.226 s\n",
      "Step 223740 (epoch   69.92), loss: 0.005348, time: 2.228 s\n",
      "Step 223760 (epoch   69.92), loss: 0.005120, time: 2.236 s\n",
      "Step 223780 (epoch   69.93), loss: 0.008159, time: 2.237 s\n",
      "Step 223800 (epoch   69.94), loss: 0.006382, time: 2.251 s, error: 0.010757\n",
      "Step 223820 (epoch   69.94), loss: 0.005826, time: 15.228 s\n",
      "Step 223840 (epoch   69.95), loss: 0.005107, time: 2.236 s\n",
      "Step 223860 (epoch   69.96), loss: 0.005590, time: 2.239 s\n",
      "Step 223880 (epoch   69.96), loss: 0.003928, time: 2.229 s\n",
      "Step 223900 (epoch   69.97), loss: 0.005802, time: 2.244 s\n",
      "Step 223920 (epoch   69.97), loss: 0.002832, time: 2.223 s\n",
      "Step 223940 (epoch   69.98), loss: 0.013812, time: 2.238 s\n",
      "Step 223960 (epoch   69.99), loss: 0.009325, time: 2.224 s\n",
      "Step 223980 (epoch   69.99), loss: 0.004158, time: 2.237 s\n",
      "Step 224000 (epoch   70.00), loss: 0.003547, time: 2.229 s, error: 0.010467\n",
      "\n",
      "Time since beginning  : 39288.618 s\n",
      "\n",
      "Step 224020 (epoch   70.01), loss: 0.004494, time: 15.111 s\n",
      "Step 224040 (epoch   70.01), loss: 0.003964, time: 2.236 s\n",
      "Step 224060 (epoch   70.02), loss: 0.006632, time: 2.257 s\n",
      "Step 224080 (epoch   70.03), loss: 0.005120, time: 2.257 s\n",
      "Step 224100 (epoch   70.03), loss: 0.006055, time: 2.246 s\n",
      "Step 224120 (epoch   70.04), loss: 0.004838, time: 2.244 s\n",
      "Step 225100 (epoch   70.34), loss: 0.004060, time: 2.244 s\n",
      "Step 225120 (epoch   70.35), loss: 0.002878, time: 2.237 s\n",
      "Step 225140 (epoch   70.36), loss: 0.005652, time: 2.250 s\n",
      "Step 225160 (epoch   70.36), loss: 0.005197, time: 2.228 s\n",
      "Step 225180 (epoch   70.37), loss: 0.004732, time: 2.251 s\n",
      "Step 225200 (epoch   70.38), loss: 0.003966, time: 2.220 s, error: 0.010525\n",
      "Step 225220 (epoch   70.38), loss: 0.006092, time: 15.026 s\n",
      "Step 225240 (epoch   70.39), loss: 0.003894, time: 2.258 s\n",
      "Step 225260 (epoch   70.39), loss: 0.003670, time: 2.249 s\n",
      "Step 225280 (epoch   70.40), loss: 0.003117, time: 2.235 s\n",
      "Step 225300 (epoch   70.41), loss: 0.005263, time: 2.233 s\n",
      "Step 225320 (epoch   70.41), loss: 0.005397, time: 2.238 s\n",
      "Step 225340 (epoch   70.42), loss: 0.003982, time: 2.238 s\n",
      "Step 225360 (epoch   70.42), loss: 0.003751, time: 2.238 s\n",
      "Step 225380 (epoch   70.43), loss: 0.003351, time: 2.245 s\n",
      "Step 225400 (epoch   70.44), loss: 0.002495, time: 2.243 s, error: 0.010942\n",
      "Step 225420 (epoch   70.44), loss: 0.005866, time: 15.065 s\n",
      "Step 225440 (epoch   70.45), loss: 0.005072, time: 2.248 s\n",
      "Step 225460 (epoch   70.46), loss: 0.002813, time: 2.244 s\n",
      "Step 225480 (epoch   70.46), loss: 0.005444, time: 2.242 s\n",
      "Step 225500 (epoch   70.47), loss: 0.004444, time: 2.258 s\n",
      "Step 225520 (epoch   70.47), loss: 0.003258, time: 2.252 s\n",
      "Step 225540 (epoch   70.48), loss: 0.005104, time: 2.235 s\n",
      "Step 225560 (epoch   70.49), loss: 0.003749, time: 2.243 s\n",
      "Step 225580 (epoch   70.49), loss: 0.004269, time: 2.246 s\n",
      "Step 225600 (epoch   70.50), loss: 0.004098, time: 2.250 s, error: 0.010297\n",
      "Step 225620 (epoch   70.51), loss: 0.003482, time: 15.137 s\n",
      "Step 225640 (epoch   70.51), loss: 0.002720, time: 2.246 s\n",
      "Step 225660 (epoch   70.52), loss: 0.003354, time: 2.235 s\n",
      "Step 225680 (epoch   70.53), loss: 0.003625, time: 2.236 s\n",
      "Step 225700 (epoch   70.53), loss: 0.005858, time: 2.243 s\n",
      "Step 225720 (epoch   70.54), loss: 0.002940, time: 2.247 s\n",
      "Step 225740 (epoch   70.54), loss: 0.003119, time: 2.242 s\n",
      "Step 225760 (epoch   70.55), loss: 0.007049, time: 2.249 s\n",
      "Step 225780 (epoch   70.56), loss: 0.003563, time: 2.238 s\n",
      "Step 225800 (epoch   70.56), loss: 0.004343, time: 2.242 s, error: 0.010689\n",
      "Step 225820 (epoch   70.57), loss: 0.003568, time: 15.041 s\n",
      "Step 225840 (epoch   70.58), loss: 0.003868, time: 2.232 s\n",
      "Step 225860 (epoch   70.58), loss: 0.005050, time: 2.247 s\n",
      "Step 225880 (epoch   70.59), loss: 0.005129, time: 2.233 s\n",
      "Step 225900 (epoch   70.59), loss: 0.008247, time: 2.232 s\n",
      "Step 225920 (epoch   70.60), loss: 0.004430, time: 2.241 s\n",
      "Step 225940 (epoch   70.61), loss: 0.005360, time: 2.238 s\n",
      "Step 225960 (epoch   70.61), loss: 0.004549, time: 2.244 s\n",
      "Step 225980 (epoch   70.62), loss: 0.005087, time: 2.240 s\n",
      "Step 226000 (epoch   70.62), loss: 0.005387, time: 2.241 s, error: 0.011058\n",
      "\n",
      "Time since beginning  : 39641.564 s\n",
      "\n",
      "Step 226020 (epoch   70.63), loss: 0.003411, time: 15.401 s\n",
      "Step 226040 (epoch   70.64), loss: 0.005290, time: 2.249 s\n",
      "Step 226060 (epoch   70.64), loss: 0.003642, time: 2.251 s\n",
      "Step 226080 (epoch   70.65), loss: 0.005902, time: 2.241 s\n",
      "Step 226100 (epoch   70.66), loss: 0.004084, time: 2.235 s\n",
      "Step 226120 (epoch   70.66), loss: 0.005567, time: 2.249 s\n",
      "Step 226140 (epoch   70.67), loss: 0.003127, time: 2.249 s\n",
      "Step 226160 (epoch   70.67), loss: 0.003076, time: 2.251 s\n",
      "Step 226180 (epoch   70.68), loss: 0.005212, time: 2.245 s\n",
      "Step 226200 (epoch   70.69), loss: 0.003441, time: 2.241 s, error: 0.010506\n",
      "Step 226220 (epoch   70.69), loss: 0.006061, time: 15.249 s\n",
      "Step 226240 (epoch   70.70), loss: 0.003489, time: 2.237 s\n",
      "Step 226260 (epoch   70.71), loss: 0.006327, time: 2.246 s\n",
      "Step 226280 (epoch   70.71), loss: 0.004967, time: 2.244 s\n",
      "Step 226300 (epoch   70.72), loss: 0.002949, time: 2.246 s\n",
      "Step 226320 (epoch   70.72), loss: 0.003136, time: 2.253 s\n",
      "Step 226340 (epoch   70.73), loss: 0.003942, time: 2.242 s\n",
      "Step 226360 (epoch   70.74), loss: 0.004790, time: 2.230 s\n",
      "Step 226380 (epoch   70.74), loss: 0.003168, time: 2.253 s\n",
      "Step 226400 (epoch   70.75), loss: 0.003769, time: 2.249 s, error: 0.010698\n",
      "Step 226420 (epoch   70.76), loss: 0.004270, time: 15.080 s\n",
      "Step 226440 (epoch   70.76), loss: 0.002387, time: 2.254 s\n",
      "Step 226460 (epoch   70.77), loss: 0.007104, time: 2.237 s\n",
      "Step 226480 (epoch   70.78), loss: 0.003849, time: 2.249 s\n",
      "Step 226500 (epoch   70.78), loss: 0.004429, time: 2.235 s\n",
      "Step 226520 (epoch   70.79), loss: 0.008123, time: 2.243 s\n",
      "Step 226540 (epoch   70.79), loss: 0.003260, time: 2.230 s\n",
      "Step 226560 (epoch   70.80), loss: 0.003652, time: 2.238 s\n",
      "Step 226580 (epoch   70.81), loss: 0.003818, time: 2.237 s\n",
      "Step 226600 (epoch   70.81), loss: 0.003174, time: 2.236 s, error: 0.010552\n",
      "Step 226620 (epoch   70.82), loss: 0.016648, time: 15.043 s\n",
      "Step 226640 (epoch   70.83), loss: 0.004267, time: 2.239 s\n",
      "Step 226660 (epoch   70.83), loss: 0.005643, time: 2.239 s\n",
      "Step 226680 (epoch   70.84), loss: 0.004953, time: 2.257 s\n",
      "Step 226700 (epoch   70.84), loss: 0.004459, time: 2.258 s\n",
      "Step 226720 (epoch   70.85), loss: 0.004394, time: 2.240 s\n",
      "Step 226740 (epoch   70.86), loss: 0.007099, time: 2.237 s\n",
      "Step 226760 (epoch   70.86), loss: 0.004773, time: 2.238 s\n",
      "Step 226780 (epoch   70.87), loss: 0.006084, time: 2.246 s\n",
      "Step 226800 (epoch   70.88), loss: 0.004740, time: 2.245 s, error: 0.010480\n",
      "Step 226820 (epoch   70.88), loss: 0.002740, time: 15.107 s\n",
      "Step 226840 (epoch   70.89), loss: 0.006523, time: 2.257 s\n",
      "Step 226860 (epoch   70.89), loss: 0.003936, time: 2.245 s\n",
      "Step 226880 (epoch   70.90), loss: 0.003694, time: 2.244 s\n",
      "Step 226900 (epoch   70.91), loss: 0.003886, time: 2.249 s\n",
      "Step 226920 (epoch   70.91), loss: 0.004512, time: 2.244 s\n",
      "Step 226940 (epoch   70.92), loss: 0.004334, time: 2.259 s\n",
      "Step 226960 (epoch   70.92), loss: 0.003882, time: 2.254 s\n",
      "Step 226980 (epoch   70.93), loss: 0.003225, time: 2.248 s\n",
      "Step 227000 (epoch   70.94), loss: 0.003978, time: 2.239 s, error: 0.010830\n",
      "Step 227020 (epoch   70.94), loss: 0.003365, time: 15.032 s\n",
      "Step 227040 (epoch   70.95), loss: 0.011920, time: 2.243 s\n",
      "Step 227060 (epoch   70.96), loss: 0.011214, time: 2.232 s\n",
      "Step 227080 (epoch   70.96), loss: 0.003250, time: 2.240 s\n",
      "Step 227100 (epoch   70.97), loss: 0.004124, time: 2.236 s\n",
      "Step 227120 (epoch   70.97), loss: 0.003375, time: 2.237 s\n",
      "Step 227140 (epoch   70.98), loss: 0.005749, time: 2.225 s\n",
      "Step 227160 (epoch   70.99), loss: 0.004617, time: 2.250 s\n",
      "Step 227180 (epoch   70.99), loss: 0.003733, time: 2.233 s\n",
      "Step 227200 (epoch   71.00), loss: 0.003274, time: 2.256 s, error: 0.010444\n",
      "Step 227220 (epoch   71.01), loss: 0.003202, time: 15.146 s\n",
      "Step 227240 (epoch   71.01), loss: 0.003860, time: 2.238 s\n",
      "Step 227260 (epoch   71.02), loss: 0.005526, time: 2.243 s\n",
      "Step 227280 (epoch   71.03), loss: 0.003648, time: 2.235 s\n",
      "Step 227300 (epoch   71.03), loss: 0.004369, time: 2.242 s\n",
      "Step 227320 (epoch   71.04), loss: 0.006042, time: 2.234 s\n",
      "Step 227340 (epoch   71.04), loss: 0.004689, time: 2.241 s\n",
      "Step 227360 (epoch   71.05), loss: 0.005366, time: 2.243 s\n",
      "Step 227380 (epoch   71.06), loss: 0.005126, time: 2.231 s\n",
      "Step 227400 (epoch   71.06), loss: 0.003497, time: 2.253 s, error: 0.010376\n",
      "Step 227420 (epoch   71.07), loss: 0.003001, time: 15.238 s\n",
      "Step 227440 (epoch   71.08), loss: 0.003568, time: 2.245 s\n",
      "Step 227460 (epoch   71.08), loss: 0.009736, time: 2.238 s\n",
      "Step 227480 (epoch   71.09), loss: 0.006206, time: 2.247 s\n",
      "Step 227500 (epoch   71.09), loss: 0.004120, time: 2.236 s\n",
      "Step 227520 (epoch   71.10), loss: 0.005482, time: 2.237 s\n",
      "Step 227540 (epoch   71.11), loss: 0.002993, time: 2.243 s\n",
      "Step 227560 (epoch   71.11), loss: 0.003562, time: 2.241 s\n",
      "Step 227580 (epoch   71.12), loss: 0.003979, time: 2.227 s\n",
      "Step 227600 (epoch   71.12), loss: 0.003467, time: 2.236 s, error: 0.010478\n",
      "Step 227620 (epoch   71.13), loss: 0.004678, time: 15.135 s\n",
      "Step 227640 (epoch   71.14), loss: 0.003033, time: 2.239 s\n",
      "Step 227660 (epoch   71.14), loss: 0.003600, time: 2.245 s\n",
      "Step 227680 (epoch   71.15), loss: 0.004921, time: 2.247 s\n",
      "Step 227700 (epoch   71.16), loss: 0.007800, time: 2.229 s\n",
      "Step 227720 (epoch   71.16), loss: 0.004464, time: 2.253 s\n",
      "Step 227740 (epoch   71.17), loss: 0.003114, time: 2.243 s\n",
      "Step 227760 (epoch   71.17), loss: 0.008820, time: 2.242 s\n",
      "Step 227780 (epoch   71.18), loss: 0.003959, time: 2.243 s\n",
      "Step 227800 (epoch   71.19), loss: 0.007379, time: 2.254 s, error: 0.010627\n",
      "Step 227820 (epoch   71.19), loss: 0.003365, time: 15.127 s\n",
      "Step 227840 (epoch   71.20), loss: 0.003788, time: 2.226 s\n",
      "Step 227860 (epoch   71.21), loss: 0.008548, time: 2.246 s\n",
      "Step 227880 (epoch   71.21), loss: 0.005336, time: 2.250 s\n",
      "Step 227900 (epoch   71.22), loss: 0.007069, time: 2.254 s\n",
      "Step 227920 (epoch   71.22), loss: 0.004815, time: 2.234 s\n",
      "Step 227940 (epoch   71.23), loss: 0.005257, time: 2.240 s\n",
      "Step 227960 (epoch   71.24), loss: 0.004584, time: 2.237 s\n",
      "Step 227980 (epoch   71.24), loss: 0.004047, time: 2.252 s\n",
      "Step 228000 (epoch   71.25), loss: 0.005127, time: 2.232 s, error: 0.010771\n",
      "\n",
      "Time since beginning  : 39994.758 s\n",
      "\n",
      "Step 228020 (epoch   71.26), loss: 0.002604, time: 15.204 s\n",
      "Step 228040 (epoch   71.26), loss: 0.003314, time: 2.253 s\n",
      "Step 228060 (epoch   71.27), loss: 0.003655, time: 2.246 s\n",
      "Step 228080 (epoch   71.28), loss: 0.004689, time: 2.242 s\n",
      "Step 228100 (epoch   71.28), loss: 0.005577, time: 2.244 s\n",
      "Step 228120 (epoch   71.29), loss: 0.006335, time: 2.253 s\n",
      "Step 228140 (epoch   71.29), loss: 0.004063, time: 2.246 s\n",
      "Step 228160 (epoch   71.30), loss: 0.003439, time: 2.243 s\n",
      "Step 228180 (epoch   71.31), loss: 0.003089, time: 2.228 s\n",
      "Step 228200 (epoch   71.31), loss: 0.003262, time: 2.255 s, error: 0.010490\n",
      "Step 228220 (epoch   71.32), loss: 0.005156, time: 15.035 s\n",
      "Step 228240 (epoch   71.33), loss: 0.004305, time: 2.236 s\n",
      "Step 228260 (epoch   71.33), loss: 0.004522, time: 2.246 s\n",
      "Step 228280 (epoch   71.34), loss: 0.003139, time: 2.233 s\n",
      "Step 228300 (epoch   71.34), loss: 0.004497, time: 2.254 s\n",
      "Step 228320 (epoch   71.35), loss: 0.005591, time: 2.240 s\n",
      "Step 228340 (epoch   71.36), loss: 0.003223, time: 2.242 s\n",
      "Step 228360 (epoch   71.36), loss: 0.005545, time: 2.225 s\n",
      "Step 228380 (epoch   71.37), loss: 0.005915, time: 2.249 s\n",
      "Step 228400 (epoch   71.38), loss: 0.005930, time: 2.249 s, error: 0.010487\n",
      "Step 228420 (epoch   71.38), loss: 0.004460, time: 15.076 s\n",
      "Step 228440 (epoch   71.39), loss: 0.004631, time: 2.246 s\n",
      "Step 228460 (epoch   71.39), loss: 0.005573, time: 2.243 s\n",
      "Step 228480 (epoch   71.40), loss: 0.005096, time: 2.240 s\n",
      "Step 228500 (epoch   71.41), loss: 0.006918, time: 2.246 s\n",
      "Step 228520 (epoch   71.41), loss: 0.004926, time: 2.251 s\n",
      "Step 228540 (epoch   71.42), loss: 0.005441, time: 2.249 s\n",
      "Step 228560 (epoch   71.42), loss: 0.003809, time: 2.233 s\n",
      "Step 228580 (epoch   71.43), loss: 0.007081, time: 2.249 s\n",
      "Step 228600 (epoch   71.44), loss: 0.005751, time: 2.240 s, error: 0.010673\n",
      "Step 228620 (epoch   71.44), loss: 0.008812, time: 15.278 s\n",
      "Step 228640 (epoch   71.45), loss: 0.005520, time: 2.240 s\n",
      "Step 228660 (epoch   71.46), loss: 0.004714, time: 2.249 s\n",
      "Step 228680 (epoch   71.46), loss: 0.003185, time: 2.244 s\n",
      "Step 228700 (epoch   71.47), loss: 0.004156, time: 2.239 s\n",
      "Step 228720 (epoch   71.47), loss: 0.006872, time: 2.249 s\n",
      "Step 228740 (epoch   71.48), loss: 0.005168, time: 2.228 s\n",
      "Step 228760 (epoch   71.49), loss: 0.005589, time: 2.242 s\n",
      "Step 228780 (epoch   71.49), loss: 0.004073, time: 2.232 s\n",
      "Step 228800 (epoch   71.50), loss: 0.006426, time: 2.245 s, error: 0.010265\n",
      "Step 228820 (epoch   71.51), loss: 0.005283, time: 15.252 s\n",
      "Step 228840 (epoch   71.51), loss: 0.003416, time: 2.240 s\n",
      "Step 228860 (epoch   71.52), loss: 0.004299, time: 2.242 s\n",
      "Step 228880 (epoch   71.53), loss: 0.003344, time: 2.232 s\n",
      "Step 228900 (epoch   71.53), loss: 0.003632, time: 2.243 s\n",
      "Step 228920 (epoch   71.54), loss: 0.003018, time: 2.236 s\n",
      "Step 228940 (epoch   71.54), loss: 0.003513, time: 2.245 s\n",
      "Step 228960 (epoch   71.55), loss: 0.005592, time: 2.231 s\n",
      "Step 228980 (epoch   71.56), loss: 0.003450, time: 2.241 s\n",
      "Step 229000 (epoch   71.56), loss: 0.003720, time: 2.236 s, error: 0.010779\n",
      "Step 229020 (epoch   71.57), loss: 0.004271, time: 15.074 s\n",
      "Step 229040 (epoch   71.58), loss: 0.006777, time: 2.258 s\n",
      "Step 229060 (epoch   71.58), loss: 0.004468, time: 2.247 s\n",
      "Step 229080 (epoch   71.59), loss: 0.003689, time: 2.249 s\n",
      "Step 229100 (epoch   71.59), loss: 0.003094, time: 2.235 s\n",
      "Step 229120 (epoch   71.60), loss: 0.004910, time: 2.244 s\n",
      "Step 229140 (epoch   71.61), loss: 0.003735, time: 2.229 s\n",
      "Step 229160 (epoch   71.61), loss: 0.005163, time: 2.249 s\n",
      "Step 229180 (epoch   71.62), loss: 0.012748, time: 2.246 s\n",
      "Step 229200 (epoch   71.62), loss: 0.005247, time: 2.243 s, error: 0.011092\n",
      "Step 229220 (epoch   71.63), loss: 0.006095, time: 15.084 s\n",
      "Step 229240 (epoch   71.64), loss: 0.004134, time: 2.232 s\n",
      "Step 229260 (epoch   71.64), loss: 0.004608, time: 2.245 s\n",
      "Step 229280 (epoch   71.65), loss: 0.006418, time: 2.243 s\n",
      "Step 229300 (epoch   71.66), loss: 0.004490, time: 2.252 s\n",
      "Step 229320 (epoch   71.66), loss: 0.002723, time: 2.246 s\n",
      "Step 229340 (epoch   71.67), loss: 0.004368, time: 2.245 s\n",
      "Step 229360 (epoch   71.67), loss: 0.006678, time: 2.254 s\n",
      "Step 229380 (epoch   71.68), loss: 0.003334, time: 2.253 s\n",
      "Step 229400 (epoch   71.69), loss: 0.004124, time: 2.240 s, error: 0.010507\n",
      "Step 229420 (epoch   71.69), loss: 0.002892, time: 15.058 s\n",
      "Step 229440 (epoch   71.70), loss: 0.007802, time: 2.249 s\n",
      "Step 229460 (epoch   71.71), loss: 0.005991, time: 2.243 s\n",
      "Step 229480 (epoch   71.71), loss: 0.005038, time: 2.239 s\n",
      "Step 229500 (epoch   71.72), loss: 0.004045, time: 2.247 s\n",
      "Step 229520 (epoch   71.72), loss: 0.004009, time: 2.246 s\n",
      "Step 229540 (epoch   71.73), loss: 0.004207, time: 2.241 s\n",
      "Step 229560 (epoch   71.74), loss: 0.003459, time: 2.254 s\n",
      "Step 229580 (epoch   71.74), loss: 0.003153, time: 2.247 s\n",
      "Step 229600 (epoch   71.75), loss: 0.005490, time: 2.245 s, error: 0.010907\n",
      "Step 229620 (epoch   71.76), loss: 0.004958, time: 15.082 s\n",
      "Step 229640 (epoch   71.76), loss: 0.003868, time: 2.250 s\n",
      "Step 229660 (epoch   71.77), loss: 0.002643, time: 2.250 s\n",
      "Step 229680 (epoch   71.78), loss: 0.005894, time: 2.226 s\n",
      "Step 229700 (epoch   71.78), loss: 0.003964, time: 2.243 s\n",
      "Step 229720 (epoch   71.79), loss: 0.003255, time: 2.234 s\n",
      "Step 229740 (epoch   71.79), loss: 0.004355, time: 2.243 s\n",
      "Step 229760 (epoch   71.80), loss: 0.003221, time: 2.232 s\n",
      "Step 229780 (epoch   71.81), loss: 0.002842, time: 2.245 s\n",
      "Step 229800 (epoch   71.81), loss: 0.004502, time: 2.243 s, error: 0.010571\n",
      "Step 229820 (epoch   71.82), loss: 0.002889, time: 15.285 s\n",
      "Step 229840 (epoch   71.83), loss: 0.002713, time: 2.239 s\n",
      "Step 229860 (epoch   71.83), loss: 0.003773, time: 2.242 s\n",
      "Step 229880 (epoch   71.84), loss: 0.002988, time: 2.257 s\n",
      "Step 229900 (epoch   71.84), loss: 0.004269, time: 2.252 s\n",
      "Step 229920 (epoch   71.85), loss: 0.003010, time: 2.256 s\n",
      "Step 229940 (epoch   71.86), loss: 0.005171, time: 2.248 s\n",
      "Step 229960 (epoch   71.86), loss: 0.003245, time: 2.243 s\n",
      "Step 229980 (epoch   71.87), loss: 0.003503, time: 2.232 s\n",
      "Step 230000 (epoch   71.88), loss: 0.003199, time: 2.228 s, error: 0.010291\n",
      "\n",
      "Time since beginning  : 40348.182 s\n",
      "\n",
      "Step 230020 (epoch   71.88), loss: 0.003727, time: 15.313 s\n",
      "Step 230040 (epoch   71.89), loss: 0.003945, time: 2.231 s\n",
      "Step 230060 (epoch   71.89), loss: 0.008133, time: 2.247 s\n",
      "Step 230080 (epoch   71.90), loss: 0.006084, time: 2.230 s\n",
      "Step 230100 (epoch   71.91), loss: 0.004611, time: 2.239 s\n",
      "Step 230120 (epoch   71.91), loss: 0.004376, time: 2.237 s\n",
      "Step 230140 (epoch   71.92), loss: 0.002539, time: 2.250 s\n",
      "Step 230160 (epoch   71.92), loss: 0.004276, time: 2.246 s\n",
      "Step 230180 (epoch   71.93), loss: 0.007256, time: 2.233 s\n",
      "Step 230200 (epoch   71.94), loss: 0.005080, time: 2.247 s, error: 0.010739\n",
      "Step 230220 (epoch   71.94), loss: 0.003124, time: 15.077 s\n",
      "Step 230240 (epoch   71.95), loss: 0.004407, time: 2.254 s\n",
      "Step 230260 (epoch   71.96), loss: 0.005818, time: 2.247 s\n",
      "Step 230280 (epoch   71.96), loss: 0.013477, time: 2.250 s\n",
      "Step 230300 (epoch   71.97), loss: 0.003727, time: 2.227 s\n",
      "Step 230320 (epoch   71.97), loss: 0.003816, time: 2.248 s\n",
      "Step 230340 (epoch   71.98), loss: 0.003512, time: 2.231 s\n",
      "Step 230360 (epoch   71.99), loss: 0.003301, time: 2.240 s\n",
      "Step 230380 (epoch   71.99), loss: 0.004099, time: 2.246 s\n",
      "Step 230400 (epoch   72.00), loss: 0.003621, time: 2.230 s, error: 0.010432\n",
      "Step 230420 (epoch   72.01), loss: 0.005124, time: 15.035 s\n",
      "Step 230440 (epoch   72.01), loss: 0.006332, time: 2.237 s\n",
      "Step 230460 (epoch   72.02), loss: 0.003036, time: 2.234 s\n",
      "Step 230480 (epoch   72.03), loss: 0.004256, time: 2.252 s\n",
      "Step 230500 (epoch   72.03), loss: 0.002590, time: 2.256 s\n",
      "Step 230520 (epoch   72.04), loss: 0.005029, time: 2.252 s\n",
      "Step 230540 (epoch   72.04), loss: 0.005779, time: 2.226 s\n",
      "Step 230560 (epoch   72.05), loss: 0.003507, time: 2.251 s\n",
      "Step 230580 (epoch   72.06), loss: 0.004781, time: 2.234 s\n",
      "Step 230600 (epoch   72.06), loss: 0.004631, time: 2.245 s, error: 0.010421\n",
      "Step 230620 (epoch   72.07), loss: 0.003127, time: 15.017 s\n",
      "Step 230640 (epoch   72.08), loss: 0.002550, time: 2.240 s\n",
      "Step 230660 (epoch   72.08), loss: 0.003417, time: 2.246 s\n",
      "Step 230680 (epoch   72.09), loss: 0.005692, time: 2.236 s\n",
      "Step 230700 (epoch   72.09), loss: 0.003494, time: 2.247 s\n",
      "Step 230720 (epoch   72.10), loss: 0.009514, time: 2.234 s\n",
      "Step 230740 (epoch   72.11), loss: 0.005103, time: 2.253 s\n",
      "Step 230760 (epoch   72.11), loss: 0.007051, time: 2.260 s\n",
      "Step 230780 (epoch   72.12), loss: 0.006896, time: 2.243 s\n",
      "Step 230800 (epoch   72.12), loss: 0.003824, time: 2.226 s, error: 0.010547\n",
      "Step 230820 (epoch   72.13), loss: 0.006326, time: 15.043 s\n",
      "Step 230840 (epoch   72.14), loss: 0.007100, time: 2.242 s\n",
      "Step 230860 (epoch   72.14), loss: 0.003492, time: 2.238 s\n",
      "Step 230880 (epoch   72.15), loss: 0.004288, time: 2.242 s\n",
      "Step 230900 (epoch   72.16), loss: 0.006119, time: 2.240 s\n",
      "Step 230920 (epoch   72.16), loss: 0.003414, time: 2.249 s\n",
      "Step 230940 (epoch   72.17), loss: 0.005404, time: 2.240 s\n",
      "Step 230960 (epoch   72.17), loss: 0.004713, time: 2.238 s\n",
      "Step 230980 (epoch   72.18), loss: 0.002835, time: 2.235 s\n",
      "Step 231000 (epoch   72.19), loss: 0.005220, time: 2.245 s, error: 0.010632\n",
      "Step 231020 (epoch   72.19), loss: 0.004939, time: 15.154 s\n",
      "Step 231040 (epoch   72.20), loss: 0.008297, time: 2.235 s\n",
      "Step 231060 (epoch   72.21), loss: 0.002931, time: 2.248 s\n",
      "Step 231080 (epoch   72.21), loss: 0.003640, time: 2.224 s\n",
      "Step 231100 (epoch   72.22), loss: 0.005534, time: 2.249 s\n",
      "Step 231120 (epoch   72.22), loss: 0.006761, time: 2.229 s\n",
      "Step 231140 (epoch   72.23), loss: 0.003800, time: 2.240 s\n",
      "Step 231160 (epoch   72.24), loss: 0.004041, time: 2.242 s\n",
      "Step 231180 (epoch   72.24), loss: 0.005188, time: 2.247 s\n",
      "Step 231200 (epoch   72.25), loss: 0.003466, time: 2.240 s, error: 0.010575\n",
      "Step 231220 (epoch   72.26), loss: 0.005650, time: 15.244 s\n",
      "Step 231240 (epoch   72.26), loss: 0.003830, time: 2.235 s\n",
      "Step 231260 (epoch   72.27), loss: 0.002873, time: 2.239 s\n",
      "Step 231280 (epoch   72.28), loss: 0.006509, time: 2.235 s\n",
      "Step 231300 (epoch   72.28), loss: 0.004241, time: 2.235 s\n",
      "Step 231320 (epoch   72.29), loss: 0.003230, time: 2.242 s\n",
      "Step 231340 (epoch   72.29), loss: 0.004839, time: 2.239 s\n",
      "Step 231360 (epoch   72.30), loss: 0.004650, time: 2.245 s\n",
      "Step 231380 (epoch   72.31), loss: 0.003039, time: 2.234 s\n",
      "Step 231400 (epoch   72.31), loss: 0.006692, time: 2.244 s, error: 0.010505\n",
      "Step 231420 (epoch   72.32), loss: 0.004253, time: 15.103 s\n",
      "Step 231440 (epoch   72.33), loss: 0.004831, time: 2.230 s\n",
      "Step 231460 (epoch   72.33), loss: 0.004570, time: 2.242 s\n",
      "Step 231480 (epoch   72.34), loss: 0.003333, time: 2.233 s\n",
      "Step 231500 (epoch   72.34), loss: 0.003162, time: 2.233 s\n",
      "Step 231520 (epoch   72.35), loss: 0.003639, time: 2.238 s\n",
      "Step 231540 (epoch   72.36), loss: 0.003873, time: 2.239 s\n",
      "Step 231560 (epoch   72.36), loss: 0.007788, time: 2.236 s\n",
      "Step 231580 (epoch   72.37), loss: 0.003799, time: 2.246 s\n",
      "Step 231600 (epoch   72.38), loss: 0.005602, time: 2.243 s, error: 0.010462\n",
      "Step 231620 (epoch   72.38), loss: 0.003157, time: 15.040 s\n",
      "Step 231640 (epoch   72.39), loss: 0.003975, time: 2.247 s\n",
      "Step 231660 (epoch   72.39), loss: 0.005331, time: 2.232 s\n",
      "Step 231680 (epoch   72.40), loss: 0.003526, time: 2.260 s\n",
      "Step 231700 (epoch   72.41), loss: 0.004915, time: 2.227 s\n",
      "Step 231720 (epoch   72.41), loss: 0.003368, time: 2.243 s\n",
      "Step 231740 (epoch   72.42), loss: 0.006577, time: 2.234 s\n",
      "Step 231760 (epoch   72.42), loss: 0.004734, time: 2.232 s\n",
      "Step 231780 (epoch   72.43), loss: 0.006094, time: 2.242 s\n",
      "Step 231800 (epoch   72.44), loss: 0.004964, time: 2.242 s, error: 0.010552\n",
      "Step 231820 (epoch   72.44), loss: 0.003515, time: 15.065 s\n",
      "Step 231840 (epoch   72.45), loss: 0.002996, time: 2.243 s\n",
      "Step 231860 (epoch   72.46), loss: 0.005132, time: 2.242 s\n",
      "Step 231880 (epoch   72.46), loss: 0.009124, time: 2.236 s\n",
      "Step 231900 (epoch   72.47), loss: 0.002744, time: 2.251 s\n",
      "Step 231920 (epoch   72.47), loss: 0.004223, time: 2.247 s\n",
      "Step 231940 (epoch   72.48), loss: 0.005668, time: 2.259 s\n",
      "Step 231960 (epoch   72.49), loss: 0.003739, time: 2.255 s\n",
      "Step 231980 (epoch   72.49), loss: 0.003931, time: 2.243 s\n",
      "Step 232000 (epoch   72.50), loss: 0.004679, time: 2.240 s, error: 0.010273\n",
      "\n",
      "Time since beginning  : 40700.761 s\n",
      "\n",
      "Step 232020 (epoch   72.51), loss: 0.003897, time: 15.107 s\n",
      "Step 232040 (epoch   72.51), loss: 0.005323, time: 2.251 s\n",
      "Step 232060 (epoch   72.52), loss: 0.004828, time: 2.241 s\n",
      "Step 232080 (epoch   72.53), loss: 0.003326, time: 2.227 s\n",
      "Step 232100 (epoch   72.53), loss: 0.004759, time: 2.247 s\n",
      "Step 232120 (epoch   72.54), loss: 0.004808, time: 2.237 s\n",
      "Step 232140 (epoch   72.54), loss: 0.003063, time: 2.239 s\n",
      "Step 232160 (epoch   72.55), loss: 0.004131, time: 2.237 s\n",
      "Step 232180 (epoch   72.56), loss: 0.003412, time: 2.237 s\n",
      "Step 232200 (epoch   72.56), loss: 0.005503, time: 2.256 s, error: 0.010762\n",
      "Step 232220 (epoch   72.57), loss: 0.004959, time: 15.046 s\n",
      "Step 232240 (epoch   72.58), loss: 0.003668, time: 2.245 s\n",
      "Step 232260 (epoch   72.58), loss: 0.005638, time: 2.238 s\n",
      "Step 232280 (epoch   72.59), loss: 0.005118, time: 2.246 s\n",
      "Step 232300 (epoch   72.59), loss: 0.006474, time: 2.255 s\n",
      "Step 232320 (epoch   72.60), loss: 0.002682, time: 2.241 s\n",
      "Step 232340 (epoch   72.61), loss: 0.006638, time: 2.255 s\n",
      "Step 232360 (epoch   72.61), loss: 0.005228, time: 2.246 s\n",
      "Step 232380 (epoch   72.62), loss: 0.005457, time: 2.233 s\n",
      "Step 232400 (epoch   72.62), loss: 0.004565, time: 2.254 s, error: 0.011073\n",
      "Step 232420 (epoch   72.63), loss: 0.002796, time: 15.253 s\n",
      "Step 232440 (epoch   72.64), loss: 0.004119, time: 2.251 s\n",
      "Step 232460 (epoch   72.64), loss: 0.004228, time: 2.256 s\n",
      "Step 232480 (epoch   72.65), loss: 0.003914, time: 2.231 s\n",
      "Step 232500 (epoch   72.66), loss: 0.005970, time: 2.238 s\n",
      "Step 232520 (epoch   72.66), loss: 0.003325, time: 2.238 s\n",
      "Step 232540 (epoch   72.67), loss: 0.004291, time: 2.254 s\n",
      "Step 232560 (epoch   72.67), loss: 0.004433, time: 2.235 s\n",
      "Step 232580 (epoch   72.68), loss: 0.004117, time: 2.244 s\n",
      "Step 232600 (epoch   72.69), loss: 0.004842, time: 2.240 s, error: 0.010448\n",
      "Step 232620 (epoch   72.69), loss: 0.002617, time: 15.215 s\n",
      "Step 232640 (epoch   72.70), loss: 0.006870, time: 2.237 s\n",
      "Step 232660 (epoch   72.71), loss: 0.005457, time: 2.248 s\n",
      "Step 232680 (epoch   72.71), loss: 0.006429, time: 2.253 s\n",
      "Step 232700 (epoch   72.72), loss: 0.005914, time: 2.240 s\n",
      "Step 232720 (epoch   72.72), loss: 0.004004, time: 2.239 s\n",
      "Step 232740 (epoch   72.73), loss: 0.003933, time: 2.240 s\n",
      "Step 232760 (epoch   72.74), loss: 0.004103, time: 2.243 s\n",
      "Step 232780 (epoch   72.74), loss: 0.002488, time: 2.256 s\n",
      "Step 232800 (epoch   72.75), loss: 0.004084, time: 2.248 s, error: 0.011011\n",
      "Step 232820 (epoch   72.76), loss: 0.003035, time: 15.094 s\n",
      "Step 232840 (epoch   72.76), loss: 0.004824, time: 2.251 s\n",
      "Step 232860 (epoch   72.77), loss: 0.004727, time: 2.248 s\n",
      "Step 232880 (epoch   72.78), loss: 0.002873, time: 2.253 s\n",
      "Step 232900 (epoch   72.78), loss: 0.005246, time: 2.236 s\n",
      "Step 232920 (epoch   72.79), loss: 0.003595, time: 2.243 s\n",
      "Step 232940 (epoch   72.79), loss: 0.003598, time: 2.234 s\n",
      "Step 232960 (epoch   72.80), loss: 0.004880, time: 2.240 s\n",
      "Step 232980 (epoch   72.81), loss: 0.004480, time: 2.245 s\n",
      "Step 233000 (epoch   72.81), loss: 0.005286, time: 2.243 s, error: 0.010666\n",
      "Step 233020 (epoch   72.82), loss: 0.003165, time: 15.015 s\n",
      "Step 233040 (epoch   72.83), loss: 0.004558, time: 2.244 s\n",
      "Step 233060 (epoch   72.83), loss: 0.004643, time: 2.234 s\n",
      "Step 233080 (epoch   72.84), loss: 0.003040, time: 2.240 s\n",
      "Step 233100 (epoch   72.84), loss: 0.005413, time: 2.238 s\n",
      "Step 233120 (epoch   72.85), loss: 0.004880, time: 2.248 s\n",
      "Step 233140 (epoch   72.86), loss: 0.004323, time: 2.233 s\n",
      "Step 233160 (epoch   72.86), loss: 0.003101, time: 2.247 s\n",
      "Step 233180 (epoch   72.87), loss: 0.005979, time: 2.230 s\n",
      "Step 233200 (epoch   72.88), loss: 0.003055, time: 2.248 s, error: 0.010225\n",
      "Step 233220 (epoch   72.88), loss: 0.006112, time: 15.020 s\n",
      "Step 233240 (epoch   72.89), loss: 0.004017, time: 2.247 s\n",
      "Step 233260 (epoch   72.89), loss: 0.004202, time: 2.235 s\n",
      "Step 233280 (epoch   72.90), loss: 0.006033, time: 2.230 s\n",
      "Step 233300 (epoch   72.91), loss: 0.002640, time: 2.238 s\n",
      "Step 233320 (epoch   72.91), loss: 0.003510, time: 2.241 s\n",
      "Step 233340 (epoch   72.92), loss: 0.004353, time: 2.243 s\n",
      "Step 233360 (epoch   72.92), loss: 0.002464, time: 2.228 s\n",
      "Step 233380 (epoch   72.93), loss: 0.003625, time: 2.259 s\n",
      "Step 233400 (epoch   72.94), loss: 0.003507, time: 2.242 s, error: 0.010634\n",
      "Step 233420 (epoch   72.94), loss: 0.008173, time: 15.026 s\n",
      "Step 233440 (epoch   72.95), loss: 0.004365, time: 2.232 s\n",
      "Step 233460 (epoch   72.96), loss: 0.004764, time: 2.238 s\n",
      "Step 233480 (epoch   72.96), loss: 0.003670, time: 2.245 s\n",
      "Step 233500 (epoch   72.97), loss: 0.003652, time: 2.222 s\n",
      "Step 233520 (epoch   72.97), loss: 0.004894, time: 2.236 s\n",
      "Step 233540 (epoch   72.98), loss: 0.004980, time: 2.226 s\n",
      "Step 233560 (epoch   72.99), loss: 0.004590, time: 2.233 s\n",
      "Step 233580 (epoch   72.99), loss: 0.003510, time: 2.231 s\n",
      "Step 233600 (epoch   73.00), loss: 0.004166, time: 2.246 s, error: 0.010469\n",
      "Step 233620 (epoch   73.01), loss: 0.003650, time: 15.273 s\n",
      "Step 233640 (epoch   73.01), loss: 0.002915, time: 2.236 s\n",
      "Step 233660 (epoch   73.02), loss: 0.006134, time: 2.249 s\n",
      "Step 233680 (epoch   73.03), loss: 0.004663, time: 2.249 s\n",
      "Step 233700 (epoch   73.03), loss: 0.004268, time: 2.241 s\n",
      "Step 233720 (epoch   73.04), loss: 0.006693, time: 2.234 s\n",
      "Step 233740 (epoch   73.04), loss: 0.003928, time: 2.230 s\n",
      "Step 233760 (epoch   73.05), loss: 0.004059, time: 2.240 s\n",
      "Step 233780 (epoch   73.06), loss: 0.002259, time: 2.229 s\n",
      "Step 233800 (epoch   73.06), loss: 0.002765, time: 2.252 s, error: 0.010475\n",
      "Step 233820 (epoch   73.07), loss: 0.003086, time: 15.243 s\n",
      "Step 233840 (epoch   73.08), loss: 0.005609, time: 2.241 s\n",
      "Step 233860 (epoch   73.08), loss: 0.003599, time: 2.232 s\n",
      "Step 233880 (epoch   73.09), loss: 0.008409, time: 2.226 s\n",
      "Step 233900 (epoch   73.09), loss: 0.004369, time: 2.245 s\n",
      "Step 233920 (epoch   73.10), loss: 0.004037, time: 2.236 s\n",
      "Step 233940 (epoch   73.11), loss: 0.003403, time: 2.245 s\n",
      "Step 233960 (epoch   73.11), loss: 0.004017, time: 2.235 s\n",
      "Step 233980 (epoch   73.12), loss: 0.005174, time: 2.247 s\n",
      "Step 234000 (epoch   73.12), loss: 0.004817, time: 2.232 s, error: 0.010509\n",
      "\n",
      "Time since beginning  : 41053.792 s\n",
      "\n",
      "Step 234020 (epoch   73.13), loss: 0.004658, time: 15.128 s\n",
      "Step 234040 (epoch   73.14), loss: 0.003464, time: 2.259 s\n",
      "Step 234060 (epoch   73.14), loss: 0.005364, time: 2.251 s\n",
      "Step 234080 (epoch   73.15), loss: 0.007022, time: 2.248 s\n",
      "Step 234100 (epoch   73.16), loss: 0.002676, time: 2.237 s\n",
      "Step 234120 (epoch   73.16), loss: 0.004695, time: 2.241 s\n",
      "Step 234140 (epoch   73.17), loss: 0.003645, time: 2.230 s\n",
      "Step 234160 (epoch   73.17), loss: 0.006478, time: 2.245 s\n",
      "Step 234180 (epoch   73.18), loss: 0.006463, time: 2.227 s\n",
      "Step 234200 (epoch   73.19), loss: 0.008102, time: 2.243 s, error: 0.010646\n",
      "Step 234220 (epoch   73.19), loss: 0.004119, time: 15.069 s\n",
      "Step 234240 (epoch   73.20), loss: 0.004172, time: 2.233 s\n",
      "Step 234260 (epoch   73.21), loss: 0.003718, time: 2.248 s\n",
      "Step 234280 (epoch   73.21), loss: 0.002780, time: 2.237 s\n",
      "Step 234300 (epoch   73.22), loss: 0.005250, time: 2.259 s\n",
      "Step 234320 (epoch   73.22), loss: 0.004676, time: 2.226 s\n",
      "Step 234340 (epoch   73.23), loss: 0.007047, time: 2.253 s\n",
      "Step 234360 (epoch   73.24), loss: 0.006397, time: 2.232 s\n",
      "Step 234380 (epoch   73.24), loss: 0.003940, time: 2.250 s\n",
      "Step 234400 (epoch   73.25), loss: 0.003196, time: 2.232 s, error: 0.010506\n",
      "Step 234420 (epoch   73.26), loss: 0.003631, time: 15.021 s\n",
      "Step 234440 (epoch   73.26), loss: 0.010217, time: 2.242 s\n",
      "Step 234460 (epoch   73.27), loss: 0.005779, time: 2.233 s\n",
      "Step 234480 (epoch   73.28), loss: 0.006027, time: 2.242 s\n",
      "Step 234500 (epoch   73.28), loss: 0.004314, time: 2.245 s\n",
      "Step 234520 (epoch   73.29), loss: 0.007670, time: 2.243 s\n",
      "Step 234540 (epoch   73.29), loss: 0.006172, time: 2.232 s\n",
      "Step 234560 (epoch   73.30), loss: 0.003935, time: 2.248 s\n",
      "Step 234580 (epoch   73.31), loss: 0.004666, time: 2.239 s\n",
      "Step 234600 (epoch   73.31), loss: 0.005057, time: 2.249 s, error: 0.010488\n",
      "Step 234620 (epoch   73.32), loss: 0.003040, time: 15.071 s\n",
      "Step 234640 (epoch   73.33), loss: 0.005134, time: 2.244 s\n",
      "Step 234660 (epoch   73.33), loss: 0.005105, time: 2.244 s\n",
      "Step 234680 (epoch   73.34), loss: 0.004309, time: 2.246 s\n",
      "Step 234700 (epoch   73.34), loss: 0.002933, time: 2.244 s\n",
      "Step 234720 (epoch   73.35), loss: 0.004341, time: 2.236 s\n",
      "Step 234740 (epoch   73.36), loss: 0.004300, time: 2.241 s\n",
      "Step 234760 (epoch   73.36), loss: 0.004577, time: 2.239 s\n",
      "Step 234780 (epoch   73.37), loss: 0.004008, time: 2.242 s\n",
      "Step 234800 (epoch   73.38), loss: 0.002899, time: 2.227 s, error: 0.010603\n",
      "Step 234820 (epoch   73.38), loss: 0.003605, time: 15.218 s\n",
      "Step 234840 (epoch   73.39), loss: 0.003663, time: 2.234 s\n",
      "Step 234860 (epoch   73.39), loss: 0.004474, time: 2.244 s\n",
      "Step 234880 (epoch   73.40), loss: 0.004169, time: 2.239 s\n",
      "Step 234900 (epoch   73.41), loss: 0.003156, time: 2.239 s\n",
      "Step 234920 (epoch   73.41), loss: 0.002450, time: 2.242 s\n",
      "Step 234940 (epoch   73.42), loss: 0.005438, time: 2.240 s\n",
      "Step 234960 (epoch   73.42), loss: 0.005480, time: 2.240 s\n",
      "Step 234980 (epoch   73.43), loss: 0.005330, time: 2.241 s\n",
      "Step 235000 (epoch   73.44), loss: 0.003823, time: 2.248 s, error: 0.010542\n",
      "Step 235020 (epoch   73.44), loss: 0.008104, time: 15.251 s\n",
      "Step 235040 (epoch   73.45), loss: 0.005226, time: 2.227 s\n",
      "Step 235060 (epoch   73.46), loss: 0.006512, time: 2.244 s\n",
      "Step 235080 (epoch   73.46), loss: 0.004478, time: 2.244 s\n",
      "Step 235100 (epoch   73.47), loss: 0.003859, time: 2.251 s\n",
      "Step 235120 (epoch   73.47), loss: 0.005971, time: 2.242 s\n",
      "Step 235140 (epoch   73.48), loss: 0.003146, time: 2.242 s\n",
      "Step 235160 (epoch   73.49), loss: 0.003303, time: 2.234 s\n",
      "Step 235180 (epoch   73.49), loss: 0.003620, time: 2.250 s\n",
      "Step 235200 (epoch   73.50), loss: 0.006292, time: 2.239 s, error: 0.010275\n",
      "Step 235220 (epoch   73.51), loss: 0.003588, time: 15.065 s\n",
      "Step 235240 (epoch   73.51), loss: 0.003237, time: 2.245 s\n",
      "Step 235260 (epoch   73.52), loss: 0.004699, time: 2.224 s\n",
      "Step 235280 (epoch   73.53), loss: 0.004693, time: 2.237 s\n",
      "Step 235300 (epoch   73.53), loss: 0.003874, time: 2.247 s\n",
      "Step 235320 (epoch   73.54), loss: 0.004233, time: 2.249 s\n",
      "Step 235340 (epoch   73.54), loss: 0.005032, time: 2.231 s\n",
      "Step 235360 (epoch   73.55), loss: 0.003822, time: 2.245 s\n",
      "Step 235380 (epoch   73.56), loss: 0.007047, time: 2.231 s\n",
      "Step 235400 (epoch   73.56), loss: 0.003557, time: 2.244 s, error: 0.010653\n",
      "Step 235420 (epoch   73.57), loss: 0.004103, time: 15.059 s\n",
      "Step 235440 (epoch   73.58), loss: 0.005055, time: 2.236 s\n",
      "Step 235460 (epoch   73.58), loss: 0.004155, time: 2.246 s\n",
      "Step 235480 (epoch   73.59), loss: 0.004648, time: 2.247 s\n",
      "Step 235500 (epoch   73.59), loss: 0.004113, time: 2.240 s\n",
      "Step 235520 (epoch   73.60), loss: 0.006900, time: 2.235 s\n",
      "Step 235540 (epoch   73.61), loss: 0.004071, time: 2.249 s\n",
      "Step 235560 (epoch   73.61), loss: 0.003817, time: 2.253 s\n",
      "Step 235580 (epoch   73.62), loss: 0.004577, time: 2.242 s\n",
      "Step 235600 (epoch   73.62), loss: 0.003566, time: 2.233 s, error: 0.011006\n",
      "Step 235620 (epoch   73.63), loss: 0.006108, time: 15.079 s\n",
      "Step 235640 (epoch   73.64), loss: 0.004748, time: 2.249 s\n",
      "Step 235660 (epoch   73.64), loss: 0.006294, time: 2.240 s\n",
      "Step 235680 (epoch   73.65), loss: 0.004259, time: 2.249 s\n",
      "Step 235700 (epoch   73.66), loss: 0.004512, time: 2.238 s\n",
      "Step 235720 (epoch   73.66), loss: 0.005454, time: 2.238 s\n",
      "Step 235740 (epoch   73.67), loss: 0.005549, time: 2.259 s\n",
      "Step 235760 (epoch   73.67), loss: 0.003784, time: 2.250 s\n",
      "Step 235780 (epoch   73.68), loss: 0.003316, time: 2.234 s\n",
      "Step 235800 (epoch   73.69), loss: 0.005767, time: 2.243 s, error: 0.010311\n",
      "Step 235820 (epoch   73.69), loss: 0.004527, time: 15.018 s\n",
      "Step 235840 (epoch   73.70), loss: 0.004940, time: 2.253 s\n",
      "Step 235860 (epoch   73.71), loss: 0.005481, time: 2.243 s\n",
      "Step 235880 (epoch   73.71), loss: 0.004483, time: 2.256 s\n",
      "Step 235900 (epoch   73.72), loss: 0.008038, time: 2.254 s\n",
      "Step 235920 (epoch   73.72), loss: 0.004713, time: 2.242 s\n",
      "Step 235940 (epoch   73.73), loss: 0.002828, time: 2.246 s\n",
      "Step 235960 (epoch   73.74), loss: 0.004874, time: 2.243 s\n",
      "Step 235980 (epoch   73.74), loss: 0.003996, time: 2.216 s\n",
      "Step 236000 (epoch   73.75), loss: 0.004355, time: 2.246 s, error: 0.010987\n",
      "\n",
      "Time since beginning  : 41406.542 s\n",
      "\n",
      "Step 236020 (epoch   73.76), loss: 0.004693, time: 15.154 s\n",
      "Step 236040 (epoch   73.76), loss: 0.003165, time: 2.242 s\n",
      "Step 236060 (epoch   73.77), loss: 0.003103, time: 2.239 s\n",
      "Step 236080 (epoch   73.78), loss: 0.003068, time: 2.228 s\n",
      "Step 236100 (epoch   73.78), loss: 0.008753, time: 2.242 s\n",
      "Step 236120 (epoch   73.79), loss: 0.004385, time: 2.241 s\n",
      "Step 236140 (epoch   73.79), loss: 0.003772, time: 2.249 s\n",
      "Step 236160 (epoch   73.80), loss: 0.003484, time: 2.232 s\n",
      "Step 236180 (epoch   73.81), loss: 0.002928, time: 2.240 s\n",
      "Step 236200 (epoch   73.81), loss: 0.003932, time: 2.238 s, error: 0.010554\n",
      "Step 236220 (epoch   73.82), loss: 0.006086, time: 15.240 s\n",
      "Step 236240 (epoch   73.83), loss: 0.005378, time: 2.234 s\n",
      "Step 236260 (epoch   73.83), loss: 0.005353, time: 2.234 s\n",
      "Step 236280 (epoch   73.84), loss: 0.004436, time: 2.242 s\n",
      "Step 236300 (epoch   73.84), loss: 0.004053, time: 2.247 s\n",
      "Step 236320 (epoch   73.85), loss: 0.007564, time: 2.248 s\n",
      "Step 236340 (epoch   73.86), loss: 0.003517, time: 2.242 s\n",
      "Step 236360 (epoch   73.86), loss: 0.003975, time: 2.244 s\n",
      "Step 236380 (epoch   73.87), loss: 0.003965, time: 2.231 s\n",
      "Step 236400 (epoch   73.88), loss: 0.003104, time: 2.247 s, error: 0.010184\n",
      "Step 236420 (epoch   73.88), loss: 0.004254, time: 15.230 s\n",
      "Step 236440 (epoch   73.89), loss: 0.004548, time: 2.255 s\n",
      "Step 236460 (epoch   73.89), loss: 0.005001, time: 2.246 s\n",
      "Step 236480 (epoch   73.90), loss: 0.003737, time: 2.234 s\n",
      "Step 236500 (epoch   73.91), loss: 0.003962, time: 2.238 s\n",
      "Step 236520 (epoch   73.91), loss: 0.003988, time: 2.231 s\n",
      "Step 236540 (epoch   73.92), loss: 0.004490, time: 2.241 s\n",
      "Step 236560 (epoch   73.92), loss: 0.003262, time: 2.236 s\n",
      "Step 236580 (epoch   73.93), loss: 0.002624, time: 2.244 s\n",
      "Step 236600 (epoch   73.94), loss: 0.004295, time: 2.257 s, error: 0.010552\n",
      "Step 236620 (epoch   73.94), loss: 0.006433, time: 15.054 s\n",
      "Step 236640 (epoch   73.95), loss: 0.004011, time: 2.234 s\n",
      "Step 236660 (epoch   73.96), loss: 0.004222, time: 2.252 s\n",
      "Step 236680 (epoch   73.96), loss: 0.007796, time: 2.246 s\n",
      "Step 236700 (epoch   73.97), loss: 0.004468, time: 2.245 s\n",
      "Step 236720 (epoch   73.97), loss: 0.004029, time: 2.236 s\n",
      "Step 236740 (epoch   73.98), loss: 0.007998, time: 2.247 s\n",
      "Step 236760 (epoch   73.99), loss: 0.003024, time: 2.237 s\n",
      "Step 236780 (epoch   73.99), loss: 0.003640, time: 2.232 s\n",
      "Step 236800 (epoch   74.00), loss: 0.003650, time: 2.237 s, error: 0.010705\n",
      "Step 236820 (epoch   74.01), loss: 0.005043, time: 15.039 s\n",
      "Step 236840 (epoch   74.01), loss: 0.006180, time: 2.245 s\n",
      "Step 236860 (epoch   74.02), loss: 0.002800, time: 2.247 s\n",
      "Step 236880 (epoch   74.03), loss: 0.002366, time: 2.242 s\n",
      "Step 236900 (epoch   74.03), loss: 0.005389, time: 2.231 s\n",
      "Step 236920 (epoch   74.04), loss: 0.005102, time: 2.258 s\n",
      "Step 236940 (epoch   74.04), loss: 0.003464, time: 2.243 s\n",
      "Step 236960 (epoch   74.05), loss: 0.003887, time: 2.239 s\n",
      "Step 236980 (epoch   74.06), loss: 0.003034, time: 2.246 s\n",
      "Step 237000 (epoch   74.06), loss: 0.004405, time: 2.232 s, error: 0.010499\n",
      "Step 237020 (epoch   74.07), loss: 0.003582, time: 15.094 s\n",
      "Step 237040 (epoch   74.08), loss: 0.002322, time: 2.249 s\n",
      "Step 237060 (epoch   74.08), loss: 0.005558, time: 2.243 s\n",
      "Step 237080 (epoch   74.09), loss: 0.005652, time: 2.247 s\n",
      "Step 237100 (epoch   74.09), loss: 0.004422, time: 2.242 s\n",
      "Step 237120 (epoch   74.10), loss: 0.003966, time: 2.245 s\n",
      "Step 237140 (epoch   74.11), loss: 0.004442, time: 2.234 s\n",
      "Step 237160 (epoch   74.11), loss: 0.004260, time: 2.250 s\n",
      "Step 237180 (epoch   74.12), loss: 0.012054, time: 2.261 s\n",
      "Step 237200 (epoch   74.12), loss: 0.003014, time: 2.249 s, error: 0.010381\n",
      "Step 237220 (epoch   74.13), loss: 0.005518, time: 15.014 s\n",
      "Step 237240 (epoch   74.14), loss: 0.004295, time: 2.245 s\n",
      "Step 237260 (epoch   74.14), loss: 0.004616, time: 2.243 s\n",
      "Step 237280 (epoch   74.15), loss: 0.004289, time: 2.229 s\n",
      "Step 237300 (epoch   74.16), loss: 0.002568, time: 2.236 s\n",
      "Step 237320 (epoch   74.16), loss: 0.003380, time: 2.234 s\n",
      "Step 237340 (epoch   74.17), loss: 0.006009, time: 2.248 s\n",
      "Step 237360 (epoch   74.17), loss: 0.003686, time: 2.247 s\n",
      "Step 237380 (epoch   74.18), loss: 0.004813, time: 2.245 s\n",
      "Step 237400 (epoch   74.19), loss: 0.003124, time: 2.236 s, error: 0.010585\n",
      "Step 237420 (epoch   74.19), loss: 0.003864, time: 15.267 s\n",
      "Step 237440 (epoch   74.20), loss: 0.004200, time: 2.247 s\n",
      "Step 237460 (epoch   74.21), loss: 0.004612, time: 2.252 s\n",
      "Step 237480 (epoch   74.21), loss: 0.003474, time: 2.239 s\n",
      "Step 237500 (epoch   74.22), loss: 0.003216, time: 2.246 s\n",
      "Step 237520 (epoch   74.22), loss: 0.003978, time: 2.246 s\n",
      "Step 237540 (epoch   74.23), loss: 0.004425, time: 2.244 s\n",
      "Step 237560 (epoch   74.24), loss: 0.004267, time: 2.248 s\n",
      "Step 237580 (epoch   74.24), loss: 0.002792, time: 2.243 s\n",
      "Step 237600 (epoch   74.25), loss: 0.004299, time: 2.247 s, error: 0.010507\n",
      "Step 237620 (epoch   74.26), loss: 0.003014, time: 15.240 s\n",
      "Step 237640 (epoch   74.26), loss: 0.003556, time: 2.244 s\n",
      "Step 237660 (epoch   74.27), loss: 0.002805, time: 2.248 s\n",
      "Step 237680 (epoch   74.28), loss: 0.003439, time: 2.246 s\n",
      "Step 237700 (epoch   74.28), loss: 0.002924, time: 2.224 s\n",
      "Step 237720 (epoch   74.29), loss: 0.003276, time: 2.238 s\n",
      "Step 237740 (epoch   74.29), loss: 0.004724, time: 2.237 s\n",
      "Step 237760 (epoch   74.30), loss: 0.002765, time: 2.243 s\n",
      "Step 237780 (epoch   74.31), loss: 0.002824, time: 2.239 s\n",
      "Step 237800 (epoch   74.31), loss: 0.002655, time: 2.243 s, error: 0.010394\n",
      "Step 237820 (epoch   74.32), loss: 0.002856, time: 15.087 s\n",
      "Step 237840 (epoch   74.33), loss: 0.005736, time: 2.259 s\n",
      "Step 237860 (epoch   74.33), loss: 0.003212, time: 2.244 s\n",
      "Step 237880 (epoch   74.34), loss: 0.006671, time: 2.242 s\n",
      "Step 237900 (epoch   74.34), loss: 0.003872, time: 2.243 s\n",
      "Step 237920 (epoch   74.35), loss: 0.003454, time: 2.238 s\n",
      "Step 237940 (epoch   74.36), loss: 0.003579, time: 2.242 s\n",
      "Step 237960 (epoch   74.36), loss: 0.003798, time: 2.254 s\n",
      "Step 237980 (epoch   74.37), loss: 0.007891, time: 2.246 s\n",
      "Step 238000 (epoch   74.38), loss: 0.004731, time: 2.235 s, error: 0.010723\n",
      "\n",
      "Time since beginning  : 41759.761 s\n",
      "\n",
      "Step 238020 (epoch   74.38), loss: 0.005121, time: 15.148 s\n",
      "Step 238040 (epoch   74.39), loss: 0.004831, time: 2.247 s\n",
      "Step 238060 (epoch   74.39), loss: 0.004879, time: 2.241 s\n",
      "Step 238080 (epoch   74.40), loss: 0.004176, time: 2.247 s\n",
      "Step 238100 (epoch   74.41), loss: 0.004652, time: 2.262 s\n",
      "Step 238120 (epoch   74.41), loss: 0.003038, time: 2.257 s\n",
      "Step 238140 (epoch   74.42), loss: 0.002758, time: 2.254 s\n",
      "Step 238160 (epoch   74.42), loss: 0.003338, time: 2.255 s\n",
      "Step 238180 (epoch   74.43), loss: 0.004187, time: 2.253 s\n",
      "Step 238200 (epoch   74.44), loss: 0.004604, time: 2.254 s, error: 0.010648\n",
      "Step 238220 (epoch   74.44), loss: 0.005065, time: 15.056 s\n",
      "Step 238240 (epoch   74.45), loss: 0.003433, time: 2.235 s\n",
      "Step 238260 (epoch   74.46), loss: 0.002864, time: 2.235 s\n",
      "Step 238280 (epoch   74.46), loss: 0.004572, time: 2.235 s\n",
      "Step 238300 (epoch   74.47), loss: 0.005247, time: 2.247 s\n",
      "Step 238320 (epoch   74.47), loss: 0.002663, time: 2.237 s\n",
      "Step 238340 (epoch   74.48), loss: 0.004819, time: 2.238 s\n",
      "Step 238360 (epoch   74.49), loss: 0.004150, time: 2.247 s\n",
      "Step 238380 (epoch   74.49), loss: 0.003454, time: 2.248 s\n",
      "Step 238400 (epoch   74.50), loss: 0.003720, time: 2.246 s, error: 0.010225\n",
      "Step 238420 (epoch   74.51), loss: 0.002539, time: 15.064 s\n",
      "Step 238440 (epoch   74.51), loss: 0.004443, time: 2.244 s\n",
      "Step 238460 (epoch   74.52), loss: 0.004345, time: 2.244 s\n",
      "Step 238480 (epoch   74.53), loss: 0.003814, time: 2.230 s\n",
      "Step 238500 (epoch   74.53), loss: 0.004016, time: 2.232 s\n",
      "Step 238520 (epoch   74.54), loss: 0.004725, time: 2.241 s\n",
      "Step 238540 (epoch   74.54), loss: 0.003623, time: 2.246 s\n",
      "Step 238560 (epoch   74.55), loss: 0.003953, time: 2.229 s\n",
      "Step 238580 (epoch   74.56), loss: 0.007537, time: 2.240 s\n",
      "Step 238600 (epoch   74.56), loss: 0.009286, time: 2.236 s, error: 0.010547\n",
      "Step 238620 (epoch   74.57), loss: 0.005752, time: 15.293 s\n",
      "Step 238640 (epoch   74.58), loss: 0.005027, time: 2.243 s\n",
      "Step 238660 (epoch   74.58), loss: 0.006055, time: 2.235 s\n",
      "Step 238680 (epoch   74.59), loss: 0.003846, time: 2.242 s\n",
      "Step 238700 (epoch   74.59), loss: 0.003253, time: 2.256 s\n",
      "Step 238720 (epoch   74.60), loss: 0.005173, time: 2.242 s\n",
      "Step 238740 (epoch   74.61), loss: 0.004560, time: 2.239 s\n",
      "Step 238760 (epoch   74.61), loss: 0.004125, time: 2.242 s\n",
      "Step 238780 (epoch   74.62), loss: 0.004834, time: 2.246 s\n",
      "Step 238800 (epoch   74.62), loss: 0.010195, time: 2.238 s, error: 0.010859\n",
      "Step 238820 (epoch   74.63), loss: 0.004501, time: 15.286 s\n",
      "Step 238840 (epoch   74.64), loss: 0.003937, time: 2.250 s\n",
      "Step 238860 (epoch   74.64), loss: 0.004300, time: 2.256 s\n",
      "Step 238880 (epoch   74.65), loss: 0.004740, time: 2.247 s\n",
      "Step 238900 (epoch   74.66), loss: 0.004753, time: 2.233 s\n",
      "Step 238920 (epoch   74.66), loss: 0.005948, time: 2.245 s\n",
      "Step 238940 (epoch   74.67), loss: 0.002935, time: 2.241 s\n",
      "Step 238960 (epoch   74.67), loss: 0.004231, time: 2.249 s\n",
      "Step 238980 (epoch   74.68), loss: 0.002981, time: 2.255 s\n",
      "Step 239000 (epoch   74.69), loss: 0.003963, time: 2.245 s, error: 0.010311\n",
      "Step 239020 (epoch   74.69), loss: 0.002827, time: 15.184 s\n",
      "Step 239040 (epoch   74.70), loss: 0.003226, time: 2.256 s\n",
      "Step 239060 (epoch   74.71), loss: 0.004127, time: 2.242 s\n",
      "Step 239080 (epoch   74.71), loss: 0.003436, time: 2.238 s\n",
      "Step 239100 (epoch   74.72), loss: 0.004101, time: 2.223 s\n",
      "Step 239120 (epoch   74.72), loss: 0.004236, time: 2.246 s\n",
      "Step 239140 (epoch   74.73), loss: 0.003833, time: 2.250 s\n",
      "Step 239160 (epoch   74.74), loss: 0.004810, time: 2.239 s\n",
      "Step 239180 (epoch   74.74), loss: 0.005694, time: 2.227 s\n",
      "Step 239200 (epoch   74.75), loss: 0.003527, time: 2.228 s, error: 0.010782\n",
      "Step 239220 (epoch   74.76), loss: 0.006522, time: 15.032 s\n",
      "Step 239240 (epoch   74.76), loss: 0.004897, time: 2.244 s\n",
      "Step 239260 (epoch   74.77), loss: 0.003165, time: 2.248 s\n",
      "Step 239280 (epoch   74.78), loss: 0.003982, time: 2.257 s\n",
      "Step 239300 (epoch   74.78), loss: 0.006112, time: 2.247 s\n",
      "Step 239320 (epoch   74.79), loss: 0.004015, time: 2.241 s\n",
      "Step 239340 (epoch   74.79), loss: 0.003563, time: 2.249 s\n",
      "Step 239360 (epoch   74.80), loss: 0.004696, time: 2.247 s\n",
      "Step 239380 (epoch   74.81), loss: 0.003506, time: 2.245 s\n",
      "Step 239400 (epoch   74.81), loss: 0.006745, time: 2.250 s, error: 0.010302\n",
      "Step 239420 (epoch   74.82), loss: 0.004301, time: 15.059 s\n",
      "Step 239440 (epoch   74.83), loss: 0.003388, time: 2.238 s\n",
      "Step 239460 (epoch   74.83), loss: 0.008843, time: 2.234 s\n",
      "Step 239480 (epoch   74.84), loss: 0.005231, time: 2.248 s\n",
      "Step 239500 (epoch   74.84), loss: 0.002953, time: 2.250 s\n",
      "Step 239520 (epoch   74.85), loss: 0.004042, time: 2.256 s\n",
      "Step 239540 (epoch   74.86), loss: 0.002906, time: 2.252 s\n",
      "Step 239560 (epoch   74.86), loss: 0.008240, time: 2.245 s\n",
      "Step 239580 (epoch   74.87), loss: 0.003583, time: 2.237 s\n",
      "Step 239600 (epoch   74.88), loss: 0.004935, time: 2.224 s, error: 0.010159\n",
      "Step 239620 (epoch   74.88), loss: 0.005060, time: 15.061 s\n",
      "Step 239640 (epoch   74.89), loss: 0.009805, time: 2.244 s\n",
      "Step 239660 (epoch   74.89), loss: 0.003608, time: 2.248 s\n",
      "Step 239680 (epoch   74.90), loss: 0.003506, time: 2.242 s\n",
      "Step 239700 (epoch   74.91), loss: 0.004124, time: 2.245 s\n",
      "Step 239720 (epoch   74.91), loss: 0.003930, time: 2.232 s\n",
      "Step 239740 (epoch   74.92), loss: 0.002466, time: 2.241 s\n",
      "Step 239760 (epoch   74.92), loss: 0.004641, time: 2.240 s\n",
      "Step 239780 (epoch   74.93), loss: 0.002858, time: 2.253 s\n",
      "Step 239800 (epoch   74.94), loss: 0.005358, time: 2.260 s, error: 0.010479\n",
      "Step 239820 (epoch   74.94), loss: 0.005536, time: 15.079 s\n",
      "Step 239840 (epoch   74.95), loss: 0.005676, time: 2.245 s\n",
      "Step 239860 (epoch   74.96), loss: 0.003552, time: 2.233 s\n",
      "Step 239880 (epoch   74.96), loss: 0.003002, time: 2.245 s\n",
      "Step 239900 (epoch   74.97), loss: 0.002899, time: 2.233 s\n",
      "Step 239920 (epoch   74.97), loss: 0.005659, time: 2.243 s\n",
      "Step 239940 (epoch   74.98), loss: 0.004211, time: 2.237 s\n",
      "Step 239960 (epoch   74.99), loss: 0.006184, time: 2.247 s\n",
      "Step 239980 (epoch   74.99), loss: 0.004331, time: 2.240 s\n",
      "Step 240000 (epoch   75.00), loss: 0.007134, time: 2.244 s, error: 0.010788\n",
      "\n",
      "Time since beginning  : 42113.141 s\n",
      "\n",
      "Step 240020 (epoch   75.01), loss: 0.003245, time: 15.353 s\n",
      "Step 240040 (epoch   75.01), loss: 0.004709, time: 2.244 s\n",
      "Step 240060 (epoch   75.02), loss: 0.003902, time: 2.231 s\n",
      "Step 240080 (epoch   75.03), loss: 0.009124, time: 2.240 s\n",
      "Step 240100 (epoch   75.03), loss: 0.003066, time: 2.245 s\n",
      "Step 240120 (epoch   75.04), loss: 0.004372, time: 2.234 s\n",
      "Step 240140 (epoch   75.04), loss: 0.005632, time: 2.242 s\n",
      "Step 240160 (epoch   75.05), loss: 0.005606, time: 2.240 s\n",
      "Step 240180 (epoch   75.06), loss: 0.005367, time: 2.235 s\n",
      "Step 240200 (epoch   75.06), loss: 0.004106, time: 2.236 s, error: 0.010335\n",
      "Step 240220 (epoch   75.07), loss: 0.003876, time: 15.208 s\n",
      "Step 240240 (epoch   75.08), loss: 0.002958, time: 2.242 s\n",
      "Step 240260 (epoch   75.08), loss: 0.005274, time: 2.247 s\n",
      "Step 240280 (epoch   75.09), loss: 0.002645, time: 2.247 s\n",
      "Step 240300 (epoch   75.09), loss: 0.004109, time: 2.246 s\n",
      "Step 240320 (epoch   75.10), loss: 0.003624, time: 2.227 s\n",
      "Step 240340 (epoch   75.11), loss: 0.004811, time: 2.240 s\n",
      "Step 240360 (epoch   75.11), loss: 0.003885, time: 2.237 s\n",
      "Step 240380 (epoch   75.12), loss: 0.002892, time: 2.243 s\n",
      "Step 240400 (epoch   75.12), loss: 0.005114, time: 2.232 s, error: 0.010265\n",
      "Step 240420 (epoch   75.13), loss: 0.004229, time: 15.070 s\n",
      "Step 240440 (epoch   75.14), loss: 0.005716, time: 2.231 s\n",
      "Step 240460 (epoch   75.14), loss: 0.005014, time: 2.261 s\n",
      "Step 240480 (epoch   75.15), loss: 0.009489, time: 2.241 s\n",
      "Step 240500 (epoch   75.16), loss: 0.003333, time: 2.243 s\n",
      "Step 240520 (epoch   75.16), loss: 0.006337, time: 2.239 s\n",
      "Step 240540 (epoch   75.17), loss: 0.004975, time: 2.232 s\n",
      "Step 240560 (epoch   75.17), loss: 0.003903, time: 2.241 s\n",
      "Step 240580 (epoch   75.18), loss: 0.005553, time: 2.245 s\n",
      "Step 240600 (epoch   75.19), loss: 0.002923, time: 2.231 s, error: 0.010503\n",
      "Step 240620 (epoch   75.19), loss: 0.003817, time: 15.024 s\n",
      "Step 240640 (epoch   75.20), loss: 0.004276, time: 2.241 s\n",
      "Step 240660 (epoch   75.21), loss: 0.004862, time: 2.245 s\n",
      "Step 240680 (epoch   75.21), loss: 0.003013, time: 2.239 s\n",
      "Step 240700 (epoch   75.22), loss: 0.004787, time: 2.244 s\n",
      "Step 240720 (epoch   75.22), loss: 0.005627, time: 2.261 s\n",
      "Step 240740 (epoch   75.23), loss: 0.005120, time: 2.249 s\n",
      "Step 240760 (epoch   75.24), loss: 0.003716, time: 2.245 s\n",
      "Step 240780 (epoch   75.24), loss: 0.003685, time: 2.232 s\n",
      "Step 240800 (epoch   75.25), loss: 0.005072, time: 2.246 s, error: 0.010466\n",
      "Step 240820 (epoch   75.26), loss: 0.003454, time: 15.021 s\n",
      "Step 240840 (epoch   75.26), loss: 0.004980, time: 2.250 s\n",
      "Step 240860 (epoch   75.27), loss: 0.003884, time: 2.248 s\n",
      "Step 240880 (epoch   75.28), loss: 0.003841, time: 2.242 s\n",
      "Step 240900 (epoch   75.28), loss: 0.004115, time: 2.235 s\n",
      "Step 240920 (epoch   75.29), loss: 0.003232, time: 2.238 s\n",
      "Step 240940 (epoch   75.29), loss: 0.007985, time: 2.242 s\n",
      "Step 240960 (epoch   75.30), loss: 0.005956, time: 2.252 s\n",
      "Step 240980 (epoch   75.31), loss: 0.003722, time: 2.251 s\n",
      "Step 241000 (epoch   75.31), loss: 0.006176, time: 2.229 s, error: 0.010309\n",
      "Step 241020 (epoch   75.32), loss: 0.005185, time: 15.068 s\n",
      "Step 241040 (epoch   75.33), loss: 0.004395, time: 2.233 s\n",
      "Step 241060 (epoch   75.33), loss: 0.003666, time: 2.241 s\n",
      "Step 241080 (epoch   75.34), loss: 0.004645, time: 2.240 s\n",
      "Step 241100 (epoch   75.34), loss: 0.003805, time: 2.239 s\n",
      "Step 241120 (epoch   75.35), loss: 0.004355, time: 2.244 s\n",
      "Step 241140 (epoch   75.36), loss: 0.003143, time: 2.244 s\n",
      "Step 241160 (epoch   75.36), loss: 0.004011, time: 2.237 s\n",
      "Step 241180 (epoch   75.37), loss: 0.005318, time: 2.236 s\n",
      "Step 241200 (epoch   75.38), loss: 0.004567, time: 2.251 s, error: 0.010844\n",
      "Step 241220 (epoch   75.38), loss: 0.006125, time: 15.224 s\n",
      "Step 241240 (epoch   75.39), loss: 0.003823, time: 2.237 s\n",
      "Step 241260 (epoch   75.39), loss: 0.002809, time: 2.247 s\n",
      "Step 241280 (epoch   75.40), loss: 0.002515, time: 2.241 s\n",
      "Step 241300 (epoch   75.41), loss: 0.004185, time: 2.240 s\n",
      "Step 241320 (epoch   75.41), loss: 0.004441, time: 2.241 s\n",
      "Step 241340 (epoch   75.42), loss: 0.003152, time: 2.243 s\n",
      "Step 241360 (epoch   75.42), loss: 0.002780, time: 2.248 s\n",
      "Step 241380 (epoch   75.43), loss: 0.005445, time: 2.237 s\n",
      "Step 241400 (epoch   75.44), loss: 0.004100, time: 2.243 s, error: 0.010657\n",
      "Step 241420 (epoch   75.44), loss: 0.003646, time: 15.231 s\n",
      "Step 241440 (epoch   75.45), loss: 0.003429, time: 2.243 s\n",
      "Step 241460 (epoch   75.46), loss: 0.003721, time: 2.240 s\n",
      "Step 241480 (epoch   75.46), loss: 0.003511, time: 2.231 s\n",
      "Step 241500 (epoch   75.47), loss: 0.003684, time: 2.238 s\n",
      "Step 241520 (epoch   75.47), loss: 0.005453, time: 2.241 s\n",
      "Step 241540 (epoch   75.48), loss: 0.003731, time: 2.245 s\n",
      "Step 241560 (epoch   75.49), loss: 0.005292, time: 2.243 s\n",
      "Step 241580 (epoch   75.49), loss: 0.004497, time: 2.250 s\n",
      "Step 241600 (epoch   75.50), loss: 0.004337, time: 2.239 s, error: 0.010136\n",
      "Step 241620 (epoch   75.51), loss: 0.005395, time: 15.076 s\n",
      "Step 241640 (epoch   75.51), loss: 0.003766, time: 2.258 s\n",
      "Step 241660 (epoch   75.52), loss: 0.006982, time: 2.235 s\n",
      "Step 241680 (epoch   75.53), loss: 0.002555, time: 2.232 s\n",
      "Step 241700 (epoch   75.53), loss: 0.004668, time: 2.245 s\n",
      "Step 241720 (epoch   75.54), loss: 0.004456, time: 2.239 s\n",
      "Step 241740 (epoch   75.54), loss: 0.003350, time: 2.241 s\n",
      "Step 241760 (epoch   75.55), loss: 0.005611, time: 2.250 s\n",
      "Step 241780 (epoch   75.56), loss: 0.006124, time: 2.255 s\n",
      "Step 241800 (epoch   75.56), loss: 0.004635, time: 2.246 s, error: 0.010462\n",
      "Step 241820 (epoch   75.57), loss: 0.004422, time: 15.082 s\n",
      "Step 241840 (epoch   75.58), loss: 0.003664, time: 2.249 s\n",
      "Step 241860 (epoch   75.58), loss: 0.003644, time: 2.238 s\n",
      "Step 241880 (epoch   75.59), loss: 0.002844, time: 2.249 s\n",
      "Step 241900 (epoch   75.59), loss: 0.003405, time: 2.256 s\n",
      "Step 241920 (epoch   75.60), loss: 0.006498, time: 2.249 s\n",
      "Step 241940 (epoch   75.61), loss: 0.004953, time: 2.248 s\n",
      "Step 241960 (epoch   75.61), loss: 0.003497, time: 2.237 s\n",
      "Step 241980 (epoch   75.62), loss: 0.004550, time: 2.235 s\n",
      "Step 242000 (epoch   75.62), loss: 0.009439, time: 2.228 s, error: 0.010745\n",
      "\n",
      "Time since beginning  : 42466.074 s\n",
      "\n",
      "Step 242020 (epoch   75.63), loss: 0.003610, time: 15.144 s\n",
      "Step 242040 (epoch   75.64), loss: 0.003677, time: 2.242 s\n",
      "Step 242060 (epoch   75.64), loss: 0.004892, time: 2.246 s\n",
      "Step 242080 (epoch   75.65), loss: 0.002774, time: 2.243 s\n",
      "Step 242100 (epoch   75.66), loss: 0.005239, time: 2.257 s\n",
      "Step 242120 (epoch   75.66), loss: 0.003177, time: 2.236 s\n",
      "Step 242140 (epoch   75.67), loss: 0.002885, time: 2.241 s\n",
      "Step 242160 (epoch   75.67), loss: 0.004731, time: 2.250 s\n",
      "Step 242180 (epoch   75.68), loss: 0.005112, time: 2.238 s\n",
      "Step 242200 (epoch   75.69), loss: 0.004128, time: 2.241 s, error: 0.010442\n",
      "Step 242220 (epoch   75.69), loss: 0.005182, time: 15.039 s\n",
      "Step 242240 (epoch   75.70), loss: 0.003063, time: 2.245 s\n",
      "Step 242260 (epoch   75.71), loss: 0.003967, time: 2.248 s\n",
      "Step 242280 (epoch   75.71), loss: 0.005959, time: 2.242 s\n",
      "Step 242300 (epoch   75.72), loss: 0.004071, time: 2.248 s\n",
      "Step 242320 (epoch   75.72), loss: 0.002236, time: 2.242 s\n",
      "Step 242340 (epoch   75.73), loss: 0.004025, time: 2.234 s\n",
      "Step 242360 (epoch   75.74), loss: 0.009419, time: 2.241 s\n",
      "Step 242380 (epoch   75.74), loss: 0.003588, time: 2.244 s\n",
      "Step 242400 (epoch   75.75), loss: 0.003744, time: 2.245 s, error: 0.010532\n",
      "Step 242420 (epoch   75.76), loss: 0.002515, time: 15.239 s\n",
      "Step 242440 (epoch   75.76), loss: 0.004842, time: 2.241 s\n",
      "Step 242460 (epoch   75.77), loss: 0.003948, time: 2.247 s\n",
      "Step 242480 (epoch   75.78), loss: 0.004045, time: 2.242 s\n",
      "Step 242500 (epoch   75.78), loss: 0.011457, time: 2.231 s\n",
      "Step 242520 (epoch   75.79), loss: 0.005028, time: 2.239 s\n",
      "Step 242540 (epoch   75.79), loss: 0.003731, time: 2.227 s\n",
      "Step 242560 (epoch   75.80), loss: 0.004752, time: 2.234 s\n",
      "Step 242580 (epoch   75.81), loss: 0.005720, time: 2.246 s\n",
      "Step 242600 (epoch   75.81), loss: 0.003451, time: 2.247 s, error: 0.010210\n",
      "Step 242620 (epoch   75.82), loss: 0.004439, time: 15.258 s\n",
      "Step 242640 (epoch   75.83), loss: 0.005494, time: 2.243 s\n",
      "Step 242660 (epoch   75.83), loss: 0.007428, time: 2.241 s\n",
      "Step 242680 (epoch   75.84), loss: 0.003795, time: 2.245 s\n",
      "Step 242700 (epoch   75.84), loss: 0.003362, time: 2.240 s\n",
      "Step 242720 (epoch   75.85), loss: 0.004905, time: 2.251 s\n",
      "Step 242740 (epoch   75.86), loss: 0.003825, time: 2.253 s\n",
      "Step 242760 (epoch   75.86), loss: 0.005096, time: 2.239 s\n",
      "Step 242780 (epoch   75.87), loss: 0.002296, time: 2.236 s\n",
      "Step 242800 (epoch   75.88), loss: 0.003417, time: 2.244 s, error: 0.010149\n",
      "Step 242820 (epoch   75.88), loss: 0.004058, time: 15.105 s\n",
      "Step 242840 (epoch   75.89), loss: 0.004532, time: 2.252 s\n",
      "Step 242860 (epoch   75.89), loss: 0.013344, time: 2.241 s\n",
      "Step 242880 (epoch   75.90), loss: 0.004183, time: 2.231 s\n",
      "Step 242900 (epoch   75.91), loss: 0.005013, time: 2.242 s\n",
      "Step 242920 (epoch   75.91), loss: 0.004291, time: 2.239 s\n",
      "Step 242940 (epoch   75.92), loss: 0.003131, time: 2.247 s\n",
      "Step 242960 (epoch   75.92), loss: 0.005904, time: 2.247 s\n",
      "Step 242980 (epoch   75.93), loss: 0.003361, time: 2.256 s\n",
      "Step 243000 (epoch   75.94), loss: 0.002712, time: 2.247 s, error: 0.010435\n",
      "Step 243020 (epoch   75.94), loss: 0.003605, time: 15.059 s\n",
      "Step 243040 (epoch   75.95), loss: 0.004521, time: 2.247 s\n",
      "Step 243060 (epoch   75.96), loss: 0.005404, time: 2.242 s\n",
      "Step 243080 (epoch   75.96), loss: 0.004465, time: 2.260 s\n",
      "Step 243100 (epoch   75.97), loss: 0.002737, time: 2.234 s\n",
      "Step 243120 (epoch   75.97), loss: 0.003260, time: 2.245 s\n",
      "Step 243140 (epoch   75.98), loss: 0.005192, time: 2.244 s\n",
      "Step 243160 (epoch   75.99), loss: 0.006434, time: 2.226 s\n",
      "Step 243180 (epoch   75.99), loss: 0.002677, time: 2.229 s\n",
      "Step 243200 (epoch   76.00), loss: 0.006358, time: 2.235 s, error: 0.010705\n",
      "Step 243220 (epoch   76.01), loss: 0.003200, time: 14.999 s\n",
      "Step 243240 (epoch   76.01), loss: 0.003359, time: 2.243 s\n",
      "Step 243260 (epoch   76.02), loss: 0.004229, time: 2.241 s\n",
      "Step 243280 (epoch   76.03), loss: 0.005463, time: 2.235 s\n",
      "Step 243300 (epoch   76.03), loss: 0.004432, time: 2.244 s\n",
      "Step 243320 (epoch   76.04), loss: 0.006393, time: 2.246 s\n",
      "Step 243340 (epoch   76.04), loss: 0.004504, time: 2.260 s\n",
      "Step 243360 (epoch   76.05), loss: 0.004724, time: 2.227 s\n",
      "Step 243380 (epoch   76.06), loss: 0.003874, time: 2.231 s\n",
      "Step 243400 (epoch   76.06), loss: 0.019689, time: 2.245 s, error: 0.010234\n",
      "Step 243420 (epoch   76.07), loss: 0.002894, time: 14.992 s\n",
      "Step 243440 (epoch   76.08), loss: 0.004977, time: 2.242 s\n",
      "Step 243460 (epoch   76.08), loss: 0.004002, time: 2.232 s\n",
      "Step 243480 (epoch   76.09), loss: 0.002551, time: 2.241 s\n",
      "Step 243500 (epoch   76.09), loss: 0.005486, time: 2.247 s\n",
      "Step 243520 (epoch   76.10), loss: 0.004875, time: 2.235 s\n",
      "Step 243540 (epoch   76.11), loss: 0.005691, time: 2.242 s\n",
      "Step 243560 (epoch   76.11), loss: 0.002702, time: 2.244 s\n",
      "Step 243580 (epoch   76.12), loss: 0.004591, time: 2.233 s\n",
      "Step 243600 (epoch   76.12), loss: 0.002968, time: 2.263 s, error: 0.010219\n",
      "Step 243620 (epoch   76.13), loss: 0.004131, time: 15.001 s\n",
      "Step 243640 (epoch   76.14), loss: 0.005891, time: 2.247 s\n",
      "Step 243660 (epoch   76.14), loss: 0.004198, time: 2.240 s\n",
      "Step 243680 (epoch   76.15), loss: 0.003967, time: 2.240 s\n",
      "Step 243700 (epoch   76.16), loss: 0.003276, time: 2.245 s\n",
      "Step 243720 (epoch   76.16), loss: 0.003701, time: 2.232 s\n",
      "Step 243740 (epoch   76.17), loss: 0.004408, time: 2.246 s\n",
      "Step 243760 (epoch   76.17), loss: 0.002883, time: 2.250 s\n",
      "Step 243780 (epoch   76.18), loss: 0.003116, time: 2.246 s\n",
      "Step 243800 (epoch   76.19), loss: 0.003664, time: 2.247 s, error: 0.010499\n",
      "Step 243820 (epoch   76.19), loss: 0.004479, time: 15.283 s\n",
      "Step 243840 (epoch   76.20), loss: 0.006223, time: 2.252 s\n",
      "Step 243860 (epoch   76.21), loss: 0.006040, time: 2.246 s\n",
      "Step 243880 (epoch   76.21), loss: 0.004554, time: 2.249 s\n",
      "Step 243900 (epoch   76.22), loss: 0.006563, time: 2.254 s\n",
      "Step 243920 (epoch   76.22), loss: 0.003489, time: 2.248 s\n",
      "Step 243940 (epoch   76.23), loss: 0.003007, time: 2.247 s\n",
      "Step 243960 (epoch   76.24), loss: 0.003662, time: 2.231 s\n",
      "Step 243980 (epoch   76.24), loss: 0.004200, time: 2.247 s\n",
      "Step 244000 (epoch   76.25), loss: 0.007042, time: 2.247 s, error: 0.010310\n",
      "\n",
      "Time since beginning  : 42819.203 s\n",
      "\n",
      "Step 244020 (epoch   76.26), loss: 0.004556, time: 15.304 s\n",
      "Step 244040 (epoch   76.26), loss: 0.005240, time: 2.239 s\n",
      "Step 244060 (epoch   76.27), loss: 0.005144, time: 2.232 s\n",
      "Step 244080 (epoch   76.28), loss: 0.004354, time: 2.249 s\n",
      "Step 244100 (epoch   76.28), loss: 0.004860, time: 2.250 s\n",
      "Step 244120 (epoch   76.29), loss: 0.002978, time: 2.248 s\n",
      "Step 244140 (epoch   76.29), loss: 0.004867, time: 2.250 s\n",
      "Step 244160 (epoch   76.30), loss: 0.003517, time: 2.249 s\n",
      "Step 244180 (epoch   76.31), loss: 0.003240, time: 2.243 s\n",
      "Step 244200 (epoch   76.31), loss: 0.002981, time: 2.242 s, error: 0.010255\n",
      "Step 244220 (epoch   76.32), loss: 0.003787, time: 15.066 s\n",
      "Step 244240 (epoch   76.33), loss: 0.003663, time: 2.243 s\n",
      "Step 244260 (epoch   76.33), loss: 0.004667, time: 2.249 s\n",
      "Step 244280 (epoch   76.34), loss: 0.004717, time: 2.234 s\n",
      "Step 244300 (epoch   76.34), loss: 0.004095, time: 2.240 s\n",
      "Step 244320 (epoch   76.35), loss: 0.002837, time: 2.230 s\n",
      "Step 244340 (epoch   76.36), loss: 0.004377, time: 2.237 s\n",
      "Step 244360 (epoch   76.36), loss: 0.002724, time: 2.241 s\n",
      "Step 244380 (epoch   76.37), loss: 0.004072, time: 2.244 s\n",
      "Step 244400 (epoch   76.38), loss: 0.005662, time: 2.244 s, error: 0.010742\n",
      "Step 244420 (epoch   76.38), loss: 0.004583, time: 15.000 s\n",
      "Step 244440 (epoch   76.39), loss: 0.005651, time: 2.241 s\n",
      "Step 244460 (epoch   76.39), loss: 0.003323, time: 2.245 s\n",
      "Step 244480 (epoch   76.40), loss: 0.004747, time: 2.238 s\n",
      "Step 244500 (epoch   76.41), loss: 0.007538, time: 2.251 s\n",
      "Step 244520 (epoch   76.41), loss: 0.005779, time: 2.254 s\n",
      "Step 244540 (epoch   76.42), loss: 0.002731, time: 2.241 s\n",
      "Step 244560 (epoch   76.42), loss: 0.002789, time: 2.238 s\n",
      "Step 244580 (epoch   76.43), loss: 0.004768, time: 2.240 s\n",
      "Step 244600 (epoch   76.44), loss: 0.006021, time: 2.243 s, error: 0.010444\n",
      "Step 244620 (epoch   76.44), loss: 0.007353, time: 14.992 s\n",
      "Step 244640 (epoch   76.45), loss: 0.004894, time: 2.240 s\n",
      "Step 244660 (epoch   76.46), loss: 0.003808, time: 2.238 s\n",
      "Step 244680 (epoch   76.46), loss: 0.003217, time: 2.241 s\n",
      "Step 244700 (epoch   76.47), loss: 0.006685, time: 2.241 s\n",
      "Step 244720 (epoch   76.47), loss: 0.005645, time: 2.244 s\n",
      "Step 244740 (epoch   76.48), loss: 0.003482, time: 2.250 s\n",
      "Step 244760 (epoch   76.49), loss: 0.004298, time: 2.252 s\n",
      "Step 244780 (epoch   76.49), loss: 0.004182, time: 2.255 s\n",
      "Step 244800 (epoch   76.50), loss: 0.003514, time: 2.238 s, error: 0.010312\n",
      "Step 244820 (epoch   76.51), loss: 0.006675, time: 15.199 s\n",
      "Step 244840 (epoch   76.51), loss: 0.004503, time: 2.255 s\n",
      "Step 244860 (epoch   76.52), loss: 0.004744, time: 2.259 s\n",
      "Step 244880 (epoch   76.53), loss: 0.002624, time: 2.256 s\n",
      "Step 244900 (epoch   76.53), loss: 0.003969, time: 2.257 s\n",
      "Step 244920 (epoch   76.54), loss: 0.003275, time: 2.257 s\n",
      "Step 244940 (epoch   76.54), loss: 0.004167, time: 2.254 s\n",
      "Step 244960 (epoch   76.55), loss: 0.004432, time: 2.255 s\n",
      "Step 244980 (epoch   76.56), loss: 0.005654, time: 2.250 s\n",
      "Step 245000 (epoch   76.56), loss: 0.005349, time: 2.252 s, error: 0.010426\n",
      "Step 245020 (epoch   76.57), loss: 0.004260, time: 15.223 s\n",
      "Step 245040 (epoch   76.58), loss: 0.004040, time: 2.238 s\n",
      "Step 245060 (epoch   76.58), loss: 0.003804, time: 2.237 s\n",
      "Step 245080 (epoch   76.59), loss: 0.005944, time: 2.240 s\n",
      "Step 245100 (epoch   76.59), loss: 0.004004, time: 2.238 s\n",
      "Step 245120 (epoch   76.60), loss: 0.003060, time: 2.232 s\n",
      "Step 245140 (epoch   76.61), loss: 0.003660, time: 2.242 s\n",
      "Step 245160 (epoch   76.61), loss: 0.003982, time: 2.246 s\n",
      "Step 245180 (epoch   76.62), loss: 0.005611, time: 2.253 s\n",
      "Step 245200 (epoch   76.62), loss: 0.006301, time: 2.232 s, error: 0.010621\n",
      "Step 245220 (epoch   76.63), loss: 0.005639, time: 15.612 s\n",
      "Step 245240 (epoch   76.64), loss: 0.003848, time: 2.254 s\n",
      "Step 245260 (epoch   76.64), loss: 0.004621, time: 2.252 s\n",
      "Step 245280 (epoch   76.65), loss: 0.003348, time: 2.239 s\n",
      "Step 245300 (epoch   76.66), loss: 0.006640, time: 2.245 s\n",
      "Step 245320 (epoch   76.66), loss: 0.003443, time: 2.245 s\n",
      "Step 245340 (epoch   76.67), loss: 0.003634, time: 2.240 s\n",
      "Step 245360 (epoch   76.67), loss: 0.006312, time: 2.258 s\n",
      "Step 245380 (epoch   76.68), loss: 0.008508, time: 2.246 s\n",
      "Step 245400 (epoch   76.69), loss: 0.004784, time: 2.248 s, error: 0.010486\n",
      "Step 245420 (epoch   76.69), loss: 0.002685, time: 15.118 s\n",
      "Step 245440 (epoch   76.70), loss: 0.004209, time: 2.244 s\n",
      "Step 245460 (epoch   76.71), loss: 0.004681, time: 2.248 s\n",
      "Step 245480 (epoch   76.71), loss: 0.003180, time: 2.246 s\n",
      "Step 245500 (epoch   76.72), loss: 0.004064, time: 2.249 s\n",
      "Step 245520 (epoch   76.72), loss: 0.004092, time: 2.233 s\n",
      "Step 245540 (epoch   76.73), loss: 0.003676, time: 2.245 s\n",
      "Step 245560 (epoch   76.74), loss: 0.006066, time: 2.236 s\n",
      "Step 245580 (epoch   76.74), loss: 0.004538, time: 2.247 s\n",
      "Step 245600 (epoch   76.75), loss: 0.004112, time: 2.232 s, error: 0.010377\n",
      "Step 245620 (epoch   76.76), loss: 0.005828, time: 15.005 s\n",
      "Step 245640 (epoch   76.76), loss: 0.003363, time: 2.235 s\n",
      "Step 245660 (epoch   76.77), loss: 0.004779, time: 2.238 s\n",
      "Step 245680 (epoch   76.78), loss: 0.003612, time: 2.258 s\n",
      "Step 245700 (epoch   76.78), loss: 0.002868, time: 2.238 s\n",
      "Step 245720 (epoch   76.79), loss: 0.004307, time: 2.242 s\n",
      "Step 245740 (epoch   76.79), loss: 0.002326, time: 2.233 s\n",
      "Step 245760 (epoch   76.80), loss: 0.003625, time: 2.242 s\n",
      "Step 245780 (epoch   76.81), loss: 0.004242, time: 2.229 s\n",
      "Step 245800 (epoch   76.81), loss: 0.005205, time: 2.244 s, error: 0.010243\n",
      "Step 245820 (epoch   76.82), loss: 0.005650, time: 15.004 s\n",
      "Step 245840 (epoch   76.83), loss: 0.003104, time: 2.241 s\n",
      "Step 245860 (epoch   76.83), loss: 0.004309, time: 2.231 s\n",
      "Step 245880 (epoch   76.84), loss: 0.004839, time: 2.245 s\n",
      "Step 245900 (epoch   76.84), loss: 0.004644, time: 2.230 s\n",
      "Step 245920 (epoch   76.85), loss: 0.004046, time: 2.240 s\n",
      "Step 245940 (epoch   76.86), loss: 0.002770, time: 2.255 s\n",
      "Step 245960 (epoch   76.86), loss: 0.004148, time: 2.230 s\n",
      "Step 245980 (epoch   76.87), loss: 0.002696, time: 2.237 s\n",
      "Step 246000 (epoch   76.88), loss: 0.002855, time: 2.238 s, error: 0.010139\n",
      "\n",
      "Time since beginning  : 43172.446 s\n",
      "\n",
      "Step 246020 (epoch   76.88), loss: 0.006356, time: 15.103 s\n",
      "Step 246040 (epoch   76.89), loss: 0.004810, time: 2.259 s\n",
      "Step 246060 (epoch   76.89), loss: 0.005610, time: 2.258 s\n",
      "Step 246080 (epoch   76.90), loss: 0.004609, time: 2.231 s\n",
      "Step 246100 (epoch   76.91), loss: 0.005020, time: 2.249 s\n",
      "Step 246120 (epoch   76.91), loss: 0.003601, time: 2.222 s\n",
      "Step 246140 (epoch   76.92), loss: 0.006161, time: 2.230 s\n",
      "Step 246160 (epoch   76.92), loss: 0.003517, time: 2.224 s\n",
      "Step 246180 (epoch   76.93), loss: 0.002914, time: 2.249 s\n",
      "Step 246200 (epoch   76.94), loss: 0.003136, time: 2.255 s, error: 0.010378\n",
      "Step 246220 (epoch   76.94), loss: 0.003987, time: 15.032 s\n",
      "Step 246240 (epoch   76.95), loss: 0.005243, time: 2.245 s\n",
      "Step 246260 (epoch   76.96), loss: 0.002879, time: 2.236 s\n",
      "Step 246280 (epoch   76.96), loss: 0.003577, time: 2.236 s\n",
      "Step 246300 (epoch   76.97), loss: 0.002672, time: 2.237 s\n",
      "Step 246320 (epoch   76.97), loss: 0.004037, time: 2.234 s\n",
      "Step 246340 (epoch   76.98), loss: 0.004895, time: 2.241 s\n",
      "Step 246360 (epoch   76.99), loss: 0.002977, time: 2.245 s\n",
      "Step 246380 (epoch   76.99), loss: 0.004088, time: 2.232 s\n",
      "Step 246400 (epoch   77.00), loss: 0.004306, time: 2.226 s, error: 0.010485\n",
      "Step 246420 (epoch   77.01), loss: 0.003994, time: 15.253 s\n",
      "Step 246440 (epoch   77.01), loss: 0.004768, time: 2.253 s\n",
      "Step 246460 (epoch   77.02), loss: 0.005634, time: 2.241 s\n",
      "Step 246480 (epoch   77.03), loss: 0.003584, time: 2.243 s\n",
      "Step 246500 (epoch   77.03), loss: 0.006348, time: 2.245 s\n",
      "Step 246520 (epoch   77.04), loss: 0.002921, time: 2.255 s\n",
      "Step 246540 (epoch   77.04), loss: 0.004599, time: 2.238 s\n",
      "Step 246560 (epoch   77.05), loss: 0.003938, time: 2.242 s\n",
      "Step 246580 (epoch   77.06), loss: 0.004063, time: 2.235 s\n",
      "Step 246600 (epoch   77.06), loss: 0.003855, time: 2.239 s, error: 0.010146\n",
      "Step 246620 (epoch   77.07), loss: 0.003842, time: 15.233 s\n",
      "Step 246640 (epoch   77.08), loss: 0.005426, time: 2.232 s\n",
      "Step 246660 (epoch   77.08), loss: 0.003908, time: 2.242 s\n",
      "Step 246680 (epoch   77.09), loss: 0.003303, time: 2.241 s\n",
      "Step 246700 (epoch   77.09), loss: 0.004600, time: 2.237 s\n",
      "Step 246720 (epoch   77.10), loss: 0.002809, time: 2.229 s\n",
      "Step 246740 (epoch   77.11), loss: 0.004091, time: 2.243 s\n",
      "Step 246760 (epoch   77.11), loss: 0.003389, time: 2.235 s\n",
      "Step 246780 (epoch   77.12), loss: 0.007773, time: 2.241 s\n",
      "Step 246800 (epoch   77.12), loss: 0.006208, time: 2.225 s, error: 0.010155\n",
      "Step 246820 (epoch   77.13), loss: 0.003478, time: 15.065 s\n",
      "Step 246840 (epoch   77.14), loss: 0.004270, time: 2.260 s\n",
      "Step 246860 (epoch   77.14), loss: 0.005286, time: 2.254 s\n",
      "Step 246880 (epoch   77.15), loss: 0.005639, time: 2.249 s\n",
      "Step 246900 (epoch   77.16), loss: 0.004981, time: 2.235 s\n",
      "Step 246920 (epoch   77.16), loss: 0.003689, time: 2.233 s\n",
      "Step 246940 (epoch   77.17), loss: 0.003518, time: 2.240 s\n",
      "Step 246960 (epoch   77.17), loss: 0.004998, time: 2.244 s\n",
      "Step 246980 (epoch   77.18), loss: 0.003087, time: 2.238 s\n",
      "Step 247000 (epoch   77.19), loss: 0.005494, time: 2.242 s, error: 0.010779\n",
      "Step 247020 (epoch   77.19), loss: 0.004182, time: 15.013 s\n",
      "Step 247040 (epoch   77.20), loss: 0.004491, time: 2.239 s\n",
      "Step 247060 (epoch   77.21), loss: 0.004196, time: 2.247 s\n",
      "Step 247080 (epoch   77.21), loss: 0.003728, time: 2.245 s\n",
      "Step 247100 (epoch   77.22), loss: 0.005486, time: 2.257 s\n",
      "Step 247120 (epoch   77.22), loss: 0.004696, time: 2.246 s\n",
      "Step 247140 (epoch   77.23), loss: 0.002939, time: 2.221 s\n",
      "Step 247160 (epoch   77.24), loss: 0.003356, time: 2.239 s\n",
      "Step 247180 (epoch   77.24), loss: 0.005487, time: 2.243 s\n",
      "Step 247200 (epoch   77.25), loss: 0.004898, time: 2.242 s, error: 0.010259\n",
      "Step 247220 (epoch   77.26), loss: 0.004834, time: 15.129 s\n",
      "Step 247240 (epoch   77.26), loss: 0.003818, time: 2.245 s\n",
      "Step 247260 (epoch   77.27), loss: 0.003846, time: 2.238 s\n",
      "Step 247280 (epoch   77.28), loss: 0.003391, time: 2.245 s\n",
      "Step 247300 (epoch   77.28), loss: 0.005258, time: 2.237 s\n",
      "Step 247320 (epoch   77.29), loss: 0.002897, time: 2.251 s\n",
      "Step 247340 (epoch   77.29), loss: 0.003613, time: 2.235 s\n",
      "Step 247360 (epoch   77.30), loss: 0.004248, time: 2.261 s\n",
      "Step 247380 (epoch   77.31), loss: 0.004659, time: 2.235 s\n",
      "Step 247400 (epoch   77.31), loss: 0.004945, time: 2.238 s, error: 0.010219\n",
      "Step 247420 (epoch   77.32), loss: 0.003262, time: 15.022 s\n",
      "Step 247440 (epoch   77.33), loss: 0.004765, time: 2.222 s\n",
      "Step 247460 (epoch   77.33), loss: 0.005711, time: 2.246 s\n",
      "Step 247480 (epoch   77.34), loss: 0.006400, time: 2.240 s\n",
      "Step 247500 (epoch   77.34), loss: 0.002345, time: 2.248 s\n",
      "Step 247520 (epoch   77.35), loss: 0.004623, time: 2.236 s\n",
      "Step 247540 (epoch   77.36), loss: 0.004400, time: 2.251 s\n",
      "Step 247560 (epoch   77.36), loss: 0.005710, time: 2.226 s\n",
      "Step 247580 (epoch   77.37), loss: 0.003489, time: 2.238 s\n",
      "Step 247600 (epoch   77.38), loss: 0.006637, time: 2.237 s, error: 0.010485\n",
      "Step 247620 (epoch   77.38), loss: 0.004977, time: 15.199 s\n",
      "Step 247640 (epoch   77.39), loss: 0.004950, time: 2.244 s\n",
      "Step 247660 (epoch   77.39), loss: 0.004180, time: 2.239 s\n",
      "Step 247680 (epoch   77.40), loss: 0.007384, time: 2.246 s\n",
      "Step 247700 (epoch   77.41), loss: 0.004088, time: 2.227 s\n",
      "Step 247720 (epoch   77.41), loss: 0.003915, time: 2.247 s\n",
      "Step 247740 (epoch   77.42), loss: 0.006237, time: 2.233 s\n",
      "Step 247760 (epoch   77.42), loss: 0.004120, time: 2.246 s\n",
      "Step 247780 (epoch   77.43), loss: 0.003132, time: 2.240 s\n",
      "Step 247800 (epoch   77.44), loss: 0.003419, time: 2.245 s, error: 0.010301\n",
      "Step 247820 (epoch   77.44), loss: 0.007223, time: 15.204 s\n",
      "Step 247840 (epoch   77.45), loss: 0.004281, time: 2.224 s\n",
      "Step 247860 (epoch   77.46), loss: 0.003496, time: 2.248 s\n",
      "Step 247880 (epoch   77.46), loss: 0.004406, time: 2.230 s\n",
      "Step 247900 (epoch   77.47), loss: 0.003579, time: 2.252 s\n",
      "Step 247920 (epoch   77.47), loss: 0.003608, time: 2.251 s\n",
      "Step 247940 (epoch   77.48), loss: 0.004193, time: 2.251 s\n",
      "Step 247960 (epoch   77.49), loss: 0.002556, time: 2.229 s\n",
      "Step 247980 (epoch   77.49), loss: 0.004433, time: 2.244 s\n",
      "Step 248000 (epoch   77.50), loss: 0.006072, time: 2.226 s, error: 0.010695\n",
      "\n",
      "Time since beginning  : 43525.420 s\n",
      "\n",
      "Step 248020 (epoch   77.51), loss: 0.003873, time: 15.183 s\n",
      "Step 248040 (epoch   77.51), loss: 0.002688, time: 2.253 s\n",
      "Step 248060 (epoch   77.52), loss: 0.004280, time: 2.248 s\n",
      "Step 248080 (epoch   77.53), loss: 0.004608, time: 2.249 s\n",
      "Step 248100 (epoch   77.53), loss: 0.003495, time: 2.248 s\n",
      "Step 248120 (epoch   77.54), loss: 0.003488, time: 2.243 s\n",
      "Step 248140 (epoch   77.54), loss: 0.002935, time: 2.248 s\n",
      "Step 248160 (epoch   77.55), loss: 0.005265, time: 2.244 s\n",
      "Step 248180 (epoch   77.56), loss: 0.004514, time: 2.241 s\n",
      "Step 248200 (epoch   77.56), loss: 0.002649, time: 2.232 s, error: 0.010396\n",
      "Step 248220 (epoch   77.57), loss: 0.003158, time: 15.019 s\n",
      "Step 248240 (epoch   77.58), loss: 0.005714, time: 2.232 s\n",
      "Step 248260 (epoch   77.58), loss: 0.004694, time: 2.247 s\n",
      "Step 248280 (epoch   77.59), loss: 0.003961, time: 2.260 s\n",
      "Step 248300 (epoch   77.59), loss: 0.006573, time: 2.250 s\n",
      "Step 248320 (epoch   77.60), loss: 0.003943, time: 2.241 s\n",
      "Step 248340 (epoch   77.61), loss: 0.003173, time: 2.235 s\n",
      "Step 248360 (epoch   77.61), loss: 0.004524, time: 2.233 s\n",
      "Step 248380 (epoch   77.62), loss: 0.004591, time: 2.234 s\n",
      "Step 248400 (epoch   77.62), loss: 0.004252, time: 2.248 s, error: 0.010506\n",
      "Step 248420 (epoch   77.63), loss: 0.004197, time: 15.021 s\n",
      "Step 248440 (epoch   77.64), loss: 0.005750, time: 2.230 s\n",
      "Step 248460 (epoch   77.64), loss: 0.003114, time: 2.232 s\n",
      "Step 248480 (epoch   77.65), loss: 0.003562, time: 2.235 s\n",
      "Step 248500 (epoch   77.66), loss: 0.004916, time: 2.229 s\n",
      "Step 248520 (epoch   77.66), loss: 0.003025, time: 2.245 s\n",
      "Step 248540 (epoch   77.67), loss: 0.005039, time: 2.255 s\n",
      "Step 248560 (epoch   77.67), loss: 0.006052, time: 2.238 s\n",
      "Step 248580 (epoch   77.68), loss: 0.003204, time: 2.247 s\n",
      "Step 248600 (epoch   77.69), loss: 0.004722, time: 2.239 s, error: 0.010455\n",
      "Step 248620 (epoch   77.69), loss: 0.003145, time: 15.043 s\n",
      "Step 248640 (epoch   77.70), loss: 0.005961, time: 2.240 s\n",
      "Step 248660 (epoch   77.71), loss: 0.004695, time: 2.231 s\n",
      "Step 248680 (epoch   77.71), loss: 0.004070, time: 2.237 s\n",
      "Step 248700 (epoch   77.72), loss: 0.002979, time: 2.243 s\n",
      "Step 248720 (epoch   77.72), loss: 0.006038, time: 2.241 s\n",
      "Step 248740 (epoch   77.73), loss: 0.002979, time: 2.244 s\n",
      "Step 248760 (epoch   77.74), loss: 0.005535, time: 2.231 s\n",
      "Step 248780 (epoch   77.74), loss: 0.004755, time: 2.256 s\n",
      "Step 248800 (epoch   77.75), loss: 0.004990, time: 2.247 s, error: 0.010271\n",
      "Step 248820 (epoch   77.76), loss: 0.005505, time: 15.038 s\n",
      "Step 248840 (epoch   77.76), loss: 0.005903, time: 2.247 s\n",
      "Step 248860 (epoch   77.77), loss: 0.004971, time: 2.233 s\n",
      "Step 248880 (epoch   77.78), loss: 0.003210, time: 2.245 s\n",
      "Step 248900 (epoch   77.78), loss: 0.003518, time: 2.244 s\n",
      "Step 248920 (epoch   77.79), loss: 0.007771, time: 2.247 s\n",
      "Step 248940 (epoch   77.79), loss: 0.003982, time: 2.235 s\n",
      "Step 248960 (epoch   77.80), loss: 0.005889, time: 2.238 s\n",
      "Step 248980 (epoch   77.81), loss: 0.003547, time: 2.244 s\n",
      "Step 249000 (epoch   77.81), loss: 0.004617, time: 2.238 s, error: 0.010331\n",
      "Step 249020 (epoch   77.82), loss: 0.004859, time: 15.282 s\n",
      "Step 249040 (epoch   77.83), loss: 0.002692, time: 2.221 s\n",
      "Step 249060 (epoch   77.83), loss: 0.005137, time: 2.241 s\n",
      "Step 249080 (epoch   77.84), loss: 0.003060, time: 2.238 s\n",
      "Step 249100 (epoch   77.84), loss: 0.006314, time: 2.240 s\n",
      "Step 249120 (epoch   77.85), loss: 0.003920, time: 2.250 s\n",
      "Step 249140 (epoch   77.86), loss: 0.004595, time: 2.243 s\n",
      "Step 249160 (epoch   77.86), loss: 0.005615, time: 2.235 s\n",
      "Step 249180 (epoch   77.87), loss: 0.003655, time: 2.243 s\n",
      "Step 249200 (epoch   77.88), loss: 0.004353, time: 2.241 s, error: 0.010080\n",
      "Step 249220 (epoch   77.88), loss: 0.004344, time: 15.226 s\n",
      "Step 249240 (epoch   77.89), loss: 0.004802, time: 2.243 s\n",
      "Step 249260 (epoch   77.89), loss: 0.004158, time: 2.227 s\n",
      "Step 249280 (epoch   77.90), loss: 0.004152, time: 2.245 s\n",
      "Step 249300 (epoch   77.91), loss: 0.005493, time: 2.249 s\n",
      "Step 249320 (epoch   77.91), loss: 0.004714, time: 2.241 s\n",
      "Step 249340 (epoch   77.92), loss: 0.003877, time: 2.229 s\n",
      "Step 249360 (epoch   77.92), loss: 0.003872, time: 2.238 s\n",
      "Step 249380 (epoch   77.93), loss: 0.003426, time: 2.239 s\n",
      "Step 249400 (epoch   77.94), loss: 0.005252, time: 2.242 s, error: 0.010415\n",
      "Step 249420 (epoch   77.94), loss: 0.003666, time: 15.092 s\n",
      "Step 249440 (epoch   77.95), loss: 0.008222, time: 2.257 s\n",
      "Step 249460 (epoch   77.96), loss: 0.003703, time: 2.246 s\n",
      "Step 249480 (epoch   77.96), loss: 0.003159, time: 2.237 s\n",
      "Step 249500 (epoch   77.97), loss: 0.002714, time: 2.247 s\n",
      "Step 249520 (epoch   77.97), loss: 0.005238, time: 2.254 s\n",
      "Step 249540 (epoch   77.98), loss: 0.002443, time: 2.229 s\n",
      "Step 249560 (epoch   77.99), loss: 0.011860, time: 2.245 s\n",
      "Step 249580 (epoch   77.99), loss: 0.004106, time: 2.242 s\n",
      "Step 249600 (epoch   78.00), loss: 0.006386, time: 2.240 s, error: 0.010313\n",
      "Step 249620 (epoch   78.01), loss: 0.004076, time: 15.048 s\n",
      "Step 249640 (epoch   78.01), loss: 0.003068, time: 2.240 s\n",
      "Step 249660 (epoch   78.02), loss: 0.004923, time: 2.242 s\n",
      "Step 249680 (epoch   78.03), loss: 0.003547, time: 2.238 s\n",
      "Step 249700 (epoch   78.03), loss: 0.004442, time: 2.250 s\n",
      "Step 249720 (epoch   78.04), loss: 0.004980, time: 2.229 s\n",
      "Step 249740 (epoch   78.04), loss: 0.005972, time: 2.246 s\n",
      "Step 249760 (epoch   78.05), loss: 0.003058, time: 2.227 s\n",
      "Step 249780 (epoch   78.06), loss: 0.003476, time: 2.244 s\n",
      "Step 249800 (epoch   78.06), loss: 0.003303, time: 2.227 s, error: 0.010200\n",
      "Step 249820 (epoch   78.07), loss: 0.003189, time: 15.041 s\n",
      "Step 249840 (epoch   78.08), loss: 0.005714, time: 2.242 s\n",
      "Step 249860 (epoch   78.08), loss: 0.005346, time: 2.243 s\n",
      "Step 249880 (epoch   78.09), loss: 0.003400, time: 2.245 s\n",
      "Step 249900 (epoch   78.09), loss: 0.003308, time: 2.233 s\n",
      "Step 249920 (epoch   78.10), loss: 0.003974, time: 2.244 s\n",
      "Step 249940 (epoch   78.11), loss: 0.003543, time: 2.250 s\n",
      "Step 249960 (epoch   78.11), loss: 0.004614, time: 2.248 s\n",
      "Step 249980 (epoch   78.12), loss: 0.003890, time: 2.239 s\n",
      "Step 250000 (epoch   78.12), loss: 0.003980, time: 2.248 s, error: 0.010161\n",
      "\n",
      "Time since beginning  : 43878.053 s\n",
      "\n",
      "Step 250020 (epoch   78.13), loss: 0.003094, time: 15.109 s\n",
      "Step 250040 (epoch   78.14), loss: 0.002943, time: 2.236 s\n",
      "Step 250060 (epoch   78.14), loss: 0.003011, time: 2.239 s\n",
      "Step 250080 (epoch   78.15), loss: 0.004552, time: 2.229 s\n",
      "Step 250100 (epoch   78.16), loss: 0.002679, time: 2.249 s\n",
      "Step 250120 (epoch   78.16), loss: 0.003810, time: 2.238 s\n",
      "Step 250140 (epoch   78.17), loss: 0.003815, time: 2.242 s\n",
      "Step 250160 (epoch   78.17), loss: 0.003302, time: 2.226 s\n",
      "Step 250180 (epoch   78.18), loss: 0.003707, time: 2.232 s\n",
      "Step 250200 (epoch   78.19), loss: 0.004978, time: 2.254 s, error: 0.010977\n",
      "Step 250220 (epoch   78.19), loss: 0.003717, time: 15.154 s\n",
      "Step 250240 (epoch   78.20), loss: 0.002982, time: 2.232 s\n",
      "Step 250260 (epoch   78.21), loss: 0.004637, time: 2.247 s\n",
      "Step 250280 (epoch   78.21), loss: 0.003048, time: 2.244 s\n",
      "Step 250300 (epoch   78.22), loss: 0.003561, time: 2.238 s\n",
      "Step 250320 (epoch   78.22), loss: 0.002988, time: 2.240 s\n",
      "Step 250340 (epoch   78.23), loss: 0.004224, time: 2.247 s\n",
      "Step 250360 (epoch   78.24), loss: 0.003633, time: 2.237 s\n",
      "Step 250380 (epoch   78.24), loss: 0.004887, time: 2.245 s\n",
      "Step 250400 (epoch   78.25), loss: 0.002113, time: 2.235 s, error: 0.010364\n",
      "Step 250420 (epoch   78.26), loss: 0.004557, time: 15.290 s\n",
      "Step 250440 (epoch   78.26), loss: 0.003176, time: 2.238 s\n",
      "Step 250460 (epoch   78.27), loss: 0.005220, time: 2.247 s\n",
      "Step 250480 (epoch   78.28), loss: 0.004205, time: 2.234 s\n",
      "Step 250500 (epoch   78.28), loss: 0.004534, time: 2.238 s\n",
      "Step 250520 (epoch   78.29), loss: 0.003344, time: 2.235 s\n",
      "Step 250540 (epoch   78.29), loss: 0.003051, time: 2.243 s\n",
      "Step 250560 (epoch   78.30), loss: 0.005700, time: 2.243 s\n",
      "Step 250580 (epoch   78.31), loss: 0.003232, time: 2.240 s\n",
      "Step 250600 (epoch   78.31), loss: 0.004892, time: 2.245 s, error: 0.010209\n",
      "Step 250620 (epoch   78.32), loss: 0.004521, time: 15.258 s\n",
      "Step 250640 (epoch   78.33), loss: 0.003615, time: 2.248 s\n",
      "Step 250660 (epoch   78.33), loss: 0.005079, time: 2.244 s\n",
      "Step 250680 (epoch   78.34), loss: 0.003570, time: 2.245 s\n",
      "Step 250700 (epoch   78.34), loss: 0.004807, time: 2.230 s\n",
      "Step 250720 (epoch   78.35), loss: 0.003715, time: 2.245 s\n",
      "Step 250740 (epoch   78.36), loss: 0.002798, time: 2.236 s\n",
      "Step 250760 (epoch   78.36), loss: 0.004047, time: 2.243 s\n",
      "Step 250780 (epoch   78.37), loss: 0.004994, time: 2.230 s\n",
      "Step 250800 (epoch   78.38), loss: 0.004081, time: 2.232 s, error: 0.010234\n",
      "Step 250820 (epoch   78.38), loss: 0.005086, time: 15.035 s\n",
      "Step 250840 (epoch   78.39), loss: 0.004279, time: 2.236 s\n",
      "Step 250860 (epoch   78.39), loss: 0.003985, time: 2.250 s\n",
      "Step 250880 (epoch   78.40), loss: 0.003562, time: 2.244 s\n",
      "Step 250900 (epoch   78.41), loss: 0.003788, time: 2.245 s\n",
      "Step 250920 (epoch   78.41), loss: 0.006356, time: 2.225 s\n",
      "Step 250940 (epoch   78.42), loss: 0.003298, time: 2.241 s\n",
      "Step 250960 (epoch   78.42), loss: 0.004546, time: 2.235 s\n",
      "Step 250980 (epoch   78.43), loss: 0.005901, time: 2.244 s\n",
      "Step 251000 (epoch   78.44), loss: 0.004034, time: 2.227 s, error: 0.010192\n",
      "Step 251020 (epoch   78.44), loss: 0.004024, time: 15.048 s\n",
      "Step 251040 (epoch   78.45), loss: 0.005642, time: 2.243 s\n",
      "Step 251060 (epoch   78.46), loss: 0.003241, time: 2.235 s\n",
      "Step 251080 (epoch   78.46), loss: 0.004342, time: 2.242 s\n",
      "Step 251100 (epoch   78.47), loss: 0.004098, time: 2.230 s\n",
      "Step 251120 (epoch   78.47), loss: 0.008309, time: 2.259 s\n",
      "Step 251140 (epoch   78.48), loss: 0.003099, time: 2.238 s\n",
      "Step 251160 (epoch   78.49), loss: 0.003482, time: 2.242 s\n",
      "Step 251180 (epoch   78.49), loss: 0.007576, time: 2.232 s\n",
      "Step 251200 (epoch   78.50), loss: 0.005920, time: 2.238 s, error: 0.010968\n",
      "Step 251220 (epoch   78.51), loss: 0.004898, time: 15.022 s\n",
      "Step 251240 (epoch   78.51), loss: 0.003037, time: 2.233 s\n",
      "Step 251260 (epoch   78.52), loss: 0.005942, time: 2.245 s\n",
      "Step 251280 (epoch   78.53), loss: 0.004130, time: 2.228 s\n",
      "Step 251300 (epoch   78.53), loss: 0.008234, time: 2.246 s\n",
      "Step 251320 (epoch   78.54), loss: 0.006511, time: 2.242 s\n",
      "Step 251340 (epoch   78.54), loss: 0.004360, time: 2.252 s\n",
      "Step 251360 (epoch   78.55), loss: 0.005244, time: 2.242 s\n",
      "Step 251380 (epoch   78.56), loss: 0.003815, time: 2.261 s\n",
      "Step 251400 (epoch   78.56), loss: 0.003903, time: 2.242 s, error: 0.010350\n",
      "Step 251420 (epoch   78.57), loss: 0.005544, time: 15.028 s\n",
      "Step 251440 (epoch   78.58), loss: 0.002957, time: 2.246 s\n",
      "Step 251460 (epoch   78.58), loss: 0.005565, time: 2.244 s\n",
      "Step 251480 (epoch   78.59), loss: 0.003383, time: 2.246 s\n",
      "Step 251500 (epoch   78.59), loss: 0.004891, time: 2.233 s\n",
      "Step 251520 (epoch   78.60), loss: 0.008488, time: 2.242 s\n",
      "Step 251540 (epoch   78.61), loss: 0.003371, time: 2.241 s\n",
      "Step 251560 (epoch   78.61), loss: 0.002982, time: 2.241 s\n",
      "Step 251580 (epoch   78.62), loss: 0.004575, time: 2.233 s\n",
      "Step 251600 (epoch   78.62), loss: 0.004977, time: 2.246 s, error: 0.010387\n",
      "Step 251620 (epoch   78.63), loss: 0.003558, time: 15.260 s\n",
      "Step 251640 (epoch   78.64), loss: 0.005162, time: 2.234 s\n",
      "Step 251660 (epoch   78.64), loss: 0.003001, time: 2.246 s\n",
      "Step 251680 (epoch   78.65), loss: 0.005003, time: 2.240 s\n",
      "Step 251700 (epoch   78.66), loss: 0.004539, time: 2.251 s\n",
      "Step 251720 (epoch   78.66), loss: 0.004811, time: 2.235 s\n",
      "Step 251740 (epoch   78.67), loss: 0.005379, time: 2.245 s\n",
      "Step 251760 (epoch   78.67), loss: 0.003565, time: 2.238 s\n",
      "Step 251780 (epoch   78.68), loss: 0.005571, time: 2.239 s\n",
      "Step 251800 (epoch   78.69), loss: 0.003821, time: 2.237 s, error: 0.010387\n",
      "Step 251820 (epoch   78.69), loss: 0.004357, time: 15.375 s\n",
      "Step 251840 (epoch   78.70), loss: 0.003616, time: 2.246 s\n",
      "Step 251860 (epoch   78.71), loss: 0.003239, time: 2.244 s\n",
      "Step 251880 (epoch   78.71), loss: 0.004023, time: 2.244 s\n",
      "Step 251900 (epoch   78.72), loss: 0.005362, time: 2.245 s\n",
      "Step 251920 (epoch   78.72), loss: 0.004680, time: 2.259 s\n",
      "Step 251940 (epoch   78.73), loss: 0.004971, time: 2.255 s\n",
      "Step 251960 (epoch   78.74), loss: 0.003847, time: 2.247 s\n",
      "Step 251980 (epoch   78.74), loss: 0.004568, time: 2.250 s\n",
      "Step 252000 (epoch   78.75), loss: 0.004517, time: 2.246 s, error: 0.010381\n",
      "\n",
      "Time since beginning  : 44231.343 s\n",
      "\n",
      "Step 252020 (epoch   78.76), loss: 0.004468, time: 15.121 s\n",
      "Step 252040 (epoch   78.76), loss: 0.006155, time: 2.253 s\n",
      "Step 252060 (epoch   78.77), loss: 0.004779, time: 2.243 s\n",
      "Step 252080 (epoch   78.78), loss: 0.004557, time: 2.244 s\n",
      "Step 252100 (epoch   78.78), loss: 0.003918, time: 2.231 s\n",
      "Step 252120 (epoch   78.79), loss: 0.003151, time: 2.244 s\n",
      "Step 252140 (epoch   78.79), loss: 0.006328, time: 2.240 s\n",
      "Step 252160 (epoch   78.80), loss: 0.004310, time: 2.241 s\n",
      "Step 252180 (epoch   78.81), loss: 0.003743, time: 2.229 s\n",
      "Step 252200 (epoch   78.81), loss: 0.003798, time: 2.243 s, error: 0.010233\n",
      "Step 252220 (epoch   78.82), loss: 0.003715, time: 15.058 s\n",
      "Step 252240 (epoch   78.83), loss: 0.002571, time: 2.251 s\n",
      "Step 252260 (epoch   78.83), loss: 0.002734, time: 2.238 s\n",
      "Step 252280 (epoch   78.84), loss: 0.003380, time: 2.261 s\n",
      "Step 252300 (epoch   78.84), loss: 0.004676, time: 2.255 s\n",
      "Step 252320 (epoch   78.85), loss: 0.002790, time: 2.249 s\n",
      "Step 252340 (epoch   78.86), loss: 0.003167, time: 2.238 s\n",
      "Step 252360 (epoch   78.86), loss: 0.003657, time: 2.252 s\n",
      "Step 252380 (epoch   78.87), loss: 0.003431, time: 2.255 s\n",
      "Step 252400 (epoch   78.88), loss: 0.003980, time: 2.251 s, error: 0.010043\n",
      "Step 252420 (epoch   78.88), loss: 0.004944, time: 15.006 s\n",
      "Step 252440 (epoch   78.89), loss: 0.004378, time: 2.230 s\n",
      "Step 252460 (epoch   78.89), loss: 0.006494, time: 2.243 s\n",
      "Step 252480 (epoch   78.90), loss: 0.008668, time: 2.244 s\n",
      "Step 252500 (epoch   78.91), loss: 0.005478, time: 2.258 s\n",
      "Step 252520 (epoch   78.91), loss: 0.003357, time: 2.255 s\n",
      "Step 252540 (epoch   78.92), loss: 0.002699, time: 2.260 s\n",
      "Step 252560 (epoch   78.92), loss: 0.005061, time: 2.260 s\n",
      "Step 252580 (epoch   78.93), loss: 0.004596, time: 2.243 s\n",
      "Step 252600 (epoch   78.94), loss: 0.002467, time: 2.244 s, error: 0.010586\n",
      "Step 252620 (epoch   78.94), loss: 0.005580, time: 15.043 s\n",
      "Step 252640 (epoch   78.95), loss: 0.003724, time: 2.226 s\n",
      "Step 252660 (epoch   78.96), loss: 0.008260, time: 2.238 s\n",
      "Step 252680 (epoch   78.96), loss: 0.005345, time: 2.240 s\n",
      "Step 252700 (epoch   78.97), loss: 0.003978, time: 2.241 s\n",
      "Step 252720 (epoch   78.97), loss: 0.003272, time: 2.246 s\n",
      "Step 252740 (epoch   78.98), loss: 0.004875, time: 2.234 s\n",
      "Step 252760 (epoch   78.99), loss: 0.001728, time: 2.250 s\n",
      "Step 252780 (epoch   78.99), loss: 0.004923, time: 2.243 s\n",
      "Step 252800 (epoch   79.00), loss: 0.003977, time: 2.262 s, error: 0.010205\n",
      "Step 252820 (epoch   79.01), loss: 0.005561, time: 15.073 s\n",
      "Step 252840 (epoch   79.01), loss: 0.002613, time: 2.243 s\n",
      "Step 252860 (epoch   79.02), loss: 0.005491, time: 2.237 s\n",
      "Step 252880 (epoch   79.03), loss: 0.003676, time: 2.233 s\n",
      "Step 252900 (epoch   79.03), loss: 0.009580, time: 2.248 s\n",
      "Step 252920 (epoch   79.04), loss: 0.003808, time: 2.243 s\n",
      "Step 252940 (epoch   79.04), loss: 0.002750, time: 2.245 s\n",
      "Step 252960 (epoch   79.05), loss: 0.003719, time: 2.236 s\n",
      "Step 252980 (epoch   79.06), loss: 0.003130, time: 2.249 s\n",
      "Step 253000 (epoch   79.06), loss: 0.003427, time: 2.242 s, error: 0.010220\n",
      "Step 253020 (epoch   79.07), loss: 0.007087, time: 15.258 s\n",
      "Step 253040 (epoch   79.08), loss: 0.003945, time: 2.246 s\n",
      "Step 253060 (epoch   79.08), loss: 0.005174, time: 2.244 s\n",
      "Step 253080 (epoch   79.09), loss: 0.005402, time: 2.242 s\n",
      "Step 253100 (epoch   79.09), loss: 0.004453, time: 2.233 s\n",
      "Step 253120 (epoch   79.10), loss: 0.005181, time: 2.256 s\n",
      "Step 253140 (epoch   79.11), loss: 0.003752, time: 2.241 s\n",
      "Step 253160 (epoch   79.11), loss: 0.002752, time: 2.240 s\n",
      "Step 253180 (epoch   79.12), loss: 0.003630, time: 2.237 s\n",
      "Step 253200 (epoch   79.12), loss: 0.003756, time: 2.245 s, error: 0.010205\n",
      "Step 253220 (epoch   79.13), loss: 0.005352, time: 15.209 s\n",
      "Step 253240 (epoch   79.14), loss: 0.003666, time: 2.233 s\n",
      "Step 253260 (epoch   79.14), loss: 0.005323, time: 2.239 s\n",
      "Step 253280 (epoch   79.15), loss: 0.005113, time: 2.233 s\n",
      "Step 253300 (epoch   79.16), loss: 0.008061, time: 2.244 s\n",
      "Step 253320 (epoch   79.16), loss: 0.003316, time: 2.234 s\n",
      "Step 253340 (epoch   79.17), loss: 0.003802, time: 2.248 s\n",
      "Step 253360 (epoch   79.17), loss: 0.003361, time: 2.241 s\n",
      "Step 253380 (epoch   79.18), loss: 0.003297, time: 2.245 s\n",
      "Step 253400 (epoch   79.19), loss: 0.003946, time: 2.232 s, error: 0.011010\n",
      "Step 253420 (epoch   79.19), loss: 0.003684, time: 15.004 s\n",
      "Step 253440 (epoch   79.20), loss: 0.003002, time: 2.254 s\n",
      "Step 253460 (epoch   79.21), loss: 0.003647, time: 2.249 s\n",
      "Step 253480 (epoch   79.21), loss: 0.004301, time: 2.250 s\n",
      "Step 253500 (epoch   79.22), loss: 0.003357, time: 2.235 s\n",
      "Step 253520 (epoch   79.22), loss: 0.003585, time: 2.248 s\n",
      "Step 253540 (epoch   79.23), loss: 0.003421, time: 2.236 s\n",
      "Step 253560 (epoch   79.24), loss: 0.005702, time: 2.242 s\n",
      "Step 253580 (epoch   79.24), loss: 0.003150, time: 2.232 s\n",
      "Step 253600 (epoch   79.25), loss: 0.005612, time: 2.243 s, error: 0.010592\n",
      "Step 253620 (epoch   79.26), loss: 0.003966, time: 15.018 s\n",
      "Step 253640 (epoch   79.26), loss: 0.003835, time: 2.228 s\n",
      "Step 253660 (epoch   79.27), loss: 0.003425, time: 2.253 s\n",
      "Step 253680 (epoch   79.28), loss: 0.004578, time: 2.235 s\n",
      "Step 253700 (epoch   79.28), loss: 0.005026, time: 2.252 s\n",
      "Step 253720 (epoch   79.29), loss: 0.004769, time: 2.254 s\n",
      "Step 253740 (epoch   79.29), loss: 0.006217, time: 2.247 s\n",
      "Step 253760 (epoch   79.30), loss: 0.004980, time: 2.234 s\n",
      "Step 253780 (epoch   79.31), loss: 0.003034, time: 2.244 s\n",
      "Step 253800 (epoch   79.31), loss: 0.003389, time: 2.244 s, error: 0.010137\n",
      "Step 253820 (epoch   79.32), loss: 0.004389, time: 15.042 s\n",
      "Step 253840 (epoch   79.33), loss: 0.003952, time: 2.247 s\n",
      "Step 253860 (epoch   79.33), loss: 0.005840, time: 2.236 s\n",
      "Step 253880 (epoch   79.34), loss: 0.004692, time: 2.245 s\n",
      "Step 253900 (epoch   79.34), loss: 0.003334, time: 2.241 s\n",
      "Step 253920 (epoch   79.35), loss: 0.003768, time: 2.244 s\n",
      "Step 253940 (epoch   79.36), loss: 0.005255, time: 2.248 s\n",
      "Step 253960 (epoch   79.36), loss: 0.004149, time: 2.254 s\n",
      "Step 253980 (epoch   79.37), loss: 0.003409, time: 2.259 s\n",
      "Step 254000 (epoch   79.38), loss: 0.003643, time: 2.246 s, error: 0.010159\n",
      "\n",
      "Time since beginning  : 44584.158 s\n",
      "\n",
      "Step 254020 (epoch   79.38), loss: 0.004199, time: 15.152 s\n",
      "Step 254040 (epoch   79.39), loss: 0.002756, time: 2.240 s\n",
      "Step 254060 (epoch   79.39), loss: 0.005755, time: 2.243 s\n",
      "Step 254080 (epoch   79.40), loss: 0.005596, time: 2.236 s\n",
      "Step 254100 (epoch   79.41), loss: 0.004594, time: 2.241 s\n",
      "Step 254120 (epoch   79.41), loss: 0.005510, time: 2.233 s\n",
      "Step 254140 (epoch   79.42), loss: 0.005712, time: 2.238 s\n",
      "Step 254160 (epoch   79.42), loss: 0.003801, time: 2.243 s\n",
      "Step 254180 (epoch   79.43), loss: 0.005452, time: 2.242 s\n",
      "Step 254200 (epoch   79.44), loss: 0.002963, time: 2.232 s, error: 0.010252\n",
      "Step 254220 (epoch   79.44), loss: 0.009224, time: 15.221 s\n",
      "Step 254240 (epoch   79.45), loss: 0.004112, time: 2.233 s\n",
      "Step 254260 (epoch   79.46), loss: 0.002608, time: 2.224 s\n",
      "Step 254280 (epoch   79.46), loss: 0.003772, time: 2.256 s\n",
      "Step 254300 (epoch   79.47), loss: 0.003690, time: 2.257 s\n",
      "Step 254320 (epoch   79.47), loss: 0.003892, time: 2.247 s\n",
      "Step 254340 (epoch   79.48), loss: 0.003056, time: 2.242 s\n",
      "Step 254360 (epoch   79.49), loss: 0.007017, time: 2.235 s\n",
      "Step 254380 (epoch   79.49), loss: 0.008706, time: 2.233 s\n",
      "Step 254400 (epoch   79.50), loss: 0.004320, time: 2.232 s, error: 0.010994\n",
      "Step 254420 (epoch   79.51), loss: 0.003270, time: 15.209 s\n",
      "Step 254440 (epoch   79.51), loss: 0.003399, time: 2.241 s\n",
      "Step 254460 (epoch   79.52), loss: 0.003014, time: 2.251 s\n",
      "Step 254480 (epoch   79.53), loss: 0.002355, time: 2.246 s\n",
      "Step 254500 (epoch   79.53), loss: 0.004531, time: 2.247 s\n",
      "Step 254520 (epoch   79.54), loss: 0.006689, time: 2.239 s\n",
      "Step 254540 (epoch   79.54), loss: 0.002984, time: 2.244 s\n",
      "Step 254560 (epoch   79.55), loss: 0.002959, time: 2.230 s\n",
      "Step 254580 (epoch   79.56), loss: 0.005260, time: 2.239 s\n",
      "Step 254600 (epoch   79.56), loss: 0.003375, time: 2.247 s, error: 0.010280\n",
      "Step 254620 (epoch   79.57), loss: 0.002995, time: 15.042 s\n",
      "Step 254640 (epoch   79.58), loss: 0.003880, time: 2.241 s\n",
      "Step 254660 (epoch   79.58), loss: 0.004201, time: 2.229 s\n",
      "Step 254680 (epoch   79.59), loss: 0.005711, time: 2.252 s\n",
      "Step 254700 (epoch   79.59), loss: 0.002820, time: 2.243 s\n",
      "Step 254720 (epoch   79.60), loss: 0.007439, time: 2.243 s\n",
      "Step 254740 (epoch   79.61), loss: 0.006164, time: 2.238 s\n",
      "Step 254760 (epoch   79.61), loss: 0.003606, time: 2.240 s\n",
      "Step 254780 (epoch   79.62), loss: 0.004221, time: 2.242 s\n",
      "Step 254800 (epoch   79.62), loss: 0.003423, time: 2.230 s, error: 0.010207\n",
      "Step 254820 (epoch   79.63), loss: 0.003544, time: 15.034 s\n",
      "Step 254840 (epoch   79.64), loss: 0.003346, time: 2.236 s\n",
      "Step 254860 (epoch   79.64), loss: 0.004840, time: 2.240 s\n",
      "Step 254880 (epoch   79.65), loss: 0.003035, time: 2.261 s\n",
      "Step 254900 (epoch   79.66), loss: 0.003755, time: 2.244 s\n",
      "Step 254920 (epoch   79.66), loss: 0.003319, time: 2.241 s\n",
      "Step 254940 (epoch   79.67), loss: 0.004155, time: 2.244 s\n",
      "Step 254960 (epoch   79.67), loss: 0.003896, time: 2.245 s\n",
      "Step 254980 (epoch   79.68), loss: 0.003125, time: 2.242 s\n",
      "Step 255000 (epoch   79.69), loss: 0.005048, time: 2.252 s, error: 0.010309\n",
      "Step 255020 (epoch   79.69), loss: 0.004356, time: 15.032 s\n",
      "Step 255040 (epoch   79.70), loss: 0.004541, time: 2.244 s\n",
      "Step 255060 (epoch   79.71), loss: 0.003158, time: 2.239 s\n",
      "Step 255080 (epoch   79.71), loss: 0.003257, time: 2.242 s\n",
      "Step 255100 (epoch   79.72), loss: 0.002742, time: 2.239 s\n",
      "Step 255120 (epoch   79.72), loss: 0.002763, time: 2.249 s\n",
      "Step 255140 (epoch   79.73), loss: 0.003855, time: 2.259 s\n",
      "Step 255160 (epoch   79.74), loss: 0.003718, time: 2.234 s\n",
      "Step 255180 (epoch   79.74), loss: 0.004020, time: 2.246 s\n",
      "Step 255200 (epoch   79.75), loss: 0.006120, time: 2.250 s, error: 0.010516\n",
      "Step 255220 (epoch   79.76), loss: 0.003026, time: 15.054 s\n",
      "Step 255240 (epoch   79.76), loss: 0.002798, time: 2.234 s\n",
      "Step 255260 (epoch   79.77), loss: 0.004375, time: 2.242 s\n",
      "Step 255280 (epoch   79.78), loss: 0.004386, time: 2.240 s\n",
      "Step 255300 (epoch   79.78), loss: 0.004752, time: 2.235 s\n",
      "Step 255320 (epoch   79.79), loss: 0.004151, time: 2.234 s\n",
      "Step 255340 (epoch   79.79), loss: 0.005776, time: 2.243 s\n",
      "Step 255360 (epoch   79.80), loss: 0.003762, time: 2.244 s\n",
      "Step 255380 (epoch   79.81), loss: 0.002886, time: 2.250 s\n",
      "Step 255400 (epoch   79.81), loss: 0.002989, time: 2.257 s, error: 0.010135\n",
      "Step 255420 (epoch   79.82), loss: 0.004055, time: 15.058 s\n",
      "Step 255440 (epoch   79.83), loss: 0.002841, time: 2.232 s\n",
      "Step 255460 (epoch   79.83), loss: 0.002799, time: 2.222 s\n",
      "Step 255480 (epoch   79.84), loss: 0.004087, time: 2.236 s\n",
      "Step 255500 (epoch   79.84), loss: 0.004746, time: 2.224 s\n",
      "Step 255520 (epoch   79.85), loss: 0.003393, time: 2.241 s\n",
      "Step 255540 (epoch   79.86), loss: 0.004652, time: 2.234 s\n",
      "Step 255560 (epoch   79.86), loss: 0.003323, time: 2.239 s\n",
      "Step 255580 (epoch   79.87), loss: 0.004502, time: 2.242 s\n",
      "Step 255600 (epoch   79.88), loss: 0.003105, time: 2.237 s, error: 0.010026\n",
      "Step 255620 (epoch   79.88), loss: 0.003308, time: 15.269 s\n",
      "Step 255640 (epoch   79.89), loss: 0.010400, time: 2.238 s\n",
      "Step 255660 (epoch   79.89), loss: 0.003995, time: 2.233 s\n",
      "Step 255680 (epoch   79.90), loss: 0.006360, time: 2.234 s\n",
      "Step 255700 (epoch   79.91), loss: 0.005121, time: 2.240 s\n",
      "Step 255720 (epoch   79.91), loss: 0.004571, time: 2.243 s\n",
      "Step 255740 (epoch   79.92), loss: 0.004009, time: 2.240 s\n",
      "Step 255760 (epoch   79.92), loss: 0.004562, time: 2.243 s\n",
      "Step 255780 (epoch   79.93), loss: 0.006634, time: 2.247 s\n",
      "Step 255800 (epoch   79.94), loss: 0.003841, time: 2.236 s, error: 0.010670\n",
      "Step 255820 (epoch   79.94), loss: 0.008771, time: 15.258 s\n",
      "Step 255840 (epoch   79.95), loss: 0.004159, time: 2.234 s\n",
      "Step 255860 (epoch   79.96), loss: 0.003555, time: 2.230 s\n",
      "Step 255880 (epoch   79.96), loss: 0.003193, time: 2.240 s\n",
      "Step 255900 (epoch   79.97), loss: 0.005406, time: 2.253 s\n",
      "Step 255920 (epoch   79.97), loss: 0.007671, time: 2.233 s\n",
      "Step 255940 (epoch   79.98), loss: 0.003862, time: 2.250 s\n",
      "Step 255960 (epoch   79.99), loss: 0.003251, time: 2.239 s\n",
      "Step 255980 (epoch   79.99), loss: 0.004041, time: 2.252 s\n",
      "Step 256000 (epoch   80.00), loss: 0.004840, time: 2.229 s, error: 0.010168\n",
      "\n",
      "Time since beginning  : 44937.109 s\n",
      "\n",
      "Step 256020 (epoch   80.01), loss: 0.002802, time: 15.121 s\n",
      "Step 256040 (epoch   80.01), loss: 0.004455, time: 2.256 s\n",
      "Step 256060 (epoch   80.02), loss: 0.006142, time: 2.251 s\n",
      "Step 256080 (epoch   80.03), loss: 0.008470, time: 2.249 s\n",
      "Step 256100 (epoch   80.03), loss: 0.003639, time: 2.243 s\n",
      "Step 256120 (epoch   80.04), loss: 0.005108, time: 2.247 s\n",
      "Step 256140 (epoch   80.04), loss: 0.003578, time: 2.229 s\n",
      "Step 256160 (epoch   80.05), loss: 0.004639, time: 2.248 s\n",
      "Step 256180 (epoch   80.06), loss: 0.005090, time: 2.227 s\n",
      "Step 256200 (epoch   80.06), loss: 0.005816, time: 2.243 s, error: 0.010304\n",
      "Step 256220 (epoch   80.07), loss: 0.004157, time: 15.271 s\n",
      "Step 256240 (epoch   80.08), loss: 0.004082, time: 2.241 s\n",
      "Step 256260 (epoch   80.08), loss: 0.004088, time: 2.240 s\n",
      "Step 256280 (epoch   80.09), loss: 0.003769, time: 2.252 s\n",
      "Step 256300 (epoch   80.09), loss: 0.003759, time: 2.248 s\n",
      "Step 256320 (epoch   80.10), loss: 0.003275, time: 2.249 s\n",
      "Step 256340 (epoch   80.11), loss: 0.003533, time: 2.238 s\n",
      "Step 256360 (epoch   80.11), loss: 0.002729, time: 2.235 s\n",
      "Step 256380 (epoch   80.12), loss: 0.003637, time: 2.242 s\n",
      "Step 256400 (epoch   80.12), loss: 0.003804, time: 2.238 s, error: 0.010188\n",
      "Step 256420 (epoch   80.13), loss: 0.003840, time: 15.106 s\n",
      "Step 256440 (epoch   80.14), loss: 0.003111, time: 2.246 s\n",
      "Step 256460 (epoch   80.14), loss: 0.006729, time: 2.245 s\n",
      "Step 256480 (epoch   80.15), loss: 0.004427, time: 2.248 s\n",
      "Step 256500 (epoch   80.16), loss: 0.003031, time: 2.236 s\n",
      "Step 256520 (epoch   80.16), loss: 0.004283, time: 2.238 s\n",
      "Step 256540 (epoch   80.17), loss: 0.003161, time: 2.249 s\n",
      "Step 256560 (epoch   80.17), loss: 0.004860, time: 2.262 s\n",
      "Step 256580 (epoch   80.18), loss: 0.003157, time: 2.254 s\n",
      "Step 256600 (epoch   80.19), loss: 0.004416, time: 2.233 s, error: 0.010950\n",
      "Step 256620 (epoch   80.19), loss: 0.002215, time: 15.028 s\n",
      "Step 256640 (epoch   80.20), loss: 0.004686, time: 2.226 s\n",
      "Step 256660 (epoch   80.21), loss: 0.004449, time: 2.241 s\n",
      "Step 256680 (epoch   80.21), loss: 0.009717, time: 2.243 s\n",
      "Step 256700 (epoch   80.22), loss: 0.004924, time: 2.234 s\n",
      "Step 256720 (epoch   80.22), loss: 0.003747, time: 2.225 s\n",
      "Step 256740 (epoch   80.23), loss: 0.003997, time: 2.238 s\n",
      "Step 256760 (epoch   80.24), loss: 0.003527, time: 2.243 s\n",
      "Step 256780 (epoch   80.24), loss: 0.007121, time: 2.238 s\n",
      "Step 256800 (epoch   80.25), loss: 0.004218, time: 2.249 s, error: 0.010596\n",
      "Step 256820 (epoch   80.26), loss: 0.004147, time: 15.187 s\n",
      "Step 256840 (epoch   80.26), loss: 0.002282, time: 2.232 s\n",
      "Step 256860 (epoch   80.27), loss: 0.004030, time: 2.250 s\n",
      "Step 256880 (epoch   80.28), loss: 0.002946, time: 2.232 s\n",
      "Step 256900 (epoch   80.28), loss: 0.003605, time: 2.239 s\n",
      "Step 256920 (epoch   80.29), loss: 0.004922, time: 2.227 s\n",
      "Step 256940 (epoch   80.29), loss: 0.003717, time: 2.228 s\n",
      "Step 256960 (epoch   80.30), loss: 0.004013, time: 2.236 s\n",
      "Step 256980 (epoch   80.31), loss: 0.003892, time: 2.242 s\n",
      "Step 257000 (epoch   80.31), loss: 0.002896, time: 2.249 s, error: 0.010027\n",
      "Step 257020 (epoch   80.32), loss: 0.007210, time: 15.216 s\n",
      "Step 257040 (epoch   80.33), loss: 0.004829, time: 2.245 s\n",
      "Step 257060 (epoch   80.33), loss: 0.005202, time: 2.235 s\n",
      "Step 257080 (epoch   80.34), loss: 0.004032, time: 2.244 s\n",
      "Step 257100 (epoch   80.34), loss: 0.004948, time: 2.236 s\n",
      "Step 257120 (epoch   80.35), loss: 0.004275, time: 2.241 s\n",
      "Step 257140 (epoch   80.36), loss: 0.006103, time: 2.246 s\n",
      "Step 257160 (epoch   80.36), loss: 0.004557, time: 2.255 s\n",
      "Step 257180 (epoch   80.37), loss: 0.005381, time: 2.254 s\n",
      "Step 257200 (epoch   80.38), loss: 0.003530, time: 2.245 s, error: 0.010154\n",
      "Step 257220 (epoch   80.38), loss: 0.008391, time: 15.082 s\n",
      "Step 257240 (epoch   80.39), loss: 0.003540, time: 2.235 s\n",
      "Step 257260 (epoch   80.39), loss: 0.004283, time: 2.247 s\n",
      "Step 257280 (epoch   80.40), loss: 0.003448, time: 2.246 s\n",
      "Step 257300 (epoch   80.41), loss: 0.004611, time: 2.241 s\n",
      "Step 257320 (epoch   80.41), loss: 0.004502, time: 2.242 s\n",
      "Step 257340 (epoch   80.42), loss: 0.003183, time: 2.246 s\n",
      "Step 257360 (epoch   80.42), loss: 0.003196, time: 2.239 s\n",
      "Step 257380 (epoch   80.43), loss: 0.004454, time: 2.226 s\n",
      "Step 257400 (epoch   80.44), loss: 0.007183, time: 2.243 s, error: 0.010503\n",
      "Step 257420 (epoch   80.44), loss: 0.003534, time: 15.128 s\n",
      "Step 257440 (epoch   80.45), loss: 0.004880, time: 2.251 s\n",
      "Step 257460 (epoch   80.46), loss: 0.002942, time: 2.255 s\n",
      "Step 257480 (epoch   80.46), loss: 0.003061, time: 2.255 s\n",
      "Step 257500 (epoch   80.47), loss: 0.004692, time: 2.235 s\n",
      "Step 257520 (epoch   80.47), loss: 0.005609, time: 2.237 s\n",
      "Step 257540 (epoch   80.48), loss: 0.004154, time: 2.242 s\n",
      "Step 257560 (epoch   80.49), loss: 0.004216, time: 2.239 s\n",
      "Step 257580 (epoch   80.49), loss: 0.004889, time: 2.234 s\n",
      "Step 257600 (epoch   80.50), loss: 0.004000, time: 2.238 s, error: 0.010746\n",
      "Step 257620 (epoch   80.51), loss: 0.003910, time: 15.081 s\n",
      "Step 257640 (epoch   80.51), loss: 0.006360, time: 2.232 s\n",
      "Step 257660 (epoch   80.52), loss: 0.003967, time: 2.230 s\n",
      "Step 257680 (epoch   80.53), loss: 0.003533, time: 2.249 s\n",
      "Step 257700 (epoch   80.53), loss: 0.007473, time: 2.240 s\n",
      "Step 257720 (epoch   80.54), loss: 0.004478, time: 2.255 s\n",
      "Step 257740 (epoch   80.54), loss: 0.004227, time: 2.254 s\n",
      "Step 257760 (epoch   80.55), loss: 0.004018, time: 2.250 s\n",
      "Step 257780 (epoch   80.56), loss: 0.003517, time: 2.249 s\n",
      "Step 257800 (epoch   80.56), loss: 0.003182, time: 2.244 s, error: 0.010181\n",
      "Step 257820 (epoch   80.57), loss: 0.003478, time: 15.082 s\n",
      "Step 257840 (epoch   80.58), loss: 0.005955, time: 2.240 s\n",
      "Step 257860 (epoch   80.58), loss: 0.004463, time: 2.228 s\n",
      "Step 257880 (epoch   80.59), loss: 0.003703, time: 2.252 s\n",
      "Step 257900 (epoch   80.59), loss: 0.003281, time: 2.242 s\n",
      "Step 257920 (epoch   80.60), loss: 0.002591, time: 2.252 s\n",
      "Step 257940 (epoch   80.61), loss: 0.003258, time: 2.238 s\n",
      "Step 257960 (epoch   80.61), loss: 0.004060, time: 2.248 s\n",
      "Step 257980 (epoch   80.62), loss: 0.005312, time: 2.261 s\n",
      "Step 258000 (epoch   80.62), loss: 0.004258, time: 2.242 s, error: 0.010182\n",
      "\n",
      "Time since beginning  : 45290.231 s\n",
      "\n",
      "Step 258020 (epoch   80.63), loss: 0.004067, time: 15.117 s\n",
      "Step 258040 (epoch   80.64), loss: 0.004085, time: 2.236 s\n",
      "Step 258060 (epoch   80.64), loss: 0.003780, time: 2.246 s\n",
      "Step 258080 (epoch   80.65), loss: 0.005348, time: 2.231 s\n",
      "Step 258100 (epoch   80.66), loss: 0.007030, time: 2.241 s\n",
      "Step 258120 (epoch   80.66), loss: 0.005633, time: 2.235 s\n",
      "Step 258140 (epoch   80.67), loss: 0.005190, time: 2.249 s\n",
      "Step 258160 (epoch   80.67), loss: 0.004861, time: 2.232 s\n",
      "Step 258180 (epoch   80.68), loss: 0.004693, time: 2.238 s\n",
      "Step 258200 (epoch   80.69), loss: 0.004924, time: 2.249 s, error: 0.010281\n",
      "Step 258220 (epoch   80.69), loss: 0.004634, time: 15.262 s\n",
      "Step 258240 (epoch   80.70), loss: 0.003725, time: 2.236 s\n",
      "Step 258260 (epoch   80.71), loss: 0.016896, time: 2.236 s\n",
      "Step 258280 (epoch   80.71), loss: 0.005188, time: 2.241 s\n",
      "Step 258300 (epoch   80.72), loss: 0.003539, time: 2.220 s\n",
      "Step 258320 (epoch   80.72), loss: 0.004552, time: 2.241 s\n",
      "Step 258340 (epoch   80.73), loss: 0.005566, time: 2.229 s\n",
      "Step 258360 (epoch   80.74), loss: 0.003339, time: 2.246 s\n",
      "Step 258380 (epoch   80.74), loss: 0.006541, time: 2.239 s\n",
      "Step 258400 (epoch   80.75), loss: 0.004196, time: 2.239 s, error: 0.010527\n",
      "Step 258420 (epoch   80.76), loss: 0.003274, time: 15.222 s\n",
      "Step 258440 (epoch   80.76), loss: 0.003884, time: 2.233 s\n",
      "Step 258460 (epoch   80.77), loss: 0.004351, time: 2.251 s\n",
      "Step 258480 (epoch   80.78), loss: 0.004649, time: 2.253 s\n",
      "Step 258500 (epoch   80.78), loss: 0.004422, time: 2.232 s\n",
      "Step 258520 (epoch   80.79), loss: 0.005439, time: 2.235 s\n",
      "Step 258540 (epoch   80.79), loss: 0.003027, time: 2.234 s\n",
      "Step 258560 (epoch   80.80), loss: 0.004429, time: 2.246 s\n",
      "Step 258580 (epoch   80.81), loss: 0.003426, time: 2.255 s\n",
      "Step 258600 (epoch   80.81), loss: 0.004644, time: 2.247 s, error: 0.010045\n",
      "Step 258620 (epoch   80.82), loss: 0.004033, time: 15.026 s\n",
      "Step 258640 (epoch   80.83), loss: 0.007043, time: 2.250 s\n",
      "Step 258660 (epoch   80.83), loss: 0.003409, time: 2.249 s\n",
      "Step 258680 (epoch   80.84), loss: 0.004692, time: 2.234 s\n",
      "Step 258700 (epoch   80.84), loss: 0.003826, time: 2.246 s\n",
      "Step 258720 (epoch   80.85), loss: 0.003134, time: 2.231 s\n",
      "Step 258740 (epoch   80.86), loss: 0.002735, time: 2.249 s\n",
      "Step 258760 (epoch   80.86), loss: 0.004032, time: 2.239 s\n",
      "Step 258780 (epoch   80.87), loss: 0.001976, time: 2.249 s\n",
      "Step 258800 (epoch   80.88), loss: 0.003807, time: 2.242 s, error: 0.010012\n",
      "Step 258820 (epoch   80.88), loss: 0.003856, time: 14.997 s\n",
      "Step 258840 (epoch   80.89), loss: 0.003032, time: 2.252 s\n",
      "Step 258860 (epoch   80.89), loss: 0.005604, time: 2.249 s\n",
      "Step 258880 (epoch   80.90), loss: 0.006454, time: 2.249 s\n",
      "Step 258900 (epoch   80.91), loss: 0.005249, time: 2.251 s\n",
      "Step 258920 (epoch   80.91), loss: 0.005169, time: 2.234 s\n",
      "Step 258940 (epoch   80.92), loss: 0.002531, time: 2.242 s\n",
      "Step 258960 (epoch   80.92), loss: 0.003403, time: 2.239 s\n",
      "Step 258980 (epoch   80.93), loss: 0.003806, time: 2.250 s\n",
      "Step 259000 (epoch   80.94), loss: 0.003524, time: 2.240 s, error: 0.010562\n",
      "Step 259020 (epoch   80.94), loss: 0.002741, time: 15.080 s\n",
      "Step 259040 (epoch   80.95), loss: 0.003860, time: 2.228 s\n",
      "Step 259060 (epoch   80.96), loss: 0.003348, time: 2.237 s\n",
      "Step 259080 (epoch   80.96), loss: 0.003489, time: 2.254 s\n",
      "Step 259100 (epoch   80.97), loss: 0.004288, time: 2.248 s\n",
      "Step 259120 (epoch   80.97), loss: 0.003921, time: 2.242 s\n",
      "Step 259140 (epoch   80.98), loss: 0.003976, time: 2.255 s\n",
      "Step 259160 (epoch   80.99), loss: 0.003539, time: 2.255 s\n",
      "Step 259180 (epoch   80.99), loss: 0.004414, time: 2.224 s\n",
      "Step 259200 (epoch   81.00), loss: 0.003004, time: 2.247 s, error: 0.010172\n",
      "Step 259220 (epoch   81.01), loss: 0.005839, time: 15.053 s\n",
      "Step 259240 (epoch   81.01), loss: 0.005463, time: 2.245 s\n",
      "Step 259260 (epoch   81.02), loss: 0.003184, time: 2.246 s\n",
      "Step 259280 (epoch   81.03), loss: 0.003997, time: 2.244 s\n",
      "Step 259300 (epoch   81.03), loss: 0.004110, time: 2.248 s\n",
      "Step 259320 (epoch   81.04), loss: 0.003267, time: 2.238 s\n",
      "Step 259340 (epoch   81.04), loss: 0.007551, time: 2.236 s\n",
      "Step 259360 (epoch   81.05), loss: 0.002890, time: 2.232 s\n",
      "Step 259380 (epoch   81.06), loss: 0.003567, time: 2.247 s\n",
      "Step 259400 (epoch   81.06), loss: 0.005443, time: 2.253 s, error: 0.010201\n",
      "Step 259420 (epoch   81.07), loss: 0.003438, time: 15.110 s\n",
      "Step 259440 (epoch   81.08), loss: 0.003732, time: 2.252 s\n",
      "Step 259460 (epoch   81.08), loss: 0.003536, time: 2.242 s\n",
      "Step 259480 (epoch   81.09), loss: 0.004204, time: 2.231 s\n",
      "Step 259500 (epoch   81.09), loss: 0.003449, time: 2.248 s\n",
      "Step 259520 (epoch   81.10), loss: 0.004011, time: 2.241 s\n",
      "Step 259540 (epoch   81.11), loss: 0.006546, time: 2.239 s\n",
      "Step 259560 (epoch   81.11), loss: 0.007289, time: 2.232 s\n",
      "Step 259580 (epoch   81.12), loss: 0.003530, time: 2.230 s\n",
      "Step 259600 (epoch   81.12), loss: 0.003064, time: 2.239 s, error: 0.010104\n",
      "Step 259620 (epoch   81.13), loss: 0.006402, time: 15.243 s\n",
      "Step 259640 (epoch   81.14), loss: 0.002114, time: 2.245 s\n",
      "Step 259660 (epoch   81.14), loss: 0.004385, time: 2.243 s\n",
      "Step 259680 (epoch   81.15), loss: 0.004270, time: 2.233 s\n",
      "Step 259700 (epoch   81.16), loss: 0.003119, time: 2.242 s\n",
      "Step 259720 (epoch   81.16), loss: 0.003373, time: 2.245 s\n",
      "Step 259740 (epoch   81.17), loss: 0.002698, time: 2.242 s\n",
      "Step 259760 (epoch   81.17), loss: 0.002442, time: 2.240 s\n",
      "Step 259780 (epoch   81.18), loss: 0.003729, time: 2.242 s\n",
      "Step 259800 (epoch   81.19), loss: 0.002681, time: 2.243 s, error: 0.010728\n",
      "Step 259820 (epoch   81.19), loss: 0.003679, time: 15.243 s\n",
      "Step 259840 (epoch   81.20), loss: 0.002623, time: 2.235 s\n",
      "Step 259860 (epoch   81.21), loss: 0.006505, time: 2.238 s\n",
      "Step 259880 (epoch   81.21), loss: 0.002831, time: 2.231 s\n",
      "Step 259900 (epoch   81.22), loss: 0.005630, time: 2.231 s\n",
      "Step 259920 (epoch   81.22), loss: 0.003276, time: 2.239 s\n",
      "Step 259940 (epoch   81.23), loss: 0.004124, time: 2.238 s\n",
      "Step 259960 (epoch   81.24), loss: 0.006828, time: 2.239 s\n",
      "Step 259980 (epoch   81.24), loss: 0.003376, time: 2.238 s\n",
      "Step 260000 (epoch   81.25), loss: 0.005010, time: 2.241 s, error: 0.010588\n",
      "\n",
      "Time since beginning  : 45643.284 s\n",
      "\n",
      "Step 260020 (epoch   81.26), loss: 0.003562, time: 15.131 s\n",
      "Step 260040 (epoch   81.26), loss: 0.008535, time: 2.249 s\n",
      "Step 260060 (epoch   81.27), loss: 0.008461, time: 2.262 s\n",
      "Step 260080 (epoch   81.28), loss: 0.004226, time: 2.255 s\n",
      "Step 260100 (epoch   81.28), loss: 0.003311, time: 2.256 s\n",
      "Step 260120 (epoch   81.29), loss: 0.007839, time: 2.253 s\n",
      "Step 260140 (epoch   81.29), loss: 0.008951, time: 2.251 s\n",
      "Step 260160 (epoch   81.30), loss: 0.004895, time: 2.233 s\n",
      "Step 260180 (epoch   81.31), loss: 0.006348, time: 2.241 s\n",
      "Step 260200 (epoch   81.31), loss: 0.004785, time: 2.231 s, error: 0.009979\n",
      "Step 260220 (epoch   81.32), loss: 0.004532, time: 15.035 s\n",
      "Step 260240 (epoch   81.33), loss: 0.003165, time: 2.245 s\n",
      "Step 260260 (epoch   81.33), loss: 0.002760, time: 2.235 s\n",
      "Step 260280 (epoch   81.34), loss: 0.002701, time: 2.245 s\n",
      "Step 260300 (epoch   81.34), loss: 0.003311, time: 2.236 s\n",
      "Step 260320 (epoch   81.35), loss: 0.003127, time: 2.262 s\n",
      "Step 260340 (epoch   81.36), loss: 0.002787, time: 2.242 s\n",
      "Step 260360 (epoch   81.36), loss: 0.006555, time: 2.245 s\n",
      "Step 260380 (epoch   81.37), loss: 0.003683, time: 2.219 s\n",
      "Step 260400 (epoch   81.38), loss: 0.002962, time: 2.241 s, error: 0.010127\n",
      "Step 260420 (epoch   81.38), loss: 0.003872, time: 15.010 s\n",
      "Step 260440 (epoch   81.39), loss: 0.005809, time: 2.235 s\n",
      "Step 260460 (epoch   81.39), loss: 0.003039, time: 2.248 s\n",
      "Step 260480 (epoch   81.40), loss: 0.003974, time: 2.249 s\n",
      "Step 260500 (epoch   81.41), loss: 0.003978, time: 2.247 s\n",
      "Step 260520 (epoch   81.41), loss: 0.003833, time: 2.230 s\n",
      "Step 260540 (epoch   81.42), loss: 0.005123, time: 2.242 s\n",
      "Step 260560 (epoch   81.42), loss: 0.004622, time: 2.241 s\n",
      "Step 260580 (epoch   81.43), loss: 0.004016, time: 2.258 s\n",
      "Step 260600 (epoch   81.44), loss: 0.005286, time: 2.248 s, error: 0.010563\n",
      "Step 260620 (epoch   81.44), loss: 0.004800, time: 15.009 s\n",
      "Step 260640 (epoch   81.45), loss: 0.003261, time: 2.247 s\n",
      "Step 260660 (epoch   81.46), loss: 0.005203, time: 2.254 s\n",
      "Step 260680 (epoch   81.46), loss: 0.011815, time: 2.245 s\n",
      "Step 260700 (epoch   81.47), loss: 0.003791, time: 2.242 s\n",
      "Step 260720 (epoch   81.47), loss: 0.003652, time: 2.239 s\n",
      "Step 260740 (epoch   81.48), loss: 0.004345, time: 2.229 s\n",
      "Step 260760 (epoch   81.49), loss: 0.003804, time: 2.233 s\n",
      "Step 260780 (epoch   81.49), loss: 0.006133, time: 2.245 s\n",
      "Step 260800 (epoch   81.50), loss: 0.003467, time: 2.228 s, error: 0.010660\n",
      "Step 260820 (epoch   81.51), loss: 0.003512, time: 15.257 s\n",
      "Step 260840 (epoch   81.51), loss: 0.011430, time: 2.237 s\n",
      "Step 260860 (epoch   81.52), loss: 0.005040, time: 2.244 s\n",
      "Step 260880 (epoch   81.53), loss: 0.002739, time: 2.234 s\n",
      "Step 260900 (epoch   81.53), loss: 0.007691, time: 2.229 s\n",
      "Step 260920 (epoch   81.54), loss: 0.003448, time: 2.247 s\n",
      "Step 260940 (epoch   81.54), loss: 0.003796, time: 2.232 s\n",
      "Step 260960 (epoch   81.55), loss: 0.003822, time: 2.240 s\n",
      "Step 260980 (epoch   81.56), loss: 0.003109, time: 2.234 s\n",
      "Step 261000 (epoch   81.56), loss: 0.001775, time: 2.243 s, error: 0.010113\n",
      "Step 261020 (epoch   81.57), loss: 0.003718, time: 15.240 s\n",
      "Step 261040 (epoch   81.58), loss: 0.005376, time: 2.226 s\n",
      "Step 261060 (epoch   81.58), loss: 0.004610, time: 2.241 s\n",
      "Step 261080 (epoch   81.59), loss: 0.004479, time: 2.242 s\n",
      "Step 261100 (epoch   81.59), loss: 0.006647, time: 2.238 s\n",
      "Step 261120 (epoch   81.60), loss: 0.002717, time: 2.234 s\n",
      "Step 261140 (epoch   81.61), loss: 0.002430, time: 2.252 s\n",
      "Step 261160 (epoch   81.61), loss: 0.003506, time: 2.233 s\n",
      "Step 261180 (epoch   81.62), loss: 0.004025, time: 2.239 s\n",
      "Step 261200 (epoch   81.62), loss: 0.003321, time: 2.226 s, error: 0.010303\n",
      "Step 261220 (epoch   81.63), loss: 0.003876, time: 15.025 s\n",
      "Step 261240 (epoch   81.64), loss: 0.004635, time: 2.245 s\n",
      "Step 261260 (epoch   81.64), loss: 0.007555, time: 2.225 s\n",
      "Step 261280 (epoch   81.65), loss: 0.002892, time: 2.238 s\n",
      "Step 261300 (epoch   81.66), loss: 0.004444, time: 2.229 s\n",
      "Step 261320 (epoch   81.66), loss: 0.004401, time: 2.235 s\n",
      "Step 261340 (epoch   81.67), loss: 0.003504, time: 2.232 s\n",
      "Step 261360 (epoch   81.67), loss: 0.002965, time: 2.248 s\n",
      "Step 261380 (epoch   81.68), loss: 0.003028, time: 2.250 s\n",
      "Step 261400 (epoch   81.69), loss: 0.003803, time: 2.243 s, error: 0.010401\n",
      "Step 261420 (epoch   81.69), loss: 0.002314, time: 15.024 s\n",
      "Step 261440 (epoch   81.70), loss: 0.003648, time: 2.253 s\n",
      "Step 261460 (epoch   81.71), loss: 0.004007, time: 2.245 s\n",
      "Step 261480 (epoch   81.71), loss: 0.004150, time: 2.256 s\n",
      "Step 261500 (epoch   81.72), loss: 0.003139, time: 2.245 s\n",
      "Step 261520 (epoch   81.72), loss: 0.004663, time: 2.240 s\n",
      "Step 261540 (epoch   81.73), loss: 0.003971, time: 2.249 s\n",
      "Step 261560 (epoch   81.74), loss: 0.006725, time: 2.252 s\n",
      "Step 261580 (epoch   81.74), loss: 0.002772, time: 2.241 s\n",
      "Step 261600 (epoch   81.75), loss: 0.004366, time: 2.242 s, error: 0.010398\n",
      "Step 261620 (epoch   81.76), loss: 0.002340, time: 15.045 s\n",
      "Step 261640 (epoch   81.76), loss: 0.003592, time: 2.241 s\n",
      "Step 261660 (epoch   81.77), loss: 0.003347, time: 2.254 s\n",
      "Step 261680 (epoch   81.78), loss: 0.006856, time: 2.242 s\n",
      "Step 261700 (epoch   81.78), loss: 0.003920, time: 2.233 s\n",
      "Step 261720 (epoch   81.79), loss: 0.007740, time: 2.233 s\n",
      "Step 261740 (epoch   81.79), loss: 0.004777, time: 2.258 s\n",
      "Step 261760 (epoch   81.80), loss: 0.004075, time: 2.239 s\n",
      "Step 261780 (epoch   81.81), loss: 0.006431, time: 2.247 s\n",
      "Step 261800 (epoch   81.81), loss: 0.003609, time: 2.243 s, error: 0.010114\n",
      "Step 261820 (epoch   81.82), loss: 0.006665, time: 15.014 s\n",
      "Step 261840 (epoch   81.83), loss: 0.002624, time: 2.238 s\n",
      "Step 261860 (epoch   81.83), loss: 0.003049, time: 2.243 s\n",
      "Step 261880 (epoch   81.84), loss: 0.003307, time: 2.242 s\n",
      "Step 261900 (epoch   81.84), loss: 0.003952, time: 2.239 s\n",
      "Step 261920 (epoch   81.85), loss: 0.005368, time: 2.243 s\n",
      "Step 261940 (epoch   81.86), loss: 0.003405, time: 2.241 s\n",
      "Step 261960 (epoch   81.86), loss: 0.004430, time: 2.243 s\n",
      "Step 261980 (epoch   81.87), loss: 0.002737, time: 2.236 s\n",
      "Step 262000 (epoch   81.88), loss: 0.005003, time: 2.258 s, error: 0.009971\n",
      "\n",
      "Time since beginning  : 45995.874 s\n",
      "\n",
      "Step 262020 (epoch   81.88), loss: 0.004974, time: 15.146 s\n",
      "Step 262040 (epoch   81.89), loss: 0.002987, time: 2.242 s\n",
      "Step 262060 (epoch   81.89), loss: 0.004451, time: 2.234 s\n",
      "Step 262080 (epoch   81.90), loss: 0.005221, time: 2.250 s\n",
      "Step 262100 (epoch   81.91), loss: 0.002636, time: 2.246 s\n",
      "Step 262120 (epoch   81.91), loss: 0.005356, time: 2.244 s\n",
      "Step 262140 (epoch   81.92), loss: 0.005590, time: 2.232 s\n",
      "Step 262160 (epoch   81.92), loss: 0.002743, time: 2.241 s\n",
      "Step 262180 (epoch   81.93), loss: 0.002481, time: 2.242 s\n",
      "Step 262200 (epoch   81.94), loss: 0.002641, time: 2.250 s, error: 0.010426\n",
      "Step 262220 (epoch   81.94), loss: 0.004132, time: 15.213 s\n",
      "Step 262240 (epoch   81.95), loss: 0.003097, time: 2.245 s\n",
      "Step 262260 (epoch   81.96), loss: 0.003119, time: 2.232 s\n",
      "Step 262280 (epoch   81.96), loss: 0.003773, time: 2.229 s\n",
      "Step 262300 (epoch   81.97), loss: 0.004668, time: 2.249 s\n",
      "Step 262320 (epoch   81.97), loss: 0.004470, time: 2.236 s\n",
      "Step 262340 (epoch   81.98), loss: 0.005954, time: 2.237 s\n",
      "Step 262360 (epoch   81.99), loss: 0.005482, time: 2.238 s\n",
      "Step 262380 (epoch   81.99), loss: 0.003077, time: 2.249 s\n",
      "Step 262400 (epoch   82.00), loss: 0.003823, time: 2.251 s, error: 0.010158\n",
      "Step 262420 (epoch   82.01), loss: 0.004476, time: 15.242 s\n",
      "Step 262440 (epoch   82.01), loss: 0.003903, time: 2.240 s\n",
      "Step 262460 (epoch   82.02), loss: 0.004673, time: 2.241 s\n",
      "Step 262480 (epoch   82.03), loss: 0.006334, time: 2.244 s\n",
      "Step 262500 (epoch   82.03), loss: 0.003012, time: 2.249 s\n",
      "Step 262520 (epoch   82.04), loss: 0.004302, time: 2.250 s\n",
      "Step 262540 (epoch   82.04), loss: 0.004320, time: 2.248 s\n",
      "Step 262560 (epoch   82.05), loss: 0.003149, time: 2.239 s\n",
      "Step 262580 (epoch   82.06), loss: 0.004132, time: 2.234 s\n",
      "Step 262600 (epoch   82.06), loss: 0.004114, time: 2.250 s, error: 0.010132\n",
      "Step 262620 (epoch   82.07), loss: 0.004446, time: 15.023 s\n",
      "Step 262640 (epoch   82.08), loss: 0.003454, time: 2.243 s\n",
      "Step 262660 (epoch   82.08), loss: 0.007323, time: 2.248 s\n",
      "Step 262680 (epoch   82.09), loss: 0.003508, time: 2.231 s\n",
      "Step 262700 (epoch   82.09), loss: 0.004749, time: 2.250 s\n",
      "Step 262720 (epoch   82.10), loss: 0.003817, time: 2.230 s\n",
      "Step 262740 (epoch   82.11), loss: 0.003595, time: 2.242 s\n",
      "Step 262760 (epoch   82.11), loss: 0.005815, time: 2.227 s\n",
      "Step 262780 (epoch   82.12), loss: 0.002979, time: 2.242 s\n",
      "Step 262800 (epoch   82.12), loss: 0.003978, time: 2.234 s, error: 0.010055\n",
      "Step 262820 (epoch   82.13), loss: 0.005206, time: 15.111 s\n",
      "Step 262840 (epoch   82.14), loss: 0.005247, time: 2.228 s\n",
      "Step 262860 (epoch   82.14), loss: 0.004739, time: 2.232 s\n",
      "Step 262880 (epoch   82.15), loss: 0.008301, time: 2.239 s\n",
      "Step 262900 (epoch   82.16), loss: 0.003417, time: 2.249 s\n",
      "Step 262920 (epoch   82.16), loss: 0.003756, time: 2.245 s\n",
      "Step 262940 (epoch   82.17), loss: 0.002347, time: 2.233 s\n",
      "Step 262960 (epoch   82.17), loss: 0.003185, time: 2.251 s\n",
      "Step 262980 (epoch   82.18), loss: 0.003852, time: 2.249 s\n",
      "Step 263000 (epoch   82.19), loss: 0.003999, time: 2.238 s, error: 0.010324\n",
      "Step 263020 (epoch   82.19), loss: 0.003781, time: 15.034 s\n",
      "Step 263040 (epoch   82.20), loss: 0.005017, time: 2.228 s\n",
      "Step 263060 (epoch   82.21), loss: 0.006676, time: 2.252 s\n",
      "Step 263080 (epoch   82.21), loss: 0.004342, time: 2.239 s\n",
      "Step 263100 (epoch   82.22), loss: 0.005339, time: 2.240 s\n",
      "Step 263120 (epoch   82.22), loss: 0.002508, time: 2.243 s\n",
      "Step 263140 (epoch   82.23), loss: 0.003713, time: 2.232 s\n",
      "Step 263160 (epoch   82.24), loss: 0.004495, time: 2.256 s\n",
      "Step 263180 (epoch   82.24), loss: 0.004856, time: 2.249 s\n",
      "Step 263200 (epoch   82.25), loss: 0.004633, time: 2.251 s, error: 0.010449\n",
      "Step 263220 (epoch   82.26), loss: 0.003189, time: 15.036 s\n",
      "Step 263240 (epoch   82.26), loss: 0.004119, time: 2.241 s\n",
      "Step 263260 (epoch   82.27), loss: 0.004190, time: 2.238 s\n",
      "Step 263280 (epoch   82.28), loss: 0.002679, time: 2.219 s\n",
      "Step 263300 (epoch   82.28), loss: 0.003245, time: 2.241 s\n",
      "Step 263320 (epoch   82.29), loss: 0.003054, time: 2.248 s\n",
      "Step 263340 (epoch   82.29), loss: 0.003772, time: 2.237 s\n",
      "Step 263360 (epoch   82.30), loss: 0.005920, time: 2.250 s\n",
      "Step 263380 (epoch   82.31), loss: 0.004159, time: 2.240 s\n",
      "Step 263400 (epoch   82.31), loss: 0.004407, time: 2.242 s, error: 0.009981\n",
      "Step 263420 (epoch   82.32), loss: 0.002603, time: 15.275 s\n",
      "Step 263440 (epoch   82.33), loss: 0.002370, time: 2.231 s\n",
      "Step 263460 (epoch   82.33), loss: 0.002744, time: 2.238 s\n",
      "Step 263480 (epoch   82.34), loss: 0.003750, time: 2.236 s\n",
      "Step 263500 (epoch   82.34), loss: 0.003136, time: 2.247 s\n",
      "Step 263520 (epoch   82.35), loss: 0.006620, time: 2.235 s\n",
      "Step 263540 (epoch   82.36), loss: 0.003297, time: 2.233 s\n",
      "Step 263560 (epoch   82.36), loss: 0.005108, time: 2.225 s\n",
      "Step 263580 (epoch   82.37), loss: 0.003125, time: 2.244 s\n",
      "Step 263600 (epoch   82.38), loss: 0.002405, time: 2.236 s, error: 0.010043\n",
      "Step 263620 (epoch   82.38), loss: 0.004076, time: 15.232 s\n",
      "Step 263640 (epoch   82.39), loss: 0.003631, time: 2.239 s\n",
      "Step 263660 (epoch   82.39), loss: 0.003529, time: 2.237 s\n",
      "Step 263680 (epoch   82.40), loss: 0.003631, time: 2.255 s\n",
      "Step 263700 (epoch   82.41), loss: 0.002686, time: 2.240 s\n",
      "Step 263720 (epoch   82.41), loss: 0.003797, time: 2.239 s\n",
      "Step 263740 (epoch   82.42), loss: 0.003515, time: 2.233 s\n",
      "Step 263760 (epoch   82.42), loss: 0.003960, time: 2.238 s\n",
      "Step 263780 (epoch   82.43), loss: 0.005407, time: 2.239 s\n",
      "Step 263800 (epoch   82.44), loss: 0.004504, time: 2.241 s, error: 0.010426\n",
      "Step 263820 (epoch   82.44), loss: 0.004418, time: 15.049 s\n",
      "Step 263840 (epoch   82.45), loss: 0.003249, time: 2.244 s\n",
      "Step 263860 (epoch   82.46), loss: 0.002331, time: 2.252 s\n",
      "Step 263880 (epoch   82.46), loss: 0.003637, time: 2.242 s\n",
      "Step 263900 (epoch   82.47), loss: 0.006377, time: 2.246 s\n",
      "Step 263920 (epoch   82.47), loss: 0.005128, time: 2.238 s\n",
      "Step 263940 (epoch   82.48), loss: 0.004844, time: 2.233 s\n",
      "Step 263960 (epoch   82.49), loss: 0.003786, time: 2.238 s\n",
      "Step 263980 (epoch   82.49), loss: 0.005087, time: 2.236 s\n",
      "Step 264000 (epoch   82.50), loss: 0.005794, time: 2.250 s, error: 0.010585\n",
      "\n",
      "Time since beginning  : 46348.838 s\n",
      "\n",
      "Step 264020 (epoch   82.51), loss: 0.003930, time: 15.070 s\n",
      "Step 264040 (epoch   82.51), loss: 0.003775, time: 2.234 s\n",
      "Step 264060 (epoch   82.52), loss: 0.006007, time: 2.252 s\n",
      "Step 264080 (epoch   82.53), loss: 0.003680, time: 2.259 s\n",
      "Step 264100 (epoch   82.53), loss: 0.008201, time: 2.241 s\n",
      "Step 264120 (epoch   82.54), loss: 0.005334, time: 2.228 s\n",
      "Step 264140 (epoch   82.54), loss: 0.004035, time: 2.230 s\n",
      "Step 264160 (epoch   82.55), loss: 0.003847, time: 2.244 s\n",
      "Step 264180 (epoch   82.56), loss: 0.003372, time: 2.238 s\n",
      "Step 264200 (epoch   82.56), loss: 0.004103, time: 2.239 s, error: 0.010059\n",
      "Step 264220 (epoch   82.57), loss: 0.004283, time: 15.086 s\n",
      "Step 264240 (epoch   82.58), loss: 0.004700, time: 2.230 s\n",
      "Step 264260 (epoch   82.58), loss: 0.004364, time: 2.246 s\n",
      "Step 264280 (epoch   82.59), loss: 0.002905, time: 2.244 s\n",
      "Step 264300 (epoch   82.59), loss: 0.003746, time: 2.245 s\n",
      "Step 264320 (epoch   82.60), loss: 0.003169, time: 2.251 s\n",
      "Step 264340 (epoch   82.61), loss: 0.003315, time: 2.260 s\n",
      "Step 264360 (epoch   82.61), loss: 0.004517, time: 2.235 s\n",
      "Step 264380 (epoch   82.62), loss: 0.003616, time: 2.225 s\n",
      "Step 264400 (epoch   82.62), loss: 0.003064, time: 2.241 s, error: 0.010485\n",
      "Step 264420 (epoch   82.63), loss: 0.004088, time: 14.999 s\n",
      "Step 264440 (epoch   82.64), loss: 0.004469, time: 2.233 s\n",
      "Step 264460 (epoch   82.64), loss: 0.002720, time: 2.236 s\n",
      "Step 264480 (epoch   82.65), loss: 0.005988, time: 2.240 s\n",
      "Step 264500 (epoch   82.66), loss: 0.004321, time: 2.238 s\n",
      "Step 264520 (epoch   82.66), loss: 0.003698, time: 2.241 s\n",
      "Step 264540 (epoch   82.67), loss: 0.005855, time: 2.254 s\n",
      "Step 264560 (epoch   82.67), loss: 0.006572, time: 2.249 s\n",
      "Step 264580 (epoch   82.68), loss: 0.003355, time: 2.251 s\n",
      "Step 264600 (epoch   82.69), loss: 0.003831, time: 2.255 s, error: 0.010543\n",
      "Step 264620 (epoch   82.69), loss: 0.022723, time: 15.098 s\n",
      "Step 264640 (epoch   82.70), loss: 0.002560, time: 2.250 s\n",
      "Step 264660 (epoch   82.71), loss: 0.008513, time: 2.245 s\n",
      "Step 264680 (epoch   82.71), loss: 0.003512, time: 2.250 s\n",
      "Step 264700 (epoch   82.72), loss: 0.004631, time: 2.226 s\n",
      "Step 264720 (epoch   82.72), loss: 0.004491, time: 2.237 s\n",
      "Step 264740 (epoch   82.73), loss: 0.003646, time: 2.218 s\n",
      "Step 264760 (epoch   82.74), loss: 0.002560, time: 2.243 s\n",
      "Step 264780 (epoch   82.74), loss: 0.002693, time: 2.242 s\n",
      "Step 264800 (epoch   82.75), loss: 0.004103, time: 2.240 s, error: 0.010355\n",
      "Step 264820 (epoch   82.76), loss: 0.003234, time: 15.206 s\n",
      "Step 264840 (epoch   82.76), loss: 0.004013, time: 2.232 s\n",
      "Step 264860 (epoch   82.77), loss: 0.006256, time: 2.247 s\n",
      "Step 264880 (epoch   82.78), loss: 0.003302, time: 2.229 s\n",
      "Step 264900 (epoch   82.78), loss: 0.003053, time: 2.236 s\n",
      "Step 264920 (epoch   82.79), loss: 0.005669, time: 2.225 s\n",
      "Step 264940 (epoch   82.79), loss: 0.003972, time: 2.250 s\n",
      "Step 264960 (epoch   82.80), loss: 0.002470, time: 2.240 s\n",
      "Step 264980 (epoch   82.81), loss: 0.004267, time: 2.233 s\n",
      "Step 265000 (epoch   82.81), loss: 0.004897, time: 2.247 s, error: 0.010208\n",
      "Step 265020 (epoch   82.82), loss: 0.004999, time: 15.350 s\n",
      "Step 265040 (epoch   82.83), loss: 0.004280, time: 2.237 s\n",
      "Step 265060 (epoch   82.83), loss: 0.005166, time: 2.246 s\n",
      "Step 265080 (epoch   82.84), loss: 0.004010, time: 2.233 s\n",
      "Step 265100 (epoch   82.84), loss: 0.003930, time: 2.244 s\n",
      "Step 265120 (epoch   82.85), loss: 0.004154, time: 2.239 s\n",
      "Step 265140 (epoch   82.86), loss: 0.004237, time: 2.235 s\n",
      "Step 265160 (epoch   82.86), loss: 0.003132, time: 2.240 s\n",
      "Step 265180 (epoch   82.87), loss: 0.003051, time: 2.229 s\n",
      "Step 265200 (epoch   82.88), loss: 0.003328, time: 2.238 s, error: 0.009925\n",
      "Step 265220 (epoch   82.88), loss: 0.006260, time: 15.053 s\n",
      "Step 265240 (epoch   82.89), loss: 0.006224, time: 2.251 s\n",
      "Step 265260 (epoch   82.89), loss: 0.003427, time: 2.244 s\n",
      "Step 265280 (epoch   82.90), loss: 0.006374, time: 2.232 s\n",
      "Step 265300 (epoch   82.91), loss: 0.003945, time: 2.239 s\n",
      "Step 265320 (epoch   82.91), loss: 0.006131, time: 2.229 s\n",
      "Step 265340 (epoch   82.92), loss: 0.005504, time: 2.241 s\n",
      "Step 265360 (epoch   82.92), loss: 0.004134, time: 2.251 s\n",
      "Step 265380 (epoch   82.93), loss: 0.003709, time: 2.243 s\n",
      "Step 265400 (epoch   82.94), loss: 0.003788, time: 2.255 s, error: 0.010403\n",
      "Step 265420 (epoch   82.94), loss: 0.007535, time: 15.023 s\n",
      "Step 265440 (epoch   82.95), loss: 0.003856, time: 2.251 s\n",
      "Step 265460 (epoch   82.96), loss: 0.003353, time: 2.227 s\n",
      "Step 265480 (epoch   82.96), loss: 0.002824, time: 2.236 s\n",
      "Step 265500 (epoch   82.97), loss: 0.003981, time: 2.259 s\n",
      "Step 265520 (epoch   82.97), loss: 0.003447, time: 2.251 s\n",
      "Step 265540 (epoch   82.98), loss: 0.004248, time: 2.240 s\n",
      "Step 265560 (epoch   82.99), loss: 0.002909, time: 2.242 s\n",
      "Step 265580 (epoch   82.99), loss: 0.004001, time: 2.247 s\n",
      "Step 265600 (epoch   83.00), loss: 0.004363, time: 2.245 s, error: 0.010085\n",
      "Step 265620 (epoch   83.01), loss: 0.003430, time: 15.005 s\n",
      "Step 265640 (epoch   83.01), loss: 0.003106, time: 2.247 s\n",
      "Step 265660 (epoch   83.02), loss: 0.003655, time: 2.245 s\n",
      "Step 265680 (epoch   83.03), loss: 0.004995, time: 2.226 s\n",
      "Step 265700 (epoch   83.03), loss: 0.003545, time: 2.242 s\n",
      "Step 265720 (epoch   83.04), loss: 0.002375, time: 2.242 s\n",
      "Step 265740 (epoch   83.04), loss: 0.004694, time: 2.237 s\n",
      "Step 265760 (epoch   83.05), loss: 0.007545, time: 2.260 s\n",
      "Step 265780 (epoch   83.06), loss: 0.010492, time: 2.244 s\n",
      "Step 265800 (epoch   83.06), loss: 0.004078, time: 2.241 s, error: 0.010120\n",
      "Step 265820 (epoch   83.07), loss: 0.002938, time: 15.057 s\n",
      "Step 265840 (epoch   83.08), loss: 0.004292, time: 2.239 s\n",
      "Step 265860 (epoch   83.08), loss: 0.004620, time: 2.241 s\n",
      "Step 265880 (epoch   83.09), loss: 0.003125, time: 2.232 s\n",
      "Step 265900 (epoch   83.09), loss: 0.003051, time: 2.233 s\n",
      "Step 265920 (epoch   83.10), loss: 0.002532, time: 2.236 s\n",
      "Step 265940 (epoch   83.11), loss: 0.003332, time: 2.241 s\n",
      "Step 265960 (epoch   83.11), loss: 0.003012, time: 2.236 s\n",
      "Step 265980 (epoch   83.12), loss: 0.005508, time: 2.251 s\n",
      "Step 266000 (epoch   83.12), loss: 0.003700, time: 2.226 s, error: 0.010041\n",
      "\n",
      "Time since beginning  : 46701.614 s\n",
      "\n",
      "Step 266020 (epoch   83.13), loss: 0.002724, time: 15.240 s\n",
      "Step 266040 (epoch   83.14), loss: 0.004598, time: 2.239 s\n",
      "Step 266060 (epoch   83.14), loss: 0.003326, time: 2.242 s\n",
      "Step 266080 (epoch   83.15), loss: 0.005845, time: 2.256 s\n",
      "Step 266100 (epoch   83.16), loss: 0.003708, time: 2.256 s\n",
      "Step 266120 (epoch   83.16), loss: 0.009907, time: 2.247 s\n",
      "Step 266140 (epoch   83.17), loss: 0.003932, time: 2.252 s\n",
      "Step 266160 (epoch   83.17), loss: 0.003937, time: 2.239 s\n",
      "Step 266180 (epoch   83.18), loss: 0.005115, time: 2.239 s\n",
      "Step 266200 (epoch   83.19), loss: 0.004042, time: 2.238 s, error: 0.010074\n",
      "Step 266220 (epoch   83.19), loss: 0.004461, time: 15.226 s\n",
      "Step 266240 (epoch   83.20), loss: 0.004331, time: 2.250 s\n",
      "Step 266260 (epoch   83.21), loss: 0.002657, time: 2.241 s\n",
      "Step 266280 (epoch   83.21), loss: 0.006287, time: 2.232 s\n",
      "Step 266300 (epoch   83.22), loss: 0.003804, time: 2.235 s\n",
      "Step 266320 (epoch   83.22), loss: 0.003760, time: 2.252 s\n",
      "Step 266340 (epoch   83.23), loss: 0.004462, time: 2.236 s\n",
      "Step 266360 (epoch   83.24), loss: 0.004015, time: 2.245 s\n",
      "Step 266380 (epoch   83.24), loss: 0.005899, time: 2.236 s\n",
      "Step 266400 (epoch   83.25), loss: 0.007441, time: 2.237 s, error: 0.010114\n",
      "Step 266420 (epoch   83.26), loss: 0.015182, time: 15.100 s\n",
      "Step 266440 (epoch   83.26), loss: 0.006198, time: 2.233 s\n",
      "Step 266460 (epoch   83.27), loss: 0.004353, time: 2.255 s\n",
      "Step 266480 (epoch   83.28), loss: 0.005364, time: 2.242 s\n",
      "Step 266500 (epoch   83.28), loss: 0.002856, time: 2.256 s\n",
      "Step 266520 (epoch   83.29), loss: 0.003287, time: 2.252 s\n",
      "Step 266540 (epoch   83.29), loss: 0.002518, time: 2.242 s\n",
      "Step 266560 (epoch   83.30), loss: 0.007540, time: 2.244 s\n",
      "Step 266580 (epoch   83.31), loss: 0.003837, time: 2.233 s\n",
      "Step 266600 (epoch   83.31), loss: 0.003272, time: 2.246 s, error: 0.009997\n",
      "Step 266620 (epoch   83.32), loss: 0.002855, time: 15.015 s\n",
      "Step 266640 (epoch   83.33), loss: 0.006522, time: 2.241 s\n",
      "Step 266660 (epoch   83.33), loss: 0.003406, time: 2.244 s\n",
      "Step 266680 (epoch   83.34), loss: 0.003388, time: 2.251 s\n",
      "Step 266700 (epoch   83.34), loss: 0.003267, time: 2.244 s\n",
      "Step 266720 (epoch   83.35), loss: 0.003810, time: 2.226 s\n",
      "Step 266740 (epoch   83.36), loss: 0.004759, time: 2.248 s\n",
      "Step 266760 (epoch   83.36), loss: 0.004036, time: 2.230 s\n",
      "Step 266780 (epoch   83.37), loss: 0.004925, time: 2.234 s\n",
      "Step 266800 (epoch   83.38), loss: 0.003549, time: 2.233 s, error: 0.009982\n",
      "Step 266820 (epoch   83.38), loss: 0.005211, time: 15.024 s\n",
      "Step 266840 (epoch   83.39), loss: 0.003782, time: 2.232 s\n",
      "Step 266860 (epoch   83.39), loss: 0.005736, time: 2.238 s\n",
      "Step 266880 (epoch   83.40), loss: 0.003151, time: 2.250 s\n",
      "Step 266900 (epoch   83.41), loss: 0.004816, time: 2.256 s\n",
      "Step 266920 (epoch   83.41), loss: 0.005029, time: 2.259 s\n",
      "Step 266940 (epoch   83.42), loss: 0.005618, time: 2.248 s\n",
      "Step 266960 (epoch   83.42), loss: 0.006919, time: 2.250 s\n",
      "Step 266980 (epoch   83.43), loss: 0.002107, time: 2.238 s\n",
      "Step 267000 (epoch   83.44), loss: 0.003869, time: 2.239 s, error: 0.010192\n",
      "Step 267020 (epoch   83.44), loss: 0.004410, time: 15.185 s\n",
      "Step 267040 (epoch   83.45), loss: 0.003110, time: 2.233 s\n",
      "Step 267060 (epoch   83.46), loss: 0.005227, time: 2.248 s\n",
      "Step 267080 (epoch   83.46), loss: 0.003254, time: 2.242 s\n",
      "Step 267100 (epoch   83.47), loss: 0.005044, time: 2.246 s\n",
      "Step 267120 (epoch   83.47), loss: 0.003369, time: 2.246 s\n",
      "Step 267140 (epoch   83.48), loss: 0.003871, time: 2.232 s\n",
      "Step 267160 (epoch   83.49), loss: 0.004899, time: 2.223 s\n",
      "Step 267180 (epoch   83.49), loss: 0.003337, time: 2.249 s\n",
      "Step 267200 (epoch   83.50), loss: 0.003357, time: 2.231 s, error: 0.010484\n",
      "Step 267220 (epoch   83.51), loss: 0.003096, time: 15.022 s\n",
      "Step 267240 (epoch   83.51), loss: 0.003505, time: 2.247 s\n",
      "Step 267260 (epoch   83.52), loss: 0.004965, time: 2.229 s\n",
      "Step 267280 (epoch   83.53), loss: 0.003816, time: 2.248 s\n",
      "Step 267300 (epoch   83.53), loss: 0.003135, time: 2.232 s\n",
      "Step 267320 (epoch   83.54), loss: 0.003536, time: 2.247 s\n",
      "Step 267340 (epoch   83.54), loss: 0.005744, time: 2.237 s\n",
      "Step 267360 (epoch   83.55), loss: 0.005946, time: 2.245 s\n",
      "Step 267380 (epoch   83.56), loss: 0.005578, time: 2.239 s\n",
      "Step 267400 (epoch   83.56), loss: 0.004543, time: 2.247 s, error: 0.010002\n",
      "Step 267420 (epoch   83.57), loss: 0.003381, time: 15.299 s\n",
      "Step 267440 (epoch   83.58), loss: 0.004229, time: 2.234 s\n",
      "Step 267460 (epoch   83.58), loss: 0.004689, time: 2.230 s\n",
      "Step 267480 (epoch   83.59), loss: 0.002712, time: 2.249 s\n",
      "Step 267500 (epoch   83.59), loss: 0.003606, time: 2.240 s\n",
      "Step 267520 (epoch   83.60), loss: 0.004188, time: 2.240 s\n",
      "Step 267540 (epoch   83.61), loss: 0.004361, time: 2.238 s\n",
      "Step 267560 (epoch   83.61), loss: 0.004299, time: 2.250 s\n",
      "Step 267580 (epoch   83.62), loss: 0.004001, time: 2.239 s\n",
      "Step 267600 (epoch   83.62), loss: 0.005805, time: 2.245 s, error: 0.010604\n",
      "Step 267620 (epoch   83.63), loss: 0.006247, time: 15.264 s\n",
      "Step 267640 (epoch   83.64), loss: 0.004140, time: 2.242 s\n",
      "Step 267660 (epoch   83.64), loss: 0.003419, time: 2.241 s\n",
      "Step 267680 (epoch   83.65), loss: 0.011264, time: 2.230 s\n",
      "Step 267700 (epoch   83.66), loss: 0.003733, time: 2.240 s\n",
      "Step 267720 (epoch   83.66), loss: 0.004973, time: 2.246 s\n",
      "Step 267740 (epoch   83.67), loss: 0.004219, time: 2.237 s\n",
      "Step 267760 (epoch   83.67), loss: 0.003142, time: 2.239 s\n",
      "Step 267780 (epoch   83.68), loss: 0.004224, time: 2.229 s\n",
      "Step 267800 (epoch   83.69), loss: 0.003965, time: 2.234 s, error: 0.010759\n",
      "Step 267820 (epoch   83.69), loss: 0.006103, time: 15.054 s\n",
      "Step 267840 (epoch   83.70), loss: 0.004472, time: 2.257 s\n",
      "Step 267860 (epoch   83.71), loss: 0.005396, time: 2.245 s\n",
      "Step 267880 (epoch   83.71), loss: 0.004832, time: 2.235 s\n",
      "Step 267900 (epoch   83.72), loss: 0.003425, time: 2.237 s\n",
      "Step 267920 (epoch   83.72), loss: 0.005209, time: 2.245 s\n",
      "Step 267940 (epoch   83.73), loss: 0.003463, time: 2.239 s\n",
      "Step 267960 (epoch   83.74), loss: 0.003166, time: 2.242 s\n",
      "Step 267980 (epoch   83.74), loss: 0.003427, time: 2.237 s\n",
      "Step 268000 (epoch   83.75), loss: 0.008227, time: 2.236 s, error: 0.010108\n",
      "\n",
      "Time since beginning  : 47054.623 s\n",
      "\n",
      "Step 268020 (epoch   83.76), loss: 0.004926, time: 15.092 s\n",
      "Step 268040 (epoch   83.76), loss: 0.003299, time: 2.239 s\n",
      "Step 268060 (epoch   83.77), loss: 0.003553, time: 2.237 s\n",
      "Step 268080 (epoch   83.78), loss: 0.004836, time: 2.236 s\n",
      "Step 268100 (epoch   83.78), loss: 0.004608, time: 2.261 s\n",
      "Step 268120 (epoch   83.79), loss: 0.005764, time: 2.246 s\n",
      "Step 268140 (epoch   83.79), loss: 0.003826, time: 2.248 s\n",
      "Step 268160 (epoch   83.80), loss: 0.002942, time: 2.237 s\n",
      "Step 268180 (epoch   83.81), loss: 0.004500, time: 2.232 s\n",
      "Step 268200 (epoch   83.81), loss: 0.003223, time: 2.235 s, error: 0.010222\n",
      "Step 268220 (epoch   83.82), loss: 0.003166, time: 15.077 s\n",
      "Step 268240 (epoch   83.83), loss: 0.003256, time: 2.239 s\n",
      "Step 268260 (epoch   83.83), loss: 0.004516, time: 2.238 s\n",
      "Step 268280 (epoch   83.84), loss: 0.003430, time: 2.234 s\n",
      "Step 268300 (epoch   83.84), loss: 0.009683, time: 2.245 s\n",
      "Step 268320 (epoch   83.85), loss: 0.002501, time: 2.244 s\n",
      "Step 268340 (epoch   83.86), loss: 0.002732, time: 2.252 s\n",
      "Step 268360 (epoch   83.86), loss: 0.004745, time: 2.258 s\n",
      "Step 268380 (epoch   83.87), loss: 0.004644, time: 2.239 s\n",
      "Step 268400 (epoch   83.88), loss: 0.003558, time: 2.234 s, error: 0.009903\n",
      "Step 268420 (epoch   83.88), loss: 0.002259, time: 15.046 s\n",
      "Step 268440 (epoch   83.89), loss: 0.003326, time: 2.242 s\n",
      "Step 268460 (epoch   83.89), loss: 0.004420, time: 2.241 s\n",
      "Step 268480 (epoch   83.90), loss: 0.003909, time: 2.223 s\n",
      "Step 268500 (epoch   83.91), loss: 0.003087, time: 2.234 s\n",
      "Step 268520 (epoch   83.91), loss: 0.004756, time: 2.242 s\n",
      "Step 268540 (epoch   83.92), loss: 0.003492, time: 2.247 s\n",
      "Step 268560 (epoch   83.92), loss: 0.003453, time: 2.242 s\n",
      "Step 268580 (epoch   83.93), loss: 0.003029, time: 2.244 s\n",
      "Step 268600 (epoch   83.94), loss: 0.004491, time: 2.254 s, error: 0.010382\n",
      "Step 268620 (epoch   83.94), loss: 0.003950, time: 15.145 s\n",
      "Step 268640 (epoch   83.95), loss: 0.004281, time: 2.240 s\n",
      "Step 268660 (epoch   83.96), loss: 0.003868, time: 2.250 s\n",
      "Step 268680 (epoch   83.96), loss: 0.002600, time: 2.243 s\n",
      "Step 268700 (epoch   83.97), loss: 0.003909, time: 2.243 s\n",
      "Step 268720 (epoch   83.97), loss: 0.008035, time: 2.240 s\n",
      "Step 268740 (epoch   83.98), loss: 0.003525, time: 2.250 s\n",
      "Step 268760 (epoch   83.99), loss: 0.004269, time: 2.237 s\n",
      "Step 268780 (epoch   83.99), loss: 0.009850, time: 2.251 s\n",
      "Step 268800 (epoch   84.00), loss: 0.003016, time: 2.233 s, error: 0.010029\n",
      "Step 268820 (epoch   84.01), loss: 0.003042, time: 15.258 s\n",
      "Step 268840 (epoch   84.01), loss: 0.004233, time: 2.244 s\n",
      "Step 268860 (epoch   84.02), loss: 0.005285, time: 2.236 s\n",
      "Step 268880 (epoch   84.03), loss: 0.003448, time: 2.240 s\n",
      "Step 268900 (epoch   84.03), loss: 0.002877, time: 2.240 s\n",
      "Step 268920 (epoch   84.04), loss: 0.004208, time: 2.243 s\n",
      "Step 268940 (epoch   84.04), loss: 0.002998, time: 2.236 s\n",
      "Step 268960 (epoch   84.05), loss: 0.004069, time: 2.233 s\n",
      "Step 268980 (epoch   84.06), loss: 0.002704, time: 2.238 s\n",
      "Step 269000 (epoch   84.06), loss: 0.002833, time: 2.248 s, error: 0.010095\n",
      "Step 269020 (epoch   84.07), loss: 0.003003, time: 15.154 s\n",
      "Step 269040 (epoch   84.08), loss: 0.003277, time: 2.244 s\n",
      "Step 269060 (epoch   84.08), loss: 0.003723, time: 2.238 s\n",
      "Step 269080 (epoch   84.09), loss: 0.004795, time: 2.235 s\n",
      "Step 269100 (epoch   84.09), loss: 0.004786, time: 2.247 s\n",
      "Step 269120 (epoch   84.10), loss: 0.004321, time: 2.231 s\n",
      "Step 269140 (epoch   84.11), loss: 0.002362, time: 2.250 s\n",
      "Step 269160 (epoch   84.11), loss: 0.008243, time: 2.230 s\n",
      "Step 269180 (epoch   84.12), loss: 0.003331, time: 2.246 s\n",
      "Step 269200 (epoch   84.12), loss: 0.002711, time: 2.234 s, error: 0.010087\n",
      "Step 269220 (epoch   84.13), loss: 0.003856, time: 15.023 s\n",
      "Step 269240 (epoch   84.14), loss: 0.003915, time: 2.238 s\n",
      "Step 269260 (epoch   84.14), loss: 0.004400, time: 2.253 s\n",
      "Step 269280 (epoch   84.15), loss: 0.003288, time: 2.255 s\n",
      "Step 269300 (epoch   84.16), loss: 0.004069, time: 2.250 s\n",
      "Step 269320 (epoch   84.16), loss: 0.004199, time: 2.239 s\n",
      "Step 269340 (epoch   84.17), loss: 0.004366, time: 2.246 s\n",
      "Step 269360 (epoch   84.17), loss: 0.004464, time: 2.250 s\n",
      "Step 269380 (epoch   84.18), loss: 0.002427, time: 2.248 s\n",
      "Step 269400 (epoch   84.19), loss: 0.003427, time: 2.243 s, error: 0.010004\n",
      "Step 269420 (epoch   84.19), loss: 0.006092, time: 15.097 s\n",
      "Step 269440 (epoch   84.20), loss: 0.004559, time: 2.249 s\n",
      "Step 269460 (epoch   84.21), loss: 0.004261, time: 2.240 s\n",
      "Step 269480 (epoch   84.21), loss: 0.002338, time: 2.250 s\n",
      "Step 269500 (epoch   84.22), loss: 0.003861, time: 2.247 s\n",
      "Step 269520 (epoch   84.22), loss: 0.005173, time: 2.257 s\n",
      "Step 269540 (epoch   84.23), loss: 0.004267, time: 2.250 s\n",
      "Step 269560 (epoch   84.24), loss: 0.005927, time: 2.245 s\n",
      "Step 269580 (epoch   84.24), loss: 0.004277, time: 2.241 s\n",
      "Step 269600 (epoch   84.25), loss: 0.004375, time: 2.250 s, error: 0.009934\n",
      "Step 269620 (epoch   84.26), loss: 0.004105, time: 15.101 s\n",
      "Step 269640 (epoch   84.26), loss: 0.005416, time: 2.227 s\n",
      "Step 269660 (epoch   84.27), loss: 0.004513, time: 2.248 s\n",
      "Step 269680 (epoch   84.28), loss: 0.003633, time: 2.245 s\n",
      "Step 269700 (epoch   84.28), loss: 0.003437, time: 2.250 s\n",
      "Step 269720 (epoch   84.29), loss: 0.005223, time: 2.236 s\n",
      "Step 269740 (epoch   84.29), loss: 0.001563, time: 2.227 s\n",
      "Step 269760 (epoch   84.30), loss: 0.004012, time: 2.246 s\n",
      "Step 269780 (epoch   84.31), loss: 0.006075, time: 2.250 s\n",
      "Step 269800 (epoch   84.31), loss: 0.004753, time: 2.238 s, error: 0.009998\n",
      "Step 269820 (epoch   84.32), loss: 0.003360, time: 14.994 s\n",
      "Step 269840 (epoch   84.33), loss: 0.006029, time: 2.243 s\n",
      "Step 269860 (epoch   84.33), loss: 0.005064, time: 2.233 s\n",
      "Step 269880 (epoch   84.34), loss: 0.003641, time: 2.247 s\n",
      "Step 269900 (epoch   84.34), loss: 0.004150, time: 2.235 s\n",
      "Step 269920 (epoch   84.35), loss: 0.006618, time: 2.252 s\n",
      "Step 269940 (epoch   84.36), loss: 0.004356, time: 2.251 s\n",
      "Step 269960 (epoch   84.36), loss: 0.005754, time: 2.253 s\n",
      "Step 269980 (epoch   84.37), loss: 0.003376, time: 2.246 s\n",
      "Step 270000 (epoch   84.38), loss: 0.007086, time: 2.242 s, error: 0.009959\n",
      "\n",
      "Time since beginning  : 47407.656 s\n",
      "\n",
      "Step 270020 (epoch   84.38), loss: 0.004340, time: 15.287 s\n",
      "Step 270040 (epoch   84.39), loss: 0.003673, time: 2.246 s\n",
      "Step 270060 (epoch   84.39), loss: 0.004154, time: 2.241 s\n",
      "Step 270080 (epoch   84.40), loss: 0.004414, time: 2.237 s\n",
      "Step 270100 (epoch   84.41), loss: 0.005043, time: 2.241 s\n",
      "Step 270120 (epoch   84.41), loss: 0.004175, time: 2.245 s\n",
      "Step 270140 (epoch   84.42), loss: 0.003568, time: 2.243 s\n",
      "Step 270160 (epoch   84.42), loss: 0.004952, time: 2.231 s\n",
      "Step 270180 (epoch   84.43), loss: 0.002697, time: 2.232 s\n",
      "Step 270200 (epoch   84.44), loss: 0.003323, time: 2.237 s, error: 0.010053\n",
      "Step 270220 (epoch   84.44), loss: 0.006364, time: 15.228 s\n",
      "Step 270240 (epoch   84.45), loss: 0.006229, time: 2.230 s\n",
      "Step 270260 (epoch   84.46), loss: 0.002685, time: 2.246 s\n",
      "Step 270280 (epoch   84.46), loss: 0.003640, time: 2.258 s\n",
      "Step 270300 (epoch   84.47), loss: 0.003758, time: 2.255 s\n",
      "Step 270320 (epoch   84.47), loss: 0.004585, time: 2.257 s\n",
      "Step 270340 (epoch   84.48), loss: 0.004602, time: 2.255 s\n",
      "Step 270360 (epoch   84.49), loss: 0.003574, time: 2.245 s\n",
      "Step 270380 (epoch   84.49), loss: 0.003991, time: 2.254 s\n",
      "Step 270400 (epoch   84.50), loss: 0.004568, time: 2.259 s, error: 0.010317\n",
      "Step 270420 (epoch   84.51), loss: 0.004509, time: 15.429 s\n",
      "Step 270440 (epoch   84.51), loss: 0.003471, time: 2.261 s\n",
      "Step 270460 (epoch   84.52), loss: 0.002957, time: 2.256 s\n",
      "Step 270480 (epoch   84.53), loss: 0.003565, time: 2.246 s\n",
      "Step 270500 (epoch   84.53), loss: 0.002104, time: 2.259 s\n",
      "Step 270520 (epoch   84.54), loss: 0.003366, time: 2.260 s\n",
      "Step 270540 (epoch   84.54), loss: 0.003823, time: 2.259 s\n",
      "Step 270560 (epoch   84.55), loss: 0.002767, time: 2.261 s\n",
      "Step 270580 (epoch   84.56), loss: 0.003910, time: 2.258 s\n",
      "Step 270600 (epoch   84.56), loss: 0.003505, time: 2.260 s, error: 0.010011\n",
      "Step 270620 (epoch   84.57), loss: 0.005372, time: 15.948 s\n",
      "Step 270640 (epoch   84.58), loss: 0.002756, time: 2.256 s\n",
      "Step 270660 (epoch   84.58), loss: 0.003593, time: 2.263 s\n",
      "Step 270680 (epoch   84.59), loss: 0.003708, time: 2.265 s\n",
      "Step 270700 (epoch   84.59), loss: 0.002277, time: 2.258 s\n",
      "Step 270720 (epoch   84.60), loss: 0.006264, time: 2.257 s\n",
      "Step 270740 (epoch   84.61), loss: 0.005419, time: 2.260 s\n",
      "Step 270760 (epoch   84.61), loss: 0.004037, time: 2.256 s\n",
      "Step 270780 (epoch   84.62), loss: 0.004307, time: 2.253 s\n",
      "Step 270800 (epoch   84.62), loss: 0.003080, time: 2.259 s, error: 0.010573\n",
      "Step 270820 (epoch   84.63), loss: 0.002684, time: 15.479 s\n",
      "Step 270840 (epoch   84.64), loss: 0.011831, time: 2.262 s\n",
      "Step 270860 (epoch   84.64), loss: 0.003472, time: 2.260 s\n",
      "Step 270880 (epoch   84.65), loss: 0.003834, time: 2.260 s\n",
      "Step 270900 (epoch   84.66), loss: 0.003515, time: 2.261 s\n",
      "Step 270920 (epoch   84.66), loss: 0.003189, time: 2.254 s\n",
      "Step 270940 (epoch   84.67), loss: 0.003999, time: 2.258 s\n",
      "Step 270960 (epoch   84.67), loss: 0.003496, time: 2.260 s\n",
      "Step 270980 (epoch   84.68), loss: 0.004024, time: 2.262 s\n",
      "Step 271000 (epoch   84.69), loss: 0.003321, time: 2.249 s, error: 0.010774\n",
      "Step 271020 (epoch   84.69), loss: 0.005130, time: 15.351 s\n",
      "Step 271040 (epoch   84.70), loss: 0.004394, time: 2.258 s\n",
      "Step 271060 (epoch   84.71), loss: 0.003921, time: 2.251 s\n",
      "Step 271080 (epoch   84.71), loss: 0.003413, time: 2.259 s\n",
      "Step 271100 (epoch   84.72), loss: 0.003077, time: 2.261 s\n",
      "Step 271120 (epoch   84.72), loss: 0.003737, time: 2.250 s\n",
      "Step 271140 (epoch   84.73), loss: 0.002613, time: 2.254 s\n",
      "Step 271160 (epoch   84.74), loss: 0.005137, time: 2.257 s\n",
      "Step 271180 (epoch   84.74), loss: 0.005447, time: 2.255 s\n",
      "Step 271200 (epoch   84.75), loss: 0.003562, time: 2.258 s, error: 0.009940\n",
      "Step 271220 (epoch   84.76), loss: 0.004210, time: 15.054 s\n",
      "Step 271240 (epoch   84.76), loss: 0.003908, time: 2.238 s\n",
      "Step 271260 (epoch   84.77), loss: 0.004946, time: 2.240 s\n",
      "Step 271280 (epoch   84.78), loss: 0.005576, time: 2.231 s\n",
      "Step 271300 (epoch   84.78), loss: 0.003039, time: 2.229 s\n",
      "Step 271320 (epoch   84.79), loss: 0.005096, time: 2.245 s\n",
      "Step 271340 (epoch   84.79), loss: 0.002652, time: 2.239 s\n",
      "Step 271360 (epoch   84.80), loss: 0.002563, time: 2.236 s\n",
      "Step 271380 (epoch   84.81), loss: 0.004920, time: 2.249 s\n",
      "Step 271400 (epoch   84.81), loss: 0.004090, time: 2.241 s, error: 0.010064\n",
      "Step 271420 (epoch   84.82), loss: 0.004544, time: 15.217 s\n",
      "Step 271440 (epoch   84.83), loss: 0.003258, time: 2.253 s\n",
      "Step 271460 (epoch   84.83), loss: 0.002949, time: 2.251 s\n",
      "Step 271480 (epoch   84.84), loss: 0.005417, time: 2.247 s\n",
      "Step 271500 (epoch   84.84), loss: 0.008636, time: 2.246 s\n",
      "Step 271520 (epoch   84.85), loss: 0.004952, time: 2.240 s\n",
      "Step 271540 (epoch   84.86), loss: 0.004361, time: 2.235 s\n",
      "Step 271560 (epoch   84.86), loss: 0.002403, time: 2.239 s\n",
      "Step 271580 (epoch   84.87), loss: 0.003224, time: 2.245 s\n",
      "Step 271600 (epoch   84.88), loss: 0.002620, time: 2.246 s, error: 0.009871\n",
      "Step 271620 (epoch   84.88), loss: 0.004298, time: 15.224 s\n",
      "Step 271640 (epoch   84.89), loss: 0.004369, time: 2.246 s\n",
      "Step 271660 (epoch   84.89), loss: 0.003508, time: 2.243 s\n",
      "Step 271680 (epoch   84.90), loss: 0.006940, time: 2.245 s\n",
      "Step 271700 (epoch   84.91), loss: 0.002589, time: 2.235 s\n",
      "Step 271720 (epoch   84.91), loss: 0.003736, time: 2.240 s\n",
      "Step 271740 (epoch   84.92), loss: 0.004596, time: 2.223 s\n",
      "Step 271760 (epoch   84.92), loss: 0.009714, time: 2.232 s\n",
      "Step 271780 (epoch   84.93), loss: 0.003832, time: 2.253 s\n",
      "Step 271800 (epoch   84.94), loss: 0.003312, time: 2.248 s, error: 0.010176\n",
      "Step 271820 (epoch   84.94), loss: 0.003423, time: 15.095 s\n",
      "Step 271840 (epoch   84.95), loss: 0.005433, time: 2.260 s\n",
      "Step 271860 (epoch   84.96), loss: 0.006539, time: 2.236 s\n",
      "Step 271880 (epoch   84.96), loss: 0.003904, time: 2.238 s\n",
      "Step 271900 (epoch   84.97), loss: 0.003985, time: 2.254 s\n",
      "Step 271920 (epoch   84.97), loss: 0.003173, time: 2.238 s\n",
      "Step 271940 (epoch   84.98), loss: 0.003809, time: 2.249 s\n",
      "Step 271960 (epoch   84.99), loss: 0.003254, time: 2.235 s\n",
      "Step 271980 (epoch   84.99), loss: 0.004093, time: 2.245 s\n",
      "Step 272000 (epoch   85.00), loss: 0.004435, time: 2.242 s, error: 0.010051\n",
      "\n",
      "Time since beginning  : 47763.199 s\n",
      "\n",
      "Step 272020 (epoch   85.01), loss: 0.004331, time: 15.107 s\n",
      "Step 272040 (epoch   85.01), loss: 0.003210, time: 2.239 s\n",
      "Step 272060 (epoch   85.02), loss: 0.008069, time: 2.239 s\n",
      "Step 272080 (epoch   85.03), loss: 0.002808, time: 2.251 s\n",
      "Step 272100 (epoch   85.03), loss: 0.005151, time: 2.261 s\n",
      "Step 272120 (epoch   85.04), loss: 0.004450, time: 2.253 s\n",
      "Step 272140 (epoch   85.04), loss: 0.004243, time: 2.235 s\n",
      "Step 272160 (epoch   85.05), loss: 0.002461, time: 2.246 s\n",
      "Step 272180 (epoch   85.06), loss: 0.003987, time: 2.237 s\n",
      "Step 272200 (epoch   85.06), loss: 0.003496, time: 2.242 s, error: 0.010047\n",
      "Step 272220 (epoch   85.07), loss: 0.004118, time: 15.075 s\n",
      "Step 272240 (epoch   85.08), loss: 0.004710, time: 2.235 s\n",
      "Step 272260 (epoch   85.08), loss: 0.001939, time: 2.248 s\n",
      "Step 272280 (epoch   85.09), loss: 0.004107, time: 2.230 s\n",
      "Step 272300 (epoch   85.09), loss: 0.003180, time: 2.245 s\n",
      "Step 272320 (epoch   85.10), loss: 0.002954, time: 2.238 s\n",
      "Step 272340 (epoch   85.11), loss: 0.005047, time: 2.241 s\n",
      "Step 272360 (epoch   85.11), loss: 0.007025, time: 2.261 s\n",
      "Step 272380 (epoch   85.12), loss: 0.004226, time: 2.247 s\n",
      "Step 272400 (epoch   85.12), loss: 0.003871, time: 2.234 s, error: 0.010204\n",
      "Step 272420 (epoch   85.13), loss: 0.002997, time: 15.019 s\n",
      "Step 272440 (epoch   85.14), loss: 0.010210, time: 2.250 s\n",
      "Step 272460 (epoch   85.14), loss: 0.003664, time: 2.245 s\n",
      "Step 272480 (epoch   85.15), loss: 0.008849, time: 2.250 s\n",
      "Step 272500 (epoch   85.16), loss: 0.003760, time: 2.231 s\n",
      "Step 272520 (epoch   85.16), loss: 0.003308, time: 2.236 s\n",
      "Step 272540 (epoch   85.17), loss: 0.004237, time: 2.236 s\n",
      "Step 272560 (epoch   85.17), loss: 0.005749, time: 2.245 s\n",
      "Step 272580 (epoch   85.18), loss: 0.003977, time: 2.241 s\n",
      "Step 272600 (epoch   85.19), loss: 0.003395, time: 2.254 s, error: 0.010063\n",
      "Step 272620 (epoch   85.19), loss: 0.003234, time: 15.258 s\n",
      "Step 272640 (epoch   85.20), loss: 0.004911, time: 2.253 s\n",
      "Step 272660 (epoch   85.21), loss: 0.004434, time: 2.257 s\n",
      "Step 272680 (epoch   85.21), loss: 0.003468, time: 2.257 s\n",
      "Step 272700 (epoch   85.22), loss: 0.003967, time: 2.242 s\n",
      "Step 272720 (epoch   85.22), loss: 0.002093, time: 2.250 s\n",
      "Step 272740 (epoch   85.23), loss: 0.003939, time: 2.239 s\n",
      "Step 272760 (epoch   85.24), loss: 0.003717, time: 2.241 s\n",
      "Step 272780 (epoch   85.24), loss: 0.006501, time: 2.242 s\n",
      "Step 272800 (epoch   85.25), loss: 0.002403, time: 2.248 s, error: 0.009969\n",
      "Step 272820 (epoch   85.26), loss: 0.004095, time: 15.222 s\n",
      "Step 272840 (epoch   85.26), loss: 0.003248, time: 2.247 s\n",
      "Step 272860 (epoch   85.27), loss: 0.003201, time: 2.242 s\n",
      "Step 272880 (epoch   85.28), loss: 0.003489, time: 2.241 s\n",
      "Step 272900 (epoch   85.28), loss: 0.003668, time: 2.235 s\n",
      "Step 272920 (epoch   85.29), loss: 0.004183, time: 2.231 s\n",
      "Step 272940 (epoch   85.29), loss: 0.005523, time: 2.240 s\n",
      "Step 272960 (epoch   85.30), loss: 0.004548, time: 2.239 s\n",
      "Step 272980 (epoch   85.31), loss: 0.005454, time: 2.248 s\n",
      "Step 273000 (epoch   85.31), loss: 0.005771, time: 2.235 s, error: 0.009939\n",
      "Step 273020 (epoch   85.32), loss: 0.003885, time: 15.108 s\n",
      "Step 273040 (epoch   85.33), loss: 0.002332, time: 2.244 s\n",
      "Step 273060 (epoch   85.33), loss: 0.004446, time: 2.243 s\n",
      "Step 273080 (epoch   85.34), loss: 0.005227, time: 2.244 s\n",
      "Step 273100 (epoch   85.34), loss: 0.003041, time: 2.236 s\n",
      "Step 273120 (epoch   85.35), loss: 0.006145, time: 2.247 s\n",
      "Step 273140 (epoch   85.36), loss: 0.003605, time: 2.243 s\n",
      "Step 273160 (epoch   85.36), loss: 0.004293, time: 2.252 s\n",
      "Step 273180 (epoch   85.37), loss: 0.004962, time: 2.251 s\n",
      "Step 273200 (epoch   85.38), loss: 0.002754, time: 2.247 s, error: 0.009937\n",
      "Step 273220 (epoch   85.38), loss: 0.004328, time: 15.091 s\n",
      "Step 273240 (epoch   85.39), loss: 0.006872, time: 2.237 s\n",
      "Step 273260 (epoch   85.39), loss: 0.005937, time: 2.255 s\n",
      "Step 273280 (epoch   85.40), loss: 0.005742, time: 2.260 s\n",
      "Step 273300 (epoch   85.41), loss: 0.002690, time: 2.249 s\n",
      "Step 273320 (epoch   85.41), loss: 0.002588, time: 2.229 s\n",
      "Step 273340 (epoch   85.42), loss: 0.003566, time: 2.247 s\n",
      "Step 273360 (epoch   85.42), loss: 0.002829, time: 2.251 s\n",
      "Step 273380 (epoch   85.43), loss: 0.003202, time: 2.244 s\n",
      "Step 273400 (epoch   85.44), loss: 0.003671, time: 2.247 s, error: 0.010045\n",
      "Step 273420 (epoch   85.44), loss: 0.005081, time: 15.034 s\n",
      "Step 273440 (epoch   85.45), loss: 0.002927, time: 2.239 s\n",
      "Step 273460 (epoch   85.46), loss: 0.003798, time: 2.237 s\n",
      "Step 273480 (epoch   85.46), loss: 0.004452, time: 2.230 s\n",
      "Step 273500 (epoch   85.47), loss: 0.003722, time: 2.246 s\n",
      "Step 273520 (epoch   85.47), loss: 0.003070, time: 2.247 s\n",
      "Step 273540 (epoch   85.48), loss: 0.003575, time: 2.261 s\n",
      "Step 273560 (epoch   85.49), loss: 0.006135, time: 2.235 s\n",
      "Step 273580 (epoch   85.49), loss: 0.003466, time: 2.241 s\n",
      "Step 273600 (epoch   85.50), loss: 0.004364, time: 2.249 s, error: 0.010251\n",
      "Step 273620 (epoch   85.51), loss: 0.003955, time: 15.004 s\n",
      "Step 273640 (epoch   85.51), loss: 0.003130, time: 2.245 s\n",
      "Step 273660 (epoch   85.52), loss: 0.003208, time: 2.247 s\n",
      "Step 273680 (epoch   85.53), loss: 0.003377, time: 2.248 s\n",
      "Step 273700 (epoch   85.53), loss: 0.003790, time: 2.253 s\n",
      "Step 273720 (epoch   85.54), loss: 0.002729, time: 2.244 s\n",
      "Step 273740 (epoch   85.54), loss: 0.004607, time: 2.239 s\n",
      "Step 273760 (epoch   85.55), loss: 0.006379, time: 2.243 s\n",
      "Step 273780 (epoch   85.56), loss: 0.006849, time: 2.255 s\n",
      "Step 273800 (epoch   85.56), loss: 0.004831, time: 2.260 s, error: 0.010030\n",
      "Step 273820 (epoch   85.57), loss: 0.003205, time: 15.058 s\n",
      "Step 273840 (epoch   85.58), loss: 0.002742, time: 2.233 s\n",
      "Step 273860 (epoch   85.58), loss: 0.003472, time: 2.243 s\n",
      "Step 273880 (epoch   85.59), loss: 0.004152, time: 2.234 s\n",
      "Step 273900 (epoch   85.59), loss: 0.003673, time: 2.237 s\n",
      "Step 273920 (epoch   85.60), loss: 0.003707, time: 2.225 s\n",
      "Step 273940 (epoch   85.61), loss: 0.004478, time: 2.244 s\n",
      "Step 273960 (epoch   85.61), loss: 0.006018, time: 2.248 s\n",
      "Step 273980 (epoch   85.62), loss: 0.003936, time: 2.242 s\n",
      "Step 274000 (epoch   85.62), loss: 0.004561, time: 2.239 s, error: 0.010473\n",
      "\n",
      "Time since beginning  : 48116.337 s\n",
      "\n",
      "Step 274020 (epoch   85.63), loss: 0.003275, time: 15.319 s\n",
      "Step 274040 (epoch   85.64), loss: 0.003350, time: 2.245 s\n",
      "Step 274060 (epoch   85.64), loss: 0.002941, time: 2.233 s\n",
      "Step 274080 (epoch   85.65), loss: 0.002940, time: 2.250 s\n",
      "Step 274100 (epoch   85.66), loss: 0.001979, time: 2.248 s\n",
      "Step 274120 (epoch   85.66), loss: 0.003748, time: 2.238 s\n",
      "Step 274140 (epoch   85.67), loss: 0.005111, time: 2.242 s\n",
      "Step 274160 (epoch   85.67), loss: 0.003449, time: 2.236 s\n",
      "Step 274180 (epoch   85.68), loss: 0.004326, time: 2.231 s\n",
      "Step 274200 (epoch   85.69), loss: 0.004583, time: 2.234 s, error: 0.010589\n",
      "Step 274220 (epoch   85.69), loss: 0.003318, time: 15.311 s\n",
      "Step 274240 (epoch   85.70), loss: 0.005548, time: 2.238 s\n",
      "Step 274260 (epoch   85.71), loss: 0.003072, time: 2.222 s\n",
      "Step 274280 (epoch   85.71), loss: 0.003675, time: 2.250 s\n",
      "Step 274300 (epoch   85.72), loss: 0.003730, time: 2.238 s\n",
      "Step 274320 (epoch   85.72), loss: 0.003162, time: 2.238 s\n",
      "Step 274340 (epoch   85.73), loss: 0.003516, time: 2.232 s\n",
      "Step 274360 (epoch   85.74), loss: 0.002670, time: 2.238 s\n",
      "Step 274380 (epoch   85.74), loss: 0.002985, time: 2.228 s\n",
      "Step 274400 (epoch   85.75), loss: 0.002745, time: 2.235 s, error: 0.009873\n",
      "Step 274420 (epoch   85.76), loss: 0.003044, time: 15.085 s\n",
      "Step 274440 (epoch   85.76), loss: 0.003862, time: 2.246 s\n",
      "Step 274460 (epoch   85.77), loss: 0.004879, time: 2.259 s\n",
      "Step 274480 (epoch   85.78), loss: 0.003196, time: 2.239 s\n",
      "Step 274500 (epoch   85.78), loss: 0.003037, time: 2.243 s\n",
      "Step 274520 (epoch   85.79), loss: 0.005811, time: 2.235 s\n",
      "Step 274540 (epoch   85.79), loss: 0.003409, time: 2.231 s\n",
      "Step 274560 (epoch   85.80), loss: 0.003709, time: 2.249 s\n",
      "Step 274580 (epoch   85.81), loss: 0.001940, time: 2.243 s\n",
      "Step 274600 (epoch   85.81), loss: 0.003560, time: 2.244 s, error: 0.009899\n",
      "Step 274620 (epoch   85.82), loss: 0.006693, time: 15.080 s\n",
      "Step 274640 (epoch   85.83), loss: 0.002477, time: 2.243 s\n",
      "Step 274660 (epoch   85.83), loss: 0.004416, time: 2.237 s\n",
      "Step 274680 (epoch   85.84), loss: 0.004922, time: 2.247 s\n",
      "Step 274700 (epoch   85.84), loss: 0.004267, time: 2.248 s\n",
      "Step 274720 (epoch   85.85), loss: 0.004860, time: 2.262 s\n",
      "Step 274740 (epoch   85.86), loss: 0.003351, time: 2.238 s\n",
      "Step 274760 (epoch   85.86), loss: 0.003241, time: 2.244 s\n",
      "Step 274780 (epoch   85.87), loss: 0.003941, time: 2.237 s\n",
      "Step 274800 (epoch   85.88), loss: 0.003659, time: 2.255 s, error: 0.009818\n",
      "Step 274820 (epoch   85.88), loss: 0.006050, time: 15.093 s\n",
      "Step 274840 (epoch   85.89), loss: 0.004314, time: 2.247 s\n",
      "Step 274860 (epoch   85.89), loss: 0.003454, time: 2.229 s\n",
      "Step 274880 (epoch   85.90), loss: 0.003879, time: 2.235 s\n",
      "Step 274900 (epoch   85.91), loss: 0.003523, time: 2.234 s\n",
      "Step 274920 (epoch   85.91), loss: 0.003629, time: 2.245 s\n",
      "Step 274940 (epoch   85.92), loss: 0.002930, time: 2.238 s\n",
      "Step 274960 (epoch   85.92), loss: 0.003434, time: 2.251 s\n",
      "Step 274980 (epoch   85.93), loss: 0.003556, time: 2.249 s\n",
      "Step 275000 (epoch   85.94), loss: 0.005464, time: 2.241 s, error: 0.010106\n",
      "Step 275020 (epoch   85.94), loss: 0.004263, time: 15.054 s\n",
      "Step 275040 (epoch   85.95), loss: 0.004466, time: 2.256 s\n",
      "Step 275060 (epoch   85.96), loss: 0.006223, time: 2.255 s\n",
      "Step 275080 (epoch   85.96), loss: 0.002960, time: 2.234 s\n",
      "Step 275100 (epoch   85.97), loss: 0.003579, time: 2.239 s\n",
      "Step 275120 (epoch   85.97), loss: 0.004781, time: 2.230 s\n",
      "Step 275140 (epoch   85.98), loss: 0.004111, time: 2.249 s\n",
      "Step 275160 (epoch   85.99), loss: 0.004158, time: 2.234 s\n",
      "Step 275180 (epoch   85.99), loss: 0.004455, time: 2.238 s\n",
      "Step 275200 (epoch   86.00), loss: 0.003740, time: 2.238 s, error: 0.010107\n",
      "Step 275220 (epoch   86.01), loss: 0.003296, time: 15.269 s\n",
      "Step 275240 (epoch   86.01), loss: 0.002892, time: 2.249 s\n",
      "Step 275260 (epoch   86.02), loss: 0.004611, time: 2.229 s\n",
      "Step 275280 (epoch   86.03), loss: 0.002340, time: 2.243 s\n",
      "Step 275300 (epoch   86.03), loss: 0.005740, time: 2.237 s\n",
      "Step 275320 (epoch   86.04), loss: 0.004387, time: 2.247 s\n",
      "Step 275340 (epoch   86.04), loss: 0.005885, time: 2.227 s\n",
      "Step 275360 (epoch   86.05), loss: 0.003841, time: 2.241 s\n",
      "Step 275380 (epoch   86.06), loss: 0.007771, time: 2.247 s\n",
      "Step 275400 (epoch   86.06), loss: 0.003112, time: 2.245 s, error: 0.010010\n",
      "Step 275420 (epoch   86.07), loss: 0.005100, time: 15.283 s\n",
      "Step 275440 (epoch   86.08), loss: 0.006923, time: 2.233 s\n",
      "Step 275460 (epoch   86.08), loss: 0.002755, time: 2.254 s\n",
      "Step 275480 (epoch   86.09), loss: 0.003158, time: 2.232 s\n",
      "Step 275500 (epoch   86.09), loss: 0.002447, time: 2.242 s\n",
      "Step 275520 (epoch   86.10), loss: 0.003050, time: 2.232 s\n",
      "Step 275540 (epoch   86.11), loss: 0.003610, time: 2.238 s\n",
      "Step 275560 (epoch   86.11), loss: 0.003511, time: 2.235 s\n",
      "Step 275580 (epoch   86.12), loss: 0.005797, time: 2.244 s\n",
      "Step 275600 (epoch   86.12), loss: 0.003961, time: 2.237 s, error: 0.010272\n",
      "Step 275620 (epoch   86.13), loss: 0.004031, time: 15.017 s\n",
      "Step 275640 (epoch   86.14), loss: 0.003362, time: 2.249 s\n",
      "Step 275660 (epoch   86.14), loss: 0.004658, time: 2.247 s\n",
      "Step 275680 (epoch   86.15), loss: 0.003248, time: 2.251 s\n",
      "Step 275700 (epoch   86.16), loss: 0.004268, time: 2.248 s\n",
      "Step 275720 (epoch   86.16), loss: 0.004336, time: 2.247 s\n",
      "Step 275740 (epoch   86.17), loss: 0.003439, time: 2.234 s\n",
      "Step 275760 (epoch   86.17), loss: 0.004593, time: 2.233 s\n",
      "Step 275780 (epoch   86.18), loss: 0.003898, time: 2.230 s\n",
      "Step 275800 (epoch   86.19), loss: 0.004105, time: 2.239 s, error: 0.010040\n",
      "Step 275820 (epoch   86.19), loss: 0.002755, time: 15.040 s\n",
      "Step 275840 (epoch   86.20), loss: 0.004094, time: 2.223 s\n",
      "Step 275860 (epoch   86.21), loss: 0.004807, time: 2.250 s\n",
      "Step 275880 (epoch   86.21), loss: 0.003778, time: 2.246 s\n",
      "Step 275900 (epoch   86.22), loss: 0.007832, time: 2.247 s\n",
      "Step 275920 (epoch   86.22), loss: 0.004762, time: 2.226 s\n",
      "Step 275940 (epoch   86.23), loss: 0.004216, time: 2.241 s\n",
      "Step 275960 (epoch   86.24), loss: 0.004030, time: 2.240 s\n",
      "Step 275980 (epoch   86.24), loss: 0.004565, time: 2.244 s\n",
      "Step 276000 (epoch   86.25), loss: 0.004644, time: 2.239 s, error: 0.010073\n",
      "\n",
      "Time since beginning  : 48469.394 s\n",
      "\n",
      "Step 276020 (epoch   86.26), loss: 0.003242, time: 15.197 s\n",
      "Step 276040 (epoch   86.26), loss: 0.005040, time: 2.253 s\n",
      "Step 276060 (epoch   86.27), loss: 0.004932, time: 2.257 s\n",
      "Step 276080 (epoch   86.28), loss: 0.003087, time: 2.234 s\n",
      "Step 276100 (epoch   86.28), loss: 0.003546, time: 2.250 s\n",
      "Step 276120 (epoch   86.29), loss: 0.003645, time: 2.234 s\n",
      "Step 276140 (epoch   86.29), loss: 0.003079, time: 2.259 s\n",
      "Step 276160 (epoch   86.30), loss: 0.003237, time: 2.250 s\n",
      "Step 276180 (epoch   86.31), loss: 0.003024, time: 2.251 s\n",
      "Step 276200 (epoch   86.31), loss: 0.003382, time: 2.252 s, error: 0.009898\n",
      "Step 276220 (epoch   86.32), loss: 0.002742, time: 15.039 s\n",
      "Step 276240 (epoch   86.33), loss: 0.002947, time: 2.247 s\n",
      "Step 276260 (epoch   86.33), loss: 0.006202, time: 2.247 s\n",
      "Step 276280 (epoch   86.34), loss: 0.003017, time: 2.240 s\n",
      "Step 276300 (epoch   86.34), loss: 0.007854, time: 2.223 s\n",
      "Step 276320 (epoch   86.35), loss: 0.005021, time: 2.233 s\n",
      "Step 276340 (epoch   86.36), loss: 0.003982, time: 2.242 s\n",
      "Step 276360 (epoch   86.36), loss: 0.004366, time: 2.238 s\n",
      "Step 276380 (epoch   86.37), loss: 0.003476, time: 2.238 s\n",
      "Step 276400 (epoch   86.38), loss: 0.005627, time: 2.257 s, error: 0.009892\n",
      "Step 276420 (epoch   86.38), loss: 0.003521, time: 15.087 s\n",
      "Step 276440 (epoch   86.39), loss: 0.003194, time: 2.240 s\n",
      "Step 276460 (epoch   86.39), loss: 0.003166, time: 2.242 s\n",
      "Step 276480 (epoch   86.40), loss: 0.002708, time: 2.230 s\n",
      "Step 276500 (epoch   86.41), loss: 0.004969, time: 2.238 s\n",
      "Step 276520 (epoch   86.41), loss: 0.003377, time: 2.230 s\n",
      "Step 276540 (epoch   86.42), loss: 0.002296, time: 2.233 s\n",
      "Step 276560 (epoch   86.42), loss: 0.004673, time: 2.241 s\n",
      "Step 276580 (epoch   86.43), loss: 0.004185, time: 2.255 s\n",
      "Step 276600 (epoch   86.44), loss: 0.012313, time: 2.254 s, error: 0.010216\n",
      "Step 276620 (epoch   86.44), loss: 0.007777, time: 15.282 s\n",
      "Step 276640 (epoch   86.45), loss: 0.002967, time: 2.255 s\n",
      "Step 276660 (epoch   86.46), loss: 0.003835, time: 2.241 s\n",
      "Step 276680 (epoch   86.46), loss: 0.004078, time: 2.252 s\n",
      "Step 276700 (epoch   86.47), loss: 0.003801, time: 2.236 s\n",
      "Step 276720 (epoch   86.47), loss: 0.002560, time: 2.252 s\n",
      "Step 276740 (epoch   86.48), loss: 0.004221, time: 2.247 s\n",
      "Step 276760 (epoch   86.49), loss: 0.004531, time: 2.251 s\n",
      "Step 276780 (epoch   86.49), loss: 0.004669, time: 2.244 s\n",
      "Step 276800 (epoch   86.50), loss: 0.003509, time: 2.253 s, error: 0.010099\n",
      "Step 276820 (epoch   86.51), loss: 0.002922, time: 15.253 s\n",
      "Step 276840 (epoch   86.51), loss: 0.003523, time: 2.246 s\n",
      "Step 276860 (epoch   86.52), loss: 0.005165, time: 2.234 s\n",
      "Step 276880 (epoch   86.53), loss: 0.004094, time: 2.248 s\n",
      "Step 276900 (epoch   86.53), loss: 0.003594, time: 2.245 s\n",
      "Step 276920 (epoch   86.54), loss: 0.004177, time: 2.228 s\n",
      "Step 276940 (epoch   86.54), loss: 0.002849, time: 2.238 s\n",
      "Step 276960 (epoch   86.55), loss: 0.003435, time: 2.237 s\n",
      "Step 276980 (epoch   86.56), loss: 0.003771, time: 2.241 s\n",
      "Step 277000 (epoch   86.56), loss: 0.006703, time: 2.239 s, error: 0.010072\n",
      "Step 277020 (epoch   86.57), loss: 0.009610, time: 15.041 s\n",
      "Step 277040 (epoch   86.58), loss: 0.003265, time: 2.246 s\n",
      "Step 277060 (epoch   86.58), loss: 0.004318, time: 2.259 s\n",
      "Step 277080 (epoch   86.59), loss: 0.005417, time: 2.249 s\n",
      "Step 277100 (epoch   86.59), loss: 0.005030, time: 2.250 s\n",
      "Step 277120 (epoch   86.60), loss: 0.004262, time: 2.247 s\n",
      "Step 277140 (epoch   86.61), loss: 0.004646, time: 2.232 s\n",
      "Step 277160 (epoch   86.61), loss: 0.002420, time: 2.245 s\n",
      "Step 277180 (epoch   86.62), loss: 0.003753, time: 2.233 s\n",
      "Step 277200 (epoch   86.62), loss: 0.003660, time: 2.245 s, error: 0.010295\n",
      "Step 277220 (epoch   86.63), loss: 0.006437, time: 15.025 s\n",
      "Step 277240 (epoch   86.64), loss: 0.003099, time: 2.235 s\n",
      "Step 277260 (epoch   86.64), loss: 0.003380, time: 2.249 s\n",
      "Step 277280 (epoch   86.65), loss: 0.005507, time: 2.237 s\n",
      "Step 277300 (epoch   86.66), loss: 0.003524, time: 2.252 s\n",
      "Step 277320 (epoch   86.66), loss: 0.004386, time: 2.253 s\n",
      "Step 277340 (epoch   86.67), loss: 0.004358, time: 2.247 s\n",
      "Step 277360 (epoch   86.67), loss: 0.003310, time: 2.229 s\n",
      "Step 277380 (epoch   86.68), loss: 0.003566, time: 2.252 s\n",
      "Step 277400 (epoch   86.69), loss: 0.004586, time: 2.249 s, error: 0.010277\n",
      "Step 277420 (epoch   86.69), loss: 0.003089, time: 15.059 s\n",
      "Step 277440 (epoch   86.70), loss: 0.004200, time: 2.244 s\n",
      "Step 277460 (epoch   86.71), loss: 0.003982, time: 2.235 s\n",
      "Step 277480 (epoch   86.71), loss: 0.004313, time: 2.252 s\n",
      "Step 277500 (epoch   86.72), loss: 0.004998, time: 2.238 s\n",
      "Step 277520 (epoch   86.72), loss: 0.004334, time: 2.240 s\n",
      "Step 277540 (epoch   86.73), loss: 0.003368, time: 2.243 s\n",
      "Step 277560 (epoch   86.74), loss: 0.002422, time: 2.246 s\n",
      "Step 277580 (epoch   86.74), loss: 0.002327, time: 2.260 s\n",
      "Step 277600 (epoch   86.75), loss: 0.006419, time: 2.245 s, error: 0.009934\n",
      "Step 277620 (epoch   86.76), loss: 0.006750, time: 15.075 s\n",
      "Step 277640 (epoch   86.76), loss: 0.003888, time: 2.221 s\n",
      "Step 277660 (epoch   86.77), loss: 0.003610, time: 2.250 s\n",
      "Step 277680 (epoch   86.78), loss: 0.003768, time: 2.254 s\n",
      "Step 277700 (epoch   86.78), loss: 0.002383, time: 2.244 s\n",
      "Step 277720 (epoch   86.79), loss: 0.004657, time: 2.247 s\n",
      "Step 277740 (epoch   86.79), loss: 0.006943, time: 2.228 s\n",
      "Step 277760 (epoch   86.80), loss: 0.002672, time: 2.242 s\n",
      "Step 277780 (epoch   86.81), loss: 0.003681, time: 2.236 s\n",
      "Step 277800 (epoch   86.81), loss: 0.004454, time: 2.235 s, error: 0.009870\n",
      "Step 277820 (epoch   86.82), loss: 0.005878, time: 15.373 s\n",
      "Step 277840 (epoch   86.83), loss: 0.004988, time: 2.249 s\n",
      "Step 277860 (epoch   86.83), loss: 0.003430, time: 2.253 s\n",
      "Step 277880 (epoch   86.84), loss: 0.006509, time: 2.239 s\n",
      "Step 277900 (epoch   86.84), loss: 0.005222, time: 2.253 s\n",
      "Step 277920 (epoch   86.85), loss: 0.003153, time: 2.247 s\n",
      "Step 277940 (epoch   86.86), loss: 0.003163, time: 2.248 s\n",
      "Step 277960 (epoch   86.86), loss: 0.002393, time: 2.235 s\n",
      "Step 277980 (epoch   86.87), loss: 0.003155, time: 2.249 s\n",
      "Step 278000 (epoch   86.88), loss: 0.003700, time: 2.238 s, error: 0.009781\n",
      "\n",
      "Time since beginning  : 48822.897 s\n",
      "\n",
      "Step 278020 (epoch   86.88), loss: 0.003060, time: 15.333 s\n",
      "Step 278040 (epoch   86.89), loss: 0.004872, time: 2.251 s\n",
      "Step 278060 (epoch   86.89), loss: 0.003890, time: 2.235 s\n",
      "Step 278080 (epoch   86.90), loss: 0.004537, time: 2.249 s\n",
      "Step 278100 (epoch   86.91), loss: 0.004085, time: 2.241 s\n",
      "Step 278120 (epoch   86.91), loss: 0.004309, time: 2.256 s\n",
      "Step 278140 (epoch   86.92), loss: 0.003807, time: 2.243 s\n",
      "Step 278160 (epoch   86.92), loss: 0.003880, time: 2.248 s\n",
      "Step 278180 (epoch   86.93), loss: 0.002314, time: 2.229 s\n",
      "Step 278200 (epoch   86.94), loss: 0.003863, time: 2.237 s, error: 0.010110\n",
      "Step 278220 (epoch   86.94), loss: 0.002985, time: 15.060 s\n",
      "Step 278240 (epoch   86.95), loss: 0.002708, time: 2.259 s\n",
      "Step 278260 (epoch   86.96), loss: 0.004193, time: 2.249 s\n",
      "Step 278280 (epoch   86.96), loss: 0.003824, time: 2.233 s\n",
      "Step 278300 (epoch   86.97), loss: 0.006402, time: 2.247 s\n",
      "Step 278320 (epoch   86.97), loss: 0.002969, time: 2.233 s\n",
      "Step 278340 (epoch   86.98), loss: 0.006405, time: 2.237 s\n",
      "Step 278360 (epoch   86.99), loss: 0.003256, time: 2.231 s\n",
      "Step 278380 (epoch   86.99), loss: 0.003626, time: 2.247 s\n",
      "Step 278400 (epoch   87.00), loss: 0.004062, time: 2.245 s, error: 0.010089\n",
      "Step 278420 (epoch   87.01), loss: 0.003073, time: 15.092 s\n",
      "Step 278440 (epoch   87.01), loss: 0.003282, time: 2.257 s\n",
      "Step 278460 (epoch   87.02), loss: 0.003376, time: 2.246 s\n",
      "Step 278480 (epoch   87.03), loss: 0.003240, time: 2.245 s\n",
      "Step 278500 (epoch   87.03), loss: 0.003898, time: 2.248 s\n",
      "Step 278520 (epoch   87.04), loss: 0.006355, time: 2.243 s\n",
      "Step 278540 (epoch   87.04), loss: 0.002633, time: 2.238 s\n",
      "Step 278560 (epoch   87.05), loss: 0.002922, time: 2.251 s\n",
      "Step 278580 (epoch   87.06), loss: 0.003128, time: 2.237 s\n",
      "Step 278600 (epoch   87.06), loss: 0.003252, time: 2.238 s, error: 0.009959\n",
      "Step 278620 (epoch   87.07), loss: 0.003533, time: 15.024 s\n",
      "Step 278640 (epoch   87.08), loss: 0.004098, time: 2.236 s\n",
      "Step 278660 (epoch   87.08), loss: 0.005999, time: 2.238 s\n",
      "Step 278680 (epoch   87.09), loss: 0.004781, time: 2.230 s\n",
      "Step 278700 (epoch   87.09), loss: 0.007213, time: 2.252 s\n",
      "Step 278720 (epoch   87.10), loss: 0.012449, time: 2.248 s\n",
      "Step 278740 (epoch   87.11), loss: 0.003657, time: 2.245 s\n",
      "Step 278760 (epoch   87.11), loss: 0.003503, time: 2.260 s\n",
      "Step 278780 (epoch   87.12), loss: 0.006004, time: 2.255 s\n",
      "Step 278800 (epoch   87.12), loss: 0.004908, time: 2.241 s, error: 0.010108\n",
      "Step 278820 (epoch   87.13), loss: 0.005371, time: 15.076 s\n",
      "Step 278840 (epoch   87.14), loss: 0.003348, time: 2.248 s\n",
      "Step 278860 (epoch   87.14), loss: 0.003372, time: 2.238 s\n",
      "Step 278880 (epoch   87.15), loss: 0.004491, time: 2.251 s\n",
      "Step 278900 (epoch   87.16), loss: 0.003311, time: 2.233 s\n",
      "Step 278920 (epoch   87.16), loss: 0.003932, time: 2.252 s\n",
      "Step 278940 (epoch   87.17), loss: 0.006111, time: 2.243 s\n",
      "Step 278960 (epoch   87.17), loss: 0.005074, time: 2.244 s\n",
      "Step 278980 (epoch   87.18), loss: 0.003589, time: 2.241 s\n",
      "Step 279000 (epoch   87.19), loss: 0.003911, time: 2.255 s, error: 0.009970\n",
      "Step 279020 (epoch   87.19), loss: 0.003805, time: 15.208 s\n",
      "Step 279040 (epoch   87.20), loss: 0.003323, time: 2.235 s\n",
      "Step 279060 (epoch   87.21), loss: 0.003150, time: 2.235 s\n",
      "Step 279080 (epoch   87.21), loss: 0.004581, time: 2.250 s\n",
      "Step 279100 (epoch   87.22), loss: 0.002329, time: 2.238 s\n",
      "Step 279120 (epoch   87.22), loss: 0.006396, time: 2.246 s\n",
      "Step 279140 (epoch   87.23), loss: 0.003261, time: 2.237 s\n",
      "Step 279160 (epoch   87.24), loss: 0.002916, time: 2.253 s\n",
      "Step 279180 (epoch   87.24), loss: 0.003106, time: 2.252 s\n",
      "Step 279200 (epoch   87.25), loss: 0.003328, time: 2.242 s, error: 0.010086\n",
      "Step 279220 (epoch   87.26), loss: 0.005142, time: 15.261 s\n",
      "Step 279240 (epoch   87.26), loss: 0.004569, time: 2.235 s\n",
      "Step 279260 (epoch   87.27), loss: 0.005598, time: 2.239 s\n",
      "Step 279280 (epoch   87.28), loss: 0.003222, time: 2.243 s\n",
      "Step 279300 (epoch   87.28), loss: 0.004677, time: 2.238 s\n",
      "Step 279320 (epoch   87.29), loss: 0.004126, time: 2.240 s\n",
      "Step 279340 (epoch   87.29), loss: 0.004174, time: 2.228 s\n",
      "Step 279360 (epoch   87.30), loss: 0.003386, time: 2.235 s\n",
      "Step 279380 (epoch   87.31), loss: 0.003991, time: 2.235 s\n",
      "Step 279400 (epoch   87.31), loss: 0.009019, time: 2.238 s, error: 0.009906\n",
      "Step 279420 (epoch   87.32), loss: 0.003368, time: 15.154 s\n",
      "Step 279440 (epoch   87.33), loss: 0.002940, time: 2.239 s\n",
      "Step 279460 (epoch   87.33), loss: 0.002494, time: 2.253 s\n",
      "Step 279480 (epoch   87.34), loss: 0.004476, time: 2.252 s\n",
      "Step 279500 (epoch   87.34), loss: 0.002715, time: 2.249 s\n",
      "Step 279520 (epoch   87.35), loss: 0.002736, time: 2.247 s\n",
      "Step 279540 (epoch   87.36), loss: 0.003695, time: 2.235 s\n",
      "Step 279560 (epoch   87.36), loss: 0.002663, time: 2.251 s\n",
      "Step 279580 (epoch   87.37), loss: 0.003951, time: 2.243 s\n",
      "Step 279600 (epoch   87.38), loss: 0.003936, time: 2.246 s, error: 0.009871\n",
      "Step 279620 (epoch   87.38), loss: 0.007672, time: 15.064 s\n",
      "Step 279640 (epoch   87.39), loss: 0.004273, time: 2.241 s\n",
      "Step 279660 (epoch   87.39), loss: 0.004875, time: 2.257 s\n",
      "Step 279680 (epoch   87.40), loss: 0.004561, time: 2.250 s\n",
      "Step 279700 (epoch   87.41), loss: 0.004196, time: 2.235 s\n",
      "Step 279720 (epoch   87.41), loss: 0.004160, time: 2.240 s\n",
      "Step 279740 (epoch   87.42), loss: 0.003184, time: 2.233 s\n",
      "Step 279760 (epoch   87.42), loss: 0.003686, time: 2.252 s\n",
      "Step 279780 (epoch   87.43), loss: 0.003638, time: 2.236 s\n",
      "Step 279800 (epoch   87.44), loss: 0.009000, time: 2.246 s, error: 0.010307\n",
      "Step 279820 (epoch   87.44), loss: 0.005597, time: 15.100 s\n",
      "Step 279840 (epoch   87.45), loss: 0.003375, time: 2.253 s\n",
      "Step 279860 (epoch   87.46), loss: 0.002108, time: 2.248 s\n",
      "Step 279880 (epoch   87.46), loss: 0.004357, time: 2.252 s\n",
      "Step 279900 (epoch   87.47), loss: 0.003259, time: 2.255 s\n",
      "Step 279920 (epoch   87.47), loss: 0.003627, time: 2.255 s\n",
      "Step 279940 (epoch   87.48), loss: 0.004639, time: 2.253 s\n",
      "Step 279960 (epoch   87.49), loss: 0.003583, time: 2.233 s\n",
      "Step 279980 (epoch   87.49), loss: 0.003730, time: 2.250 s\n",
      "Step 280000 (epoch   87.50), loss: 0.004349, time: 2.245 s, error: 0.010020\n",
      "\n",
      "Time since beginning  : 49176.018 s\n",
      "\n",
      "Step 280020 (epoch   87.51), loss: 0.002522, time: 15.122 s\n",
      "Step 280040 (epoch   87.51), loss: 0.006873, time: 2.242 s\n",
      "Step 280060 (epoch   87.52), loss: 0.006046, time: 2.242 s\n",
      "Step 280080 (epoch   87.53), loss: 0.003898, time: 2.249 s\n",
      "Step 280100 (epoch   87.53), loss: 0.002789, time: 2.235 s\n",
      "Step 280120 (epoch   87.54), loss: 0.005065, time: 2.249 s\n",
      "Step 280140 (epoch   87.54), loss: 0.004437, time: 2.243 s\n",
      "Step 280160 (epoch   87.55), loss: 0.002721, time: 2.242 s\n",
      "Step 280180 (epoch   87.56), loss: 0.004639, time: 2.259 s\n",
      "Step 280200 (epoch   87.56), loss: 0.003622, time: 2.259 s, error: 0.010079\n",
      "Step 280220 (epoch   87.57), loss: 0.003893, time: 15.050 s\n",
      "Step 280240 (epoch   87.58), loss: 0.005232, time: 2.226 s\n",
      "Step 280260 (epoch   87.58), loss: 0.002630, time: 2.232 s\n",
      "Step 280280 (epoch   87.59), loss: 0.002157, time: 2.243 s\n",
      "Step 280300 (epoch   87.59), loss: 0.003329, time: 2.231 s\n",
      "Step 280320 (epoch   87.60), loss: 0.004857, time: 2.250 s\n",
      "Step 280340 (epoch   87.61), loss: 0.003729, time: 2.236 s\n",
      "Step 280360 (epoch   87.61), loss: 0.007337, time: 2.239 s\n",
      "Step 280380 (epoch   87.62), loss: 0.004248, time: 2.235 s\n",
      "Step 280400 (epoch   87.62), loss: 0.003057, time: 2.234 s, error: 0.010191\n",
      "Step 280420 (epoch   87.63), loss: 0.003777, time: 15.205 s\n",
      "Step 280440 (epoch   87.64), loss: 0.005246, time: 2.234 s\n",
      "Step 280460 (epoch   87.64), loss: 0.003943, time: 2.243 s\n",
      "Step 280480 (epoch   87.65), loss: 0.005391, time: 2.241 s\n",
      "Step 280500 (epoch   87.66), loss: 0.002870, time: 2.248 s\n",
      "Step 280520 (epoch   87.66), loss: 0.016113, time: 2.245 s\n",
      "Step 280540 (epoch   87.67), loss: 0.003917, time: 2.237 s\n",
      "Step 280560 (epoch   87.67), loss: 0.002935, time: 2.235 s\n",
      "Step 280580 (epoch   87.68), loss: 0.003823, time: 2.253 s\n",
      "Step 280600 (epoch   87.69), loss: 0.005866, time: 2.237 s, error: 0.010060\n",
      "Step 280620 (epoch   87.69), loss: 0.003639, time: 15.211 s\n",
      "Step 280640 (epoch   87.70), loss: 0.003036, time: 2.241 s\n",
      "Step 280660 (epoch   87.71), loss: 0.004756, time: 2.241 s\n",
      "Step 280680 (epoch   87.71), loss: 0.004778, time: 2.241 s\n",
      "Step 280700 (epoch   87.72), loss: 0.003566, time: 2.226 s\n",
      "Step 280720 (epoch   87.72), loss: 0.005455, time: 2.252 s\n",
      "Step 280740 (epoch   87.73), loss: 0.004129, time: 2.234 s\n",
      "Step 280760 (epoch   87.74), loss: 0.004602, time: 2.250 s\n",
      "Step 280780 (epoch   87.74), loss: 0.004902, time: 2.235 s\n",
      "Step 280800 (epoch   87.75), loss: 0.003680, time: 2.247 s, error: 0.009965\n",
      "Step 280820 (epoch   87.76), loss: 0.002970, time: 15.048 s\n",
      "Step 280840 (epoch   87.76), loss: 0.002548, time: 2.247 s\n",
      "Step 280860 (epoch   87.77), loss: 0.004012, time: 2.255 s\n",
      "Step 280880 (epoch   87.78), loss: 0.004211, time: 2.228 s\n",
      "Step 280900 (epoch   87.78), loss: 0.003041, time: 2.229 s\n",
      "Step 280920 (epoch   87.79), loss: 0.005335, time: 2.240 s\n",
      "Step 280940 (epoch   87.79), loss: 0.003347, time: 2.229 s\n",
      "Step 280960 (epoch   87.80), loss: 0.003475, time: 2.226 s\n",
      "Step 280980 (epoch   87.81), loss: 0.003703, time: 2.245 s\n",
      "Step 281000 (epoch   87.81), loss: 0.004452, time: 2.236 s, error: 0.010170\n",
      "Step 281020 (epoch   87.82), loss: 0.005594, time: 15.131 s\n",
      "Step 281040 (epoch   87.83), loss: 0.002460, time: 2.242 s\n",
      "Step 281060 (epoch   87.83), loss: 0.003242, time: 2.242 s\n",
      "Step 281080 (epoch   87.84), loss: 0.004655, time: 2.248 s\n",
      "Step 281100 (epoch   87.84), loss: 0.004691, time: 2.258 s\n",
      "Step 281120 (epoch   87.85), loss: 0.003209, time: 2.254 s\n",
      "Step 281140 (epoch   87.86), loss: 0.003907, time: 2.237 s\n",
      "Step 281160 (epoch   87.86), loss: 0.002870, time: 2.248 s\n",
      "Step 281180 (epoch   87.87), loss: 0.003933, time: 2.235 s\n",
      "Step 281200 (epoch   87.88), loss: 0.003712, time: 2.254 s, error: 0.009774\n",
      "Step 281220 (epoch   87.88), loss: 0.003883, time: 15.121 s\n",
      "Step 281240 (epoch   87.89), loss: 0.004966, time: 2.247 s\n",
      "Step 281260 (epoch   87.89), loss: 0.002750, time: 2.240 s\n",
      "Step 281280 (epoch   87.90), loss: 0.003594, time: 2.244 s\n",
      "Step 281300 (epoch   87.91), loss: 0.005463, time: 2.227 s\n",
      "Step 281320 (epoch   87.91), loss: 0.002968, time: 2.241 s\n",
      "Step 281340 (epoch   87.92), loss: 0.003101, time: 2.251 s\n",
      "Step 281360 (epoch   87.92), loss: 0.003498, time: 2.249 s\n",
      "Step 281380 (epoch   87.93), loss: 0.002477, time: 2.252 s\n",
      "Step 281400 (epoch   87.94), loss: 0.005062, time: 2.249 s, error: 0.010133\n",
      "Step 281420 (epoch   87.94), loss: 0.005317, time: 15.016 s\n",
      "Step 281440 (epoch   87.95), loss: 0.003385, time: 2.232 s\n",
      "Step 281460 (epoch   87.96), loss: 0.002863, time: 2.241 s\n",
      "Step 281480 (epoch   87.96), loss: 0.006431, time: 2.222 s\n",
      "Step 281500 (epoch   87.97), loss: 0.002604, time: 2.240 s\n",
      "Step 281520 (epoch   87.97), loss: 0.004023, time: 2.241 s\n",
      "Step 281540 (epoch   87.98), loss: 0.002530, time: 2.241 s\n",
      "Step 281560 (epoch   87.99), loss: 0.004623, time: 2.254 s\n",
      "Step 281580 (epoch   87.99), loss: 0.005878, time: 2.250 s\n",
      "Step 281600 (epoch   88.00), loss: 0.008202, time: 2.226 s, error: 0.009977\n",
      "Step 281620 (epoch   88.01), loss: 0.002941, time: 15.177 s\n",
      "Step 281640 (epoch   88.01), loss: 0.003432, time: 2.243 s\n",
      "Step 281660 (epoch   88.02), loss: 0.004592, time: 2.244 s\n",
      "Step 281680 (epoch   88.03), loss: 0.003126, time: 2.237 s\n",
      "Step 281700 (epoch   88.03), loss: 0.003276, time: 2.234 s\n",
      "Step 281720 (epoch   88.04), loss: 0.005037, time: 2.249 s\n",
      "Step 281740 (epoch   88.04), loss: 0.002367, time: 2.240 s\n",
      "Step 281760 (epoch   88.05), loss: 0.004717, time: 2.236 s\n",
      "Step 281780 (epoch   88.06), loss: 0.005544, time: 2.253 s\n",
      "Step 281800 (epoch   88.06), loss: 0.004919, time: 2.245 s, error: 0.009900\n",
      "Step 281820 (epoch   88.07), loss: 0.003457, time: 15.221 s\n",
      "Step 281840 (epoch   88.08), loss: 0.003548, time: 2.247 s\n",
      "Step 281860 (epoch   88.08), loss: 0.003674, time: 2.238 s\n",
      "Step 281880 (epoch   88.09), loss: 0.004140, time: 2.247 s\n",
      "Step 281900 (epoch   88.09), loss: 0.002987, time: 2.251 s\n",
      "Step 281920 (epoch   88.10), loss: 0.004729, time: 2.247 s\n",
      "Step 281940 (epoch   88.11), loss: 0.004977, time: 2.242 s\n",
      "Step 281960 (epoch   88.11), loss: 0.001965, time: 2.245 s\n",
      "Step 281980 (epoch   88.12), loss: 0.004096, time: 2.241 s\n",
      "Step 282000 (epoch   88.12), loss: 0.004267, time: 2.240 s, error: 0.009923\n",
      "\n",
      "Time since beginning  : 49529.129 s\n",
      "\n",
      "Step 282020 (epoch   88.13), loss: 0.002781, time: 15.191 s\n",
      "Step 282040 (epoch   88.14), loss: 0.005442, time: 2.232 s\n",
      "Step 282060 (epoch   88.14), loss: 0.005286, time: 2.248 s\n",
      "Step 282080 (epoch   88.15), loss: 0.005701, time: 2.234 s\n",
      "Step 282100 (epoch   88.16), loss: 0.004129, time: 2.256 s\n",
      "Step 282120 (epoch   88.16), loss: 0.005474, time: 2.250 s\n",
      "Step 282140 (epoch   88.17), loss: 0.003001, time: 2.239 s\n",
      "Step 282160 (epoch   88.17), loss: 0.006135, time: 2.231 s\n",
      "Step 282180 (epoch   88.18), loss: 0.005997, time: 2.247 s\n",
      "Step 282200 (epoch   88.19), loss: 0.003130, time: 2.235 s, error: 0.009967\n",
      "Step 282220 (epoch   88.19), loss: 0.003256, time: 15.051 s\n",
      "Step 282240 (epoch   88.20), loss: 0.004292, time: 2.236 s\n",
      "Step 282260 (epoch   88.21), loss: 0.004561, time: 2.236 s\n",
      "Step 282280 (epoch   88.21), loss: 0.002558, time: 2.263 s\n",
      "Step 282300 (epoch   88.22), loss: 0.003431, time: 2.232 s\n",
      "Step 282320 (epoch   88.22), loss: 0.002233, time: 2.230 s\n",
      "Step 282340 (epoch   88.23), loss: 0.002601, time: 2.224 s\n",
      "Step 282360 (epoch   88.24), loss: 0.003224, time: 2.247 s\n",
      "Step 282380 (epoch   88.24), loss: 0.002711, time: 2.242 s\n",
      "Step 282400 (epoch   88.25), loss: 0.004604, time: 2.250 s, error: 0.009989\n",
      "Step 282420 (epoch   88.26), loss: 0.005096, time: 15.097 s\n",
      "Step 282440 (epoch   88.26), loss: 0.004820, time: 2.244 s\n",
      "Step 282460 (epoch   88.27), loss: 0.003167, time: 2.250 s\n",
      "Step 282480 (epoch   88.28), loss: 0.003038, time: 2.242 s\n",
      "Step 282500 (epoch   88.28), loss: 0.003786, time: 2.248 s\n",
      "Step 282520 (epoch   88.29), loss: 0.004768, time: 2.244 s\n",
      "Step 282540 (epoch   88.29), loss: 0.004501, time: 2.260 s\n",
      "Step 282560 (epoch   88.30), loss: 0.004688, time: 2.251 s\n",
      "Step 282580 (epoch   88.31), loss: 0.004189, time: 2.243 s\n",
      "Step 282600 (epoch   88.31), loss: 0.003050, time: 2.238 s, error: 0.009887\n",
      "Step 282620 (epoch   88.32), loss: 0.003122, time: 15.052 s\n",
      "Step 282640 (epoch   88.33), loss: 0.005156, time: 2.237 s\n",
      "Step 282660 (epoch   88.33), loss: 0.003414, time: 2.240 s\n",
      "Step 282680 (epoch   88.34), loss: 0.002231, time: 2.250 s\n",
      "Step 282700 (epoch   88.34), loss: 0.003158, time: 2.228 s\n",
      "Step 282720 (epoch   88.35), loss: 0.003617, time: 2.242 s\n",
      "Step 282740 (epoch   88.36), loss: 0.005484, time: 2.234 s\n",
      "Step 282760 (epoch   88.36), loss: 0.003005, time: 2.243 s\n",
      "Step 282780 (epoch   88.37), loss: 0.003861, time: 2.247 s\n",
      "Step 282800 (epoch   88.38), loss: 0.003926, time: 2.257 s, error: 0.009876\n",
      "Step 282820 (epoch   88.38), loss: 0.003370, time: 15.043 s\n",
      "Step 282840 (epoch   88.39), loss: 0.010511, time: 2.240 s\n",
      "Step 282860 (epoch   88.39), loss: 0.007274, time: 2.247 s\n",
      "Step 282880 (epoch   88.40), loss: 0.005271, time: 2.244 s\n",
      "Step 282900 (epoch   88.41), loss: 0.003704, time: 2.244 s\n",
      "Step 282920 (epoch   88.41), loss: 0.005384, time: 2.236 s\n",
      "Step 282940 (epoch   88.42), loss: 0.003888, time: 2.242 s\n",
      "Step 282960 (epoch   88.42), loss: 0.004044, time: 2.234 s\n",
      "Step 282980 (epoch   88.43), loss: 0.003002, time: 2.242 s\n",
      "Step 283000 (epoch   88.44), loss: 0.005063, time: 2.248 s, error: 0.010126\n",
      "Step 283020 (epoch   88.44), loss: 0.003224, time: 15.304 s\n",
      "Step 283040 (epoch   88.45), loss: 0.003340, time: 2.239 s\n",
      "Step 283060 (epoch   88.46), loss: 0.003866, time: 2.239 s\n",
      "Step 283080 (epoch   88.46), loss: 0.004356, time: 2.248 s\n",
      "Step 283100 (epoch   88.47), loss: 0.007488, time: 2.232 s\n",
      "Step 283120 (epoch   88.47), loss: 0.003542, time: 2.249 s\n",
      "Step 283140 (epoch   88.48), loss: 0.002682, time: 2.235 s\n",
      "Step 283160 (epoch   88.49), loss: 0.003193, time: 2.252 s\n",
      "Step 283180 (epoch   88.49), loss: 0.003862, time: 2.235 s\n",
      "Step 283200 (epoch   88.50), loss: 0.005048, time: 2.244 s, error: 0.009943\n",
      "Step 283220 (epoch   88.51), loss: 0.003155, time: 15.306 s\n",
      "Step 283240 (epoch   88.51), loss: 0.011197, time: 2.255 s\n",
      "Step 283260 (epoch   88.52), loss: 0.004672, time: 2.254 s\n",
      "Step 283280 (epoch   88.53), loss: 0.003438, time: 2.258 s\n",
      "Step 283300 (epoch   88.53), loss: 0.006934, time: 2.254 s\n",
      "Step 283320 (epoch   88.54), loss: 0.004940, time: 2.242 s\n",
      "Step 283340 (epoch   88.54), loss: 0.004693, time: 2.250 s\n",
      "Step 283360 (epoch   88.55), loss: 0.003196, time: 2.250 s\n",
      "Step 283380 (epoch   88.56), loss: 0.005469, time: 2.246 s\n",
      "Step 283400 (epoch   88.56), loss: 0.004015, time: 2.242 s, error: 0.010125\n",
      "Step 283420 (epoch   88.57), loss: 0.004242, time: 15.056 s\n",
      "Step 283440 (epoch   88.58), loss: 0.003345, time: 2.256 s\n",
      "Step 283460 (epoch   88.58), loss: 0.005655, time: 2.252 s\n",
      "Step 283480 (epoch   88.59), loss: 0.002917, time: 2.237 s\n",
      "Step 283500 (epoch   88.59), loss: 0.004469, time: 2.238 s\n",
      "Step 283520 (epoch   88.60), loss: 0.004266, time: 2.239 s\n",
      "Step 283540 (epoch   88.61), loss: 0.006709, time: 2.229 s\n",
      "Step 283560 (epoch   88.61), loss: 0.004317, time: 2.239 s\n",
      "Step 283580 (epoch   88.62), loss: 0.006821, time: 2.227 s\n",
      "Step 283600 (epoch   88.62), loss: 0.002829, time: 2.248 s, error: 0.010106\n",
      "Step 283620 (epoch   88.63), loss: 0.003934, time: 15.025 s\n",
      "Step 283640 (epoch   88.64), loss: 0.003268, time: 2.247 s\n",
      "Step 283660 (epoch   88.64), loss: 0.003902, time: 2.238 s\n",
      "Step 283680 (epoch   88.65), loss: 0.007278, time: 2.248 s\n",
      "Step 283700 (epoch   88.66), loss: 0.002476, time: 2.251 s\n",
      "Step 283720 (epoch   88.66), loss: 0.002891, time: 2.258 s\n",
      "Step 283740 (epoch   88.67), loss: 0.002946, time: 2.236 s\n",
      "Step 283760 (epoch   88.67), loss: 0.003452, time: 2.242 s\n",
      "Step 283780 (epoch   88.68), loss: 0.003566, time: 2.233 s\n",
      "Step 283800 (epoch   88.69), loss: 0.004586, time: 2.233 s, error: 0.009995\n",
      "Step 283820 (epoch   88.69), loss: 0.007978, time: 14.986 s\n",
      "Step 283840 (epoch   88.70), loss: 0.002874, time: 2.238 s\n",
      "Step 283860 (epoch   88.71), loss: 0.003099, time: 2.249 s\n",
      "Step 283880 (epoch   88.71), loss: 0.004101, time: 2.242 s\n",
      "Step 283900 (epoch   88.72), loss: 0.004925, time: 2.249 s\n",
      "Step 283920 (epoch   88.72), loss: 0.004555, time: 2.236 s\n",
      "Step 283940 (epoch   88.73), loss: 0.002679, time: 2.246 s\n",
      "Step 283960 (epoch   88.74), loss: 0.004881, time: 2.251 s\n",
      "Step 283980 (epoch   88.74), loss: 0.002940, time: 2.256 s\n",
      "Step 284000 (epoch   88.75), loss: 0.003708, time: 2.245 s, error: 0.009855\n",
      "\n",
      "Time since beginning  : 49882.072 s\n",
      "\n",
      "Step 284020 (epoch   88.76), loss: 0.003614, time: 15.101 s\n",
      "Step 284040 (epoch   88.76), loss: 0.003679, time: 2.251 s\n",
      "Step 284060 (epoch   88.77), loss: 0.003107, time: 2.233 s\n",
      "Step 284080 (epoch   88.78), loss: 0.003873, time: 2.250 s\n",
      "Step 284100 (epoch   88.78), loss: 0.004059, time: 2.222 s\n",
      "Step 284120 (epoch   88.79), loss: 0.005161, time: 2.245 s\n",
      "Step 284140 (epoch   88.79), loss: 0.002900, time: 2.243 s\n",
      "Step 284160 (epoch   88.80), loss: 0.005246, time: 2.249 s\n",
      "Step 284180 (epoch   88.81), loss: 0.002949, time: 2.246 s\n",
      "Step 284200 (epoch   88.81), loss: 0.003816, time: 2.247 s, error: 0.010435\n",
      "Step 284220 (epoch   88.82), loss: 0.005512, time: 15.268 s\n",
      "Step 284240 (epoch   88.83), loss: 0.002556, time: 2.239 s\n",
      "Step 284260 (epoch   88.83), loss: 0.004201, time: 2.252 s\n",
      "Step 284280 (epoch   88.84), loss: 0.004044, time: 2.241 s\n",
      "Step 284300 (epoch   88.84), loss: 0.002748, time: 2.248 s\n",
      "Step 284320 (epoch   88.85), loss: 0.003151, time: 2.239 s\n",
      "Step 284340 (epoch   88.86), loss: 0.005779, time: 2.246 s\n",
      "Step 284360 (epoch   88.86), loss: 0.003262, time: 2.234 s\n",
      "Step 284380 (epoch   88.87), loss: 0.003716, time: 2.246 s\n",
      "Step 284400 (epoch   88.88), loss: 0.003853, time: 2.235 s, error: 0.009755\n",
      "Step 284420 (epoch   88.88), loss: 0.003389, time: 15.259 s\n",
      "Step 284440 (epoch   88.89), loss: 0.007934, time: 2.240 s\n",
      "Step 284460 (epoch   88.89), loss: 0.003380, time: 2.240 s\n",
      "Step 284480 (epoch   88.90), loss: 0.007217, time: 2.247 s\n",
      "Step 284500 (epoch   88.91), loss: 0.003988, time: 2.240 s\n",
      "Step 284520 (epoch   88.91), loss: 0.005246, time: 2.257 s\n",
      "Step 284540 (epoch   88.92), loss: 0.003950, time: 2.234 s\n",
      "Step 284560 (epoch   88.92), loss: 0.003335, time: 2.240 s\n",
      "Step 284580 (epoch   88.93), loss: 0.002853, time: 2.235 s\n",
      "Step 284600 (epoch   88.94), loss: 0.003285, time: 2.239 s, error: 0.010103\n",
      "Step 284620 (epoch   88.94), loss: 0.005742, time: 15.059 s\n",
      "Step 284640 (epoch   88.95), loss: 0.002816, time: 2.254 s\n",
      "Step 284660 (epoch   88.96), loss: 0.003885, time: 2.254 s\n",
      "Step 284680 (epoch   88.96), loss: 0.004209, time: 2.251 s\n",
      "Step 284700 (epoch   88.97), loss: 0.005350, time: 2.243 s\n",
      "Step 284720 (epoch   88.97), loss: 0.004226, time: 2.251 s\n",
      "Step 284740 (epoch   88.98), loss: 0.003047, time: 2.245 s\n",
      "Step 284760 (epoch   88.99), loss: 0.002599, time: 2.244 s\n",
      "Step 284780 (epoch   88.99), loss: 0.004364, time: 2.244 s\n",
      "Step 284800 (epoch   89.00), loss: 0.002528, time: 2.253 s, error: 0.009870\n",
      "Step 284820 (epoch   89.01), loss: 0.003021, time: 15.062 s\n",
      "Step 284840 (epoch   89.01), loss: 0.005885, time: 2.235 s\n",
      "Step 284860 (epoch   89.02), loss: 0.004259, time: 2.246 s\n",
      "Step 284880 (epoch   89.03), loss: 0.003398, time: 2.260 s\n",
      "Step 284900 (epoch   89.03), loss: 0.003563, time: 2.249 s\n",
      "Step 284920 (epoch   89.04), loss: 0.003800, time: 2.245 s\n",
      "Step 284940 (epoch   89.04), loss: 0.003097, time: 2.249 s\n",
      "Step 284960 (epoch   89.05), loss: 0.006339, time: 2.244 s\n",
      "Step 284980 (epoch   89.06), loss: 0.005434, time: 2.239 s\n",
      "Step 285000 (epoch   89.06), loss: 0.004372, time: 2.246 s, error: 0.009836\n",
      "Step 285020 (epoch   89.07), loss: 0.002983, time: 15.023 s\n",
      "Step 285040 (epoch   89.08), loss: 0.003044, time: 2.243 s\n",
      "Step 285060 (epoch   89.08), loss: 0.003835, time: 2.243 s\n",
      "Step 285080 (epoch   89.09), loss: 0.003477, time: 2.227 s\n",
      "Step 285100 (epoch   89.09), loss: 0.003989, time: 2.251 s\n",
      "Step 285120 (epoch   89.10), loss: 0.002514, time: 2.244 s\n",
      "Step 285140 (epoch   89.11), loss: 0.003356, time: 2.258 s\n",
      "Step 285160 (epoch   89.11), loss: 0.006181, time: 2.246 s\n",
      "Step 285180 (epoch   89.12), loss: 0.003217, time: 2.253 s\n",
      "Step 285200 (epoch   89.12), loss: 0.005086, time: 2.250 s, error: 0.009924\n",
      "Step 285220 (epoch   89.13), loss: 0.004476, time: 15.024 s\n",
      "Step 285240 (epoch   89.14), loss: 0.003140, time: 2.242 s\n",
      "Step 285260 (epoch   89.14), loss: 0.004195, time: 2.247 s\n",
      "Step 285280 (epoch   89.15), loss: 0.004052, time: 2.244 s\n",
      "Step 285300 (epoch   89.16), loss: 0.005188, time: 2.246 s\n",
      "Step 285320 (epoch   89.16), loss: 0.003355, time: 2.253 s\n",
      "Step 285340 (epoch   89.17), loss: 0.004424, time: 2.254 s\n",
      "Step 285360 (epoch   89.17), loss: 0.005432, time: 2.236 s\n",
      "Step 285380 (epoch   89.18), loss: 0.006132, time: 2.225 s\n",
      "Step 285400 (epoch   89.19), loss: 0.004675, time: 2.252 s, error: 0.009967\n",
      "Step 285420 (epoch   89.19), loss: 0.003105, time: 15.076 s\n",
      "Step 285440 (epoch   89.20), loss: 0.005527, time: 2.239 s\n",
      "Step 285460 (epoch   89.21), loss: 0.004031, time: 2.238 s\n",
      "Step 285480 (epoch   89.21), loss: 0.004976, time: 2.232 s\n",
      "Step 285500 (epoch   89.22), loss: 0.003956, time: 2.244 s\n",
      "Step 285520 (epoch   89.22), loss: 0.002586, time: 2.242 s\n",
      "Step 285540 (epoch   89.23), loss: 0.003261, time: 2.230 s\n",
      "Step 285560 (epoch   89.24), loss: 0.003625, time: 2.226 s\n",
      "Step 285580 (epoch   89.24), loss: 0.004080, time: 2.248 s\n",
      "Step 285600 (epoch   89.25), loss: 0.004239, time: 2.234 s, error: 0.009855\n",
      "Step 285620 (epoch   89.26), loss: 0.002508, time: 15.292 s\n",
      "Step 285640 (epoch   89.26), loss: 0.005244, time: 2.238 s\n",
      "Step 285660 (epoch   89.27), loss: 0.003760, time: 2.236 s\n",
      "Step 285680 (epoch   89.28), loss: 0.004243, time: 2.250 s\n",
      "Step 285700 (epoch   89.28), loss: 0.002883, time: 2.235 s\n",
      "Step 285720 (epoch   89.29), loss: 0.003635, time: 2.250 s\n",
      "Step 285740 (epoch   89.29), loss: 0.003767, time: 2.235 s\n",
      "Step 285760 (epoch   89.30), loss: 0.004256, time: 2.237 s\n",
      "Step 285780 (epoch   89.31), loss: 0.003667, time: 2.231 s\n",
      "Step 285800 (epoch   89.31), loss: 0.003907, time: 2.239 s, error: 0.009877\n",
      "Step 285820 (epoch   89.32), loss: 0.003319, time: 15.203 s\n",
      "Step 285840 (epoch   89.33), loss: 0.003058, time: 2.225 s\n",
      "Step 285860 (epoch   89.33), loss: 0.003678, time: 2.245 s\n",
      "Step 285880 (epoch   89.34), loss: 0.003484, time: 2.232 s\n",
      "Step 285900 (epoch   89.34), loss: 0.005093, time: 2.246 s\n",
      "Step 285920 (epoch   89.35), loss: 0.003914, time: 2.237 s\n",
      "Step 285940 (epoch   89.36), loss: 0.007974, time: 2.249 s\n",
      "Step 285960 (epoch   89.36), loss: 0.008916, time: 2.255 s\n",
      "Step 285980 (epoch   89.37), loss: 0.003129, time: 2.248 s\n",
      "Step 286000 (epoch   89.38), loss: 0.003187, time: 2.248 s, error: 0.009888\n",
      "\n",
      "Time since beginning  : 50235.339 s\n",
      "\n",
      "Step 286020 (epoch   89.38), loss: 0.003785, time: 15.151 s\n",
      "Step 286040 (epoch   89.39), loss: 0.012122, time: 2.241 s\n",
      "Step 286060 (epoch   89.39), loss: 0.003781, time: 2.259 s\n",
      "Step 286080 (epoch   89.40), loss: 0.004203, time: 2.254 s\n",
      "Step 286100 (epoch   89.41), loss: 0.006076, time: 2.254 s\n",
      "Step 286120 (epoch   89.41), loss: 0.005330, time: 2.247 s\n",
      "Step 286140 (epoch   89.42), loss: 0.003248, time: 2.223 s\n",
      "Step 286160 (epoch   89.42), loss: 0.005745, time: 2.239 s\n",
      "Step 286180 (epoch   89.43), loss: 0.002520, time: 2.246 s\n",
      "Step 286200 (epoch   89.44), loss: 0.007464, time: 2.241 s, error: 0.009901\n",
      "Step 286220 (epoch   89.44), loss: 0.005391, time: 15.118 s\n",
      "Step 286240 (epoch   89.45), loss: 0.003298, time: 2.255 s\n",
      "Step 286260 (epoch   89.46), loss: 0.003217, time: 2.256 s\n",
      "Step 286280 (epoch   89.46), loss: 0.004262, time: 2.245 s\n",
      "Step 286300 (epoch   89.47), loss: 0.006511, time: 2.248 s\n",
      "Step 286320 (epoch   89.47), loss: 0.003563, time: 2.257 s\n",
      "Step 286340 (epoch   89.48), loss: 0.004278, time: 2.243 s\n",
      "Step 286360 (epoch   89.49), loss: 0.002931, time: 2.244 s\n",
      "Step 286380 (epoch   89.49), loss: 0.005373, time: 2.236 s\n",
      "Step 286400 (epoch   89.50), loss: 0.003522, time: 2.236 s, error: 0.009873\n",
      "Step 286420 (epoch   89.51), loss: 0.004211, time: 15.027 s\n",
      "Step 286440 (epoch   89.51), loss: 0.004245, time: 2.246 s\n",
      "Step 286460 (epoch   89.52), loss: 0.004199, time: 2.247 s\n",
      "Step 286480 (epoch   89.53), loss: 0.004235, time: 2.247 s\n",
      "Step 286500 (epoch   89.53), loss: 0.003224, time: 2.245 s\n",
      "Step 286520 (epoch   89.54), loss: 0.003475, time: 2.230 s\n",
      "Step 286540 (epoch   89.54), loss: 0.004061, time: 2.246 s\n",
      "Step 286560 (epoch   89.55), loss: 0.002641, time: 2.255 s\n",
      "Step 286580 (epoch   89.56), loss: 0.005083, time: 2.262 s\n",
      "Step 286600 (epoch   89.56), loss: 0.004591, time: 2.243 s, error: 0.010180\n",
      "Step 286620 (epoch   89.57), loss: 0.003618, time: 15.047 s\n",
      "Step 286640 (epoch   89.58), loss: 0.004228, time: 2.240 s\n",
      "Step 286660 (epoch   89.58), loss: 0.003398, time: 2.235 s\n",
      "Step 286680 (epoch   89.59), loss: 0.003914, time: 2.250 s\n",
      "Step 286700 (epoch   89.59), loss: 0.003211, time: 2.244 s\n",
      "Step 286720 (epoch   89.60), loss: 0.003773, time: 2.254 s\n",
      "Step 286740 (epoch   89.61), loss: 0.002809, time: 2.251 s\n",
      "Step 286760 (epoch   89.61), loss: 0.004972, time: 2.237 s\n",
      "Step 286780 (epoch   89.62), loss: 0.003462, time: 2.251 s\n",
      "Step 286800 (epoch   89.62), loss: 0.005569, time: 2.234 s, error: 0.009924\n",
      "Step 286820 (epoch   89.63), loss: 0.002574, time: 15.298 s\n",
      "Step 286840 (epoch   89.64), loss: 0.002284, time: 2.254 s\n",
      "Step 286860 (epoch   89.64), loss: 0.003895, time: 2.246 s\n",
      "Step 286880 (epoch   89.65), loss: 0.003038, time: 2.255 s\n",
      "Step 286900 (epoch   89.66), loss: 0.003676, time: 2.240 s\n",
      "Step 286920 (epoch   89.66), loss: 0.003854, time: 2.249 s\n",
      "Step 286940 (epoch   89.67), loss: 0.007190, time: 2.233 s\n",
      "Step 286960 (epoch   89.67), loss: 0.003644, time: 2.247 s\n",
      "Step 286980 (epoch   89.68), loss: 0.003850, time: 2.246 s\n",
      "Step 287000 (epoch   89.69), loss: 0.003937, time: 2.246 s, error: 0.009940\n",
      "Step 287020 (epoch   89.69), loss: 0.003878, time: 15.214 s\n",
      "Step 287040 (epoch   89.70), loss: 0.004660, time: 2.236 s\n",
      "Step 287060 (epoch   89.71), loss: 0.002207, time: 2.242 s\n",
      "Step 287080 (epoch   89.71), loss: 0.002256, time: 2.231 s\n",
      "Step 287100 (epoch   89.72), loss: 0.005524, time: 2.241 s\n",
      "Step 287120 (epoch   89.72), loss: 0.002791, time: 2.254 s\n",
      "Step 287140 (epoch   89.73), loss: 0.003211, time: 2.243 s\n",
      "Step 287160 (epoch   89.74), loss: 0.003636, time: 2.247 s\n",
      "Step 287180 (epoch   89.74), loss: 0.002598, time: 2.238 s\n",
      "Step 287200 (epoch   89.75), loss: 0.004391, time: 2.248 s, error: 0.009759\n",
      "Step 287220 (epoch   89.76), loss: 0.003554, time: 15.025 s\n",
      "Step 287240 (epoch   89.76), loss: 0.003502, time: 2.258 s\n",
      "Step 287260 (epoch   89.77), loss: 0.002552, time: 2.253 s\n",
      "Step 287280 (epoch   89.78), loss: 0.002882, time: 2.250 s\n",
      "Step 287300 (epoch   89.78), loss: 0.004017, time: 2.242 s\n",
      "Step 287320 (epoch   89.79), loss: 0.003393, time: 2.241 s\n",
      "Step 287340 (epoch   89.79), loss: 0.004831, time: 2.224 s\n",
      "Step 287360 (epoch   89.80), loss: 0.004202, time: 2.253 s\n",
      "Step 287380 (epoch   89.81), loss: 0.004533, time: 2.244 s\n",
      "Step 287400 (epoch   89.81), loss: 0.002735, time: 2.245 s, error: 0.010386\n",
      "Step 287420 (epoch   89.82), loss: 0.003778, time: 15.115 s\n",
      "Step 287440 (epoch   89.83), loss: 0.002923, time: 2.237 s\n",
      "Step 287460 (epoch   89.83), loss: 0.003336, time: 2.235 s\n",
      "Step 287480 (epoch   89.84), loss: 0.002991, time: 2.250 s\n",
      "Step 287500 (epoch   89.84), loss: 0.003267, time: 2.255 s\n",
      "Step 287520 (epoch   89.85), loss: 0.005003, time: 2.241 s\n",
      "Step 287540 (epoch   89.86), loss: 0.004975, time: 2.237 s\n",
      "Step 287560 (epoch   89.86), loss: 0.004381, time: 2.236 s\n",
      "Step 287580 (epoch   89.87), loss: 0.002451, time: 2.235 s\n",
      "Step 287600 (epoch   89.88), loss: 0.003458, time: 2.228 s, error: 0.009750\n",
      "Step 287620 (epoch   89.88), loss: 0.005991, time: 15.040 s\n",
      "Step 287640 (epoch   89.89), loss: 0.002844, time: 2.249 s\n",
      "Step 287660 (epoch   89.89), loss: 0.003773, time: 2.234 s\n",
      "Step 287680 (epoch   89.90), loss: 0.005684, time: 2.253 s\n",
      "Step 287700 (epoch   89.91), loss: 0.004389, time: 2.236 s\n",
      "Step 287720 (epoch   89.91), loss: 0.004632, time: 2.251 s\n",
      "Step 287740 (epoch   89.92), loss: 0.003826, time: 2.254 s\n",
      "Step 287760 (epoch   89.92), loss: 0.006712, time: 2.258 s\n",
      "Step 287780 (epoch   89.93), loss: 0.005813, time: 2.251 s\n",
      "Step 287800 (epoch   89.94), loss: 0.005118, time: 2.231 s, error: 0.010038\n",
      "Step 287820 (epoch   89.94), loss: 0.004618, time: 15.026 s\n",
      "Step 287840 (epoch   89.95), loss: 0.004656, time: 2.244 s\n",
      "Step 287860 (epoch   89.96), loss: 0.003710, time: 2.250 s\n",
      "Step 287880 (epoch   89.96), loss: 0.004365, time: 2.255 s\n",
      "Step 287900 (epoch   89.97), loss: 0.002543, time: 2.245 s\n",
      "Step 287920 (epoch   89.97), loss: 0.011195, time: 2.247 s\n",
      "Step 287940 (epoch   89.98), loss: 0.008114, time: 2.248 s\n",
      "Step 287960 (epoch   89.99), loss: 0.003569, time: 2.246 s\n",
      "Step 287980 (epoch   89.99), loss: 0.003034, time: 2.240 s\n",
      "Step 288000 (epoch   90.00), loss: 0.003446, time: 2.253 s, error: 0.009817\n",
      "\n",
      "Time since beginning  : 50588.556 s\n",
      "\n",
      "Step 288020 (epoch   90.01), loss: 0.003554, time: 15.257 s\n",
      "Step 288040 (epoch   90.01), loss: 0.005780, time: 2.232 s\n",
      "Step 288060 (epoch   90.02), loss: 0.003928, time: 2.248 s\n",
      "Step 288080 (epoch   90.03), loss: 0.003495, time: 2.232 s\n",
      "Step 288100 (epoch   90.03), loss: 0.003890, time: 2.228 s\n",
      "Step 288120 (epoch   90.04), loss: 0.003273, time: 2.238 s\n",
      "Step 288140 (epoch   90.04), loss: 0.003969, time: 2.235 s\n",
      "Step 288160 (epoch   90.05), loss: 0.005091, time: 2.246 s\n",
      "Step 288180 (epoch   90.06), loss: 0.005756, time: 2.238 s\n",
      "Step 288200 (epoch   90.06), loss: 0.003689, time: 2.250 s, error: 0.009802\n",
      "Step 288220 (epoch   90.07), loss: 0.005886, time: 15.274 s\n",
      "Step 288240 (epoch   90.08), loss: 0.003480, time: 2.245 s\n",
      "Step 288260 (epoch   90.08), loss: 0.002712, time: 2.241 s\n",
      "Step 288280 (epoch   90.09), loss: 0.003020, time: 2.246 s\n",
      "Step 288300 (epoch   90.09), loss: 0.004830, time: 2.255 s\n",
      "Step 288320 (epoch   90.10), loss: 0.005721, time: 2.256 s\n",
      "Step 288340 (epoch   90.11), loss: 0.008762, time: 2.249 s\n",
      "Step 288360 (epoch   90.11), loss: 0.005387, time: 2.256 s\n",
      "Step 288380 (epoch   90.12), loss: 0.003231, time: 2.255 s\n",
      "Step 288400 (epoch   90.12), loss: 0.004809, time: 2.255 s, error: 0.009951\n",
      "Step 288420 (epoch   90.13), loss: 0.004006, time: 15.275 s\n",
      "Step 288440 (epoch   90.14), loss: 0.002362, time: 2.244 s\n",
      "Step 288460 (epoch   90.14), loss: 0.003964, time: 2.248 s\n",
      "Step 288480 (epoch   90.15), loss: 0.004959, time: 2.249 s\n",
      "Step 288500 (epoch   90.16), loss: 0.003755, time: 2.245 s\n",
      "Step 288520 (epoch   90.16), loss: 0.006899, time: 2.238 s\n",
      "Step 288540 (epoch   90.17), loss: 0.003333, time: 2.250 s\n",
      "Step 288560 (epoch   90.17), loss: 0.003551, time: 2.245 s\n",
      "Step 288580 (epoch   90.18), loss: 0.003512, time: 2.254 s\n",
      "Step 288600 (epoch   90.19), loss: 0.002632, time: 2.246 s, error: 0.009996\n",
      "Step 288620 (epoch   90.19), loss: 0.003697, time: 15.066 s\n",
      "Step 288640 (epoch   90.20), loss: 0.002639, time: 2.231 s\n",
      "Step 288660 (epoch   90.21), loss: 0.004751, time: 2.259 s\n",
      "Step 288680 (epoch   90.21), loss: 0.003335, time: 2.240 s\n",
      "Step 288700 (epoch   90.22), loss: 0.007335, time: 2.251 s\n",
      "Step 288720 (epoch   90.22), loss: 0.004182, time: 2.239 s\n",
      "Step 288740 (epoch   90.23), loss: 0.004315, time: 2.238 s\n",
      "Step 288760 (epoch   90.24), loss: 0.003662, time: 2.234 s\n",
      "Step 288780 (epoch   90.24), loss: 0.004330, time: 2.234 s\n",
      "Step 288800 (epoch   90.25), loss: 0.003245, time: 2.250 s, error: 0.009797\n",
      "Step 288820 (epoch   90.26), loss: 0.003612, time: 15.059 s\n",
      "Step 288840 (epoch   90.26), loss: 0.003532, time: 2.236 s\n",
      "Step 288860 (epoch   90.27), loss: 0.003029, time: 2.236 s\n",
      "Step 288880 (epoch   90.28), loss: 0.004527, time: 2.240 s\n",
      "Step 288900 (epoch   90.28), loss: 0.003656, time: 2.240 s\n",
      "Step 288920 (epoch   90.29), loss: 0.004443, time: 2.256 s\n",
      "Step 288940 (epoch   90.29), loss: 0.003385, time: 2.256 s\n",
      "Step 288960 (epoch   90.30), loss: 0.003153, time: 2.237 s\n",
      "Step 288980 (epoch   90.31), loss: 0.004366, time: 2.230 s\n",
      "Step 289000 (epoch   90.31), loss: 0.003222, time: 2.226 s, error: 0.009913\n",
      "Step 289020 (epoch   90.32), loss: 0.002562, time: 15.124 s\n",
      "Step 289040 (epoch   90.33), loss: 0.007394, time: 2.248 s\n",
      "Step 289060 (epoch   90.33), loss: 0.004051, time: 2.244 s\n",
      "Step 289080 (epoch   90.34), loss: 0.003662, time: 2.244 s\n",
      "Step 289100 (epoch   90.34), loss: 0.002605, time: 2.233 s\n",
      "Step 289120 (epoch   90.35), loss: 0.004911, time: 2.243 s\n",
      "Step 289140 (epoch   90.36), loss: 0.004321, time: 2.237 s\n",
      "Step 289160 (epoch   90.36), loss: 0.004148, time: 2.243 s\n",
      "Step 289180 (epoch   90.37), loss: 0.003801, time: 2.255 s\n",
      "Step 289200 (epoch   90.38), loss: 0.005282, time: 2.249 s, error: 0.009866\n",
      "Step 289220 (epoch   90.38), loss: 0.003270, time: 15.047 s\n",
      "Step 289240 (epoch   90.39), loss: 0.003152, time: 2.253 s\n",
      "Step 289260 (epoch   90.39), loss: 0.002852, time: 2.244 s\n",
      "Step 289280 (epoch   90.40), loss: 0.004515, time: 2.239 s\n",
      "Step 289300 (epoch   90.41), loss: 0.003797, time: 2.244 s\n",
      "Step 289320 (epoch   90.41), loss: 0.003819, time: 2.243 s\n",
      "Step 289340 (epoch   90.42), loss: 0.003209, time: 2.249 s\n",
      "Step 289360 (epoch   90.42), loss: 0.002896, time: 2.248 s\n",
      "Step 289380 (epoch   90.43), loss: 0.002096, time: 2.248 s\n",
      "Step 289400 (epoch   90.44), loss: 0.004662, time: 2.246 s, error: 0.009847\n",
      "Step 289420 (epoch   90.44), loss: 0.003836, time: 15.221 s\n",
      "Step 289440 (epoch   90.45), loss: 0.002255, time: 2.249 s\n",
      "Step 289460 (epoch   90.46), loss: 0.004148, time: 2.246 s\n",
      "Step 289480 (epoch   90.46), loss: 0.004091, time: 2.246 s\n",
      "Step 289500 (epoch   90.47), loss: 0.002961, time: 2.241 s\n",
      "Step 289520 (epoch   90.47), loss: 0.004669, time: 2.238 s\n",
      "Step 289540 (epoch   90.48), loss: 0.003438, time: 2.245 s\n",
      "Step 289560 (epoch   90.49), loss: 0.003807, time: 2.248 s\n",
      "Step 289580 (epoch   90.49), loss: 0.003243, time: 2.244 s\n",
      "Step 289600 (epoch   90.50), loss: 0.003008, time: 2.223 s, error: 0.009836\n",
      "Step 289620 (epoch   90.51), loss: 0.002322, time: 15.231 s\n",
      "Step 289640 (epoch   90.51), loss: 0.002868, time: 2.242 s\n",
      "Step 289660 (epoch   90.52), loss: 0.003768, time: 2.242 s\n",
      "Step 289680 (epoch   90.53), loss: 0.005357, time: 2.254 s\n",
      "Step 289700 (epoch   90.53), loss: 0.002164, time: 2.251 s\n",
      "Step 289720 (epoch   90.54), loss: 0.002907, time: 2.255 s\n",
      "Step 289740 (epoch   90.54), loss: 0.006845, time: 2.240 s\n",
      "Step 289760 (epoch   90.55), loss: 0.003385, time: 2.243 s\n",
      "Step 289780 (epoch   90.56), loss: 0.003551, time: 2.235 s\n",
      "Step 289800 (epoch   90.56), loss: 0.002710, time: 2.250 s, error: 0.010218\n",
      "Step 289820 (epoch   90.57), loss: 0.003153, time: 15.041 s\n",
      "Step 289840 (epoch   90.58), loss: 0.004550, time: 2.261 s\n",
      "Step 289860 (epoch   90.58), loss: 0.004315, time: 2.245 s\n",
      "Step 289880 (epoch   90.59), loss: 0.006078, time: 2.237 s\n",
      "Step 289900 (epoch   90.59), loss: 0.003707, time: 2.239 s\n",
      "Step 289920 (epoch   90.60), loss: 0.004687, time: 2.239 s\n",
      "Step 289940 (epoch   90.61), loss: 0.004087, time: 2.248 s\n",
      "Step 289960 (epoch   90.61), loss: 0.004609, time: 2.236 s\n",
      "Step 289980 (epoch   90.62), loss: 0.004732, time: 2.252 s\n",
      "Step 290000 (epoch   90.62), loss: 0.002697, time: 2.227 s, error: 0.009860\n",
      "\n",
      "Time since beginning  : 50942.050 s\n",
      "\n",
      "Step 290020 (epoch   90.63), loss: 0.005175, time: 15.215 s\n",
      "Step 290040 (epoch   90.64), loss: 0.003606, time: 2.232 s\n",
      "Step 290060 (epoch   90.64), loss: 0.005056, time: 2.238 s\n",
      "Step 290080 (epoch   90.65), loss: 0.003231, time: 2.251 s\n",
      "Step 290100 (epoch   90.66), loss: 0.005043, time: 2.262 s\n",
      "Step 290120 (epoch   90.66), loss: 0.003144, time: 2.241 s\n",
      "Step 290140 (epoch   90.67), loss: 0.002757, time: 2.240 s\n",
      "Step 290160 (epoch   90.67), loss: 0.004617, time: 2.246 s\n",
      "Step 290180 (epoch   90.68), loss: 0.003186, time: 2.230 s\n",
      "Step 290200 (epoch   90.69), loss: 0.005494, time: 2.245 s, error: 0.009889\n",
      "Step 290220 (epoch   90.69), loss: 0.003468, time: 15.068 s\n",
      "Step 290240 (epoch   90.70), loss: 0.005743, time: 2.245 s\n",
      "Step 290260 (epoch   90.71), loss: 0.004040, time: 2.245 s\n",
      "Step 290280 (epoch   90.71), loss: 0.002831, time: 2.239 s\n",
      "Step 290300 (epoch   90.72), loss: 0.002640, time: 2.234 s\n",
      "Step 290320 (epoch   90.72), loss: 0.003066, time: 2.243 s\n",
      "Step 290340 (epoch   90.73), loss: 0.004709, time: 2.253 s\n",
      "Step 290360 (epoch   90.74), loss: 0.002983, time: 2.259 s\n",
      "Step 290380 (epoch   90.74), loss: 0.002957, time: 2.246 s\n",
      "Step 290400 (epoch   90.75), loss: 0.003265, time: 2.237 s, error: 0.009765\n",
      "Step 290420 (epoch   90.76), loss: 0.001773, time: 15.038 s\n",
      "Step 290440 (epoch   90.76), loss: 0.006323, time: 2.249 s\n",
      "Step 290460 (epoch   90.77), loss: 0.003867, time: 2.235 s\n",
      "Step 290480 (epoch   90.78), loss: 0.003518, time: 2.235 s\n",
      "Step 290500 (epoch   90.78), loss: 0.006634, time: 2.233 s\n",
      "Step 290520 (epoch   90.79), loss: 0.003099, time: 2.233 s\n",
      "Step 290540 (epoch   90.79), loss: 0.002948, time: 2.227 s\n",
      "Step 290560 (epoch   90.80), loss: 0.003302, time: 2.247 s\n",
      "Step 290580 (epoch   90.81), loss: 0.003190, time: 2.238 s\n",
      "Step 290600 (epoch   90.81), loss: 0.015107, time: 2.236 s, error: 0.010129\n",
      "Step 290620 (epoch   90.82), loss: 0.004459, time: 15.281 s\n",
      "Step 290640 (epoch   90.83), loss: 0.004863, time: 2.235 s\n",
      "Step 290660 (epoch   90.83), loss: 0.004361, time: 2.257 s\n",
      "Step 290680 (epoch   90.84), loss: 0.003828, time: 2.255 s\n",
      "Step 290700 (epoch   90.84), loss: 0.003945, time: 2.257 s\n",
      "Step 290720 (epoch   90.85), loss: 0.005606, time: 2.248 s\n",
      "Step 290740 (epoch   90.86), loss: 0.004381, time: 2.241 s\n",
      "Step 290760 (epoch   90.86), loss: 0.005632, time: 2.241 s\n",
      "Step 290780 (epoch   90.87), loss: 0.004038, time: 2.239 s\n",
      "Step 290800 (epoch   90.88), loss: 0.002230, time: 2.242 s, error: 0.009730\n",
      "Step 290820 (epoch   90.88), loss: 0.004751, time: 15.338 s\n",
      "Step 290840 (epoch   90.89), loss: 0.003294, time: 2.250 s\n",
      "Step 290860 (epoch   90.89), loss: 0.003218, time: 2.248 s\n",
      "Step 290880 (epoch   90.90), loss: 0.003694, time: 2.249 s\n",
      "Step 290900 (epoch   90.91), loss: 0.003577, time: 2.224 s\n",
      "Step 290920 (epoch   90.91), loss: 0.004026, time: 2.242 s\n",
      "Step 290940 (epoch   90.92), loss: 0.003271, time: 2.247 s\n",
      "Step 290960 (epoch   90.92), loss: 0.002799, time: 2.229 s\n",
      "Step 290980 (epoch   90.93), loss: 0.002882, time: 2.228 s\n",
      "Step 291000 (epoch   90.94), loss: 0.002829, time: 2.230 s, error: 0.009961\n",
      "Step 291020 (epoch   90.94), loss: 0.010420, time: 15.175 s\n",
      "Step 291040 (epoch   90.95), loss: 0.009824, time: 2.250 s\n",
      "Step 291060 (epoch   90.96), loss: 0.002787, time: 2.239 s\n",
      "Step 291080 (epoch   90.96), loss: 0.003509, time: 2.253 s\n",
      "Step 291100 (epoch   90.97), loss: 0.003602, time: 2.255 s\n",
      "Step 291120 (epoch   90.97), loss: 0.006001, time: 2.243 s\n",
      "Step 291140 (epoch   90.98), loss: 0.004023, time: 2.245 s\n",
      "Step 291160 (epoch   90.99), loss: 0.003468, time: 2.242 s\n",
      "Step 291180 (epoch   90.99), loss: 0.002523, time: 2.240 s\n",
      "Step 291200 (epoch   91.00), loss: 0.003030, time: 2.252 s, error: 0.009836\n",
      "Step 291220 (epoch   91.01), loss: 0.003475, time: 15.047 s\n",
      "Step 291240 (epoch   91.01), loss: 0.005099, time: 2.241 s\n",
      "Step 291260 (epoch   91.02), loss: 0.003630, time: 2.251 s\n",
      "Step 291280 (epoch   91.03), loss: 0.003714, time: 2.245 s\n",
      "Step 291300 (epoch   91.03), loss: 0.005443, time: 2.238 s\n",
      "Step 291320 (epoch   91.04), loss: 0.004656, time: 2.234 s\n",
      "Step 291340 (epoch   91.04), loss: 0.005169, time: 2.236 s\n",
      "Step 291360 (epoch   91.05), loss: 0.003626, time: 2.254 s\n",
      "Step 291380 (epoch   91.06), loss: 0.003170, time: 2.249 s\n",
      "Step 291400 (epoch   91.06), loss: 0.002627, time: 2.246 s, error: 0.009882\n",
      "Step 291420 (epoch   91.07), loss: 0.003075, time: 15.097 s\n",
      "Step 291440 (epoch   91.08), loss: 0.009470, time: 2.233 s\n",
      "Step 291460 (epoch   91.08), loss: 0.005212, time: 2.238 s\n",
      "Step 291480 (epoch   91.09), loss: 0.003536, time: 2.238 s\n",
      "Step 291500 (epoch   91.09), loss: 0.005022, time: 2.247 s\n",
      "Step 291520 (epoch   91.10), loss: 0.002727, time: 2.246 s\n",
      "Step 291540 (epoch   91.11), loss: 0.003328, time: 2.260 s\n",
      "Step 291560 (epoch   91.11), loss: 0.003855, time: 2.248 s\n",
      "Step 291580 (epoch   91.12), loss: 0.003106, time: 2.254 s\n",
      "Step 291600 (epoch   91.12), loss: 0.004221, time: 2.248 s, error: 0.010037\n",
      "Step 291620 (epoch   91.13), loss: 0.002965, time: 15.074 s\n",
      "Step 291640 (epoch   91.14), loss: 0.003185, time: 2.246 s\n",
      "Step 291660 (epoch   91.14), loss: 0.004206, time: 2.242 s\n",
      "Step 291680 (epoch   91.15), loss: 0.006529, time: 2.248 s\n",
      "Step 291700 (epoch   91.16), loss: 0.003921, time: 2.242 s\n",
      "Step 291720 (epoch   91.16), loss: 0.002819, time: 2.245 s\n",
      "Step 291740 (epoch   91.17), loss: 0.008176, time: 2.254 s\n",
      "Step 291760 (epoch   91.17), loss: 0.003472, time: 2.254 s\n",
      "Step 291780 (epoch   91.18), loss: 0.005934, time: 2.256 s\n",
      "Step 291800 (epoch   91.19), loss: 0.003251, time: 2.256 s, error: 0.010063\n",
      "Step 291820 (epoch   91.19), loss: 0.003689, time: 14.997 s\n",
      "Step 291840 (epoch   91.20), loss: 0.006746, time: 2.237 s\n",
      "Step 291860 (epoch   91.21), loss: 0.003852, time: 2.243 s\n",
      "Step 291880 (epoch   91.21), loss: 0.006653, time: 2.234 s\n",
      "Step 291900 (epoch   91.22), loss: 0.003964, time: 2.239 s\n",
      "Step 291920 (epoch   91.22), loss: 0.005102, time: 2.250 s\n",
      "Step 291940 (epoch   91.23), loss: 0.003904, time: 2.227 s\n",
      "Step 291960 (epoch   91.24), loss: 0.003699, time: 2.247 s\n",
      "Step 291980 (epoch   91.24), loss: 0.004402, time: 2.235 s\n",
      "Step 292000 (epoch   91.25), loss: 0.002174, time: 2.252 s, error: 0.009829\n",
      "\n",
      "Time since beginning  : 51295.453 s\n",
      "\n",
      "Step 292020 (epoch   91.26), loss: 0.002889, time: 15.389 s\n",
      "Step 292040 (epoch   91.26), loss: 0.003572, time: 2.255 s\n",
      "Step 292060 (epoch   91.27), loss: 0.003900, time: 2.245 s\n",
      "Step 292080 (epoch   91.28), loss: 0.004862, time: 2.235 s\n",
      "Step 292100 (epoch   91.28), loss: 0.005590, time: 2.251 s\n",
      "Step 292120 (epoch   91.29), loss: 0.002981, time: 2.226 s\n",
      "Step 292140 (epoch   91.29), loss: 0.002999, time: 2.245 s\n",
      "Step 292160 (epoch   91.30), loss: 0.002430, time: 2.238 s\n",
      "Step 292180 (epoch   91.31), loss: 0.003026, time: 2.245 s\n",
      "Step 292200 (epoch   91.31), loss: 0.004458, time: 2.234 s, error: 0.009996\n",
      "Step 292220 (epoch   91.32), loss: 0.004045, time: 15.230 s\n",
      "Step 292240 (epoch   91.33), loss: 0.003369, time: 2.249 s\n",
      "Step 292260 (epoch   91.33), loss: 0.002636, time: 2.244 s\n",
      "Step 292280 (epoch   91.34), loss: 0.004218, time: 2.237 s\n",
      "Step 292300 (epoch   91.34), loss: 0.005200, time: 2.234 s\n",
      "Step 292320 (epoch   91.35), loss: 0.002690, time: 2.250 s\n",
      "Step 292340 (epoch   91.36), loss: 0.005063, time: 2.237 s\n",
      "Step 292360 (epoch   91.36), loss: 0.005011, time: 2.233 s\n",
      "Step 292380 (epoch   91.37), loss: 0.005122, time: 2.243 s\n",
      "Step 292400 (epoch   91.38), loss: 0.003777, time: 2.249 s, error: 0.009839\n",
      "Step 292420 (epoch   91.38), loss: 0.003824, time: 15.029 s\n",
      "Step 292440 (epoch   91.39), loss: 0.004733, time: 2.254 s\n",
      "Step 292460 (epoch   91.39), loss: 0.003831, time: 2.254 s\n",
      "Step 292480 (epoch   91.40), loss: 0.005371, time: 2.222 s\n",
      "Step 292500 (epoch   91.41), loss: 0.005060, time: 2.251 s\n",
      "Step 292520 (epoch   91.41), loss: 0.005508, time: 2.249 s\n",
      "Step 292540 (epoch   91.42), loss: 0.002925, time: 2.249 s\n",
      "Step 292560 (epoch   91.42), loss: 0.006466, time: 2.232 s\n",
      "Step 292580 (epoch   91.43), loss: 0.004518, time: 2.239 s\n",
      "Step 292600 (epoch   91.44), loss: 0.007995, time: 2.237 s, error: 0.009913\n",
      "Step 292620 (epoch   91.44), loss: 0.005300, time: 15.074 s\n",
      "Step 292640 (epoch   91.45), loss: 0.004339, time: 2.240 s\n",
      "Step 292660 (epoch   91.46), loss: 0.002598, time: 2.251 s\n",
      "Step 292680 (epoch   91.46), loss: 0.003740, time: 2.249 s\n",
      "Step 292700 (epoch   91.47), loss: 0.006561, time: 2.254 s\n",
      "Step 292720 (epoch   91.47), loss: 0.004245, time: 2.253 s\n",
      "Step 292740 (epoch   91.48), loss: 0.004904, time: 2.249 s\n",
      "Step 292760 (epoch   91.49), loss: 0.003476, time: 2.239 s\n",
      "Step 292780 (epoch   91.49), loss: 0.005361, time: 2.241 s\n",
      "Step 292800 (epoch   91.50), loss: 0.004108, time: 2.252 s, error: 0.009810\n",
      "Step 292820 (epoch   91.51), loss: 0.002975, time: 15.081 s\n",
      "Step 292840 (epoch   91.51), loss: 0.003390, time: 2.241 s\n",
      "Step 292860 (epoch   91.52), loss: 0.003227, time: 2.236 s\n",
      "Step 292880 (epoch   91.53), loss: 0.002888, time: 2.237 s\n",
      "Step 292900 (epoch   91.53), loss: 0.002840, time: 2.239 s\n",
      "Step 292920 (epoch   91.54), loss: 0.003153, time: 2.237 s\n",
      "Step 292940 (epoch   91.54), loss: 0.005663, time: 2.244 s\n",
      "Step 292960 (epoch   91.55), loss: 0.002811, time: 2.245 s\n",
      "Step 292980 (epoch   91.56), loss: 0.002865, time: 2.255 s\n",
      "Step 293000 (epoch   91.56), loss: 0.003831, time: 2.240 s, error: 0.010176\n",
      "Step 293020 (epoch   91.57), loss: 0.005903, time: 15.044 s\n",
      "Step 293040 (epoch   91.58), loss: 0.004001, time: 2.259 s\n",
      "Step 293060 (epoch   91.58), loss: 0.002159, time: 2.257 s\n",
      "Step 293080 (epoch   91.59), loss: 0.002677, time: 2.246 s\n",
      "Step 293100 (epoch   91.59), loss: 0.004311, time: 2.248 s\n",
      "Step 293120 (epoch   91.60), loss: 0.003339, time: 2.253 s\n",
      "Step 293140 (epoch   91.61), loss: 0.004589, time: 2.252 s\n",
      "Step 293160 (epoch   91.61), loss: 0.012307, time: 2.249 s\n",
      "Step 293180 (epoch   91.62), loss: 0.004633, time: 2.251 s\n",
      "Step 293200 (epoch   91.62), loss: 0.005345, time: 2.241 s, error: 0.009856\n",
      "Step 293220 (epoch   91.63), loss: 0.003264, time: 15.222 s\n",
      "Step 293240 (epoch   91.64), loss: 0.003424, time: 2.242 s\n",
      "Step 293260 (epoch   91.64), loss: 0.005505, time: 2.224 s\n",
      "Step 293280 (epoch   91.65), loss: 0.003326, time: 2.247 s\n",
      "Step 293300 (epoch   91.66), loss: 0.002386, time: 2.237 s\n",
      "Step 293320 (epoch   91.66), loss: 0.003260, time: 2.240 s\n",
      "Step 293340 (epoch   91.67), loss: 0.005894, time: 2.237 s\n",
      "Step 293360 (epoch   91.67), loss: 0.002693, time: 2.248 s\n",
      "Step 293380 (epoch   91.68), loss: 0.003568, time: 2.241 s\n",
      "Step 293400 (epoch   91.69), loss: 0.002709, time: 2.241 s, error: 0.009847\n",
      "Step 293420 (epoch   91.69), loss: 0.006542, time: 15.299 s\n",
      "Step 293440 (epoch   91.70), loss: 0.005434, time: 2.238 s\n",
      "Step 293460 (epoch   91.71), loss: 0.004104, time: 2.248 s\n",
      "Step 293480 (epoch   91.71), loss: 0.003255, time: 2.228 s\n",
      "Step 293500 (epoch   91.72), loss: 0.003545, time: 2.246 s\n",
      "Step 293520 (epoch   91.72), loss: 0.003684, time: 2.252 s\n",
      "Step 293540 (epoch   91.73), loss: 0.003100, time: 2.250 s\n",
      "Step 293560 (epoch   91.74), loss: 0.002565, time: 2.245 s\n",
      "Step 293580 (epoch   91.74), loss: 0.004763, time: 2.244 s\n",
      "Step 293600 (epoch   91.75), loss: 0.003878, time: 2.241 s, error: 0.009831\n",
      "Step 293620 (epoch   91.76), loss: 0.003059, time: 15.107 s\n",
      "Step 293640 (epoch   91.76), loss: 0.002571, time: 2.245 s\n",
      "Step 293660 (epoch   91.77), loss: 0.006070, time: 2.237 s\n",
      "Step 293680 (epoch   91.78), loss: 0.003182, time: 2.254 s\n",
      "Step 293700 (epoch   91.78), loss: 0.003189, time: 2.249 s\n",
      "Step 293720 (epoch   91.79), loss: 0.003774, time: 2.251 s\n",
      "Step 293740 (epoch   91.79), loss: 0.002912, time: 2.229 s\n",
      "Step 293760 (epoch   91.80), loss: 0.002447, time: 2.250 s\n",
      "Step 293780 (epoch   91.81), loss: 0.004163, time: 2.258 s\n",
      "Step 293800 (epoch   91.81), loss: 0.002745, time: 2.244 s, error: 0.009880\n",
      "Step 293820 (epoch   91.82), loss: 0.002111, time: 15.088 s\n",
      "Step 293840 (epoch   91.83), loss: 0.003701, time: 2.240 s\n",
      "Step 293860 (epoch   91.83), loss: 0.002641, time: 2.252 s\n",
      "Step 293880 (epoch   91.84), loss: 0.003820, time: 2.260 s\n",
      "Step 293900 (epoch   91.84), loss: 0.002715, time: 2.245 s\n",
      "Step 293920 (epoch   91.85), loss: 0.004193, time: 2.225 s\n",
      "Step 293940 (epoch   91.86), loss: 0.003265, time: 2.230 s\n",
      "Step 293960 (epoch   91.86), loss: 0.002877, time: 2.245 s\n",
      "Step 293980 (epoch   91.87), loss: 0.002698, time: 2.245 s\n",
      "Step 294000 (epoch   91.88), loss: 0.003061, time: 2.244 s, error: 0.009737\n",
      "\n",
      "Time since beginning  : 51648.692 s\n",
      "\n",
      "Step 294020 (epoch   91.88), loss: 0.003295, time: 15.116 s\n",
      "Step 294040 (epoch   91.89), loss: 0.005822, time: 2.250 s\n",
      "Step 294060 (epoch   91.89), loss: 0.004874, time: 2.243 s\n",
      "Step 294080 (epoch   91.90), loss: 0.003930, time: 2.247 s\n",
      "Step 294100 (epoch   91.91), loss: 0.004043, time: 2.241 s\n",
      "Step 294120 (epoch   91.91), loss: 0.002154, time: 2.230 s\n",
      "Step 294140 (epoch   91.92), loss: 0.003784, time: 2.260 s\n",
      "Step 294160 (epoch   91.92), loss: 0.006298, time: 2.250 s\n",
      "Step 294180 (epoch   91.93), loss: 0.004186, time: 2.252 s\n",
      "Step 294200 (epoch   91.94), loss: 0.002540, time: 2.252 s, error: 0.009899\n",
      "Step 294220 (epoch   91.94), loss: 0.003999, time: 15.007 s\n",
      "Step 294240 (epoch   91.95), loss: 0.005943, time: 2.234 s\n",
      "Step 294260 (epoch   91.96), loss: 0.012484, time: 2.238 s\n",
      "Step 294280 (epoch   91.96), loss: 0.003422, time: 2.235 s\n",
      "Step 294300 (epoch   91.97), loss: 0.003184, time: 2.244 s\n",
      "Step 294320 (epoch   91.97), loss: 0.002705, time: 2.236 s\n",
      "Step 294340 (epoch   91.98), loss: 0.002846, time: 2.253 s\n",
      "Step 294360 (epoch   91.99), loss: 0.003265, time: 2.245 s\n",
      "Step 294380 (epoch   91.99), loss: 0.002458, time: 2.249 s\n",
      "Step 294400 (epoch   92.00), loss: 0.004088, time: 2.249 s, error: 0.009862\n",
      "Step 294420 (epoch   92.01), loss: 0.005530, time: 15.052 s\n",
      "Step 294440 (epoch   92.01), loss: 0.002687, time: 2.236 s\n",
      "Step 294460 (epoch   92.02), loss: 0.003649, time: 2.232 s\n",
      "Step 294480 (epoch   92.03), loss: 0.002220, time: 2.244 s\n",
      "Step 294500 (epoch   92.03), loss: 0.003412, time: 2.243 s\n",
      "Step 294520 (epoch   92.04), loss: 0.004944, time: 2.243 s\n",
      "Step 294540 (epoch   92.04), loss: 0.003716, time: 2.239 s\n",
      "Step 294560 (epoch   92.05), loss: 0.003965, time: 2.251 s\n",
      "Step 294580 (epoch   92.06), loss: 0.003729, time: 2.235 s\n",
      "Step 294600 (epoch   92.06), loss: 0.003007, time: 2.254 s, error: 0.009951\n",
      "Step 294620 (epoch   92.07), loss: 0.002026, time: 15.197 s\n",
      "Step 294640 (epoch   92.08), loss: 0.002953, time: 2.237 s\n",
      "Step 294660 (epoch   92.08), loss: 0.004734, time: 2.240 s\n",
      "Step 294680 (epoch   92.09), loss: 0.003719, time: 2.241 s\n",
      "Step 294700 (epoch   92.09), loss: 0.006994, time: 2.236 s\n",
      "Step 294720 (epoch   92.10), loss: 0.003992, time: 2.253 s\n",
      "Step 294740 (epoch   92.11), loss: 0.005785, time: 2.248 s\n",
      "Step 294760 (epoch   92.11), loss: 0.006545, time: 2.248 s\n",
      "Step 294780 (epoch   92.12), loss: 0.003318, time: 2.227 s\n",
      "Step 294800 (epoch   92.12), loss: 0.006047, time: 2.253 s, error: 0.009996\n",
      "Step 294820 (epoch   92.13), loss: 0.004988, time: 15.231 s\n",
      "Step 294840 (epoch   92.14), loss: 0.003807, time: 2.240 s\n",
      "Step 294860 (epoch   92.14), loss: 0.003532, time: 2.241 s\n",
      "Step 294880 (epoch   92.15), loss: 0.005963, time: 2.259 s\n",
      "Step 294900 (epoch   92.16), loss: 0.003293, time: 2.258 s\n",
      "Step 294920 (epoch   92.16), loss: 0.004348, time: 2.254 s\n",
      "Step 294940 (epoch   92.17), loss: 0.004144, time: 2.239 s\n",
      "Step 294960 (epoch   92.17), loss: 0.002326, time: 2.251 s\n",
      "Step 294980 (epoch   92.18), loss: 0.004335, time: 2.235 s\n",
      "Step 295000 (epoch   92.19), loss: 0.004203, time: 2.247 s, error: 0.010116\n",
      "Step 295020 (epoch   92.19), loss: 0.007163, time: 15.086 s\n",
      "Step 295040 (epoch   92.20), loss: 0.002771, time: 2.244 s\n",
      "Step 295060 (epoch   92.21), loss: 0.002877, time: 2.257 s\n",
      "Step 295080 (epoch   92.21), loss: 0.005461, time: 2.232 s\n",
      "Step 295100 (epoch   92.22), loss: 0.005779, time: 2.251 s\n",
      "Step 295120 (epoch   92.22), loss: 0.003294, time: 2.249 s\n",
      "Step 295140 (epoch   92.23), loss: 0.003557, time: 2.241 s\n",
      "Step 295160 (epoch   92.24), loss: 0.004678, time: 2.242 s\n",
      "Step 295180 (epoch   92.24), loss: 0.003283, time: 2.248 s\n",
      "Step 295200 (epoch   92.25), loss: 0.004834, time: 2.226 s, error: 0.009869\n",
      "Step 295220 (epoch   92.26), loss: 0.003021, time: 15.030 s\n",
      "Step 295240 (epoch   92.26), loss: 0.002848, time: 2.250 s\n",
      "Step 295260 (epoch   92.27), loss: 0.005858, time: 2.249 s\n",
      "Step 295280 (epoch   92.28), loss: 0.004026, time: 2.255 s\n",
      "Step 295300 (epoch   92.28), loss: 0.002745, time: 2.256 s\n",
      "Step 295320 (epoch   92.29), loss: 0.003606, time: 2.249 s\n",
      "Step 295340 (epoch   92.29), loss: 0.003839, time: 2.253 s\n",
      "Step 295360 (epoch   92.30), loss: 0.002996, time: 2.243 s\n",
      "Step 295380 (epoch   92.31), loss: 0.005457, time: 2.254 s\n",
      "Step 295400 (epoch   92.31), loss: 0.003434, time: 2.249 s, error: 0.010039\n",
      "Step 295420 (epoch   92.32), loss: 0.004738, time: 15.114 s\n",
      "Step 295440 (epoch   92.33), loss: 0.004063, time: 2.244 s\n",
      "Step 295460 (epoch   92.33), loss: 0.003139, time: 2.238 s\n",
      "Step 295480 (epoch   92.34), loss: 0.002878, time: 2.245 s\n",
      "Step 295500 (epoch   92.34), loss: 0.003236, time: 2.237 s\n",
      "Step 295520 (epoch   92.35), loss: 0.003429, time: 2.249 s\n",
      "Step 295540 (epoch   92.36), loss: 0.007190, time: 2.242 s\n",
      "Step 295560 (epoch   92.36), loss: 0.003598, time: 2.248 s\n",
      "Step 295580 (epoch   92.37), loss: 0.004436, time: 2.256 s\n",
      "Step 295600 (epoch   92.38), loss: 0.002731, time: 2.244 s, error: 0.009804\n",
      "Step 295620 (epoch   92.38), loss: 0.003834, time: 15.015 s\n",
      "Step 295640 (epoch   92.39), loss: 0.004401, time: 2.231 s\n",
      "Step 295660 (epoch   92.39), loss: 0.003268, time: 2.236 s\n",
      "Step 295680 (epoch   92.40), loss: 0.004506, time: 2.243 s\n",
      "Step 295700 (epoch   92.41), loss: 0.002916, time: 2.243 s\n",
      "Step 295720 (epoch   92.41), loss: 0.006450, time: 2.235 s\n",
      "Step 295740 (epoch   92.42), loss: 0.004122, time: 2.239 s\n",
      "Step 295760 (epoch   92.42), loss: 0.005308, time: 2.256 s\n",
      "Step 295780 (epoch   92.43), loss: 0.004269, time: 2.252 s\n",
      "Step 295800 (epoch   92.44), loss: 0.002792, time: 2.248 s, error: 0.010062\n",
      "Step 295820 (epoch   92.44), loss: 0.002678, time: 15.227 s\n",
      "Step 295840 (epoch   92.45), loss: 0.004332, time: 2.245 s\n",
      "Step 295860 (epoch   92.46), loss: 0.007004, time: 2.237 s\n",
      "Step 295880 (epoch   92.46), loss: 0.002397, time: 2.241 s\n",
      "Step 295900 (epoch   92.47), loss: 0.003849, time: 2.250 s\n",
      "Step 295920 (epoch   92.47), loss: 0.005467, time: 2.247 s\n",
      "Step 295940 (epoch   92.48), loss: 0.003145, time: 2.241 s\n",
      "Step 295960 (epoch   92.49), loss: 0.003368, time: 2.242 s\n",
      "Step 295980 (epoch   92.49), loss: 0.004156, time: 2.225 s\n",
      "Step 296000 (epoch   92.50), loss: 0.003493, time: 2.240 s, error: 0.009769\n",
      "\n",
      "Time since beginning  : 52001.961 s\n",
      "\n",
      "Step 296020 (epoch   92.51), loss: 0.004575, time: 15.297 s\n",
      "Step 296040 (epoch   92.51), loss: 0.004380, time: 2.236 s\n",
      "Step 296060 (epoch   92.52), loss: 0.002724, time: 2.241 s\n",
      "Step 296080 (epoch   92.53), loss: 0.003842, time: 2.238 s\n",
      "Step 296100 (epoch   92.53), loss: 0.004287, time: 2.232 s\n",
      "Step 296120 (epoch   92.54), loss: 0.002919, time: 2.232 s\n",
      "Step 296140 (epoch   92.54), loss: 0.004082, time: 2.256 s\n",
      "Step 296160 (epoch   92.55), loss: 0.002339, time: 2.241 s\n",
      "Step 296180 (epoch   92.56), loss: 0.004546, time: 2.246 s\n",
      "Step 296200 (epoch   92.56), loss: 0.003955, time: 2.250 s, error: 0.010178\n",
      "Step 296220 (epoch   92.57), loss: 0.002894, time: 15.037 s\n",
      "Step 296240 (epoch   92.58), loss: 0.004049, time: 2.251 s\n",
      "Step 296260 (epoch   92.58), loss: 0.004016, time: 2.247 s\n",
      "Step 296280 (epoch   92.59), loss: 0.005307, time: 2.238 s\n",
      "Step 296300 (epoch   92.59), loss: 0.002214, time: 2.246 s\n",
      "Step 296320 (epoch   92.60), loss: 0.005343, time: 2.251 s\n",
      "Step 296340 (epoch   92.61), loss: 0.004135, time: 2.240 s\n",
      "Step 296360 (epoch   92.61), loss: 0.004799, time: 2.228 s\n",
      "Step 296380 (epoch   92.62), loss: 0.004239, time: 2.245 s\n",
      "Step 296400 (epoch   92.62), loss: 0.002433, time: 2.242 s, error: 0.009861\n",
      "Step 296420 (epoch   92.63), loss: 0.003494, time: 15.064 s\n",
      "Step 296440 (epoch   92.64), loss: 0.003802, time: 2.253 s\n",
      "Step 296460 (epoch   92.64), loss: 0.003236, time: 2.232 s\n",
      "Step 296480 (epoch   92.65), loss: 0.005270, time: 2.252 s\n",
      "Step 296500 (epoch   92.66), loss: 0.002756, time: 2.252 s\n",
      "Step 296520 (epoch   92.66), loss: 0.003981, time: 2.245 s\n",
      "Step 296540 (epoch   92.67), loss: 0.003901, time: 2.253 s\n",
      "Step 296560 (epoch   92.67), loss: 0.003468, time: 2.249 s\n",
      "Step 296580 (epoch   92.68), loss: 0.004555, time: 2.252 s\n",
      "Step 296600 (epoch   92.69), loss: 0.002100, time: 2.237 s, error: 0.009863\n",
      "Step 296620 (epoch   92.69), loss: 0.005760, time: 15.034 s\n",
      "Step 296640 (epoch   92.70), loss: 0.005319, time: 2.241 s\n",
      "Step 296660 (epoch   92.71), loss: 0.005622, time: 2.237 s\n",
      "Step 296680 (epoch   92.71), loss: 0.005345, time: 2.237 s\n",
      "Step 296700 (epoch   92.72), loss: 0.003741, time: 2.224 s\n",
      "Step 296720 (epoch   92.72), loss: 0.003433, time: 2.244 s\n",
      "Step 296740 (epoch   92.73), loss: 0.003914, time: 2.257 s\n",
      "Step 296760 (epoch   92.74), loss: 0.002399, time: 2.257 s\n",
      "Step 296780 (epoch   92.74), loss: 0.003379, time: 2.234 s\n",
      "Step 296800 (epoch   92.75), loss: 0.002476, time: 2.247 s, error: 0.009906\n",
      "Step 296820 (epoch   92.76), loss: 0.004583, time: 15.065 s\n",
      "Step 296840 (epoch   92.76), loss: 0.004416, time: 2.242 s\n",
      "Step 296860 (epoch   92.77), loss: 0.002574, time: 2.249 s\n",
      "Step 296880 (epoch   92.78), loss: 0.004476, time: 2.236 s\n",
      "Step 296900 (epoch   92.78), loss: 0.003442, time: 2.247 s\n",
      "Step 296920 (epoch   92.79), loss: 0.003373, time: 2.241 s\n",
      "Step 296940 (epoch   92.79), loss: 0.003694, time: 2.239 s\n",
      "Step 296960 (epoch   92.80), loss: 0.003776, time: 2.237 s\n",
      "Step 296980 (epoch   92.81), loss: 0.004777, time: 2.252 s\n",
      "Step 297000 (epoch   92.81), loss: 0.002411, time: 2.258 s, error: 0.009732\n",
      "Step 297020 (epoch   92.82), loss: 0.003921, time: 15.091 s\n",
      "Step 297040 (epoch   92.83), loss: 0.004017, time: 2.246 s\n",
      "Step 297060 (epoch   92.83), loss: 0.002990, time: 2.239 s\n",
      "Step 297080 (epoch   92.84), loss: 0.004384, time: 2.242 s\n",
      "Step 297100 (epoch   92.84), loss: 0.003798, time: 2.234 s\n",
      "Step 297120 (epoch   92.85), loss: 0.003867, time: 2.245 s\n",
      "Step 297140 (epoch   92.86), loss: 0.002325, time: 2.250 s\n",
      "Step 297160 (epoch   92.86), loss: 0.004875, time: 2.246 s\n",
      "Step 297180 (epoch   92.87), loss: 0.002582, time: 2.250 s\n",
      "Step 297200 (epoch   92.88), loss: 0.005668, time: 2.246 s, error: 0.009767\n",
      "Step 297220 (epoch   92.88), loss: 0.003434, time: 15.256 s\n",
      "Step 297240 (epoch   92.89), loss: 0.003524, time: 2.231 s\n",
      "Step 297260 (epoch   92.89), loss: 0.005115, time: 2.245 s\n",
      "Step 297280 (epoch   92.90), loss: 0.002457, time: 2.243 s\n",
      "Step 297300 (epoch   92.91), loss: 0.003330, time: 2.244 s\n",
      "Step 297320 (epoch   92.91), loss: 0.003409, time: 2.242 s\n",
      "Step 297340 (epoch   92.92), loss: 0.002176, time: 2.241 s\n",
      "Step 297360 (epoch   92.92), loss: 0.004076, time: 2.251 s\n",
      "Step 297380 (epoch   92.93), loss: 0.002734, time: 2.253 s\n",
      "Step 297400 (epoch   92.94), loss: 0.007155, time: 2.252 s, error: 0.009894\n",
      "Step 297420 (epoch   92.94), loss: 0.003696, time: 15.171 s\n",
      "Step 297440 (epoch   92.95), loss: 0.004126, time: 2.238 s\n",
      "Step 297460 (epoch   92.96), loss: 0.003191, time: 2.257 s\n",
      "Step 297480 (epoch   92.96), loss: 0.003351, time: 2.251 s\n",
      "Step 297500 (epoch   92.97), loss: 0.003954, time: 2.242 s\n",
      "Step 297520 (epoch   92.97), loss: 0.003882, time: 2.230 s\n",
      "Step 297540 (epoch   92.98), loss: 0.004075, time: 2.237 s\n",
      "Step 297560 (epoch   92.99), loss: 0.003046, time: 2.233 s\n",
      "Step 297580 (epoch   92.99), loss: 0.003458, time: 2.244 s\n",
      "Step 297600 (epoch   93.00), loss: 0.003395, time: 2.236 s, error: 0.009912\n",
      "Step 297620 (epoch   93.01), loss: 0.002558, time: 15.044 s\n",
      "Step 297640 (epoch   93.01), loss: 0.006034, time: 2.242 s\n",
      "Step 297660 (epoch   93.02), loss: 0.004571, time: 2.258 s\n",
      "Step 297680 (epoch   93.03), loss: 0.003602, time: 2.252 s\n",
      "Step 297700 (epoch   93.03), loss: 0.006996, time: 2.231 s\n",
      "Step 297720 (epoch   93.04), loss: 0.003455, time: 2.244 s\n",
      "Step 297740 (epoch   93.04), loss: 0.003732, time: 2.239 s\n",
      "Step 297760 (epoch   93.05), loss: 0.001795, time: 2.232 s\n",
      "Step 297780 (epoch   93.06), loss: 0.002344, time: 2.248 s\n",
      "Step 297800 (epoch   93.06), loss: 0.002775, time: 2.246 s, error: 0.009966\n",
      "Step 297820 (epoch   93.07), loss: 0.005719, time: 15.084 s\n",
      "Step 297840 (epoch   93.08), loss: 0.003231, time: 2.243 s\n",
      "Step 297860 (epoch   93.08), loss: 0.007447, time: 2.243 s\n",
      "Step 297880 (epoch   93.09), loss: 0.003475, time: 2.234 s\n",
      "Step 297900 (epoch   93.09), loss: 0.003636, time: 2.244 s\n",
      "Step 297920 (epoch   93.10), loss: 0.002963, time: 2.258 s\n",
      "Step 297940 (epoch   93.11), loss: 0.003337, time: 2.246 s\n",
      "Step 297960 (epoch   93.11), loss: 0.004721, time: 2.241 s\n",
      "Step 297980 (epoch   93.12), loss: 0.004637, time: 2.246 s\n",
      "Step 298000 (epoch   93.12), loss: 0.004612, time: 2.234 s, error: 0.009948\n",
      "\n",
      "Time since beginning  : 52354.855 s\n",
      "\n",
      "Step 298020 (epoch   93.13), loss: 0.002850, time: 15.127 s\n",
      "Step 298040 (epoch   93.14), loss: 0.004766, time: 2.237 s\n",
      "Step 298060 (epoch   93.14), loss: 0.006340, time: 2.251 s\n",
      "Step 298080 (epoch   93.15), loss: 0.002322, time: 2.242 s\n",
      "Step 298100 (epoch   93.16), loss: 0.004363, time: 2.244 s\n",
      "Step 298120 (epoch   93.16), loss: 0.003147, time: 2.227 s\n",
      "Step 298140 (epoch   93.17), loss: 0.005512, time: 2.245 s\n",
      "Step 298160 (epoch   93.17), loss: 0.005219, time: 2.240 s\n",
      "Step 298180 (epoch   93.18), loss: 0.007293, time: 2.252 s\n",
      "Step 298200 (epoch   93.19), loss: 0.003477, time: 2.249 s, error: 0.010154\n",
      "Step 298220 (epoch   93.19), loss: 0.003688, time: 15.051 s\n",
      "Step 298240 (epoch   93.20), loss: 0.003301, time: 2.255 s\n",
      "Step 298260 (epoch   93.21), loss: 0.002620, time: 2.243 s\n",
      "Step 298280 (epoch   93.21), loss: 0.004828, time: 2.253 s\n",
      "Step 298300 (epoch   93.22), loss: 0.004078, time: 2.256 s\n",
      "Step 298320 (epoch   93.22), loss: 0.006498, time: 2.255 s\n",
      "Step 298340 (epoch   93.23), loss: 0.005251, time: 2.238 s\n",
      "Step 298360 (epoch   93.24), loss: 0.003380, time: 2.246 s\n",
      "Step 298380 (epoch   93.24), loss: 0.002579, time: 2.236 s\n",
      "Step 298400 (epoch   93.25), loss: 0.003098, time: 2.252 s, error: 0.009902\n",
      "Step 298420 (epoch   93.26), loss: 0.008868, time: 15.242 s\n",
      "Step 298440 (epoch   93.26), loss: 0.005770, time: 2.243 s\n",
      "Step 298460 (epoch   93.27), loss: 0.005228, time: 2.254 s\n",
      "Step 298480 (epoch   93.28), loss: 0.004012, time: 2.258 s\n",
      "Step 298500 (epoch   93.28), loss: 0.006410, time: 2.246 s\n",
      "Step 298520 (epoch   93.29), loss: 0.005822, time: 2.255 s\n",
      "Step 298540 (epoch   93.29), loss: 0.003690, time: 2.248 s\n",
      "Step 298560 (epoch   93.30), loss: 0.004683, time: 2.244 s\n",
      "Step 298580 (epoch   93.31), loss: 0.004337, time: 2.243 s\n",
      "Step 298600 (epoch   93.31), loss: 0.002736, time: 2.249 s, error: 0.009992\n",
      "Step 298620 (epoch   93.32), loss: 0.004174, time: 15.311 s\n",
      "Step 298640 (epoch   93.33), loss: 0.004761, time: 2.250 s\n",
      "Step 298660 (epoch   93.33), loss: 0.003600, time: 2.254 s\n",
      "Step 298680 (epoch   93.34), loss: 0.002544, time: 2.246 s\n",
      "Step 298700 (epoch   93.34), loss: 0.003599, time: 2.247 s\n",
      "Step 298720 (epoch   93.35), loss: 0.003752, time: 2.251 s\n",
      "Step 298740 (epoch   93.36), loss: 0.003539, time: 2.251 s\n",
      "Step 298760 (epoch   93.36), loss: 0.003507, time: 2.238 s\n",
      "Step 298780 (epoch   93.37), loss: 0.002688, time: 2.245 s\n",
      "Step 298800 (epoch   93.38), loss: 0.002633, time: 2.234 s, error: 0.009817\n",
      "Step 298820 (epoch   93.38), loss: 0.003078, time: 15.115 s\n",
      "Step 298840 (epoch   93.39), loss: 0.003915, time: 2.252 s\n",
      "Step 298860 (epoch   93.39), loss: 0.004047, time: 2.255 s\n",
      "Step 298880 (epoch   93.40), loss: 0.002887, time: 2.243 s\n",
      "Step 298900 (epoch   93.41), loss: 0.002375, time: 2.230 s\n",
      "Step 298920 (epoch   93.41), loss: 0.004767, time: 2.236 s\n",
      "Step 298940 (epoch   93.42), loss: 0.003777, time: 2.248 s\n",
      "Step 298960 (epoch   93.42), loss: 0.004803, time: 2.233 s\n",
      "Step 298980 (epoch   93.43), loss: 0.003194, time: 2.234 s\n",
      "Step 299000 (epoch   93.44), loss: 0.005902, time: 2.233 s, error: 0.010033\n",
      "Step 299020 (epoch   93.44), loss: 0.004452, time: 15.033 s\n",
      "Step 299040 (epoch   93.45), loss: 0.005071, time: 2.243 s\n",
      "Step 299060 (epoch   93.46), loss: 0.004482, time: 2.232 s\n",
      "Step 299080 (epoch   93.46), loss: 0.003509, time: 2.243 s\n",
      "Step 299100 (epoch   93.47), loss: 0.004619, time: 2.258 s\n",
      "Step 299120 (epoch   93.47), loss: 0.002781, time: 2.251 s\n",
      "Step 299140 (epoch   93.48), loss: 0.003203, time: 2.231 s\n",
      "Step 299160 (epoch   93.49), loss: 0.003173, time: 2.237 s\n",
      "Step 299180 (epoch   93.49), loss: 0.004955, time: 2.227 s\n",
      "Step 299200 (epoch   93.50), loss: 0.002534, time: 2.238 s, error: 0.009749\n",
      "Step 299220 (epoch   93.51), loss: 0.002712, time: 15.116 s\n",
      "Step 299240 (epoch   93.51), loss: 0.004355, time: 2.244 s\n",
      "Step 299260 (epoch   93.52), loss: 0.003680, time: 2.252 s\n",
      "Step 299280 (epoch   93.53), loss: 0.003480, time: 2.234 s\n",
      "Step 299300 (epoch   93.53), loss: 0.004140, time: 2.255 s\n",
      "Step 299320 (epoch   93.54), loss: 0.004720, time: 2.241 s\n",
      "Step 299340 (epoch   93.54), loss: 0.002975, time: 2.254 s\n",
      "Step 299360 (epoch   93.55), loss: 0.005173, time: 2.246 s\n",
      "Step 299380 (epoch   93.56), loss: 0.002824, time: 2.243 s\n",
      "Step 299400 (epoch   93.56), loss: 0.003482, time: 2.238 s, error: 0.010142\n",
      "Step 299420 (epoch   93.57), loss: 0.004232, time: 15.043 s\n",
      "Step 299440 (epoch   93.58), loss: 0.003365, time: 2.241 s\n",
      "Step 299460 (epoch   93.58), loss: 0.004227, time: 2.238 s\n",
      "Step 299480 (epoch   93.59), loss: 0.003633, time: 2.241 s\n",
      "Step 299500 (epoch   93.59), loss: 0.005854, time: 2.245 s\n",
      "Step 299520 (epoch   93.60), loss: 0.004066, time: 2.240 s\n",
      "Step 299540 (epoch   93.61), loss: 0.003431, time: 2.251 s\n",
      "Step 299560 (epoch   93.61), loss: 0.004029, time: 2.245 s\n",
      "Step 299580 (epoch   93.62), loss: 0.003181, time: 2.242 s\n",
      "Step 299600 (epoch   93.62), loss: 0.005245, time: 2.246 s, error: 0.009863\n",
      "Step 299620 (epoch   93.63), loss: 0.004014, time: 15.120 s\n",
      "Step 299640 (epoch   93.64), loss: 0.005693, time: 2.241 s\n",
      "Step 299660 (epoch   93.64), loss: 0.003565, time: 2.239 s\n",
      "Step 299680 (epoch   93.65), loss: 0.003518, time: 2.243 s\n",
      "Step 299700 (epoch   93.66), loss: 0.004576, time: 2.239 s\n",
      "Step 299720 (epoch   93.66), loss: 0.004198, time: 2.248 s\n",
      "Step 299740 (epoch   93.67), loss: 0.003377, time: 2.254 s\n",
      "Step 299760 (epoch   93.67), loss: 0.002666, time: 2.246 s\n",
      "Step 299780 (epoch   93.68), loss: 0.005040, time: 2.228 s\n",
      "Step 299800 (epoch   93.69), loss: 0.003820, time: 2.244 s, error: 0.009892\n",
      "Step 299820 (epoch   93.69), loss: 0.004281, time: 15.234 s\n",
      "Step 299840 (epoch   93.70), loss: 0.004921, time: 2.255 s\n",
      "Step 299860 (epoch   93.71), loss: 0.003657, time: 2.246 s\n",
      "Step 299880 (epoch   93.71), loss: 0.007578, time: 2.245 s\n",
      "Step 299900 (epoch   93.72), loss: 0.004186, time: 2.241 s\n",
      "Step 299920 (epoch   93.72), loss: 0.002518, time: 2.246 s\n",
      "Step 299940 (epoch   93.73), loss: 0.004999, time: 2.243 s\n",
      "Step 299960 (epoch   93.74), loss: 0.003259, time: 2.238 s\n",
      "Step 299980 (epoch   93.74), loss: 0.003854, time: 2.241 s\n",
      "Step 300000 (epoch   93.75), loss: 0.004070, time: 2.238 s, error: 0.009924\n",
      "\n",
      "Time since beginning  : 52708.282 s\n",
      "\n",
      "Step 300020 (epoch   93.76), loss: 0.002962, time: 15.234 s\n",
      "Step 300040 (epoch   93.76), loss: 0.002876, time: 2.239 s\n",
      "Step 300060 (epoch   93.77), loss: 0.003191, time: 2.243 s\n",
      "Step 300080 (epoch   93.78), loss: 0.008379, time: 2.250 s\n",
      "Step 300100 (epoch   93.78), loss: 0.003925, time: 2.252 s\n",
      "Step 300120 (epoch   93.79), loss: 0.002959, time: 2.251 s\n",
      "Step 300140 (epoch   93.79), loss: 0.002724, time: 2.245 s\n",
      "Step 300160 (epoch   93.80), loss: 0.002837, time: 2.243 s\n",
      "Step 300180 (epoch   93.81), loss: 0.003830, time: 2.235 s\n",
      "Step 300200 (epoch   93.81), loss: 0.004787, time: 2.252 s, error: 0.009797\n",
      "Step 300220 (epoch   93.82), loss: 0.004656, time: 15.039 s\n",
      "Step 300240 (epoch   93.83), loss: 0.004933, time: 2.241 s\n",
      "Step 300260 (epoch   93.83), loss: 0.003694, time: 2.252 s\n",
      "Step 300280 (epoch   93.84), loss: 0.003787, time: 2.254 s\n",
      "Step 300300 (epoch   93.84), loss: 0.006156, time: 2.232 s\n",
      "Step 300320 (epoch   93.85), loss: 0.003505, time: 2.237 s\n",
      "Step 300340 (epoch   93.86), loss: 0.003118, time: 2.245 s\n",
      "Step 300360 (epoch   93.86), loss: 0.004091, time: 2.226 s\n",
      "Step 300380 (epoch   93.87), loss: 0.002637, time: 2.238 s\n",
      "Step 300400 (epoch   93.88), loss: 0.003545, time: 2.237 s, error: 0.009858\n",
      "Step 300420 (epoch   93.88), loss: 0.004021, time: 15.002 s\n",
      "Step 300440 (epoch   93.89), loss: 0.004135, time: 2.243 s\n",
      "Step 300460 (epoch   93.89), loss: 0.003434, time: 2.243 s\n",
      "Step 300480 (epoch   93.90), loss: 0.003297, time: 2.247 s\n",
      "Step 300500 (epoch   93.91), loss: 0.003723, time: 2.234 s\n",
      "Step 300520 (epoch   93.91), loss: 0.003720, time: 2.258 s\n",
      "Step 300540 (epoch   93.92), loss: 0.002865, time: 2.253 s\n",
      "Step 300560 (epoch   93.92), loss: 0.001693, time: 2.248 s\n",
      "Step 300580 (epoch   93.93), loss: 0.003656, time: 2.236 s\n",
      "Step 300600 (epoch   93.94), loss: 0.005505, time: 2.245 s, error: 0.009893\n",
      "Step 300620 (epoch   93.94), loss: 0.003496, time: 15.080 s\n",
      "Step 300640 (epoch   93.95), loss: 0.003794, time: 2.243 s\n",
      "Step 300660 (epoch   93.96), loss: 0.006878, time: 2.242 s\n",
      "Step 300680 (epoch   93.96), loss: 0.004623, time: 2.243 s\n",
      "Step 300700 (epoch   93.97), loss: 0.003312, time: 2.249 s\n",
      "Step 300720 (epoch   93.97), loss: 0.006443, time: 2.233 s\n",
      "Step 300740 (epoch   93.98), loss: 0.003045, time: 2.247 s\n",
      "Step 300760 (epoch   93.99), loss: 0.003496, time: 2.238 s\n",
      "Step 300780 (epoch   93.99), loss: 0.003237, time: 2.244 s\n",
      "Step 300800 (epoch   94.00), loss: 0.004579, time: 2.244 s, error: 0.009903\n",
      "Step 300820 (epoch   94.01), loss: 0.004806, time: 15.046 s\n",
      "Step 300840 (epoch   94.01), loss: 0.003213, time: 2.252 s\n",
      "Step 300860 (epoch   94.02), loss: 0.002284, time: 2.244 s\n",
      "Step 300880 (epoch   94.03), loss: 0.004830, time: 2.255 s\n",
      "Step 300900 (epoch   94.03), loss: 0.004366, time: 2.229 s\n",
      "Step 300920 (epoch   94.04), loss: 0.002706, time: 2.244 s\n",
      "Step 300940 (epoch   94.04), loss: 0.003577, time: 2.242 s\n",
      "Step 300960 (epoch   94.05), loss: 0.002530, time: 2.241 s\n",
      "Step 300980 (epoch   94.06), loss: 0.004794, time: 2.234 s\n",
      "Step 301000 (epoch   94.06), loss: 0.003253, time: 2.243 s, error: 0.009942\n",
      "Step 301020 (epoch   94.07), loss: 0.002000, time: 15.193 s\n",
      "Step 301040 (epoch   94.08), loss: 0.004644, time: 2.253 s\n",
      "Step 301060 (epoch   94.08), loss: 0.004760, time: 2.236 s\n",
      "Step 301080 (epoch   94.09), loss: 0.003918, time: 2.243 s\n",
      "Step 301100 (epoch   94.09), loss: 0.003586, time: 2.235 s\n",
      "Step 301120 (epoch   94.10), loss: 0.003649, time: 2.239 s\n",
      "Step 301140 (epoch   94.11), loss: 0.003854, time: 2.242 s\n",
      "Step 301160 (epoch   94.11), loss: 0.010072, time: 2.240 s\n",
      "Step 301180 (epoch   94.12), loss: 0.002941, time: 2.228 s\n",
      "Step 301200 (epoch   94.12), loss: 0.004286, time: 2.239 s, error: 0.009917\n",
      "Step 301220 (epoch   94.13), loss: 0.004228, time: 15.246 s\n",
      "Step 301240 (epoch   94.14), loss: 0.004004, time: 2.250 s\n",
      "Step 301260 (epoch   94.14), loss: 0.003741, time: 2.247 s\n",
      "Step 301280 (epoch   94.15), loss: 0.002471, time: 2.257 s\n",
      "Step 301300 (epoch   94.16), loss: 0.002444, time: 2.254 s\n",
      "Step 301320 (epoch   94.16), loss: 0.005211, time: 2.246 s\n",
      "Step 301340 (epoch   94.17), loss: 0.003167, time: 2.243 s\n",
      "Step 301360 (epoch   94.17), loss: 0.003890, time: 2.242 s\n",
      "Step 301380 (epoch   94.18), loss: 0.002637, time: 2.235 s\n",
      "Step 301400 (epoch   94.19), loss: 0.003234, time: 2.229 s, error: 0.010105\n",
      "Step 301420 (epoch   94.19), loss: 0.004043, time: 15.051 s\n",
      "Step 301440 (epoch   94.20), loss: 0.004110, time: 2.257 s\n",
      "Step 301460 (epoch   94.21), loss: 0.003068, time: 2.255 s\n",
      "Step 301480 (epoch   94.21), loss: 0.002685, time: 2.237 s\n",
      "Step 301500 (epoch   94.22), loss: 0.003337, time: 2.235 s\n",
      "Step 301520 (epoch   94.22), loss: 0.004330, time: 2.245 s\n",
      "Step 301540 (epoch   94.23), loss: 0.003802, time: 2.247 s\n",
      "Step 301560 (epoch   94.24), loss: 0.002525, time: 2.228 s\n",
      "Step 301580 (epoch   94.24), loss: 0.003410, time: 2.251 s\n",
      "Step 301600 (epoch   94.25), loss: 0.002753, time: 2.254 s, error: 0.009858\n",
      "Step 301620 (epoch   94.26), loss: 0.002983, time: 15.020 s\n",
      "Step 301640 (epoch   94.26), loss: 0.002644, time: 2.254 s\n",
      "Step 301660 (epoch   94.27), loss: 0.003392, time: 2.251 s\n",
      "Step 301680 (epoch   94.28), loss: 0.003215, time: 2.236 s\n",
      "Step 301700 (epoch   94.28), loss: 0.002717, time: 2.257 s\n",
      "Step 301720 (epoch   94.29), loss: 0.004041, time: 2.252 s\n",
      "Step 301740 (epoch   94.29), loss: 0.002525, time: 2.253 s\n",
      "Step 301760 (epoch   94.30), loss: 0.002618, time: 2.242 s\n",
      "Step 301780 (epoch   94.31), loss: 0.002084, time: 2.248 s\n",
      "Step 301800 (epoch   94.31), loss: 0.002527, time: 2.253 s, error: 0.009921\n",
      "Step 301820 (epoch   94.32), loss: 0.004358, time: 15.023 s\n",
      "Step 301840 (epoch   94.33), loss: 0.002921, time: 2.252 s\n",
      "Step 301860 (epoch   94.33), loss: 0.005382, time: 2.246 s\n",
      "Step 301880 (epoch   94.34), loss: 0.002774, time: 2.247 s\n",
      "Step 301900 (epoch   94.34), loss: 0.003021, time: 2.252 s\n",
      "Step 301920 (epoch   94.35), loss: 0.003408, time: 2.252 s\n",
      "Step 301940 (epoch   94.36), loss: 0.003258, time: 2.246 s\n",
      "Step 301960 (epoch   94.36), loss: 0.007659, time: 2.245 s\n",
      "Step 301980 (epoch   94.37), loss: 0.003868, time: 2.237 s\n",
      "Step 302000 (epoch   94.38), loss: 0.004278, time: 2.236 s, error: 0.009865\n",
      "\n",
      "Time since beginning  : 53061.135 s\n",
      "\n",
      "Step 302020 (epoch   94.38), loss: 0.003230, time: 15.188 s\n",
      "Step 302040 (epoch   94.39), loss: 0.004434, time: 2.234 s\n",
      "Step 302060 (epoch   94.39), loss: 0.003667, time: 2.235 s\n",
      "Step 302080 (epoch   94.40), loss: 0.003699, time: 2.249 s\n",
      "Step 302100 (epoch   94.41), loss: 0.002564, time: 2.224 s\n",
      "Step 302120 (epoch   94.41), loss: 0.002548, time: 2.242 s\n",
      "Step 302140 (epoch   94.42), loss: 0.002953, time: 2.256 s\n",
      "Step 302160 (epoch   94.42), loss: 0.003714, time: 2.249 s\n",
      "Step 302180 (epoch   94.43), loss: 0.004072, time: 2.237 s\n",
      "Step 302200 (epoch   94.44), loss: 0.004201, time: 2.244 s, error: 0.009963\n",
      "Step 302220 (epoch   94.44), loss: 0.002548, time: 15.265 s\n",
      "Step 302240 (epoch   94.45), loss: 0.002603, time: 2.228 s\n",
      "Step 302260 (epoch   94.46), loss: 0.003942, time: 2.245 s\n",
      "Step 302280 (epoch   94.46), loss: 0.004442, time: 2.246 s\n",
      "Step 302300 (epoch   94.47), loss: 0.001996, time: 2.245 s\n",
      "Step 302320 (epoch   94.47), loss: 0.004504, time: 2.234 s\n",
      "Step 302340 (epoch   94.48), loss: 0.003892, time: 2.246 s\n",
      "Step 302360 (epoch   94.49), loss: 0.003272, time: 2.226 s\n",
      "Step 302380 (epoch   94.49), loss: 0.003773, time: 2.243 s\n",
      "Step 302400 (epoch   94.50), loss: 0.002126, time: 2.235 s, error: 0.009813\n",
      "Step 302420 (epoch   94.51), loss: 0.003811, time: 15.221 s\n",
      "Step 302440 (epoch   94.51), loss: 0.003259, time: 2.249 s\n",
      "Step 302460 (epoch   94.52), loss: 0.003246, time: 2.234 s\n",
      "Step 302480 (epoch   94.53), loss: 0.003433, time: 2.244 s\n",
      "Step 302500 (epoch   94.53), loss: 0.004458, time: 2.239 s\n",
      "Step 302520 (epoch   94.54), loss: 0.002900, time: 2.238 s\n",
      "Step 302540 (epoch   94.54), loss: 0.003537, time: 2.232 s\n",
      "Step 302560 (epoch   94.55), loss: 0.006007, time: 2.244 s\n",
      "Step 302580 (epoch   94.56), loss: 0.008407, time: 2.256 s\n",
      "Step 302600 (epoch   94.56), loss: 0.005126, time: 2.240 s, error: 0.010105\n",
      "Step 302620 (epoch   94.57), loss: 0.004422, time: 15.149 s\n",
      "Step 302640 (epoch   94.58), loss: 0.005657, time: 2.245 s\n",
      "Step 302660 (epoch   94.58), loss: 0.002978, time: 2.241 s\n",
      "Step 302680 (epoch   94.59), loss: 0.002623, time: 2.235 s\n",
      "Step 302700 (epoch   94.59), loss: 0.004245, time: 2.232 s\n",
      "Step 302720 (epoch   94.60), loss: 0.003925, time: 2.242 s\n",
      "Step 302740 (epoch   94.61), loss: 0.003622, time: 2.243 s\n",
      "Step 302760 (epoch   94.61), loss: 0.004348, time: 2.253 s\n",
      "Step 302780 (epoch   94.62), loss: 0.008458, time: 2.255 s\n",
      "Step 302800 (epoch   94.62), loss: 0.003979, time: 2.244 s, error: 0.009947\n",
      "Step 302820 (epoch   94.63), loss: 0.003331, time: 15.038 s\n",
      "Step 302840 (epoch   94.64), loss: 0.003893, time: 2.243 s\n",
      "Step 302860 (epoch   94.64), loss: 0.004607, time: 2.245 s\n",
      "Step 302880 (epoch   94.65), loss: 0.003572, time: 2.252 s\n",
      "Step 302900 (epoch   94.66), loss: 0.004489, time: 2.240 s\n",
      "Step 302920 (epoch   94.66), loss: 0.002908, time: 2.254 s\n",
      "Step 302940 (epoch   94.67), loss: 0.003993, time: 2.240 s\n",
      "Step 302960 (epoch   94.67), loss: 0.002939, time: 2.245 s\n",
      "Step 302980 (epoch   94.68), loss: 0.003979, time: 2.248 s\n",
      "Step 303000 (epoch   94.69), loss: 0.002656, time: 2.244 s, error: 0.009900\n",
      "Step 303020 (epoch   94.69), loss: 0.002814, time: 15.053 s\n",
      "Step 303040 (epoch   94.70), loss: 0.003726, time: 2.248 s\n",
      "Step 303060 (epoch   94.71), loss: 0.003196, time: 2.242 s\n",
      "Step 303080 (epoch   94.71), loss: 0.003618, time: 2.245 s\n",
      "Step 303100 (epoch   94.72), loss: 0.003763, time: 2.232 s\n",
      "Step 303120 (epoch   94.72), loss: 0.003318, time: 2.242 s\n",
      "Step 303140 (epoch   94.73), loss: 0.003807, time: 2.261 s\n",
      "Step 303160 (epoch   94.74), loss: 0.005550, time: 2.240 s\n",
      "Step 303180 (epoch   94.74), loss: 0.003168, time: 2.243 s\n",
      "Step 303200 (epoch   94.75), loss: 0.005970, time: 2.234 s, error: 0.009935\n",
      "Step 303220 (epoch   94.76), loss: 0.004347, time: 15.047 s\n",
      "Step 303240 (epoch   94.76), loss: 0.003011, time: 2.245 s\n",
      "Step 303260 (epoch   94.77), loss: 0.003328, time: 2.243 s\n",
      "Step 303280 (epoch   94.78), loss: 0.006188, time: 2.256 s\n",
      "Step 303300 (epoch   94.78), loss: 0.003563, time: 2.255 s\n",
      "Step 303320 (epoch   94.79), loss: 0.002635, time: 2.242 s\n",
      "Step 303340 (epoch   94.79), loss: 0.003667, time: 2.255 s\n",
      "Step 303360 (epoch   94.80), loss: 0.002478, time: 2.238 s\n",
      "Step 303380 (epoch   94.81), loss: 0.006911, time: 2.240 s\n",
      "Step 303400 (epoch   94.81), loss: 0.003666, time: 2.245 s, error: 0.010038\n",
      "Step 303420 (epoch   94.82), loss: 0.003077, time: 15.040 s\n",
      "Step 303440 (epoch   94.83), loss: 0.006554, time: 2.239 s\n",
      "Step 303460 (epoch   94.83), loss: 0.004831, time: 2.242 s\n",
      "Step 303480 (epoch   94.84), loss: 0.002555, time: 2.235 s\n",
      "Step 303500 (epoch   94.84), loss: 0.003792, time: 2.235 s\n",
      "Step 303520 (epoch   94.85), loss: 0.002331, time: 2.241 s\n",
      "Step 303540 (epoch   94.86), loss: 0.007039, time: 2.237 s\n",
      "Step 303560 (epoch   94.86), loss: 0.002999, time: 2.240 s\n",
      "Step 303580 (epoch   94.87), loss: 0.004037, time: 2.249 s\n",
      "Step 303600 (epoch   94.88), loss: 0.003686, time: 2.239 s, error: 0.009910\n",
      "Step 303620 (epoch   94.88), loss: 0.009514, time: 15.286 s\n",
      "Step 303640 (epoch   94.89), loss: 0.003429, time: 2.237 s\n",
      "Step 303660 (epoch   94.89), loss: 0.003474, time: 2.231 s\n",
      "Step 303680 (epoch   94.90), loss: 0.003312, time: 2.244 s\n",
      "Step 303700 (epoch   94.91), loss: 0.003612, time: 2.239 s\n",
      "Step 303720 (epoch   94.91), loss: 0.002058, time: 2.249 s\n",
      "Step 303740 (epoch   94.92), loss: 0.004640, time: 2.226 s\n",
      "Step 303760 (epoch   94.92), loss: 0.002443, time: 2.233 s\n",
      "Step 303780 (epoch   94.93), loss: 0.004726, time: 2.238 s\n",
      "Step 303800 (epoch   94.94), loss: 0.004429, time: 2.245 s, error: 0.009942\n",
      "Step 303820 (epoch   94.94), loss: 0.004953, time: 15.281 s\n",
      "Step 303840 (epoch   94.95), loss: 0.003194, time: 2.238 s\n",
      "Step 303860 (epoch   94.96), loss: 0.002655, time: 2.244 s\n",
      "Step 303880 (epoch   94.96), loss: 0.002653, time: 2.233 s\n",
      "Step 303900 (epoch   94.97), loss: 0.005534, time: 2.233 s\n",
      "Step 303920 (epoch   94.97), loss: 0.003132, time: 2.231 s\n",
      "Step 303940 (epoch   94.98), loss: 0.005405, time: 2.238 s\n",
      "Step 303960 (epoch   94.99), loss: 0.003972, time: 2.256 s\n",
      "Step 303980 (epoch   94.99), loss: 0.006364, time: 2.254 s\n",
      "Step 304000 (epoch   95.00), loss: 0.003046, time: 2.245 s, error: 0.009759\n",
      "\n",
      "Time since beginning  : 53414.438 s\n",
      "\n",
      "Step 304020 (epoch   95.01), loss: 0.005094, time: 15.122 s\n",
      "Step 304040 (epoch   95.01), loss: 0.003246, time: 2.244 s\n",
      "Step 304060 (epoch   95.02), loss: 0.008133, time: 2.256 s\n",
      "Step 304080 (epoch   95.03), loss: 0.002671, time: 2.245 s\n",
      "Step 304100 (epoch   95.03), loss: 0.003866, time: 2.229 s\n",
      "Step 304120 (epoch   95.04), loss: 0.004863, time: 2.239 s\n",
      "Step 304140 (epoch   95.04), loss: 0.004850, time: 2.247 s\n",
      "Step 304160 (epoch   95.05), loss: 0.005134, time: 2.246 s\n",
      "Step 304180 (epoch   95.06), loss: 0.003398, time: 2.235 s\n",
      "Step 304200 (epoch   95.06), loss: 0.003154, time: 2.242 s, error: 0.009914\n",
      "Step 304220 (epoch   95.07), loss: 0.002966, time: 15.029 s\n",
      "Step 304240 (epoch   95.08), loss: 0.004493, time: 2.247 s\n",
      "Step 304260 (epoch   95.08), loss: 0.002198, time: 2.255 s\n",
      "Step 304280 (epoch   95.09), loss: 0.004141, time: 2.250 s\n",
      "Step 304300 (epoch   95.09), loss: 0.003327, time: 2.252 s\n",
      "Step 304320 (epoch   95.10), loss: 0.004243, time: 2.254 s\n",
      "Step 304340 (epoch   95.11), loss: 0.003501, time: 2.249 s\n",
      "Step 304360 (epoch   95.11), loss: 0.002380, time: 2.247 s\n",
      "Step 304380 (epoch   95.12), loss: 0.003999, time: 2.237 s\n",
      "Step 304400 (epoch   95.12), loss: 0.003104, time: 2.226 s, error: 0.009895\n",
      "Step 304420 (epoch   95.13), loss: 0.004876, time: 15.017 s\n",
      "Step 304440 (epoch   95.14), loss: 0.004344, time: 2.243 s\n",
      "Step 304460 (epoch   95.14), loss: 0.007800, time: 2.233 s\n",
      "Step 304480 (epoch   95.15), loss: 0.002925, time: 2.244 s\n",
      "Step 304500 (epoch   95.16), loss: 0.005869, time: 2.240 s\n",
      "Step 304520 (epoch   95.16), loss: 0.004082, time: 2.245 s\n",
      "Step 304540 (epoch   95.17), loss: 0.003308, time: 2.238 s\n",
      "Step 304560 (epoch   95.17), loss: 0.005490, time: 2.243 s\n",
      "Step 304580 (epoch   95.18), loss: 0.002245, time: 2.253 s\n",
      "Step 304600 (epoch   95.19), loss: 0.003353, time: 2.242 s, error: 0.009977\n",
      "Step 304620 (epoch   95.19), loss: 0.004017, time: 15.126 s\n",
      "Step 304640 (epoch   95.20), loss: 0.004532, time: 2.249 s\n",
      "Step 304660 (epoch   95.21), loss: 0.002329, time: 2.252 s\n",
      "Step 304680 (epoch   95.21), loss: 0.004023, time: 2.240 s\n",
      "Step 304700 (epoch   95.22), loss: 0.004873, time: 2.245 s\n",
      "Step 304720 (epoch   95.22), loss: 0.004762, time: 2.234 s\n",
      "Step 304740 (epoch   95.23), loss: 0.002991, time: 2.233 s\n",
      "Step 304760 (epoch   95.24), loss: 0.002781, time: 2.244 s\n",
      "Step 304780 (epoch   95.24), loss: 0.004464, time: 2.251 s\n",
      "Step 304800 (epoch   95.25), loss: 0.003049, time: 2.247 s, error: 0.009768\n",
      "Step 304820 (epoch   95.26), loss: 0.003666, time: 15.293 s\n",
      "Step 304840 (epoch   95.26), loss: 0.003469, time: 2.246 s\n",
      "Step 304860 (epoch   95.27), loss: 0.003453, time: 2.233 s\n",
      "Step 304880 (epoch   95.28), loss: 0.003849, time: 2.241 s\n",
      "Step 304900 (epoch   95.28), loss: 0.002769, time: 2.247 s\n",
      "Step 304920 (epoch   95.29), loss: 0.006222, time: 2.240 s\n",
      "Step 304940 (epoch   95.29), loss: 0.005146, time: 2.257 s\n",
      "Step 304960 (epoch   95.30), loss: 0.002958, time: 2.241 s\n",
      "Step 304980 (epoch   95.31), loss: 0.006104, time: 2.254 s\n",
      "Step 305000 (epoch   95.31), loss: 0.004093, time: 2.245 s, error: 0.009785\n",
      "Step 305020 (epoch   95.32), loss: 0.003614, time: 15.337 s\n",
      "Step 305040 (epoch   95.33), loss: 0.003217, time: 2.253 s\n",
      "Step 305060 (epoch   95.33), loss: 0.004687, time: 2.242 s\n",
      "Step 305080 (epoch   95.34), loss: 0.002877, time: 2.250 s\n",
      "Step 305100 (epoch   95.34), loss: 0.004217, time: 2.240 s\n",
      "Step 305120 (epoch   95.35), loss: 0.002543, time: 2.250 s\n",
      "Step 305140 (epoch   95.36), loss: 0.002980, time: 2.242 s\n",
      "Step 305160 (epoch   95.36), loss: 0.004251, time: 2.241 s\n",
      "Step 305180 (epoch   95.37), loss: 0.004389, time: 2.246 s\n",
      "Step 305200 (epoch   95.38), loss: 0.004898, time: 2.237 s, error: 0.009828\n",
      "Step 305220 (epoch   95.38), loss: 0.003543, time: 15.039 s\n",
      "Step 305240 (epoch   95.39), loss: 0.002490, time: 2.258 s\n",
      "Step 305260 (epoch   95.39), loss: 0.002159, time: 2.246 s\n",
      "Step 305280 (epoch   95.40), loss: 0.003627, time: 2.240 s\n",
      "Step 305300 (epoch   95.41), loss: 0.003770, time: 2.259 s\n",
      "Step 305320 (epoch   95.41), loss: 0.002507, time: 2.231 s\n",
      "Step 305340 (epoch   95.42), loss: 0.002138, time: 2.234 s\n",
      "Step 305360 (epoch   95.42), loss: 0.004519, time: 2.243 s\n",
      "Step 305380 (epoch   95.43), loss: 0.003848, time: 2.232 s\n",
      "Step 305400 (epoch   95.44), loss: 0.002890, time: 2.229 s, error: 0.009792\n",
      "Step 305420 (epoch   95.44), loss: 0.003006, time: 15.123 s\n",
      "Step 305440 (epoch   95.45), loss: 0.003207, time: 2.246 s\n",
      "Step 305460 (epoch   95.46), loss: 0.003216, time: 2.235 s\n",
      "Step 305480 (epoch   95.46), loss: 0.003168, time: 2.254 s\n",
      "Step 305500 (epoch   95.47), loss: 0.004550, time: 2.254 s\n",
      "Step 305520 (epoch   95.47), loss: 0.003129, time: 2.240 s\n",
      "Step 305540 (epoch   95.48), loss: 0.004936, time: 2.240 s\n",
      "Step 305560 (epoch   95.49), loss: 0.004113, time: 2.243 s\n",
      "Step 305580 (epoch   95.49), loss: 0.004266, time: 2.233 s\n",
      "Step 305600 (epoch   95.50), loss: 0.005018, time: 2.229 s, error: 0.009948\n",
      "Step 305620 (epoch   95.51), loss: 0.003050, time: 15.069 s\n",
      "Step 305640 (epoch   95.51), loss: 0.006256, time: 2.243 s\n",
      "Step 305660 (epoch   95.52), loss: 0.002065, time: 2.232 s\n",
      "Step 305680 (epoch   95.53), loss: 0.004830, time: 2.248 s\n",
      "Step 305700 (epoch   95.53), loss: 0.004242, time: 2.235 s\n",
      "Step 305720 (epoch   95.54), loss: 0.003096, time: 2.240 s\n",
      "Step 305740 (epoch   95.54), loss: 0.004609, time: 2.247 s\n",
      "Step 305760 (epoch   95.55), loss: 0.004084, time: 2.237 s\n",
      "Step 305780 (epoch   95.56), loss: 0.003806, time: 2.237 s\n",
      "Step 305800 (epoch   95.56), loss: 0.004088, time: 2.253 s, error: 0.009923\n",
      "Step 305820 (epoch   95.57), loss: 0.003441, time: 15.117 s\n",
      "Step 305840 (epoch   95.58), loss: 0.003171, time: 2.233 s\n",
      "Step 305860 (epoch   95.58), loss: 0.002351, time: 2.253 s\n",
      "Step 305880 (epoch   95.59), loss: 0.003086, time: 2.238 s\n",
      "Step 305900 (epoch   95.59), loss: 0.004869, time: 2.248 s\n",
      "Step 305920 (epoch   95.60), loss: 0.004095, time: 2.234 s\n",
      "Step 305940 (epoch   95.61), loss: 0.003425, time: 2.253 s\n",
      "Step 305960 (epoch   95.61), loss: 0.003673, time: 2.230 s\n",
      "Step 305980 (epoch   95.62), loss: 0.008114, time: 2.244 s\n",
      "Step 306000 (epoch   95.62), loss: 0.003125, time: 2.259 s, error: 0.010129\n",
      "\n",
      "Time since beginning  : 53767.718 s\n",
      "\n",
      "Step 306020 (epoch   95.63), loss: 0.003142, time: 15.227 s\n",
      "Step 306040 (epoch   95.64), loss: 0.004581, time: 2.245 s\n",
      "Step 306060 (epoch   95.64), loss: 0.002552, time: 2.230 s\n",
      "Step 306080 (epoch   95.65), loss: 0.003933, time: 2.237 s\n",
      "Step 306100 (epoch   95.66), loss: 0.002831, time: 2.233 s\n",
      "Step 306120 (epoch   95.66), loss: 0.002367, time: 2.240 s\n",
      "Step 306140 (epoch   95.67), loss: 0.004391, time: 2.239 s\n",
      "Step 306160 (epoch   95.67), loss: 0.003868, time: 2.235 s\n",
      "Step 306180 (epoch   95.68), loss: 0.003672, time: 2.237 s\n",
      "Step 306200 (epoch   95.69), loss: 0.004405, time: 2.246 s, error: 0.009790\n",
      "Step 306220 (epoch   95.69), loss: 0.002466, time: 15.231 s\n",
      "Step 306240 (epoch   95.70), loss: 0.002818, time: 2.224 s\n",
      "Step 306260 (epoch   95.71), loss: 0.005374, time: 2.251 s\n",
      "Step 306280 (epoch   95.71), loss: 0.004075, time: 2.241 s\n",
      "Step 306300 (epoch   95.72), loss: 0.002147, time: 2.239 s\n",
      "Step 306320 (epoch   95.72), loss: 0.003566, time: 2.254 s\n",
      "Step 306340 (epoch   95.73), loss: 0.008907, time: 2.248 s\n",
      "Step 306360 (epoch   95.74), loss: 0.003051, time: 2.249 s\n",
      "Step 306380 (epoch   95.74), loss: 0.003328, time: 2.244 s\n",
      "Step 306400 (epoch   95.75), loss: 0.002006, time: 2.247 s, error: 0.009821\n",
      "Step 306420 (epoch   95.76), loss: 0.004122, time: 15.166 s\n",
      "Step 306440 (epoch   95.76), loss: 0.003198, time: 2.247 s\n",
      "Step 306460 (epoch   95.77), loss: 0.003819, time: 2.242 s\n",
      "Step 306480 (epoch   95.78), loss: 0.009862, time: 2.243 s\n",
      "Step 306500 (epoch   95.78), loss: 0.004302, time: 2.242 s\n",
      "Step 306520 (epoch   95.79), loss: 0.003133, time: 2.253 s\n",
      "Step 306540 (epoch   95.79), loss: 0.004251, time: 2.250 s\n",
      "Step 306560 (epoch   95.80), loss: 0.004520, time: 2.251 s\n",
      "Step 306580 (epoch   95.81), loss: 0.003048, time: 2.232 s\n",
      "Step 306600 (epoch   95.81), loss: 0.004465, time: 2.245 s, error: 0.010132\n",
      "Step 306620 (epoch   95.82), loss: 0.005517, time: 15.047 s\n",
      "Step 306640 (epoch   95.83), loss: 0.004919, time: 2.249 s\n",
      "Step 306660 (epoch   95.83), loss: 0.003442, time: 2.259 s\n",
      "Step 306680 (epoch   95.84), loss: 0.002970, time: 2.249 s\n",
      "Step 306700 (epoch   95.84), loss: 0.004333, time: 2.251 s\n",
      "Step 306720 (epoch   95.85), loss: 0.003730, time: 2.244 s\n",
      "Step 306740 (epoch   95.86), loss: 0.004393, time: 2.243 s\n",
      "Step 306760 (epoch   95.86), loss: 0.002112, time: 2.232 s\n",
      "Step 306780 (epoch   95.87), loss: 0.003133, time: 2.248 s\n",
      "Step 306800 (epoch   95.88), loss: 0.003817, time: 2.236 s, error: 0.009865\n",
      "Step 306820 (epoch   95.88), loss: 0.003716, time: 15.073 s\n",
      "Step 306840 (epoch   95.89), loss: 0.013161, time: 2.233 s\n",
      "Step 306860 (epoch   95.89), loss: 0.003472, time: 2.241 s\n",
      "Step 306880 (epoch   95.90), loss: 0.005076, time: 2.235 s\n",
      "Step 306900 (epoch   95.91), loss: 0.003517, time: 2.251 s\n",
      "Step 306920 (epoch   95.91), loss: 0.002817, time: 2.254 s\n",
      "Step 306940 (epoch   95.92), loss: 0.004981, time: 2.248 s\n",
      "Step 306960 (epoch   95.92), loss: 0.003116, time: 2.241 s\n",
      "Step 306980 (epoch   95.93), loss: 0.002751, time: 2.249 s\n",
      "Step 307000 (epoch   95.94), loss: 0.002759, time: 2.250 s, error: 0.010038\n",
      "Step 307020 (epoch   95.94), loss: 0.004090, time: 15.207 s\n",
      "Step 307040 (epoch   95.95), loss: 0.004688, time: 2.248 s\n",
      "Step 307060 (epoch   95.96), loss: 0.003741, time: 2.243 s\n",
      "Step 307080 (epoch   95.96), loss: 0.002294, time: 2.246 s\n",
      "Step 307100 (epoch   95.97), loss: 0.003325, time: 2.254 s\n",
      "Step 307120 (epoch   95.97), loss: 0.004667, time: 2.252 s\n",
      "Step 307140 (epoch   95.98), loss: 0.005575, time: 2.252 s\n",
      "Step 307160 (epoch   95.99), loss: 0.002374, time: 2.232 s\n",
      "Step 307180 (epoch   95.99), loss: 0.005712, time: 2.257 s\n",
      "Step 307200 (epoch   96.00), loss: 0.002497, time: 2.237 s, error: 0.009821\n",
      "Step 307220 (epoch   96.01), loss: 0.003056, time: 15.053 s\n",
      "Step 307240 (epoch   96.01), loss: 0.003821, time: 2.235 s\n",
      "Step 307260 (epoch   96.02), loss: 0.004784, time: 2.246 s\n",
      "Step 307280 (epoch   96.03), loss: 0.004458, time: 2.252 s\n",
      "Step 307300 (epoch   96.03), loss: 0.005219, time: 2.243 s\n",
      "Step 307320 (epoch   96.04), loss: 0.003586, time: 2.237 s\n",
      "Step 307340 (epoch   96.04), loss: 0.004864, time: 2.246 s\n",
      "Step 307360 (epoch   96.05), loss: 0.003367, time: 2.253 s\n",
      "Step 307380 (epoch   96.06), loss: 0.016880, time: 2.243 s\n",
      "Step 307400 (epoch   96.06), loss: 0.002664, time: 2.250 s, error: 0.009833\n",
      "Step 307420 (epoch   96.07), loss: 0.004458, time: 15.304 s\n",
      "Step 307440 (epoch   96.08), loss: 0.003363, time: 2.230 s\n",
      "Step 307460 (epoch   96.08), loss: 0.002140, time: 2.234 s\n",
      "Step 307480 (epoch   96.09), loss: 0.004627, time: 2.233 s\n",
      "Step 307500 (epoch   96.09), loss: 0.003982, time: 2.238 s\n",
      "Step 307520 (epoch   96.10), loss: 0.004882, time: 2.251 s\n",
      "Step 307540 (epoch   96.11), loss: 0.002489, time: 2.253 s\n",
      "Step 307560 (epoch   96.11), loss: 0.003487, time: 2.243 s\n",
      "Step 307580 (epoch   96.12), loss: 0.002566, time: 2.254 s\n",
      "Step 307600 (epoch   96.12), loss: 0.003761, time: 2.251 s, error: 0.009805\n",
      "Step 307620 (epoch   96.13), loss: 0.004377, time: 15.252 s\n",
      "Step 307640 (epoch   96.14), loss: 0.003734, time: 2.246 s\n",
      "Step 307660 (epoch   96.14), loss: 0.003446, time: 2.251 s\n",
      "Step 307680 (epoch   96.15), loss: 0.003644, time: 2.243 s\n",
      "Step 307700 (epoch   96.16), loss: 0.003254, time: 2.236 s\n",
      "Step 307720 (epoch   96.16), loss: 0.003943, time: 2.250 s\n",
      "Step 307740 (epoch   96.17), loss: 0.002760, time: 2.237 s\n",
      "Step 307760 (epoch   96.17), loss: 0.002860, time: 2.249 s\n",
      "Step 307780 (epoch   96.18), loss: 0.003248, time: 2.224 s\n",
      "Step 307800 (epoch   96.19), loss: 0.004041, time: 2.251 s, error: 0.009822\n",
      "Step 307820 (epoch   96.19), loss: 0.005346, time: 15.008 s\n",
      "Step 307840 (epoch   96.20), loss: 0.005033, time: 2.257 s\n",
      "Step 307860 (epoch   96.21), loss: 0.003982, time: 2.230 s\n",
      "Step 307880 (epoch   96.21), loss: 0.005820, time: 2.224 s\n",
      "Step 307900 (epoch   96.22), loss: 0.003279, time: 2.239 s\n",
      "Step 307920 (epoch   96.22), loss: 0.003044, time: 2.228 s\n",
      "Step 307940 (epoch   96.23), loss: 0.004057, time: 2.245 s\n",
      "Step 307960 (epoch   96.24), loss: 0.003038, time: 2.238 s\n",
      "Step 307980 (epoch   96.24), loss: 0.006474, time: 2.244 s\n",
      "Step 308000 (epoch   96.25), loss: 0.004364, time: 2.255 s, error: 0.009715\n",
      "\n",
      "Time since beginning  : 54121.158 s\n",
      "\n",
      "Step 308020 (epoch   96.26), loss: 0.004688, time: 15.193 s\n",
      "Step 308040 (epoch   96.26), loss: 0.004371, time: 2.252 s\n",
      "Step 308060 (epoch   96.27), loss: 0.003577, time: 2.240 s\n",
      "Step 308080 (epoch   96.28), loss: 0.004079, time: 2.238 s\n",
      "Step 308100 (epoch   96.28), loss: 0.002765, time: 2.254 s\n",
      "Step 308120 (epoch   96.29), loss: 0.004224, time: 2.240 s\n",
      "Step 308140 (epoch   96.29), loss: 0.003311, time: 2.229 s\n",
      "Step 308160 (epoch   96.30), loss: 0.002620, time: 2.240 s\n",
      "Step 308180 (epoch   96.31), loss: 0.002804, time: 2.239 s\n",
      "Step 308200 (epoch   96.31), loss: 0.003536, time: 2.236 s, error: 0.009796\n",
      "Step 308220 (epoch   96.32), loss: 0.003613, time: 15.123 s\n",
      "Step 308240 (epoch   96.33), loss: 0.004692, time: 2.251 s\n",
      "Step 308260 (epoch   96.33), loss: 0.004364, time: 2.241 s\n",
      "Step 308280 (epoch   96.34), loss: 0.003590, time: 2.232 s\n",
      "Step 308300 (epoch   96.34), loss: 0.002921, time: 2.244 s\n",
      "Step 308320 (epoch   96.35), loss: 0.003802, time: 2.253 s\n",
      "Step 308340 (epoch   96.36), loss: 0.002571, time: 2.255 s\n",
      "Step 308360 (epoch   96.36), loss: 0.003365, time: 2.247 s\n",
      "Step 308380 (epoch   96.37), loss: 0.004664, time: 2.256 s\n",
      "Step 308400 (epoch   96.38), loss: 0.004036, time: 2.256 s, error: 0.009777\n",
      "Step 308420 (epoch   96.38), loss: 0.004461, time: 15.099 s\n",
      "Step 308440 (epoch   96.39), loss: 0.003025, time: 2.238 s\n",
      "Step 308460 (epoch   96.39), loss: 0.003896, time: 2.245 s\n",
      "Step 308480 (epoch   96.40), loss: 0.006451, time: 2.239 s\n",
      "Step 308500 (epoch   96.41), loss: 0.005306, time: 2.248 s\n",
      "Step 308520 (epoch   96.41), loss: 0.002092, time: 2.253 s\n",
      "Step 308540 (epoch   96.42), loss: 0.002264, time: 2.240 s\n",
      "Step 308560 (epoch   96.42), loss: 0.004934, time: 2.223 s\n",
      "Step 308580 (epoch   96.43), loss: 0.006077, time: 2.250 s\n",
      "Step 308600 (epoch   96.44), loss: 0.006669, time: 2.247 s, error: 0.009756\n",
      "Step 308620 (epoch   96.44), loss: 0.004513, time: 15.124 s\n",
      "Step 308640 (epoch   96.45), loss: 0.002908, time: 2.259 s\n",
      "Step 308660 (epoch   96.46), loss: 0.003117, time: 2.250 s\n",
      "Step 308680 (epoch   96.46), loss: 0.005889, time: 2.258 s\n",
      "Step 308700 (epoch   96.47), loss: 0.004507, time: 2.255 s\n",
      "Step 308720 (epoch   96.47), loss: 0.002817, time: 2.255 s\n",
      "Step 308740 (epoch   96.48), loss: 0.003446, time: 2.239 s\n",
      "Step 308760 (epoch   96.49), loss: 0.003383, time: 2.241 s\n",
      "Step 308780 (epoch   96.49), loss: 0.003919, time: 2.247 s\n",
      "Step 308800 (epoch   96.50), loss: 0.005419, time: 2.240 s, error: 0.010044\n",
      "Step 308820 (epoch   96.51), loss: 0.004403, time: 15.268 s\n",
      "Step 308840 (epoch   96.51), loss: 0.004569, time: 2.254 s\n",
      "Step 308860 (epoch   96.52), loss: 0.002245, time: 2.251 s\n",
      "Step 308880 (epoch   96.53), loss: 0.003399, time: 2.256 s\n",
      "Step 308900 (epoch   96.53), loss: 0.002932, time: 2.250 s\n",
      "Step 308920 (epoch   96.54), loss: 0.003013, time: 2.241 s\n",
      "Step 308940 (epoch   96.54), loss: 0.004425, time: 2.239 s\n",
      "Step 308960 (epoch   96.55), loss: 0.004938, time: 2.248 s\n",
      "Step 308980 (epoch   96.56), loss: 0.004174, time: 2.241 s\n",
      "Step 309000 (epoch   96.56), loss: 0.004172, time: 2.251 s, error: 0.009849\n",
      "Step 309020 (epoch   96.57), loss: 0.003798, time: 15.305 s\n",
      "Step 309040 (epoch   96.58), loss: 0.003462, time: 2.248 s\n",
      "Step 309060 (epoch   96.58), loss: 0.005078, time: 2.257 s\n",
      "Step 309080 (epoch   96.59), loss: 0.003103, time: 2.252 s\n",
      "Step 309100 (epoch   96.59), loss: 0.002820, time: 2.247 s\n",
      "Step 309120 (epoch   96.60), loss: 0.003381, time: 2.244 s\n",
      "Step 309140 (epoch   96.61), loss: 0.003800, time: 2.250 s\n",
      "Step 309160 (epoch   96.61), loss: 0.004865, time: 2.234 s\n",
      "Step 309180 (epoch   96.62), loss: 0.005011, time: 2.248 s\n",
      "Step 309200 (epoch   96.62), loss: 0.004446, time: 2.251 s, error: 0.010171\n",
      "Step 309220 (epoch   96.63), loss: 0.003024, time: 15.163 s\n",
      "Step 309240 (epoch   96.64), loss: 0.004639, time: 2.219 s\n",
      "Step 309260 (epoch   96.64), loss: 0.002558, time: 2.259 s\n",
      "Step 309280 (epoch   96.65), loss: 0.006664, time: 2.244 s\n",
      "Step 309300 (epoch   96.66), loss: 0.003004, time: 2.243 s\n",
      "Step 309320 (epoch   96.66), loss: 0.003746, time: 2.253 s\n",
      "Step 309340 (epoch   96.67), loss: 0.005228, time: 2.250 s\n",
      "Step 309360 (epoch   96.67), loss: 0.007853, time: 2.246 s\n",
      "Step 309380 (epoch   96.68), loss: 0.003995, time: 2.245 s\n",
      "Step 309400 (epoch   96.69), loss: 0.002345, time: 2.252 s, error: 0.009695\n",
      "Step 309420 (epoch   96.69), loss: 0.003399, time: 15.024 s\n",
      "Step 309440 (epoch   96.70), loss: 0.003110, time: 2.244 s\n",
      "Step 309460 (epoch   96.71), loss: 0.002649, time: 2.241 s\n",
      "Step 309480 (epoch   96.71), loss: 0.003432, time: 2.243 s\n",
      "Step 309500 (epoch   96.72), loss: 0.003705, time: 2.252 s\n",
      "Step 309520 (epoch   96.72), loss: 0.003251, time: 2.259 s\n",
      "Step 309540 (epoch   96.73), loss: 0.004491, time: 2.254 s\n",
      "Step 309560 (epoch   96.74), loss: 0.003801, time: 2.243 s\n",
      "Step 309580 (epoch   96.74), loss: 0.003642, time: 2.243 s\n",
      "Step 309600 (epoch   96.75), loss: 0.005647, time: 2.253 s, error: 0.009762\n",
      "Step 309620 (epoch   96.76), loss: 0.003124, time: 15.054 s\n",
      "Step 309640 (epoch   96.76), loss: 0.004206, time: 2.246 s\n",
      "Step 309660 (epoch   96.77), loss: 0.003483, time: 2.242 s\n",
      "Step 309680 (epoch   96.78), loss: 0.002612, time: 2.252 s\n",
      "Step 309700 (epoch   96.78), loss: 0.003421, time: 2.247 s\n",
      "Step 309720 (epoch   96.79), loss: 0.002072, time: 2.250 s\n",
      "Step 309740 (epoch   96.79), loss: 0.003082, time: 2.244 s\n",
      "Step 309760 (epoch   96.80), loss: 0.003909, time: 2.252 s\n",
      "Step 309780 (epoch   96.81), loss: 0.004656, time: 2.247 s\n",
      "Step 309800 (epoch   96.81), loss: 0.006174, time: 2.253 s, error: 0.010049\n",
      "Step 309820 (epoch   96.82), loss: 0.002897, time: 15.051 s\n",
      "Step 309840 (epoch   96.83), loss: 0.004255, time: 2.241 s\n",
      "Step 309860 (epoch   96.83), loss: 0.004708, time: 2.246 s\n",
      "Step 309880 (epoch   96.84), loss: 0.004356, time: 2.233 s\n",
      "Step 309900 (epoch   96.84), loss: 0.003526, time: 2.246 s\n",
      "Step 309920 (epoch   96.85), loss: 0.002843, time: 2.238 s\n",
      "Step 309940 (epoch   96.86), loss: 0.003853, time: 2.233 s\n",
      "Step 309960 (epoch   96.86), loss: 0.002484, time: 2.236 s\n",
      "Step 309980 (epoch   96.87), loss: 0.002562, time: 2.249 s\n",
      "Step 310000 (epoch   96.88), loss: 0.006479, time: 2.227 s, error: 0.009748\n",
      "\n",
      "Time since beginning  : 54474.796 s\n",
      "\n",
      "Step 310020 (epoch   96.88), loss: 0.004177, time: 15.346 s\n",
      "Step 310040 (epoch   96.89), loss: 0.004588, time: 2.257 s\n",
      "Step 310060 (epoch   96.89), loss: 0.003623, time: 2.243 s\n",
      "Step 310080 (epoch   96.90), loss: 0.004683, time: 2.241 s\n",
      "Step 310100 (epoch   96.91), loss: 0.003420, time: 2.244 s\n",
      "Step 310120 (epoch   96.91), loss: 0.005719, time: 2.237 s\n",
      "Step 310140 (epoch   96.92), loss: 0.002946, time: 2.229 s\n",
      "Step 310160 (epoch   96.92), loss: 0.002643, time: 2.255 s\n",
      "Step 310180 (epoch   96.93), loss: 0.002785, time: 2.234 s\n",
      "Step 310200 (epoch   96.94), loss: 0.003326, time: 2.243 s, error: 0.010117\n",
      "Step 310220 (epoch   96.94), loss: 0.004125, time: 15.309 s\n",
      "Step 310240 (epoch   96.95), loss: 0.002973, time: 2.240 s\n",
      "Step 310260 (epoch   96.96), loss: 0.002962, time: 2.246 s\n",
      "Step 310280 (epoch   96.96), loss: 0.002355, time: 2.236 s\n",
      "Step 310300 (epoch   96.97), loss: 0.003950, time: 2.244 s\n",
      "Step 310320 (epoch   96.97), loss: 0.004598, time: 2.228 s\n",
      "Step 310340 (epoch   96.98), loss: 0.002793, time: 2.249 s\n",
      "Step 310360 (epoch   96.99), loss: 0.003542, time: 2.231 s\n",
      "Step 310380 (epoch   96.99), loss: 0.004096, time: 2.229 s\n",
      "Step 310400 (epoch   97.00), loss: 0.003545, time: 2.231 s, error: 0.010086\n",
      "Step 310420 (epoch   97.01), loss: 0.003671, time: 15.163 s\n",
      "Step 310440 (epoch   97.01), loss: 0.004614, time: 2.260 s\n",
      "Step 310460 (epoch   97.02), loss: 0.002975, time: 2.233 s\n",
      "Step 310480 (epoch   97.03), loss: 0.006130, time: 2.246 s\n",
      "Step 310500 (epoch   97.03), loss: 0.002145, time: 2.229 s\n",
      "Step 310520 (epoch   97.04), loss: 0.003731, time: 2.239 s\n",
      "Step 310540 (epoch   97.04), loss: 0.002991, time: 2.241 s\n",
      "Step 310560 (epoch   97.05), loss: 0.003743, time: 2.224 s\n",
      "Step 310580 (epoch   97.06), loss: 0.003113, time: 2.248 s\n",
      "Step 310600 (epoch   97.06), loss: 0.003405, time: 2.229 s, error: 0.009758\n",
      "Step 310620 (epoch   97.07), loss: 0.005626, time: 15.063 s\n",
      "Step 310640 (epoch   97.08), loss: 0.003438, time: 2.243 s\n",
      "Step 310660 (epoch   97.08), loss: 0.002682, time: 2.240 s\n",
      "Step 310680 (epoch   97.09), loss: 0.004355, time: 2.240 s\n",
      "Step 310700 (epoch   97.09), loss: 0.002720, time: 2.257 s\n",
      "Step 310720 (epoch   97.10), loss: 0.003785, time: 2.244 s\n",
      "Step 310740 (epoch   97.11), loss: 0.003072, time: 2.244 s\n",
      "Step 310760 (epoch   97.11), loss: 0.006903, time: 2.244 s\n",
      "Step 310780 (epoch   97.12), loss: 0.006141, time: 2.238 s\n",
      "Step 310800 (epoch   97.12), loss: 0.002806, time: 2.243 s, error: 0.009752\n",
      "Step 310820 (epoch   97.13), loss: 0.003379, time: 15.051 s\n",
      "Step 310840 (epoch   97.14), loss: 0.004834, time: 2.244 s\n",
      "Step 310860 (epoch   97.14), loss: 0.005246, time: 2.235 s\n",
      "Step 310880 (epoch   97.15), loss: 0.004708, time: 2.245 s\n",
      "Step 310900 (epoch   97.16), loss: 0.003251, time: 2.233 s\n",
      "Step 310920 (epoch   97.16), loss: 0.002869, time: 2.233 s\n",
      "Step 310940 (epoch   97.17), loss: 0.004497, time: 2.254 s\n",
      "Step 310960 (epoch   97.17), loss: 0.002787, time: 2.255 s\n",
      "Step 310980 (epoch   97.18), loss: 0.004292, time: 2.241 s\n",
      "Step 311000 (epoch   97.19), loss: 0.003883, time: 2.253 s, error: 0.009753\n",
      "Step 311020 (epoch   97.19), loss: 0.004374, time: 15.046 s\n",
      "Step 311040 (epoch   97.20), loss: 0.003663, time: 2.244 s\n",
      "Step 311060 (epoch   97.21), loss: 0.003735, time: 2.258 s\n",
      "Step 311080 (epoch   97.21), loss: 0.005196, time: 2.239 s\n",
      "Step 311100 (epoch   97.22), loss: 0.004524, time: 2.249 s\n",
      "Step 311120 (epoch   97.22), loss: 0.003349, time: 2.251 s\n",
      "Step 311140 (epoch   97.23), loss: 0.002863, time: 2.245 s\n",
      "Step 311160 (epoch   97.24), loss: 0.004595, time: 2.247 s\n",
      "Step 311180 (epoch   97.24), loss: 0.003911, time: 2.240 s\n",
      "Step 311200 (epoch   97.25), loss: 0.003968, time: 2.243 s, error: 0.009693\n",
      "Step 311220 (epoch   97.26), loss: 0.003292, time: 15.179 s\n",
      "Step 311240 (epoch   97.26), loss: 0.003171, time: 2.234 s\n",
      "Step 311260 (epoch   97.27), loss: 0.002693, time: 2.233 s\n",
      "Step 311280 (epoch   97.28), loss: 0.004018, time: 2.245 s\n",
      "Step 311300 (epoch   97.28), loss: 0.002506, time: 2.237 s\n",
      "Step 311320 (epoch   97.29), loss: 0.003124, time: 2.254 s\n",
      "Step 311340 (epoch   97.29), loss: 0.003840, time: 2.251 s\n",
      "Step 311360 (epoch   97.30), loss: 0.004118, time: 2.239 s\n",
      "Step 311380 (epoch   97.31), loss: 0.004598, time: 2.236 s\n",
      "Step 311400 (epoch   97.31), loss: 0.002944, time: 2.244 s, error: 0.010033\n",
      "Step 311420 (epoch   97.32), loss: 0.003750, time: 15.299 s\n",
      "Step 311440 (epoch   97.33), loss: 0.005173, time: 2.237 s\n",
      "Step 311460 (epoch   97.33), loss: 0.005361, time: 2.251 s\n",
      "Step 311480 (epoch   97.34), loss: 0.002307, time: 2.235 s\n",
      "Step 311500 (epoch   97.34), loss: 0.004428, time: 2.236 s\n",
      "Step 311520 (epoch   97.35), loss: 0.003690, time: 2.234 s\n",
      "Step 311540 (epoch   97.36), loss: 0.005384, time: 2.238 s\n",
      "Step 311560 (epoch   97.36), loss: 0.003354, time: 2.251 s\n",
      "Step 311580 (epoch   97.37), loss: 0.005922, time: 2.244 s\n",
      "Step 311600 (epoch   97.38), loss: 0.004354, time: 2.249 s, error: 0.009803\n",
      "Step 311620 (epoch   97.38), loss: 0.003836, time: 15.124 s\n",
      "Step 311640 (epoch   97.39), loss: 0.003940, time: 2.244 s\n",
      "Step 311660 (epoch   97.39), loss: 0.007123, time: 2.251 s\n",
      "Step 311680 (epoch   97.40), loss: 0.003775, time: 2.238 s\n",
      "Step 311700 (epoch   97.41), loss: 0.003084, time: 2.255 s\n",
      "Step 311720 (epoch   97.41), loss: 0.005455, time: 2.248 s\n",
      "Step 311740 (epoch   97.42), loss: 0.003486, time: 2.249 s\n",
      "Step 311760 (epoch   97.42), loss: 0.002707, time: 2.250 s\n",
      "Step 311780 (epoch   97.43), loss: 0.003118, time: 2.257 s\n",
      "Step 311800 (epoch   97.44), loss: 0.005825, time: 2.242 s, error: 0.009885\n",
      "Step 311820 (epoch   97.44), loss: 0.004226, time: 15.018 s\n",
      "Step 311840 (epoch   97.45), loss: 0.002920, time: 2.249 s\n",
      "Step 311860 (epoch   97.46), loss: 0.003908, time: 2.237 s\n",
      "Step 311880 (epoch   97.46), loss: 0.003125, time: 2.252 s\n",
      "Step 311900 (epoch   97.47), loss: 0.003288, time: 2.226 s\n",
      "Step 311920 (epoch   97.47), loss: 0.003822, time: 2.243 s\n",
      "Step 311940 (epoch   97.48), loss: 0.002506, time: 2.241 s\n",
      "Step 311960 (epoch   97.49), loss: 0.003426, time: 2.250 s\n",
      "Step 311980 (epoch   97.49), loss: 0.004484, time: 2.245 s\n",
      "Step 312000 (epoch   97.50), loss: 0.002955, time: 2.235 s, error: 0.009940\n",
      "\n",
      "Time since beginning  : 54828.142 s\n",
      "\n",
      "Step 312020 (epoch   97.51), loss: 0.002569, time: 15.236 s\n",
      "Step 312040 (epoch   97.51), loss: 0.004016, time: 2.255 s\n",
      "Step 312060 (epoch   97.52), loss: 0.004029, time: 2.253 s\n",
      "Step 312080 (epoch   97.53), loss: 0.003053, time: 2.244 s\n",
      "Step 312100 (epoch   97.53), loss: 0.002875, time: 2.249 s\n",
      "Step 312120 (epoch   97.54), loss: 0.002636, time: 2.255 s\n",
      "Step 312140 (epoch   97.54), loss: 0.004733, time: 2.257 s\n",
      "Step 312160 (epoch   97.55), loss: 0.003390, time: 2.240 s\n",
      "Step 312180 (epoch   97.56), loss: 0.002395, time: 2.229 s\n",
      "Step 312200 (epoch   97.56), loss: 0.002570, time: 2.252 s, error: 0.009846\n",
      "Step 312220 (epoch   97.57), loss: 0.005285, time: 15.003 s\n",
      "Step 312240 (epoch   97.58), loss: 0.004310, time: 2.239 s\n",
      "Step 312260 (epoch   97.58), loss: 0.003574, time: 2.236 s\n",
      "Step 312280 (epoch   97.59), loss: 0.005805, time: 2.237 s\n",
      "Step 312300 (epoch   97.59), loss: 0.003351, time: 2.249 s\n",
      "Step 312320 (epoch   97.60), loss: 0.002838, time: 2.236 s\n",
      "Step 312340 (epoch   97.61), loss: 0.004170, time: 2.249 s\n",
      "Step 312360 (epoch   97.61), loss: 0.004127, time: 2.251 s\n",
      "Step 312380 (epoch   97.62), loss: 0.003886, time: 2.256 s\n",
      "Step 312400 (epoch   97.62), loss: 0.003305, time: 2.257 s, error: 0.010156\n",
      "Step 312420 (epoch   97.63), loss: 0.005543, time: 15.143 s\n",
      "Step 312440 (epoch   97.64), loss: 0.003087, time: 2.237 s\n",
      "Step 312460 (epoch   97.64), loss: 0.003600, time: 2.232 s\n",
      "Step 312480 (epoch   97.65), loss: 0.004093, time: 2.235 s\n",
      "Step 312500 (epoch   97.66), loss: 0.002625, time: 2.242 s\n",
      "Step 312520 (epoch   97.66), loss: 0.004595, time: 2.228 s\n",
      "Step 312540 (epoch   97.67), loss: 0.005419, time: 2.246 s\n",
      "Step 312560 (epoch   97.67), loss: 0.003359, time: 2.230 s\n",
      "Step 312580 (epoch   97.68), loss: 0.003929, time: 2.248 s\n",
      "Step 312600 (epoch   97.69), loss: 0.002604, time: 2.240 s, error: 0.009682\n",
      "Step 312620 (epoch   97.69), loss: 0.005181, time: 15.231 s\n",
      "Step 312640 (epoch   97.70), loss: 0.004195, time: 2.245 s\n",
      "Step 312660 (epoch   97.71), loss: 0.004041, time: 2.237 s\n",
      "Step 312680 (epoch   97.71), loss: 0.002979, time: 2.256 s\n",
      "Step 312700 (epoch   97.72), loss: 0.005773, time: 2.241 s\n",
      "Step 312720 (epoch   97.72), loss: 0.002783, time: 2.232 s\n",
      "Step 312740 (epoch   97.73), loss: 0.004652, time: 2.222 s\n",
      "Step 312760 (epoch   97.74), loss: 0.004211, time: 2.230 s\n",
      "Step 312780 (epoch   97.74), loss: 0.004631, time: 2.238 s\n",
      "Step 312800 (epoch   97.75), loss: 0.004556, time: 2.257 s, error: 0.009730\n",
      "Step 312820 (epoch   97.76), loss: 0.005034, time: 15.275 s\n",
      "Step 312840 (epoch   97.76), loss: 0.004029, time: 2.234 s\n",
      "Step 312860 (epoch   97.77), loss: 0.002918, time: 2.243 s\n",
      "Step 312880 (epoch   97.78), loss: 0.003184, time: 2.234 s\n",
      "Step 312900 (epoch   97.78), loss: 0.009238, time: 2.249 s\n",
      "Step 312920 (epoch   97.79), loss: 0.003300, time: 2.232 s\n",
      "Step 312940 (epoch   97.79), loss: 0.005034, time: 2.246 s\n",
      "Step 312960 (epoch   97.80), loss: 0.003098, time: 2.235 s\n",
      "Step 312980 (epoch   97.81), loss: 0.003997, time: 2.242 s\n",
      "Step 313000 (epoch   97.81), loss: 0.004693, time: 2.235 s, error: 0.009877\n",
      "Step 313020 (epoch   97.82), loss: 0.001963, time: 15.027 s\n",
      "Step 313040 (epoch   97.83), loss: 0.004589, time: 2.245 s\n",
      "Step 313060 (epoch   97.83), loss: 0.002676, time: 2.250 s\n",
      "Step 313080 (epoch   97.84), loss: 0.005849, time: 2.240 s\n",
      "Step 313100 (epoch   97.84), loss: 0.003854, time: 2.255 s\n",
      "Step 313120 (epoch   97.85), loss: 0.004021, time: 2.254 s\n",
      "Step 313140 (epoch   97.86), loss: 0.005138, time: 2.243 s\n",
      "Step 313160 (epoch   97.86), loss: 0.003046, time: 2.231 s\n",
      "Step 313180 (epoch   97.87), loss: 0.004411, time: 2.245 s\n",
      "Step 313200 (epoch   97.88), loss: 0.004050, time: 2.234 s, error: 0.009685\n",
      "Step 313220 (epoch   97.88), loss: 0.004566, time: 15.051 s\n",
      "Step 313240 (epoch   97.89), loss: 0.004186, time: 2.239 s\n",
      "Step 313260 (epoch   97.89), loss: 0.003127, time: 2.225 s\n",
      "Step 313280 (epoch   97.90), loss: 0.004864, time: 2.233 s\n",
      "Step 313300 (epoch   97.91), loss: 0.004841, time: 2.255 s\n",
      "Step 313320 (epoch   97.91), loss: 0.002987, time: 2.246 s\n",
      "Step 313340 (epoch   97.92), loss: 0.003358, time: 2.238 s\n",
      "Step 313360 (epoch   97.92), loss: 0.003121, time: 2.239 s\n",
      "Step 313380 (epoch   97.93), loss: 0.004701, time: 2.225 s\n",
      "Step 313400 (epoch   97.94), loss: 0.003299, time: 2.247 s, error: 0.010257\n",
      "Step 313420 (epoch   97.94), loss: 0.007245, time: 15.043 s\n",
      "Step 313440 (epoch   97.95), loss: 0.003190, time: 2.254 s\n",
      "Step 313460 (epoch   97.96), loss: 0.002683, time: 2.238 s\n",
      "Step 313480 (epoch   97.96), loss: 0.001867, time: 2.237 s\n",
      "Step 313500 (epoch   97.97), loss: 0.005084, time: 2.242 s\n",
      "Step 313520 (epoch   97.97), loss: 0.002147, time: 2.229 s\n",
      "Step 313540 (epoch   97.98), loss: 0.010709, time: 2.242 s\n",
      "Step 313560 (epoch   97.99), loss: 0.003766, time: 2.255 s\n",
      "Step 313580 (epoch   97.99), loss: 0.005603, time: 2.250 s\n",
      "Step 313600 (epoch   98.00), loss: 0.003301, time: 2.240 s, error: 0.010297\n",
      "Step 313620 (epoch   98.01), loss: 0.002788, time: 15.129 s\n",
      "Step 313640 (epoch   98.01), loss: 0.004101, time: 2.243 s\n",
      "Step 313660 (epoch   98.02), loss: 0.003260, time: 2.238 s\n",
      "Step 313680 (epoch   98.03), loss: 0.003629, time: 2.245 s\n",
      "Step 313700 (epoch   98.03), loss: 0.004145, time: 2.239 s\n",
      "Step 313720 (epoch   98.04), loss: 0.004936, time: 2.244 s\n",
      "Step 313740 (epoch   98.04), loss: 0.002818, time: 2.233 s\n",
      "Step 313760 (epoch   98.05), loss: 0.003033, time: 2.229 s\n",
      "Step 313780 (epoch   98.06), loss: 0.002902, time: 2.226 s\n",
      "Step 313800 (epoch   98.06), loss: 0.002730, time: 2.237 s, error: 0.009712\n",
      "Step 313820 (epoch   98.07), loss: 0.005606, time: 15.349 s\n",
      "Step 313840 (epoch   98.08), loss: 0.004861, time: 2.236 s\n",
      "Step 313860 (epoch   98.08), loss: 0.003035, time: 2.245 s\n",
      "Step 313880 (epoch   98.09), loss: 0.002949, time: 2.239 s\n",
      "Step 313900 (epoch   98.09), loss: 0.003343, time: 2.246 s\n",
      "Step 313920 (epoch   98.10), loss: 0.003235, time: 2.239 s\n",
      "Step 313940 (epoch   98.11), loss: 0.004289, time: 2.240 s\n",
      "Step 313960 (epoch   98.11), loss: 0.003286, time: 2.236 s\n",
      "Step 313980 (epoch   98.12), loss: 0.003667, time: 2.253 s\n",
      "Step 314000 (epoch   98.12), loss: 0.002778, time: 2.241 s, error: 0.009749\n",
      "\n",
      "Time since beginning  : 55181.444 s\n",
      "\n",
      "Step 314020 (epoch   98.13), loss: 0.002393, time: 15.314 s\n",
      "Step 314040 (epoch   98.14), loss: 0.002545, time: 2.239 s\n",
      "Step 314060 (epoch   98.14), loss: 0.003723, time: 2.235 s\n",
      "Step 314080 (epoch   98.15), loss: 0.002657, time: 2.245 s\n",
      "Step 314100 (epoch   98.16), loss: 0.003351, time: 2.227 s\n",
      "Step 314120 (epoch   98.16), loss: 0.003722, time: 2.256 s\n",
      "Step 314140 (epoch   98.17), loss: 0.002678, time: 2.256 s\n",
      "Step 314160 (epoch   98.17), loss: 0.003310, time: 2.245 s\n",
      "Step 314180 (epoch   98.18), loss: 0.004212, time: 2.244 s\n",
      "Step 314200 (epoch   98.19), loss: 0.003222, time: 2.235 s, error: 0.009732\n",
      "Step 314220 (epoch   98.19), loss: 0.002802, time: 15.031 s\n",
      "Step 314240 (epoch   98.20), loss: 0.003967, time: 2.248 s\n",
      "Step 314260 (epoch   98.21), loss: 0.002806, time: 2.243 s\n",
      "Step 314280 (epoch   98.21), loss: 0.002987, time: 2.238 s\n",
      "Step 314300 (epoch   98.22), loss: 0.002404, time: 2.224 s\n",
      "Step 314320 (epoch   98.22), loss: 0.003504, time: 2.247 s\n",
      "Step 314340 (epoch   98.23), loss: 0.003470, time: 2.234 s\n",
      "Step 314360 (epoch   98.24), loss: 0.004332, time: 2.243 s\n",
      "Step 314380 (epoch   98.24), loss: 0.002014, time: 2.231 s\n",
      "Step 314400 (epoch   98.25), loss: 0.004150, time: 2.245 s, error: 0.009706\n",
      "Step 314420 (epoch   98.26), loss: 0.002647, time: 15.024 s\n",
      "Step 314440 (epoch   98.26), loss: 0.004386, time: 2.243 s\n",
      "Step 314460 (epoch   98.27), loss: 0.003566, time: 2.257 s\n",
      "Step 314480 (epoch   98.28), loss: 0.003980, time: 2.258 s\n",
      "Step 314500 (epoch   98.28), loss: 0.003281, time: 2.248 s\n",
      "Step 314520 (epoch   98.29), loss: 0.003056, time: 2.230 s\n",
      "Step 314540 (epoch   98.29), loss: 0.004701, time: 2.239 s\n",
      "Step 314560 (epoch   98.30), loss: 0.003088, time: 2.224 s\n",
      "Step 314580 (epoch   98.31), loss: 0.004595, time: 2.244 s\n",
      "Step 314600 (epoch   98.31), loss: 0.003670, time: 2.254 s, error: 0.010293\n",
      "Step 314620 (epoch   98.32), loss: 0.003062, time: 15.102 s\n",
      "Step 314640 (epoch   98.33), loss: 0.004388, time: 2.241 s\n",
      "Step 314660 (epoch   98.33), loss: 0.002952, time: 2.225 s\n",
      "Step 314680 (epoch   98.34), loss: 0.004427, time: 2.240 s\n",
      "Step 314700 (epoch   98.34), loss: 0.003544, time: 2.243 s\n",
      "Step 314720 (epoch   98.35), loss: 0.002644, time: 2.244 s\n",
      "Step 314740 (epoch   98.36), loss: 0.003732, time: 2.260 s\n",
      "Step 314760 (epoch   98.36), loss: 0.004538, time: 2.246 s\n",
      "Step 314780 (epoch   98.37), loss: 0.003623, time: 2.240 s\n",
      "Step 314800 (epoch   98.38), loss: 0.004165, time: 2.235 s, error: 0.009805\n",
      "Step 314820 (epoch   98.38), loss: 0.003645, time: 15.220 s\n",
      "Step 314840 (epoch   98.39), loss: 0.003539, time: 2.237 s\n",
      "Step 314860 (epoch   98.39), loss: 0.003119, time: 2.236 s\n",
      "Step 314880 (epoch   98.40), loss: 0.003526, time: 2.243 s\n",
      "Step 314900 (epoch   98.41), loss: 0.004993, time: 2.240 s\n",
      "Step 314920 (epoch   98.41), loss: 0.003077, time: 2.242 s\n",
      "Step 314940 (epoch   98.42), loss: 0.004189, time: 2.237 s\n",
      "Step 314960 (epoch   98.42), loss: 0.005538, time: 2.240 s\n",
      "Step 314980 (epoch   98.43), loss: 0.003569, time: 2.237 s\n",
      "Step 315000 (epoch   98.44), loss: 0.003772, time: 2.261 s, error: 0.010046\n",
      "Step 315020 (epoch   98.44), loss: 0.004767, time: 15.017 s\n",
      "Step 315040 (epoch   98.45), loss: 0.002726, time: 2.249 s\n",
      "Step 315060 (epoch   98.46), loss: 0.003102, time: 2.242 s\n",
      "Step 315080 (epoch   98.46), loss: 0.003452, time: 2.238 s\n",
      "Step 315100 (epoch   98.47), loss: 0.008056, time: 2.243 s\n",
      "Step 315120 (epoch   98.47), loss: 0.003160, time: 2.240 s\n",
      "Step 315140 (epoch   98.48), loss: 0.003136, time: 2.234 s\n",
      "Step 315160 (epoch   98.49), loss: 0.006122, time: 2.253 s\n",
      "Step 315180 (epoch   98.49), loss: 0.003289, time: 2.256 s\n",
      "Step 315200 (epoch   98.50), loss: 0.004943, time: 2.244 s, error: 0.009924\n",
      "Step 315220 (epoch   98.51), loss: 0.002845, time: 15.267 s\n",
      "Step 315240 (epoch   98.51), loss: 0.004984, time: 2.246 s\n",
      "Step 315260 (epoch   98.52), loss: 0.003282, time: 2.253 s\n",
      "Step 315280 (epoch   98.53), loss: 0.007340, time: 2.256 s\n",
      "Step 315300 (epoch   98.53), loss: 0.006039, time: 2.256 s\n",
      "Step 315320 (epoch   98.54), loss: 0.004095, time: 2.253 s\n",
      "Step 315340 (epoch   98.54), loss: 0.005132, time: 2.243 s\n",
      "Step 315360 (epoch   98.55), loss: 0.003116, time: 2.254 s\n",
      "Step 315380 (epoch   98.56), loss: 0.002834, time: 2.245 s\n",
      "Step 315400 (epoch   98.56), loss: 0.004702, time: 2.250 s, error: 0.009853\n",
      "Step 315420 (epoch   98.57), loss: 0.002510, time: 15.198 s\n",
      "Step 315440 (epoch   98.58), loss: 0.004432, time: 2.248 s\n",
      "Step 315460 (epoch   98.58), loss: 0.003190, time: 2.247 s\n",
      "Step 315480 (epoch   98.59), loss: 0.004274, time: 2.247 s\n",
      "Step 315500 (epoch   98.59), loss: 0.007788, time: 2.236 s\n",
      "Step 315520 (epoch   98.60), loss: 0.003010, time: 2.232 s\n",
      "Step 315540 (epoch   98.61), loss: 0.002576, time: 2.234 s\n",
      "Step 315560 (epoch   98.61), loss: 0.003982, time: 2.231 s\n",
      "Step 315580 (epoch   98.62), loss: 0.004376, time: 2.247 s\n",
      "Step 315600 (epoch   98.62), loss: 0.003118, time: 2.245 s, error: 0.010047\n",
      "Step 315620 (epoch   98.63), loss: 0.004418, time: 15.044 s\n",
      "Step 315640 (epoch   98.64), loss: 0.002487, time: 2.249 s\n",
      "Step 315660 (epoch   98.64), loss: 0.005044, time: 2.252 s\n",
      "Step 315680 (epoch   98.65), loss: 0.003953, time: 2.241 s\n",
      "Step 315700 (epoch   98.66), loss: 0.004238, time: 2.250 s\n",
      "Step 315720 (epoch   98.66), loss: 0.005521, time: 2.246 s\n",
      "Step 315740 (epoch   98.67), loss: 0.003225, time: 2.242 s\n",
      "Step 315760 (epoch   98.67), loss: 0.004555, time: 2.235 s\n",
      "Step 315780 (epoch   98.68), loss: 0.003035, time: 2.244 s\n",
      "Step 315800 (epoch   98.69), loss: 0.003139, time: 2.234 s, error: 0.009708\n",
      "Step 315820 (epoch   98.69), loss: 0.003720, time: 15.134 s\n",
      "Step 315840 (epoch   98.70), loss: 0.002829, time: 2.230 s\n",
      "Step 315860 (epoch   98.71), loss: 0.003296, time: 2.243 s\n",
      "Step 315880 (epoch   98.71), loss: 0.004357, time: 2.230 s\n",
      "Step 315900 (epoch   98.72), loss: 0.004112, time: 2.250 s\n",
      "Step 315920 (epoch   98.72), loss: 0.004226, time: 2.257 s\n",
      "Step 315940 (epoch   98.73), loss: 0.003094, time: 2.247 s\n",
      "Step 315960 (epoch   98.74), loss: 0.003689, time: 2.251 s\n",
      "Step 315980 (epoch   98.74), loss: 0.004437, time: 2.252 s\n",
      "Step 316000 (epoch   98.75), loss: 0.003625, time: 2.252 s, error: 0.009724\n",
      "\n",
      "Time since beginning  : 55534.533 s\n",
      "\n",
      "Step 316020 (epoch   98.76), loss: 0.006108, time: 15.139 s\n",
      "Step 316040 (epoch   98.76), loss: 0.004078, time: 2.251 s\n",
      "Step 316060 (epoch   98.77), loss: 0.003947, time: 2.236 s\n",
      "Step 316080 (epoch   98.78), loss: 0.003930, time: 2.248 s\n",
      "Step 316100 (epoch   98.78), loss: 0.002729, time: 2.239 s\n",
      "Step 316120 (epoch   98.79), loss: 0.005988, time: 2.242 s\n",
      "Step 316140 (epoch   98.79), loss: 0.003261, time: 2.232 s\n",
      "Step 316160 (epoch   98.80), loss: 0.003453, time: 2.243 s\n",
      "Step 316180 (epoch   98.81), loss: 0.002996, time: 2.256 s\n",
      "Step 316200 (epoch   98.81), loss: 0.003478, time: 2.247 s, error: 0.009643\n",
      "Step 316220 (epoch   98.82), loss: 0.002039, time: 15.051 s\n",
      "Step 316240 (epoch   98.83), loss: 0.002405, time: 2.237 s\n",
      "Step 316260 (epoch   98.83), loss: 0.002823, time: 2.247 s\n",
      "Step 316280 (epoch   98.84), loss: 0.003930, time: 2.256 s\n",
      "Step 316300 (epoch   98.84), loss: 0.002547, time: 2.248 s\n",
      "Step 316320 (epoch   98.85), loss: 0.002915, time: 2.231 s\n",
      "Step 316340 (epoch   98.86), loss: 0.003368, time: 2.242 s\n",
      "Step 316360 (epoch   98.86), loss: 0.002831, time: 2.246 s\n",
      "Step 316380 (epoch   98.87), loss: 0.003515, time: 2.237 s\n",
      "Step 316400 (epoch   98.88), loss: 0.003822, time: 2.249 s, error: 0.009860\n",
      "Step 316420 (epoch   98.88), loss: 0.003877, time: 15.246 s\n",
      "Step 316440 (epoch   98.89), loss: 0.004954, time: 2.243 s\n",
      "Step 316460 (epoch   98.89), loss: 0.007902, time: 2.249 s\n",
      "Step 316480 (epoch   98.90), loss: 0.004848, time: 2.243 s\n",
      "Step 316500 (epoch   98.91), loss: 0.003188, time: 2.254 s\n",
      "Step 316520 (epoch   98.91), loss: 0.002340, time: 2.248 s\n",
      "Step 316540 (epoch   98.92), loss: 0.005813, time: 2.247 s\n",
      "Step 316560 (epoch   98.92), loss: 0.003999, time: 2.235 s\n",
      "Step 316580 (epoch   98.93), loss: 0.002001, time: 2.242 s\n",
      "Step 316600 (epoch   98.94), loss: 0.004359, time: 2.219 s, error: 0.010338\n",
      "Step 316620 (epoch   98.94), loss: 0.003391, time: 15.226 s\n",
      "Step 316640 (epoch   98.95), loss: 0.007885, time: 2.225 s\n",
      "Step 316660 (epoch   98.96), loss: 0.004833, time: 2.228 s\n",
      "Step 316680 (epoch   98.96), loss: 0.003795, time: 2.237 s\n",
      "Step 316700 (epoch   98.97), loss: 0.002887, time: 2.232 s\n",
      "Step 316720 (epoch   98.97), loss: 0.003995, time: 2.249 s\n",
      "Step 316740 (epoch   98.98), loss: 0.001584, time: 2.239 s\n",
      "Step 316760 (epoch   98.99), loss: 0.004385, time: 2.243 s\n",
      "Step 316780 (epoch   98.99), loss: 0.003495, time: 2.243 s\n",
      "Step 316800 (epoch   99.00), loss: 0.004484, time: 2.246 s, error: 0.010219\n",
      "Step 316820 (epoch   99.01), loss: 0.002348, time: 15.079 s\n",
      "Step 316840 (epoch   99.01), loss: 0.005438, time: 2.246 s\n",
      "Step 316860 (epoch   99.02), loss: 0.003682, time: 2.240 s\n",
      "Step 316880 (epoch   99.03), loss: 0.008332, time: 2.255 s\n",
      "Step 316900 (epoch   99.03), loss: 0.003490, time: 2.252 s\n",
      "Step 316920 (epoch   99.04), loss: 0.002688, time: 2.242 s\n",
      "Step 316940 (epoch   99.04), loss: 0.003365, time: 2.239 s\n",
      "Step 316960 (epoch   99.05), loss: 0.002753, time: 2.242 s\n",
      "Step 316980 (epoch   99.06), loss: 0.002656, time: 2.239 s\n",
      "Step 317000 (epoch   99.06), loss: 0.005540, time: 2.226 s, error: 0.009680\n",
      "Step 317020 (epoch   99.07), loss: 0.003066, time: 15.021 s\n",
      "Step 317040 (epoch   99.08), loss: 0.004028, time: 2.243 s\n",
      "Step 317060 (epoch   99.08), loss: 0.004797, time: 2.238 s\n",
      "Step 317080 (epoch   99.09), loss: 0.004306, time: 2.248 s\n",
      "Step 317100 (epoch   99.09), loss: 0.004645, time: 2.242 s\n",
      "Step 317120 (epoch   99.10), loss: 0.003403, time: 2.232 s\n",
      "Step 317140 (epoch   99.11), loss: 0.002484, time: 2.248 s\n",
      "Step 317160 (epoch   99.11), loss: 0.003472, time: 2.235 s\n",
      "Step 317180 (epoch   99.12), loss: 0.003446, time: 2.246 s\n",
      "Step 317200 (epoch   99.12), loss: 0.005597, time: 2.232 s, error: 0.009767\n",
      "Step 317220 (epoch   99.13), loss: 0.003676, time: 15.119 s\n",
      "Step 317240 (epoch   99.14), loss: 0.004789, time: 2.238 s\n",
      "Step 317260 (epoch   99.14), loss: 0.004987, time: 2.225 s\n",
      "Step 317280 (epoch   99.15), loss: 0.006517, time: 2.246 s\n",
      "Step 317300 (epoch   99.16), loss: 0.003069, time: 2.234 s\n",
      "Step 317320 (epoch   99.16), loss: 0.003398, time: 2.255 s\n",
      "Step 317340 (epoch   99.17), loss: 0.002459, time: 2.249 s\n",
      "Step 317360 (epoch   99.17), loss: 0.002850, time: 2.253 s\n",
      "Step 317380 (epoch   99.18), loss: 0.003297, time: 2.238 s\n",
      "Step 317400 (epoch   99.19), loss: 0.003259, time: 2.238 s, error: 0.009804\n",
      "Step 317420 (epoch   99.19), loss: 0.002355, time: 15.046 s\n",
      "Step 317440 (epoch   99.20), loss: 0.003642, time: 2.231 s\n",
      "Step 317460 (epoch   99.21), loss: 0.004082, time: 2.243 s\n",
      "Step 317480 (epoch   99.21), loss: 0.003148, time: 2.235 s\n",
      "Step 317500 (epoch   99.22), loss: 0.003077, time: 2.250 s\n",
      "Step 317520 (epoch   99.22), loss: 0.002851, time: 2.234 s\n",
      "Step 317540 (epoch   99.23), loss: 0.004740, time: 2.255 s\n",
      "Step 317560 (epoch   99.24), loss: 0.002436, time: 2.246 s\n",
      "Step 317580 (epoch   99.24), loss: 0.004769, time: 2.243 s\n",
      "Step 317600 (epoch   99.25), loss: 0.003691, time: 2.257 s, error: 0.009701\n",
      "Step 317620 (epoch   99.26), loss: 0.003203, time: 15.110 s\n",
      "Step 317640 (epoch   99.26), loss: 0.002909, time: 2.236 s\n",
      "Step 317660 (epoch   99.27), loss: 0.004143, time: 2.236 s\n",
      "Step 317680 (epoch   99.28), loss: 0.004798, time: 2.237 s\n",
      "Step 317700 (epoch   99.28), loss: 0.003847, time: 2.229 s\n",
      "Step 317720 (epoch   99.29), loss: 0.006011, time: 2.246 s\n",
      "Step 317740 (epoch   99.29), loss: 0.003797, time: 2.228 s\n",
      "Step 317760 (epoch   99.30), loss: 0.002477, time: 2.237 s\n",
      "Step 317780 (epoch   99.31), loss: 0.002999, time: 2.236 s\n",
      "Step 317800 (epoch   99.31), loss: 0.004029, time: 2.242 s, error: 0.010418\n",
      "Step 317820 (epoch   99.32), loss: 0.003247, time: 15.263 s\n",
      "Step 317840 (epoch   99.33), loss: 0.004479, time: 2.250 s\n",
      "Step 317860 (epoch   99.33), loss: 0.004269, time: 2.256 s\n",
      "Step 317880 (epoch   99.34), loss: 0.003144, time: 2.244 s\n",
      "Step 317900 (epoch   99.34), loss: 0.003393, time: 2.249 s\n",
      "Step 317920 (epoch   99.35), loss: 0.004630, time: 2.230 s\n",
      "Step 317940 (epoch   99.36), loss: 0.003390, time: 2.249 s\n",
      "Step 317960 (epoch   99.36), loss: 0.002753, time: 2.231 s\n",
      "Step 317980 (epoch   99.37), loss: 0.003013, time: 2.241 s\n",
      "Step 318000 (epoch   99.38), loss: 0.003418, time: 2.235 s, error: 0.009807\n",
      "\n",
      "Time since beginning  : 55887.706 s\n",
      "\n",
      "Step 318020 (epoch   99.38), loss: 0.002160, time: 15.326 s\n",
      "Step 318040 (epoch   99.39), loss: 0.004843, time: 2.243 s\n",
      "Step 318060 (epoch   99.39), loss: 0.005231, time: 2.249 s\n",
      "Step 318080 (epoch   99.40), loss: 0.004231, time: 2.232 s\n",
      "Step 318100 (epoch   99.41), loss: 0.004622, time: 2.242 s\n",
      "Step 318120 (epoch   99.41), loss: 0.005094, time: 2.234 s\n",
      "Step 318140 (epoch   99.42), loss: 0.003641, time: 2.246 s\n",
      "Step 318160 (epoch   99.42), loss: 0.004768, time: 2.239 s\n",
      "Step 318180 (epoch   99.43), loss: 0.002480, time: 2.235 s\n",
      "Step 318200 (epoch   99.44), loss: 0.008324, time: 2.230 s, error: 0.010077\n",
      "Step 318220 (epoch   99.44), loss: 0.003777, time: 15.058 s\n",
      "Step 318240 (epoch   99.45), loss: 0.002347, time: 2.246 s\n",
      "Step 318260 (epoch   99.46), loss: 0.003464, time: 2.244 s\n",
      "Step 318280 (epoch   99.46), loss: 0.003292, time: 2.240 s\n",
      "Step 318300 (epoch   99.47), loss: 0.003161, time: 2.229 s\n",
      "Step 318320 (epoch   99.47), loss: 0.003048, time: 2.243 s\n",
      "Step 318340 (epoch   99.48), loss: 0.005944, time: 2.242 s\n",
      "Step 318360 (epoch   99.49), loss: 0.007657, time: 2.242 s\n",
      "Step 318380 (epoch   99.49), loss: 0.003497, time: 2.236 s\n",
      "Step 318400 (epoch   99.50), loss: 0.002938, time: 2.225 s, error: 0.009870\n",
      "Step 318420 (epoch   99.51), loss: 0.002890, time: 15.040 s\n",
      "Step 318440 (epoch   99.51), loss: 0.002790, time: 2.240 s\n",
      "Step 318460 (epoch   99.52), loss: 0.002380, time: 2.229 s\n",
      "Step 318480 (epoch   99.53), loss: 0.003754, time: 2.238 s\n",
      "Step 318500 (epoch   99.53), loss: 0.004725, time: 2.235 s\n",
      "Step 318520 (epoch   99.54), loss: 0.002539, time: 2.244 s\n",
      "Step 318540 (epoch   99.54), loss: 0.002704, time: 2.248 s\n",
      "Step 318560 (epoch   99.55), loss: 0.004087, time: 2.250 s\n",
      "Step 318580 (epoch   99.56), loss: 0.002544, time: 2.248 s\n",
      "Step 318600 (epoch   99.56), loss: 0.002705, time: 2.240 s, error: 0.009779\n",
      "Step 318620 (epoch   99.57), loss: 0.003197, time: 15.003 s\n",
      "Step 318640 (epoch   99.58), loss: 0.003752, time: 2.243 s\n",
      "Step 318660 (epoch   99.58), loss: 0.004705, time: 2.241 s\n",
      "Step 318680 (epoch   99.59), loss: 0.002518, time: 2.240 s\n",
      "Step 318700 (epoch   99.59), loss: 0.008041, time: 2.234 s\n",
      "Step 318720 (epoch   99.60), loss: 0.005329, time: 2.240 s\n",
      "Step 318740 (epoch   99.61), loss: 0.003030, time: 2.242 s\n",
      "Step 318760 (epoch   99.61), loss: 0.003966, time: 2.240 s\n",
      "Step 318780 (epoch   99.62), loss: 0.003358, time: 2.255 s\n",
      "Step 318800 (epoch   99.62), loss: 0.003039, time: 2.236 s, error: 0.009927\n",
      "Step 318820 (epoch   99.63), loss: 0.003387, time: 15.004 s\n",
      "Step 318840 (epoch   99.64), loss: 0.003714, time: 2.241 s\n",
      "Step 318860 (epoch   99.64), loss: 0.002549, time: 2.245 s\n",
      "Step 318880 (epoch   99.65), loss: 0.002945, time: 2.255 s\n",
      "Step 318900 (epoch   99.66), loss: 0.002771, time: 2.244 s\n",
      "Step 318920 (epoch   99.66), loss: 0.003717, time: 2.237 s\n",
      "Step 318940 (epoch   99.67), loss: 0.003332, time: 2.259 s\n",
      "Step 318960 (epoch   99.67), loss: 0.002825, time: 2.245 s\n",
      "Step 318980 (epoch   99.68), loss: 0.004128, time: 2.242 s\n",
      "Step 319000 (epoch   99.69), loss: 0.003485, time: 2.223 s, error: 0.009690\n",
      "Step 319020 (epoch   99.69), loss: 0.003975, time: 15.241 s\n",
      "Step 319040 (epoch   99.70), loss: 0.002974, time: 2.241 s\n",
      "Step 319060 (epoch   99.71), loss: 0.003159, time: 2.245 s\n",
      "Step 319080 (epoch   99.71), loss: 0.002309, time: 2.251 s\n",
      "Step 319100 (epoch   99.72), loss: 0.002550, time: 2.243 s\n",
      "Step 319120 (epoch   99.72), loss: 0.002904, time: 2.246 s\n",
      "Step 319140 (epoch   99.73), loss: 0.003422, time: 2.223 s\n",
      "Step 319160 (epoch   99.74), loss: 0.003614, time: 2.238 s\n",
      "Step 319180 (epoch   99.74), loss: 0.004824, time: 2.238 s\n",
      "Step 319200 (epoch   99.75), loss: 0.002707, time: 2.242 s, error: 0.009758\n",
      "Step 319220 (epoch   99.76), loss: 0.002172, time: 15.267 s\n",
      "Step 319240 (epoch   99.76), loss: 0.003636, time: 2.226 s\n",
      "Step 319260 (epoch   99.77), loss: 0.003661, time: 2.252 s\n",
      "Step 319280 (epoch   99.78), loss: 0.004296, time: 2.241 s\n",
      "Step 319300 (epoch   99.78), loss: 0.003539, time: 2.244 s\n",
      "Step 319320 (epoch   99.79), loss: 0.005255, time: 2.229 s\n",
      "Step 319340 (epoch   99.79), loss: 0.003372, time: 2.240 s\n",
      "Step 319360 (epoch   99.80), loss: 0.002619, time: 2.239 s\n",
      "Step 319380 (epoch   99.81), loss: 0.002697, time: 2.240 s\n",
      "Step 319400 (epoch   99.81), loss: 0.003186, time: 2.241 s, error: 0.009622\n",
      "Step 319420 (epoch   99.82), loss: 0.002302, time: 15.025 s\n",
      "Step 319440 (epoch   99.83), loss: 0.002230, time: 2.259 s\n",
      "Step 319460 (epoch   99.83), loss: 0.003210, time: 2.243 s\n",
      "Step 319480 (epoch   99.84), loss: 0.004189, time: 2.243 s\n",
      "Step 319500 (epoch   99.84), loss: 0.003101, time: 2.236 s\n",
      "Step 319520 (epoch   99.85), loss: 0.004573, time: 2.239 s\n",
      "Step 319540 (epoch   99.86), loss: 0.002837, time: 2.236 s\n",
      "Step 319560 (epoch   99.86), loss: 0.003231, time: 2.228 s\n",
      "Step 319580 (epoch   99.87), loss: 0.002743, time: 2.249 s\n",
      "Step 319600 (epoch   99.88), loss: 0.002884, time: 2.254 s, error: 0.010087\n",
      "Step 319620 (epoch   99.88), loss: 0.009291, time: 15.061 s\n",
      "Step 319640 (epoch   99.89), loss: 0.003667, time: 2.246 s\n",
      "Step 319660 (epoch   99.89), loss: 0.005613, time: 2.245 s\n",
      "Step 319680 (epoch   99.90), loss: 0.004146, time: 2.242 s\n",
      "Step 319700 (epoch   99.91), loss: 0.004460, time: 2.253 s\n",
      "Step 319720 (epoch   99.91), loss: 0.003799, time: 2.240 s\n",
      "Step 319740 (epoch   99.92), loss: 0.003785, time: 2.234 s\n",
      "Step 319760 (epoch   99.92), loss: 0.006025, time: 2.242 s\n",
      "Step 319780 (epoch   99.93), loss: 0.003178, time: 2.244 s\n",
      "Step 319800 (epoch   99.94), loss: 0.008042, time: 2.230 s, error: 0.010196\n",
      "Step 319820 (epoch   99.94), loss: 0.003661, time: 15.022 s\n",
      "Step 319840 (epoch   99.95), loss: 0.002969, time: 2.232 s\n",
      "Step 319860 (epoch   99.96), loss: 0.002832, time: 2.237 s\n",
      "Step 319880 (epoch   99.96), loss: 0.004347, time: 2.233 s\n",
      "Step 319900 (epoch   99.97), loss: 0.006186, time: 2.253 s\n",
      "Step 319920 (epoch   99.97), loss: 0.002826, time: 2.254 s\n",
      "Step 319940 (epoch   99.98), loss: 0.003181, time: 2.244 s\n",
      "Step 319960 (epoch   99.99), loss: 0.003664, time: 2.247 s\n",
      "Step 319980 (epoch   99.99), loss: 0.004729, time: 2.244 s\n",
      "\n",
      "-> Test error         : 0.010185\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./saves/c9h7n.ckpt-320000'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build an initialization operation.\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Register a model saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Load the previous checkpoint if existed.\n",
    "checkpoint_path = restore_latest_from_ckpt(save_path)\n",
    "load = False\n",
    "if load and checkpoint_path:\n",
    "  saver.restore(sess, checkpoint_path)\n",
    "  print(\"Restore from the lastest checkpoint file %s ...\" % checkpoint_path)\n",
    "  print(\"\")\n",
    "\n",
    "print(\"Initialized!\")\n",
    "print(\"\")\n",
    "print(\"Training Samples      :\", len(X_train))\n",
    "print(\"Batch Size            :\", batch_size)\n",
    "print(\"Number of Epochs      :\", num_epochs)\n",
    "print(\"Log Frequency         :\", log_frequency)\n",
    "print(\"Eval Frequency        :\", eval_frequency)\n",
    "print(\"\")\n",
    "\n",
    "# Save the graph\n",
    "saver.save(sess, save_path=chk_file, global_step=0)\n",
    "\n",
    "tic = time.time()\n",
    "tstart = time.time()\n",
    "\n",
    "# Keep the validation errors locally\n",
    "valid_hists = []\n",
    "\n",
    "# Loop through training steps.\n",
    "for step in range(int(num_epochs * len(X_train)) // batch_size):\n",
    "  \n",
    "  # Compute the offset of the current minibatch in the data.\n",
    "  # The dataset was already shuffled in assignment 1 so we do not need to\n",
    "  # randomize it.\n",
    "  offset = (step * batch_size) % (len(X_train) - batch_size)\n",
    "  batch_dataset = X_train[offset: (offset + batch_size), ...]\n",
    "  batch_targets = y_train[offset: (offset + batch_size), ...]\n",
    "  \n",
    "  # Build the feed dict to feed previous defined placeholders.\n",
    "  feed_dict = {\n",
    "    X_batch: batch_dataset, \n",
    "    y_batch: batch_targets, \n",
    "    keep_prob: 0.7, \n",
    "    dense_keep_prob: 0.5\n",
    "  }\n",
    "  \n",
    "  # Run the optimization session.\n",
    "  sess.run([optimizer], feed_dict=feed_dict)\n",
    "  \n",
    "  # Run the step decay function\n",
    "  if step > 0 and step % (len(X_train) // batch_size) == 0:\n",
    "    sess.run([global_epoch_op])\n",
    "  \n",
    "  # Save the training accuracy every 100 steps.\n",
    "  if step % log_frequency == 0:\n",
    "    summary, error = sess.run([merged, loss], feed_dict=feed_dict)\n",
    "    elapsed_time = time.time() - tic\n",
    "    tic = time.time()\n",
    "    eval_run = (step % eval_frequency == 0) and (step > 0)\n",
    "    if not eval_run:\n",
    "      print(\"Step %6d (epoch %7.2f), loss: %.6f, time: %.3f s\" % (\n",
    "        step, float(step) * batch_size / len(X_train), error, elapsed_time))\n",
    "    writer.add_summary(summary, step)\n",
    "    # Every `eval_frequency` steps we shall take several extra operations,\n",
    "    # including printing the validation accuracy and updating the learning\n",
    "    # rate.\n",
    "    if eval_run:\n",
    "      lr = sess.run(learning_rate)\n",
    "      valid_error = root_mean_squred(y_test - eval_in_batches(X_test))\n",
    "      valid_hists.append([step, valid_error])\n",
    "      print(\"Step %6d (epoch %7.2f), loss: %.6f, time: %.3f s, error: %.6f\" % (\n",
    "        step, float(step) * batch_size / len(X_train), error, elapsed_time, valid_error))\n",
    "    sys.stdout.flush()\n",
    "  \n",
    "  # Save the trained model every 1000 steps.\n",
    "  if step % save_frequency == 0:\n",
    "    print(\"\")\n",
    "    print(\"Time since beginning  : %.3f s\" % (time.time() - tstart))\n",
    "    print(\"\")\n",
    "    saver.save(sess, save_path=chk_file, global_step=global_step)\n",
    "\n",
    "# Close the writer\n",
    "writer.close()\n",
    "\n",
    "# Finally the training is completed. Now let me see if this MBE model can\n",
    "# really estimate DFT energies.\n",
    "print(\"\")\n",
    "print(\"-> Test error         : %.6f\" % root_mean_squred(y_test - eval_in_batches(X_test)))\n",
    "print(\"\")\n",
    "\n",
    "# Do not forget to save the model one last time!\n",
    "saver.save(sess, save_path=chk_file, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.3 Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this section we will display some figures to analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f1f6fc7dfd0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtoAAAGuCAYAAACutaF5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f6fe989e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the smoothed validation errors\n",
    "xy = np.asarray(valid_hists)\n",
    "size = 10\n",
    "x, y = xy[:, 0], smooth(xy[:, 1], size)[:len(xy)]\n",
    "fig, ax = plt.subplots(1, 1, figsize=[12, 7])\n",
    "ax.plot(x, y, \"r-\")\n",
    "ax.set_ylabel(\"Validation RMSE\", fontsize=12)\n",
    "ax.set_xlabel(\"Steps\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Transform the estimates and targets back to energies\n",
    "y_train_est = eval_in_batches(X_train)\n",
    "y_test_est = eval_in_batches(X_test)\n",
    "energies_train = scaler.inverse_transform(y_train)\n",
    "energies_train_pred = scaler.inverse_transform(y_train_est)\n",
    "energies_test = scaler.inverse_transform(y_test)\n",
    "energies_test_pred = scaler.inverse_transform(y_test_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8EAAAH2CAYAAABQqI48AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrlWcukKxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f148dd67ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the regression figure using ``seaborn.regplot``.\n",
    "fig, axes = plt.subplots(1, 2, figsize=[16, 8], sharey=False)\n",
    "\n",
    "def plot_regression(y_true, y_pred, iax, title):\n",
    "  r2 = r2_score(y_true, y_pred)\n",
    "  mae = mean_abs_error(y_true, y_pred)\n",
    "  diff = y_true - y_pred\n",
    "  stddev = np.var(diff)\n",
    "  ax = sns.regplot(x=y_true.flatten(), y=y_pred.flatten(), ax=axes[iax])\n",
    "  ax.set_ylabel(\"DFTB Energy (eV)\", fontsize=12)\n",
    "  ax.set_xlabel(\"DNN Energy (eV)\", fontsize=12)\n",
    "  ax.set_title(\"%s, $R^2$=%.3f, MAE=%.3f, stddev=%.3f\" % (title, r2, mae, stddev), fontsize=12)\n",
    "\n",
    "# Plot the training result\n",
    "plot_regression(energies_train, energies_train_pred, 0, \"Training Data\")\n",
    "\n",
    "# Plot the testing result\n",
    "plot_regression(energies_test, energies_test_pred, 1, \"Testing Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
